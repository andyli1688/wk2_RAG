{
  "0": "Speech and Language Processing\nAn Introduction to Natural Language Processing,\nComputational Linguistics, and Speech Recognition\nThird Edition draft\nDaniel Jurafsky\nStanford University\nJames H. Martin\nUniversity of Colorado at Boulder\nCopyright c⃝2018\nDraft of September 23, 2018. Comments and typos welcome!",
  "1": "Summary of Contents\n1\nIntroduction. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n9\n2\nRegular Expressions, Text Normalization, Edit Distance. . . . . . . . . 10\n3\nN-gram Language Models . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n37\n4\nNaive Bayes and Sentiment Classiﬁcation . . . . . . . . . . . . . . . . . . . . . . . 63\n5\nLogistic Regression . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 82\n6\nVector Semantics . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 101\n7\nNeural Networks and Neural Language Models . . . . . . . . . . . . . . . . . 131\n8\nPart-of-Speech Tagging . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 151\n9\nSequence Processing with Recurrent Networks. . . . . . . . . . . . . . . . . . 177\n10 Formal Grammars of English . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 194\n11 Syntactic Parsing . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 223\n12 Statistical Parsing . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 237\n13 Dependency Parsing . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 270\n14 The Representation of Sentence Meaning . . . . . . . . . . . . . . . . . . . . . . . 295\n15 Computational Semantics . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 325\n16 Semantic Parsing . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 326\n17 Information Extraction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 327\n18 Semantic Role Labeling. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 356\n19 Lexicons for Sentiment, Affect, and Connotation . . . . . . . . . . . . . . . . 378\n20 Coreference Resolution and Entity Linking . . . . . . . . . . . . . . . . . . . . . 399\n21 Discourse Coherence. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .400\n22 Machine Translation. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 401\n23 Question Answering . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 402",
  "2": "22 Machine Translation. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 401\n23 Question Answering . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 402\n24 Dialog Systems and Chatbots. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .422\n25 Advanced Dialog Systems . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 446\n26 Speech Recognition and Synthesis . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 461\nAppendix\n463\nA Hidden Markov Models. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 464\nB\nSpelling Correction and the Noisy Channel . . . . . . . . . . . . . . . . . . . . . 480\nC WordNet: Word Relations, Senses, and Disambiguation . . . . . . . . . 493\nBibliography. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 517\nAuthor Index . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 543\nSubject Index . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 551\n2",
  "3": "Contents\n1\nIntroduction\n9\n2\nRegular Expressions, Text Normalization, Edit Distance\n10\n2.1\nRegular Expressions . . . . . . . . . . . . . . . . . . . . . . . . .\n11\n2.2\nWords\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n19\n2.3\nCorpora\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n21\n2.4\nText Normalization\n. . . . . . . . . . . . . . . . . . . . . . . . .\n22\n2.5\nMinimum Edit Distance . . . . . . . . . . . . . . . . . . . . . . .\n30\n2.6\nSummary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n34\nBibliographical and Historical Notes . . . . . . . . . . . . . . . . . . . .\n34\nExercises\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n35\n3\nN-gram Language Models\n37\n3.1\nN-Grams . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n38\n3.2\nEvaluating Language Models . . . . . . . . . . . . . . . . . . . .\n43\n3.3\nGeneralization and Zeros\n. . . . . . . . . . . . . . . . . . . . . .\n45\n3.4\nSmoothing . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n49\n3.5\nKneser-Ney Smoothing . . . . . . . . . . . . . . . . . . . . . . .\n53\n3.6\nThe Web and Stupid Backoff\n. . . . . . . . . . . . . . . . . . . .\n55\n3.7\nAdvanced: Perplexity’s Relation to Entropy\n. . . . . . . . . . . .\n56\n3.8\nSummary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n60\nBibliographical and Historical Notes . . . . . . . . . . . . . . . . . . . .\n60\nExercises\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n61\n4\nNaive Bayes and Sentiment Classiﬁcation\n63\n4.1\nNaive Bayes Classiﬁers . . . . . . . . . . . . . . . . . . . . . . .\n65\n4.2\nTraining the Naive Bayes Classiﬁer . . . . . . . . . . . . . . . . .\n67\n4.3\nWorked example . . . . . . . . . . . . . . . . . . . . . . . . . . .\n68\n4.4\nOptimizing for Sentiment Analysis . . . . . . . . . . . . . . . . .\n70\n4.5\nNaive Bayes for other text classiﬁcation tasks\n. . . . . . . . . . .\n71\n4.6\nNaive Bayes as a Language Model\n. . . . . . . . . . . . . . . . .\n72\n4.7\nEvaluation: Precision, Recall, F-measure . . . . . . . . . . . . . .\n73\n4.8\nTest sets and Cross-validation . . . . . . . . . . . . . . . . . . . .\n76\n4.9\nStatistical Signiﬁcance Testing\n. . . . . . . . . . . . . . . . . . .\n77\n4.10\nAdvanced: Feature Selection",
  "4": "73\n4.8\nTest sets and Cross-validation . . . . . . . . . . . . . . . . . . . .\n76\n4.9\nStatistical Signiﬁcance Testing\n. . . . . . . . . . . . . . . . . . .\n77\n4.10\nAdvanced: Feature Selection\n. . . . . . . . . . . . . . . . . . . .\n79\n4.11\nSummary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n79\nBibliographical and Historical Notes . . . . . . . . . . . . . . . . . . . .\n80\nExercises\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n81\n5\nLogistic Regression\n82\n5.1\nClassiﬁcation: the sigmoid\n. . . . . . . . . . . . . . . . . . . . .\n83\n5.2\nLearning in Logistic Regression . . . . . . . . . . . . . . . . . . .\n87\n5.3\nThe cross-entropy loss function . . . . . . . . . . . . . . . . . . .\n88\n5.4\nGradient Descent\n. . . . . . . . . . . . . . . . . . . . . . . . . .\n89\n5.5\nRegularization . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n93\n5.6\nMultinomial logistic regression . . . . . . . . . . . . . . . . . . .\n95\n5.7\nInterpreting models\n. . . . . . . . . . . . . . . . . . . . . . . . .\n97\n5.8\nAdvanced: Deriving the Gradient Equation . . . . . . . . . . . . .\n98\n5.9\nSummary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n99\n3",
  "5": "4\nCONTENTS\nBibliographical and Historical Notes . . . . . . . . . . . . . . . . . . . .\n99\nExercises\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 100\n6\nVector Semantics\n101\n6.1\nLexical Semantics . . . . . . . . . . . . . . . . . . . . . . . . . . 102\n6.2\nVector Semantics\n. . . . . . . . . . . . . . . . . . . . . . . . . . 106\n6.3\nWords and Vectors . . . . . . . . . . . . . . . . . . . . . . . . . . 108\n6.4\nCosine for measuring similarity . . . . . . . . . . . . . . . . . . . 111\n6.5\nTF-IDF: Weighing terms in the vector\n. . . . . . . . . . . . . . . 112\n6.6\nApplications of the tf-idf vector model . . . . . . . . . . . . . . . 115\n6.7\nOptional: Pointwise Mutual Information (PMI) . . . . . . . . . . . 116\n6.8\nWord2vec\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 118\n6.9\nVisualizing Embeddings . . . . . . . . . . . . . . . . . . . . . . . 123\n6.10\nSemantic properties of embeddings . . . . . . . . . . . . . . . . . 123\n6.11\nBias and Embeddings . . . . . . . . . . . . . . . . . . . . . . . . 125\n6.12\nEvaluating Vector Models . . . . . . . . . . . . . . . . . . . . . . 126\n6.13\nSummary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 127\nBibliographical and Historical Notes . . . . . . . . . . . . . . . . . . . . 127\nExercises\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 130\n7\nNeural Networks and Neural Language Models\n131\n7.1\nUnits . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 132\n7.2\nThe XOR problem . . . . . . . . . . . . . . . . . . . . . . . . . . 134\n7.3\nFeed-Forward Neural Networks . . . . . . . . . . . . . . . . . . . 137\n7.4\nTraining Neural Nets\n. . . . . . . . . . . . . . . . . . . . . . . . 140\n7.5\nNeural Language Models . . . . . . . . . . . . . . . . . . . . . . 145\n7.6\nSummary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 149\nBibliographical and Historical Notes . . . . . . . . . . . . . . . . . . . . 149\n8\nPart-of-Speech Tagging\n151\n8.1\n(Mostly) English Word Classes . . . . . . . . . . . . . . . . . . . 151\n8.2\nThe Penn Treebank Part-of-Speech Tagset\n. . . . . . . . . . . . . 154\n8.3\nPart-of-Speech Tagging . . . . . . . . . . . . . . . . . . . . . . . 156\n8.4\nHMM Part-of-Speech Tagging\n. . . . . . . . . . . . . . . . . . . 157\n8.5",
  "6": ". . . . . . . . . . . . . 154\n8.3\nPart-of-Speech Tagging . . . . . . . . . . . . . . . . . . . . . . . 156\n8.4\nHMM Part-of-Speech Tagging\n. . . . . . . . . . . . . . . . . . . 157\n8.5\nMaximum Entropy Markov Models . . . . . . . . . . . . . . . . . 167\n8.6\nBidirectionality\n. . . . . . . . . . . . . . . . . . . . . . . . . . . 171\n8.7\nPart-of-Speech Tagging for Other Languages . . . . . . . . . . . . 172\n8.8\nSummary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 173\nBibliographical and Historical Notes . . . . . . . . . . . . . . . . . . . . 174\nExercises\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 175\n9\nSequence Processing with Recurrent Networks\n177\n9.1\nSimple Recurrent Networks . . . . . . . . . . . . . . . . . . . . . 177\n9.2\nApplications of RNNs . . . . . . . . . . . . . . . . . . . . . . . . 184\n9.3\nDeep Networks: Stacked and Bidirectional RNNs\n. . . . . . . . . 186\n9.4\nManaging Context in RNNs: LSTMs and GRUs . . . . . . . . . . 188\n9.5\nWords, Characters and Byte-Pairs . . . . . . . . . . . . . . . . . . 192\n9.6\nSummary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 193\n10 Formal Grammars of English\n194\n10.1\nConstituency . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 194\n10.2\nContext-Free Grammars . . . . . . . . . . . . . . . . . . . . . . . 195",
  "7": "CONTENTS\n5\n10.3\nSome Grammar Rules for English . . . . . . . . . . . . . . . . . . 200\n10.4\nTreebanks\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 207\n10.5\nGrammar Equivalence and Normal Form . . . . . . . . . . . . . . 213\n10.6\nLexicalized Grammars . . . . . . . . . . . . . . . . . . . . . . . . 214\n10.7\nSummary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 219\nBibliographical and Historical Notes . . . . . . . . . . . . . . . . . . . . 220\nExercises\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 221\n11 Syntactic Parsing\n223\n11.1\nAmbiguity . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 223\n11.2\nCKY Parsing: A Dynamic Programming Approach\n. . . . . . . . 225\n11.3\nPartial Parsing . . . . . . . . . . . . . . . . . . . . . . . . . . . . 231\n11.4\nSummary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 234\nBibliographical and Historical Notes . . . . . . . . . . . . . . . . . . . . 235\nExercises\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 236\n12 Statistical Parsing\n237\n12.1\nProbabilistic Context-Free Grammars . . . . . . . . . . . . . . . . 238\n12.2\nProbabilistic CKY Parsing of PCFGs . . . . . . . . . . . . . . . . 242\n12.3\nWays to Learn PCFG Rule Probabilities\n. . . . . . . . . . . . . . 243\n12.4\nProblems with PCFGs . . . . . . . . . . . . . . . . . . . . . . . . 245\n12.5\nImproving PCFGs by Splitting Non-Terminals . . . . . . . . . . . 248\n12.6\nProbabilistic Lexicalized CFGs . . . . . . . . . . . . . . . . . . . 250\n12.7\nProbabilistic CCG Parsing . . . . . . . . . . . . . . . . . . . . . . 255\n12.8\nEvaluating Parsers . . . . . . . . . . . . . . . . . . . . . . . . . . 263\n12.9\nHuman Parsing . . . . . . . . . . . . . . . . . . . . . . . . . . . . 264\n12.10 Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 266\nBibliographical and Historical Notes . . . . . . . . . . . . . . . . . . . . 267\nExercises\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 268\n13 Dependency Parsing\n270\n13.1\nDependency Relations . . . . . . . . . . . . . . . . . . . . . . . . 271\n13.2\nDependency Formalisms . . . . . . . . . . . . . . . . . . . . . . . 273\n13.3\nDependency Treebanks\n. . . . . . . . . . . . . . . . . . . . . . . 274\n13.4\nTransition-Based Dependency Parsing\n. . . . . . . . . . . . . . . 275\n13.5\nGraph-Based Dependency Parsing",
  "8": "Dependency Formalisms . . . . . . . . . . . . . . . . . . . . . . . 273\n13.3\nDependency Treebanks\n. . . . . . . . . . . . . . . . . . . . . . . 274\n13.4\nTransition-Based Dependency Parsing\n. . . . . . . . . . . . . . . 275\n13.5\nGraph-Based Dependency Parsing\n. . . . . . . . . . . . . . . . . 286\n13.6\nEvaluation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 291\n13.7\nSummary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 292\nBibliographical and Historical Notes . . . . . . . . . . . . . . . . . . . . 292\nExercises\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 294\n14 The Representation of Sentence Meaning\n295\n14.1\nComputational Desiderata for Representations . . . . . . . . . . . 297\n14.2\nModel-Theoretic Semantics . . . . . . . . . . . . . . . . . . . . . 301\n14.3\nFirst-Order Logic\n. . . . . . . . . . . . . . . . . . . . . . . . . . 304\n14.4\nEvent and State Representations . . . . . . . . . . . . . . . . . . . 311\n14.5\nDescription Logics . . . . . . . . . . . . . . . . . . . . . . . . . . 316\n14.6\nSummary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 321\nBibliographical and Historical Notes . . . . . . . . . . . . . . . . . . . . 322\nExercises\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 323\n15 Computational Semantics\n325",
  "9": "6\nCONTENTS\n16 Semantic Parsing\n326\n17 Information Extraction\n327\n17.1\nNamed Entity Recognition\n. . . . . . . . . . . . . . . . . . . . . 328\n17.2\nRelation Extraction\n. . . . . . . . . . . . . . . . . . . . . . . . . 334\n17.3\nExtracting Times . . . . . . . . . . . . . . . . . . . . . . . . . . . 344\n17.4\nExtracting Events and their Times . . . . . . . . . . . . . . . . . . 348\n17.5\nTemplate Filling . . . . . . . . . . . . . . . . . . . . . . . . . . . 351\n17.6\nSummary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 353\nBibliographical and Historical Notes . . . . . . . . . . . . . . . . . . . . 354\nExercises\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 355\n18 Semantic Role Labeling\n356\n18.1\nSemantic Roles\n. . . . . . . . . . . . . . . . . . . . . . . . . . . 357\n18.2\nDiathesis Alternations . . . . . . . . . . . . . . . . . . . . . . . . 358\n18.3\nSemantic Roles: Problems with Thematic Roles . . . . . . . . . . 359\n18.4\nThe Proposition Bank . . . . . . . . . . . . . . . . . . . . . . . . 360\n18.5\nFrameNet\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 362\n18.6\nSemantic Role Labeling . . . . . . . . . . . . . . . . . . . . . . . 364\n18.7\nSelectional Restrictions . . . . . . . . . . . . . . . . . . . . . . . 368\n18.8\nPrimitive Decomposition of Predicates . . . . . . . . . . . . . . . 372\n18.9\nSummary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 374\nBibliographical and Historical Notes . . . . . . . . . . . . . . . . . . . . 374\nExercises\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 377\n19 Lexicons for Sentiment, Affect, and Connotation\n378\n19.1\nDeﬁning Emotion . . . . . . . . . . . . . . . . . . . . . . . . . . 379\n19.2\nAvailable Sentiment and Affect Lexicons . . . . . . . . . . . . . . 381\n19.3\nCreating affect lexicons by human labeling . . . . . . . . . . . . . 382\n19.4\nSemi-supervised induction of affect lexicons . . . . . . . . . . . . 384\n19.5\nSupervised learning of word sentiment . . . . . . . . . . . . . . . 387\n19.6\nUsing Lexicons for Sentiment Recognition . . . . . . . . . . . . . 391\n19.7\nOther tasks: Personality . . . . . . . . . . . . . . . . . . . . . . . 392\n19.8\nAffect Recognition . . . . . . . . . . . . . . . . . . . . . . . . . . 393\n19.9\nConnotation Frames . . . . . . . . . . . . . . . . . . . . . . . . . 395\n19.10 Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 396",
  "10": "19.9\nConnotation Frames . . . . . . . . . . . . . . . . . . . . . . . . . 395\n19.10 Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 396\nBibliographical and Historical Notes . . . . . . . . . . . . . . . . . . . . 397\n20 Coreference Resolution and Entity Linking\n399\n21 Discourse Coherence\n400\n22 Machine Translation\n401\n23 Question Answering\n402\n23.1\nIR-based Factoid Question Answering\n. . . . . . . . . . . . . . . 403\n23.2\nKnowledge-based Question Answering . . . . . . . . . . . . . . . 411\n23.3\nUsing multiple information sources: IBM’s Watson\n. . . . . . . . 415\n23.4\nEvaluation of Factoid Answers\n. . . . . . . . . . . . . . . . . . . 418\nBibliographical and Historical Notes . . . . . . . . . . . . . . . . . . . . 420\nExercises\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 421\n24 Dialog Systems and Chatbots\n422",
  "11": "CONTENTS\n7\n24.1\nChatbots . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 425\n24.2\nFrame Based Dialog Agents . . . . . . . . . . . . . . . . . . . . . 430\n24.3\nVoiceXML . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 438\n24.4\nEvaluating Dialog Systems\n. . . . . . . . . . . . . . . . . . . . . 441\n24.5\nDialog System Design . . . . . . . . . . . . . . . . . . . . . . . . 442\n24.6\nSummary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 444\nBibliographical and Historical Notes . . . . . . . . . . . . . . . . . . . . 444\nExercises\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 445\n25 Advanced Dialog Systems\n446\n25.1\nDialog Acts\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . 447\n25.2\nDialog State: Interpreting Dialog Acts\n. . . . . . . . . . . . . . . 452\n25.3\nDialog Policy\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . 454\n25.4\nA simple policy based on local context . . . . . . . . . . . . . . . 456\n25.5\nNatural language generation in the dialog-state model . . . . . . . 456\n25.6\nDeep Reinforcement Learning for Dialog . . . . . . . . . . . . . . 459\n25.7\nSummary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 459\nBibliographical and Historical Notes . . . . . . . . . . . . . . . . . . . . 459\n26 Speech Recognition and Synthesis\n461\nAppendix\n463\nA Hidden Markov Models\n464\nA.1\nMarkov Chains . . . . . . . . . . . . . . . . . . . . . . . . . . . . 464\nA.2\nThe Hidden Markov Model . . . . . . . . . . . . . . . . . . . . . 465\nA.3\nLikelihood Computation: The Forward Algorithm . . . . . . . . . 467\nA.4\nDecoding: The Viterbi Algorithm . . . . . . . . . . . . . . . . . . 471\nA.5\nHMM Training: The Forward-Backward Algorithm . . . . . . . . 473\nA.6\nSummary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 479\nBibliographical and Historical Notes . . . . . . . . . . . . . . . . . . . . 479\nB\nSpelling Correction and the Noisy Channel\n480\nB.1\nThe Noisy Channel Model . . . . . . . . . . . . . . . . . . . . . . 481\nB.2\nReal-word spelling errors . . . . . . . . . . . . . . . . . . . . . . 486\nB.3\nNoisy Channel Model: The State of the Art . . . . . . . . . . . . . 488\nBibliographical and Historical Notes . . . . . . . . . . . . . . . . . . . . 491\nExercises\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 492\nC WordNet: Word Relations, Senses, and Disambiguation\n493\nC.1",
  "12": "Bibliographical and Historical Notes . . . . . . . . . . . . . . . . . . . . 491\nExercises\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 492\nC WordNet: Word Relations, Senses, and Disambiguation\n493\nC.1\nWord Senses . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 493\nC.2\nWordNet: A Database of Lexical Relations . . . . . . . . . . . . . 497\nC.3\nWord Similarity: Thesaurus Methods . . . . . . . . . . . . . . . . 498\nC.4\nWord Sense Disambiguation: Overview . . . . . . . . . . . . . . . 504\nC.5\nSupervised Word Sense Disambiguation\n. . . . . . . . . . . . . . 505\nC.6\nWSD: Dictionary and Thesaurus Methods\n. . . . . . . . . . . . . 507\nC.7\nSemi-Supervised WSD: Bootstrapping . . . . . . . . . . . . . . . 510\nC.8\nUnsupervised Word Sense Induction\n. . . . . . . . . . . . . . . . 511\nC.9\nSummary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 512\nBibliographical and Historical Notes . . . . . . . . . . . . . . . . . . . . 513\nExercises\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 514",
  "13": "8\nCONTENTS\nBibliography\n517\nAuthor Index\n543\nSubject Index\n551",
  "14": "CHAPTER\n1\nIntroduction\nPlaceholder\n9",
  "15": "10\nCHAPTER 2\n•\nREGULAR EXPRESSIONS, TEXT NORMALIZATION, EDIT DISTANCE\nCHAPTER\n2\nRegular\nExpressions,\nText\nNormalization, Edit Distance\nUser:\nYou are like my father in some ways.\nELIZA: WHAT RESEMBLANCE DO YOU SEE\nUser:\nYou are not very aggressive but I think you don’t want me to\nnotice that.\nELIZA: WHAT MAKES YOU THINK I AM NOT VERY AGGRESSIVE\nUser:\nYou don’t argue with me.\nELIZA: WHY DO YOU THINK I DON’T ARGUE WITH YOU\nUser:\nYou are afraid of me.\nELIZA: DOES IT PLEASE YOU TO BELIEVE I AM AFRAID OF YOU\nWeizenbaum (1966)\nThe dialogue above is from ELIZA, an early natural language processing sys-\nELIZA\ntem that could carry on a limited conversation with a user by imitating the responses\nof a Rogerian psychotherapist (Weizenbaum, 1966). ELIZA is a surprisingly simple\nprogram that uses pattern matching to recognize phrases like “You are X” and trans-\nlate them into suitable outputs like “What makes you think I am X?”. This simple\ntechnique succeeds in this domain because ELIZA doesn’t actually need to know\nanything to mimic a Rogerian psychotherapist. As Weizenbaum notes, this is one\nof the few dialogue genres where listeners can act as if they know nothing of the\nworld. Eliza’s mimicry of human conversation was remarkably successful: many\npeople who interacted with ELIZA came to believe that it really understood them\nand their problems, many continued to believe in ELIZA’s abilities even after the\nprogram’s operation was explained to them (Weizenbaum, 1976), and even today\nsuch chatbots are a fun diversion.\nchatbots\nOf course modern conversational agents are much more than a diversion; they\ncan answer questions, book ﬂights, or ﬁnd restaurants, functions for which they rely\non a much more sophisticated understanding of the user’s intent, as we will see in\nChapter 24. Nonetheless, the simple pattern-based methods that powered ELIZA\nand other chatbots play a crucial role in natural language processing.\nWe’ll begin with the most important tool for describing text patterns: the regular\nexpression. Regular expressions can be used to specify strings we might want to\nextract from a document, from transforming “You are X” in Eliza above, to deﬁning\nstrings like $199 or $24.99 for extracting tables of prices from a document.\nWe’ll then turn to a set of tasks collectively called text normalization, in which\ntext\nnormalization\nregular expressions play an important part. Normalizing text means converting it\nto a more convenient, standard form. For example, most of what we are going to\ndo with language relies on ﬁrst separating out or tokenizing words from running\ntext, the task of tokenization. English words are often separated from each other\ntokenization\nby whitespace, but whitespace is not always sufﬁcient. New York and rock ’n’ roll\nare sometimes treated as large words despite the fact that they contain spaces, while\nsometimes we’ll need to separate I’m into the two words I and am. For processing\ntweets or texts we’ll need to tokenize emoticons like :) or hashtags like #nlproc.\nSome languages, like Chinese, don’t have spaces between words, so word tokeniza-\ntion becomes more difﬁcult.",
  "16": "2.1\n•\nREGULAR EXPRESSIONS\n11\nAnother part of text normalization is lemmatization, the task of determining\nlemmatization\nthat two words have the same root, despite their surface differences. For example,\nthe words sang, sung, and sings are forms of the verb sing. The word sing is the\ncommon lemma of these words, and a lemmatizer maps from all of these to sing.\nLemmatization is essential for processing morphologically complex languages like\nArabic. Stemming refers to a simpler version of lemmatization in which we mainly\nstemming\njust strip sufﬁxes from the end of the word. Text normalization also includes sen-\ntence segmentation: breaking up a text into individual sentences, using cues like\nsentence\nsegmentation\nperiods or exclamation points.\nFinally, we’ll need to compare words and other strings. We’ll introduce a metric\ncalled edit distance that measures how similar two strings are based on the number\nof edits (insertions, deletions, substitutions) it takes to change one string into the\nother. Edit distance is an algorithm with applications throughout language process-\ning, from spelling correction to speech recognition to coreference resolution.\n2.1\nRegular Expressions\nSIR ANDREW:\nHer C’s, her U’s and her T’s: why that?\nShakespeare, Twelfth Night\nOne of the unsung successes in standardization in computer science has been the\nregular expression (RE), a language for specifying text search strings. This prac-\nregular\nexpression\ntical language is used in every computer language, word processor, and text pro-\ncessing tools like the Unix tools grep or Emacs. Formally, a regular expression is\nan algebraic notation for characterizing a set of strings. They are particularly use-\nful for searching in texts, when we have a pattern to search for and a corpus of\ncorpus\ntexts to search through. A regular expression search function will search through the\ncorpus, returning all texts that match the pattern. The corpus can be a single docu-\nment or a collection. For example, the Unix command-line tool grep takes a regular\nexpression and returns every line of the input document that matches the expression.\nA search can be designed to return every match on a line, if there are more than\none, or just the ﬁrst match. In the following examples we generally underline the\nexact part of the pattern that matches the regular expression and show only the ﬁrst\nmatch. We’ll show regular expressions delimited by slashes but note that slashes are\nnot part of the regular expressions.\nRegular expressions come in many variants. We’ll be describing extended regu-\nlar expressions; different regular expression parsers may only recognize subsets of\nthese, or treat some expressions slightly differently. Using an online regular expres-\nsion tester is a handy way to test out your expressions and explore these variations.\n2.1.1\nBasic Regular Expression Patterns\nThe simplest kind of regular expression is a sequence of simple characters. To search\nfor woodchuck, we type /woodchuck/. The expression /Buttercup/ matches any\nstring containing the substring Buttercup; grep with that expression would return the\nline I’m called little Buttercup. The search string can consist of a single character\n(like /!/) or a sequence of characters (like /urgl/).\nRegular expressions are case sensitive; lower case /s/ is distinct from upper\ncase /S/ (/s/ matches a lower case s but not an upper case S). This means that\nthe pattern /woodchucks/ will not match the string Woodchucks. We can solve this",
  "17": "12\nCHAPTER 2\n•\nREGULAR EXPRESSIONS, TEXT NORMALIZATION, EDIT DISTANCE\nRE\nExample Patterns Matched\n/woodchucks/\n“interesting links to woodchucks and lemurs”\n/a/\n“Mary Ann stopped by Mona’s”\n/!/\n“You’ve left the burglar behind again!” said Nori\nFigure 2.1\nSome simple regex searches.\nproblem with the use of the square braces [ and ]. The string of characters inside the\nbraces speciﬁes a disjunction of characters to match. For example, Fig. 2.2 shows\nthat the pattern /[wW]/ matches patterns containing either w or W.\nRE\nMatch\nExample Patterns\n/[wW]oodchuck/\nWoodchuck or woodchuck\n“Woodchuck”\n/[abc]/\n‘a’, ‘b’, or ‘c’\n“In uomini, in soldati”\n/[1234567890]/\nany digit\n“plenty of 7 to 5”\nFigure 2.2\nThe use of the brackets [] to specify a disjunction of characters.\nThe regular expression /[1234567890]/ speciﬁed any single digit. While such\nclasses of characters as digits or letters are important building blocks in expressions,\nthey can get awkward (e.g., it’s inconvenient to specify\n/[ABCDEFGHIJKLMNOPQRSTUVWXYZ]/\nto mean “any capital letter”). In cases where there is a well-deﬁned sequence asso-\nciated with a set of characters, the brackets can be used with the dash (-) to specify\nany one character in a range. The pattern /[2-5]/ speciﬁes any one of the charac-\nrange\nters 2, 3, 4, or 5. The pattern /[b-g]/ speciﬁes one of the characters b, c, d, e, f, or\ng. Some other examples are shown in Fig. 2.3.\nRE\nMatch\nExample Patterns Matched\n/[A-Z]/\nan upper case letter\n“we should call it ‘Drenched Blossoms’ ”\n/[a-z]/\na lower case letter\n“my beans were impatient to be hoed!”\n/[0-9]/\na single digit\n“Chapter 1: Down the Rabbit Hole”\nFigure 2.3\nThe use of the brackets [] plus the dash - to specify a range.\nThe square braces can also be used to specify what a single character cannot be,\nby use of the caret ˆ. If the caret ˆ is the ﬁrst symbol after the open square brace [,\nthe resulting pattern is negated. For example, the pattern /[ˆa]/ matches any single\ncharacter (including special characters) except a. This is only true when the caret\nis the ﬁrst symbol after the open square brace. If it occurs anywhere else, it usually\nstands for a caret; Fig. 2.4 shows some examples.\nRE\nMatch (single characters)\nExample Patterns Matched\n/[ˆA-Z]/\nnot an upper case letter\n“Oyfn pripetchik”\n/[ˆSs]/\nneither ‘S’ nor ‘s’\n“I have no exquisite reason for’t”\n/[ˆ\\.]/\nnot a period\n“our resident Djinn”\n/[eˆ]/\neither ‘e’ or ‘ˆ’\n“look up ˆ now”\n/aˆb/\nthe pattern ‘aˆb’\n“look up aˆ b now”\nFigure 2.4\nThe caret ˆ for negation or just to mean ˆ. See below re: the backslash for escaping the period.\nHow can we talk about optional elements, like an optional s in woodchuck and\nwoodchucks? We can’t use the square brackets, because while they allow us to say\n“s or S”, they don’t allow us to say “s or nothing”. For this we use the question mark\n/?/, which means “the preceding character or nothing”, as shown in Fig. 2.5.",
  "18": "2.1\n•\nREGULAR EXPRESSIONS\n13\nRE\nMatch\nExample Patterns Matched\n/woodchucks?/\nwoodchuck or woodchucks\n“woodchuck”\n/colou?r/\ncolor or colour\n“colour”\nFigure 2.5\nThe question mark ? marks optionality of the previous expression.\nWe can think of the question mark as meaning “zero or one instances of the\nprevious character”. That is, it’s a way of specifying how many of something that\nwe want, something that is very important in regular expressions. For example,\nconsider the language of certain sheep, which consists of strings that look like the\nfollowing:\nbaa!\nbaaa!\nbaaaa!\nbaaaaa!\n...\nThis language consists of strings with a b, followed by at least two a’s, followed\nby an exclamation point. The set of operators that allows us to say things like “some\nnumber of as” are based on the asterisk or *, commonly called the Kleene * (gen-\nKleene *\nerally pronounced “cleany star”). The Kleene star means “zero or more occurrences\nof the immediately previous character or regular expression”. So /a*/ means “any\nstring of zero or more as”. This will match a or aaaaaa, but it will also match Off\nMinor since the string Off Minor has zero a’s. So the regular expression for matching\none or more a is /aa*/, meaning one a followed by zero or more as. More complex\npatterns can also be repeated. So /[ab]*/ means “zero or more a’s or b’s” (not\n“zero or more right square braces”). This will match strings like aaaa or ababab or\nbbbb.\nFor specifying multiple digits (useful for ﬁnding prices) we can extend /[0-9]/,\nthe regular expression for a single digit.\nAn integer (a string of digits) is thus\n/[0-9][0-9]*/. (Why isn’t it just /[0-9]*/?)\nSometimes it’s annoying to have to write the regular expression for digits twice,\nso there is a shorter way to specify “at least one” of some character. This is the\nKleene +, which means “one or more occurrences of the immediately preceding\nKleene +\ncharacter or regular expression”. Thus, the expression /[0-9]+/ is the normal way\nto specify “a sequence of digits”. There are thus two ways to specify the sheep\nlanguage: /baaa*!/ or /baa+!/.\nOne very important special character is the period (/./), a wildcard expression\nthat matches any single character (except a carriage return), as shown in Fig. 2.6.\nRE\nMatch\nExample Matches\n/beg.n/\nany character between beg and n\nbegin, beg’n, begun\nFigure 2.6\nThe use of the period . to specify any character.\nThe wildcard is often used together with the Kleene star to mean “any string of\ncharacters”. For example, suppose we want to ﬁnd any line in which a particular\nword, for example, aardvark, appears twice. We can specify this with the regular\nexpression /aardvark.*aardvark/.\nAnchors are special characters that anchor regular expressions to particular places\nAnchors\nin a string. The most common anchors are the caret ˆ and the dollar sign $. The caret\nˆ matches the start of a line. The pattern /ˆThe/ matches the word The only at the",
  "19": "14\nCHAPTER 2\n•\nREGULAR EXPRESSIONS, TEXT NORMALIZATION, EDIT DISTANCE\nstart of a line. Thus, the caret ˆ has three uses: to match the start of a line, to in-\ndicate a negation inside of square brackets, and just to mean a caret. (What are the\ncontexts that allow grep or Python to know which function a given caret is supposed\nto have?) The dollar sign $ matches the end of a line. So the pattern ␣$ is a useful\npattern for matching a space at the end of a line, and /ˆThe dog\\.$/ matches a\nline that contains only the phrase The dog. (We have to use the backslash here since\nwe want the . to mean “period” and not the wildcard.)\nThere are also two other anchors: \\b matches a word boundary, and \\B matches\na non-boundary. Thus, /\\bthe\\b/ matches the word the but not the word other.\nMore technically, a “word” for the purposes of a regular expression is deﬁned as any\nsequence of digits, underscores, or letters; this is based on the deﬁnition of “words”\nin programming languages. For example, /\\b99\\b/ will match the string 99 in\nThere are 99 bottles of beer on the wall (because 99 follows a space) but not 99 in\nThere are 299 bottles of beer on the wall (since 99 follows a number). But it will\nmatch 99 in $99 (since 99 follows a dollar sign ($), which is not a digit, underscore,\nor letter).\n2.1.2\nDisjunction, Grouping, and Precedence\nSuppose we need to search for texts about pets; perhaps we are particularly interested\nin cats and dogs. In such a case, we might want to search for either the string cat or\nthe string dog. Since we can’t use the square brackets to search for “cat or dog” (why\ncan’t we say /[catdog]/?), we need a new operator, the disjunction operator, also\ndisjunction\ncalled the pipe symbol |. The pattern /cat|dog/ matches either the string cat or\nthe string dog.\nSometimes we need to use this disjunction operator in the midst of a larger se-\nquence. For example, suppose I want to search for information about pet ﬁsh for\nmy cousin David. How can I specify both guppy and guppies? We cannot simply\nsay /guppy|ies/, because that would match only the strings guppy and ies. This\nis because sequences like guppy take precedence over the disjunction operator |.\nPrecedence\nTo make the disjunction operator apply only to a speciﬁc pattern, we need to use the\nparenthesis operators ( and ). Enclosing a pattern in parentheses makes it act like\na single character for the purposes of neighboring operators like the pipe | and the\nKleene*. So the pattern /gupp(y|ies)/ would specify that we meant the disjunc-\ntion only to apply to the sufﬁxes y and ies.\nThe parenthesis operator ( is also useful when we are using counters like the\nKleene*. Unlike the | operator, the Kleene* operator applies by default only to\na single character, not to a whole sequence. Suppose we want to match repeated\ninstances of a string. Perhaps we have a line that has column labels of the form\nColumn 1 Column 2 Column 3. The expression /Column␣[0-9]+␣*/ will not\nmatch any number of columns; instead, it will match a single column followed by\nany number of spaces! The star here applies only to the space ␣that precedes it,\nnot to the whole sequence. With the parentheses, we could write the expression\n/(Column␣[0-9]+␣*)*/ to match the word Column, followed by a number and\noptional spaces, the whole pattern repeated any number of times.\nThis idea that one operator may take precedence over another, requiring us to\nsometimes use parentheses to specify what we mean, is formalized by the operator\nprecedence hierarchy for regular expressions. The following table gives the order\noperator\nprecedence\nof RE operator precedence, from highest precedence to lowest precedence.",
  "20": "2.1\n•\nREGULAR EXPRESSIONS\n15\nParenthesis\n()\nCounters\n* + ? {}\nSequences and anchors\nthe ˆmy end$\nDisjunction\n|\nThus,\nbecause\ncounters\nhave\na\nhigher\nprecedence\nthan\nsequences,\n/the*/ matches theeeee but not thethe. Because sequences have a higher prece-\ndence than disjunction, /the|any/ matches the or any but not theny.\nPatterns can be ambiguous in another way. Consider the expression /[a-z]*/\nwhen matching against the text once upon a time. Since /[a-z]*/ matches zero or\nmore letters, this expression could match nothing, or just the ﬁrst letter o, on, onc,\nor once. In these cases regular expressions always match the largest string they can;\nwe say that patterns are greedy, expanding to cover as much of a string as they can.\ngreedy\nThere are, however, ways to enforce non-greedy matching, using another mean-\nnon-greedy\ning of the ? qualiﬁer. The operator *? is a Kleene star that matches as little text as\n*?\npossible. The operator +? is a Kleene plus that matches as little text as possible.\n+?\n2.1.3\nA Simple Example\nSuppose we wanted to write a RE to ﬁnd cases of the English article the. A simple\n(but incorrect) pattern might be:\n/the/\nOne problem is that this pattern will miss the word when it begins a sentence\nand hence is capitalized (i.e., The). This might lead us to the following pattern:\n/[tT]he/\nBut we will still incorrectly return texts with the embedded in other words (e.g.,\nother or theology). So we need to specify that we want instances with a word bound-\nary on both sides:\n/\\b[tT]he\\b/\nSuppose we wanted to do this without the use of /\\b/. We might want this since\n/\\b/ won’t treat underscores and numbers as word boundaries; but we might want\nto ﬁnd the in some context where it might also have underlines or numbers nearby\n(the or the25). We need to specify that we want instances in which there are no\nalphabetic letters on either side of the the:\n/[ˆa-zA-Z][tT]he[ˆa-zA-Z]/\nBut there is still one more problem with this pattern: it won’t ﬁnd the word the\nwhen it begins a line. This is because the regular expression [ˆa-zA-Z], which\nwe used to avoid embedded instances of the, implies that there must be some single\n(although non-alphabetic) character before the the. We can avoid this by specify-\ning that before the the we require either the beginning-of-line or a non-alphabetic\ncharacter, and the same at the end of the line:\n/(ˆ|[ˆa-zA-Z])[tT]he([ˆa-zA-Z]|$)/\nThe process we just went through was based on ﬁxing two kinds of errors: false\npositives, strings that we incorrectly matched like other or there, and false nega-\nfalse positives\ntives, strings that we incorrectly missed, like The. Addressing these two kinds of\nfalse negatives",
  "21": "16\nCHAPTER 2\n•\nREGULAR EXPRESSIONS, TEXT NORMALIZATION, EDIT DISTANCE\nerrors comes up again and again in implementing speech and language processing\nsystems. Reducing the overall error rate for an application thus involves two antag-\nonistic efforts:\n• Increasing precision (minimizing false positives)\n• Increasing recall (minimizing false negatives)\n2.1.4\nA More Complex Example\nLet’s try out a more signiﬁcant example of the power of REs. Suppose we want to\nbuild an application to help a user buy a computer on the Web. The user might want\n“any machine with at least 6 GHz and 500 GB of disk space for less than $1000”.\nTo do this kind of retrieval, we ﬁrst need to be able to look for expressions like 6\nGHz or 500 GB or Mac or $999.99. In the rest of this section we’ll work out some\nsimple regular expressions for this task.\nFirst, let’s complete our regular expression for prices. Here’s a regular expres-\nsion for a dollar sign followed by a string of digits:\n/$[0-9]+/\nNote that the $ character has a different function here than the end-of-line function\nwe discussed earlier. Most regular expression parsers are smart enough to realize\nthat $ here doesn’t mean end-of-line. (As a thought experiment, think about how\nregex parsers might ﬁgure out the function of $ from the context.)\nNow we just need to deal with fractions of dollars. We’ll add a decimal point\nand two digits afterwards:\n/$[0-9]+\\.[0-9][0-9]/\nThis pattern only allows $199.99 but not $199. We need to make the cents\noptional and to make sure we’re at a word boundary:\n/(ˆ|\\W)$[0-9]+(\\.[0-9][0-9])?\\b/\nOne last catch! This pattern allows prices like $199999.99 which would be far\ntoo expensive! We need to limit the dollar\n/(ˆ|\\W)$[0-9]{0,3}(\\.[0-9][0-9])?\\b/\nHow about speciﬁcations for > 6GHz processor speed? Here’s a pattern for that:\n/\\b[6-9]+␣*(GHz|[Gg]igahertz)\\b/\nNote that we use /␣*/ to mean “zero or more spaces” since there might always\nbe extra spaces lying around. For disk space, we’ll need to allow for optional frac-\ntions again (5.5 GB); note the use of ? for making the ﬁnal s optional:\n/\\b[0-9]+(\\.[0-9]+)?␣*(GB|[Gg]igabytes?)\\b/\nModifying this regular expression so that it only matches more than 500 GB is\nleft as an exercise for the reader.\n2.1.5\nMore Operators\nFigure 2.7 shows some aliases for common ranges, which can be used mainly to\nsave typing. Besides the Kleene * and Kleene + we can also use explicit numbers as",
  "22": "2.1\n•\nREGULAR EXPRESSIONS\n17\ncounters, by enclosing them in curly brackets. The regular expression /{3}/ means\n“exactly 3 occurrences of the previous character or expression”. So /a\\.{24}z/\nwill match a followed by 24 dots followed by z (but not a followed by 23 or 25 dots\nfollowed by a z).\nRE\nExpansion\nMatch\nFirst Matches\n\\d\n[0-9]\nany digit\nParty␣of␣5\n\\D\n[ˆ0-9]\nany non-digit\nBlue␣moon\n\\w\n[a-zA-Z0-9_]\nany alphanumeric/underscore\nDaiyu\n\\W\n[ˆ\\w]\na non-alphanumeric\n!!!!\n\\s\n[␣\\r\\t\\n\\f]\nwhitespace (space, tab)\n\\S\n[ˆ\\s]\nNon-whitespace\nin␣Concord\nFigure 2.7\nAliases for common sets of characters.\nA range of numbers can also be speciﬁed. So /{n,m}/ speciﬁes from n to m\noccurrences of the previous char or expression, and /{n,}/ means at least n occur-\nrences of the previous expression. REs for counting are summarized in Fig. 2.8.\nRE\nMatch\n*\nzero or more occurrences of the previous char or expression\n+\none or more occurrences of the previous char or expression\n?\nexactly zero or one occurrence of the previous char or expression\n{n}\nn occurrences of the previous char or expression\n{n,m}\nfrom n to m occurrences of the previous char or expression\n{n,}\nat least n occurrences of the previous char or expression\n{,m}\nup to m occurrences of the previous char or expression\nFigure 2.8\nRegular expression operators for counting.\nFinally, certain special characters are referred to by special notation based on the\nbackslash (\\) (see Fig. 2.9). The most common of these are the newline character\nNewline\n\\n and the tab character \\t. To refer to characters that are special themselves (like\n., *, [, and \\), precede them with a backslash, (i.e., /\\./, /\\*/, /\\[/, and /\\\\/).\nRE\nMatch\nFirst Patterns Matched\n\\*\nan asterisk “*”\n“K*A*P*L*A*N”\n\\.\na period “.”\n“Dr. Livingston, I presume”\n\\?\na question mark\n“Why don’t they come and lend a hand?”\n\\n\na newline\n\\t\na tab\nFigure 2.9\nSome characters that need to be backslashed.\n2.1.6\nRegular Expression Substitution, Capture Groups, and ELIZA\nAn important use of regular expressions is in substitutions. For example, the substi-\nsubstitution\ntution operator s/regexp1/pattern/ used in Python and in Unix commands like\nvim or sed allows a string characterized by a regular expression to be replaced by\nanother string:\ns/colour/color/\nIt is often useful to be able to refer to a particular subpart of the string matching\nthe ﬁrst pattern. For example, suppose we wanted to put angle brackets around all",
  "23": "18\nCHAPTER 2\n•\nREGULAR EXPRESSIONS, TEXT NORMALIZATION, EDIT DISTANCE\nintegers in a text, for example, changing the 35 boxes to the <35> boxes. We’d\nlike a way to refer to the integer we’ve found so that we can easily add the brackets.\nTo do this, we put parentheses ( and ) around the ﬁrst pattern and use the number\noperator \\1 in the second pattern to refer back. Here’s how it looks:\ns/([0-9]+)/<\\1>/\nThe parenthesis and number operators can also specify that a certain string or\nexpression must occur twice in the text. For example, suppose we are looking for\nthe pattern “the Xer they were, the Xer they will be”, where we want to constrain\nthe two X’s to be the same string. We do this by surrounding the ﬁrst X with the\nparenthesis operator, and replacing the second X with the number operator \\1, as\nfollows:\n/the (.*)er they were, the \\1er they will be/\nHere the \\1 will be replaced by whatever string matched the ﬁrst item in paren-\ntheses. So this will match the bigger they were, the bigger they will be but not the\nbigger they were, the faster they will be.\nThis use of parentheses to store a pattern in memory is called a capture group.\ncapture group\nEvery time a capture group is used (i.e., parentheses surround a pattern), the re-\nsulting match is stored in a numbered register. If you match two different sets of\nregister\nparentheses, \\2 means whatever matched the second capture group. Thus\n/the (.*)er they (.*), the \\1er we \\2/\nwill match the faster they ran, the faster we ran but not the faster they ran, the faster\nwe ate. Similarly, the third capture group is stored in \\3, the fourth is \\4, and so on.\nParentheses thus have a double function in regular expressions; they are used to\ngroup terms for specifying the order in which operators should apply, and they are\nused to capture something in a register. Occasionally we might want to use parenthe-\nses for grouping, but don’t want to capture the resulting pattern in a register. In that\ncase we use a non-capturing group, which is speciﬁed by putting the commands\nnon-capturing\ngroup\n?: after the open paren, in the form (?: pattern ).\n/(?:some|a few) (people|cats) like some \\1/\nwill match some cats like some cats but not some cats like some a few.\nSubstitutions and capture groups are very useful in implementing simple chat-\nbots like ELIZA (Weizenbaum, 1966). Recall that ELIZA simulates a Rogerian\npsychologist by carrying on conversations like the following:\nUser1:\nMen are all alike.\nELIZA1: IN WHAT WAY\nUser2:\nThey’re always bugging us about something or other.\nELIZA2: CAN YOU THINK OF A SPECIFIC EXAMPLE\nUser3:\nWell, my boyfriend made me come here.\nELIZA3: YOUR BOYFRIEND MADE YOU COME HERE\nUser4:\nHe says I’m depressed much of the time.\nELIZA4: I AM SORRY TO HEAR YOU ARE DEPRESSED\nELIZA works by having a series or cascade of regular expression substitutions\neach of which matches and changes some part of the input lines. Input lines are\nﬁrst uppercased. The ﬁrst substitutions then change all instances of MY to YOUR,\nand I’M to YOU ARE, and so on. The next set of substitutions matches and replaces\nother patterns in the input. Here are some examples:",
  "24": "2.2\n•\nWORDS\n19\ns/.* I’M (depressed|sad) .*/I AM SORRY TO HEAR YOU ARE \\1/\ns/.* I AM (depressed|sad) .*/WHY DO YOU THINK YOU ARE \\1/\ns/.* all .*/IN WHAT WAY/\ns/.* always .*/CAN YOU THINK OF A SPECIFIC EXAMPLE/\nSince multiple substitutions can apply to a given input, substitutions are assigned\na rank and applied in order. Creating patterns is the topic of Exercise 2.3, and we\nreturn to the details of the ELIZA architecture in Chapter 24.\n2.1.7\nLookahead assertions\nFinally, there will be times when we need to predict the future: look ahead in the\ntext to see if some pattern matches, but not advance the match cursor, so that we can\nthen deal with the pattern if it occurs.\nThese lookahead assertions make use of the (? syntax that we saw in the previ-\nlookahead\nous section for non-capture groups. The operator (?= pattern) is true if pattern\noccurs, but is zero-width, i.e. the match pointer doesn’t advance. The operator\nzero-width\n(?! pattern) only returns true if a pattern does not match, but again is zero-width\nand doesn’t advance the cursor. Negative lookahead is commonly used when we\nare parsing some complex pattern but want to rule out a special case. For example\nsuppose we want to match, at the beginning of a line, any single word that doesn’t\nstart with “Volcano”. We can use negative lookahead to do this:\n/ˆ(?!Volcano)[A-Za-z]+/\n2.2\nWords\nBefore we talk about processing words, we need to decide what counts as a word.\nLet’s start by looking at one particular corpus (plural corpora), a computer-readable\ncorpus\ncorpora\ncollection of text or speech. For example the Brown corpus is a million-word col-\nlection of samples from 500 written English texts from different genres (newspa-\nper, ﬁction, non-ﬁction, academic, etc.), assembled at Brown University in 1963–64\n(Kuˇcera and Francis, 1967). How many words are in the following Brown sentence?\nHe stepped out into the hall, was delighted to encounter a water brother.\nThis sentence has 13 words if we don’t count punctuation marks as words, 15\nif we count punctuation. Whether we treat period (“.”), comma (“,”), and so on as\nwords depends on the task. Punctuation is critical for ﬁnding boundaries of things\n(commas, periods, colons) and for identifying some aspects of meaning (question\nmarks, exclamation marks, quotation marks). For some tasks, like part-of-speech\ntagging or parsing or speech synthesis, we sometimes treat punctuation marks as if\nthey were separate words.\nThe Switchboard corpus of American English telephone conversations between\nstrangers was collected in the early 1990s; it contains 2430 conversations averaging\n6 minutes each, totaling 240 hours of speech and about 3 million words (Godfrey\net al., 1992). Such corpora of spoken language don’t have punctuation but do intro-\nduce other complications with regard to deﬁning words. Let’s look at one utterance\nfrom Switchboard; an utterance is the spoken correlate of a sentence:\nutterance\nI do uh main- mainly business data processing",
  "25": "20\nCHAPTER 2\n•\nREGULAR EXPRESSIONS, TEXT NORMALIZATION, EDIT DISTANCE\nThis utterance has two kinds of disﬂuencies. The broken-off word main- is\ndisﬂuency\ncalled a fragment. Words like uh and um are called ﬁllers or ﬁlled pauses. Should\nfragment\nﬁlled pause\nwe consider these to be words? Again, it depends on the application. If we are\nbuilding a speech transcription system, we might want to eventually strip out the\ndisﬂuencies.\nBut we also sometimes keep disﬂuencies around. Disﬂuencies like uh or um\nare actually helpful in speech recognition in predicting the upcoming word, because\nthey may signal that the speaker is restarting the clause or idea, and so for speech\nrecognition they are treated as regular words. Because people use different disﬂu-\nencies they can also be a cue to speaker identiﬁcation. In fact Clark and Fox Tree\n(2002) showed that uh and um have different meanings. What do you think they are?\nAre capitalized tokens like They and uncapitalized tokens like they the same\nword? These are lumped together in some tasks (speech recognition), while for part-\nof-speech or named-entity tagging, capitalization is a useful feature and is retained.\nHow about inﬂected forms like cats versus cat? These two words have the same\nlemma cat but are different wordforms. A lemma is a set of lexical forms having\nlemma\nthe same stem, the same major part-of-speech, and the same word sense. The word-\nform is the full inﬂected or derived form of the word. For morphologically complex\nwordform\nlanguages like Arabic, we often need to deal with lemmatization. For many tasks in\nEnglish, however, wordforms are sufﬁcient.\nHow many words are there in English? To answer this question we need to\ndistinguish two ways of talking about words. Types are the number of distinct words\nword type\nin a corpus; if the set of words in the vocabulary is V, the number of types is the\nvocabulary size |V|. Tokens are the total number N of running words. If we ignore\nword token\npunctuation, the following Brown sentence has 16 tokens and 14 types:\nThey picnicked by the pool, then lay back on the grass and looked at the stars.\nWhen we speak about the number of words in the language, we are generally\nreferring to word types.\nCorpus\nTokens = N Types = |V|\nShakespeare\n884 thousand 31 thousand\nBrown corpus\n1 million 38 thousand\nSwitchboard telephone conversations\n2.4 million 20 thousand\nCOCA\n440 million\n2 million\nGoogle N-grams\n1 trillion\n13 million\nFigure 2.10\nRough numbers of types and tokens for some English language corpora. The\nlargest, the Google N-grams corpus, contains 13 million types, but this count only includes\ntypes appearing 40 or more times, so the true number would be much larger.\nFig. 2.10 shows the rough numbers of types and tokens computed from some\npopular English corpora. The larger the corpora we look at, the more word types\nwe ﬁnd, and in fact this relationship between the number of types |V| and number\nof tokens N is called Herdan’s Law (Herdan, 1960) or Heaps’ Law (Heaps, 1978)\nHerdan’s Law\nHeaps’ Law\nafter its discoverers (in linguistics and information retrieval respectively). It is shown\nin Eq. 2.1, where k and β are positive constants, and 0 < β < 1.\n|V| = kNβ\n(2.1)\nThe value of β depends on the corpus size and the genre, but at least for the\nlarge corpora in Fig. 2.10, β ranges from .67 to .75. Roughly then we can say that",
  "26": "2.3\n•\nCORPORA\n21\nthe vocabulary size for a text goes up signiﬁcantly faster than the square root of its\nlength in words.\nAnother measure of the number of words in the language is the number of lem-\nmas instead of wordform types. Dictionaries can help in giving lemma counts; dic-\ntionary entries or boldface forms are a very rough upper bound on the number of\nlemmas (since some lemmas have multiple boldface forms). The 1989 edition of the\nOxford English Dictionary had 615,000 entries.\n2.3\nCorpora\nWords don’t appear out of nowhere. Any particular piece of text that we study\nis produced by one or more speciﬁc speakers or writers, in a speciﬁc dialect of a\nspeciﬁc language, at a speciﬁc time, in a speciﬁc place, for a speciﬁc function.\nPerhaps the most important dimension of variation is the language. NLP algo-\nrithms are most useful when they apply across many languages. The world has 7097\nlanguages at the time of this writing, according to the online Ethnologue catalog\n(Simons and Fennig, 2018). Most NLP tools tend to be developed for the ofﬁcial\nlanguages of large industrialized nations (Chinese, English, Spanish, Arabic, etc.),\nbut we don’t want to limit tools to just these few languages. Furthermore, most lan-\nguages also have multiple varieties, such as dialects spoken in different regions or\nby different social groups. Thus, for example, if we’re processing text in African\nAmerican Vernacular English (AAVE), a dialect spoken by millions of people in the\nAAVE\nUnited States, it’s important to make use of NLP tools that function with that dialect.\nTwitter posts written in AAVE make use of constructions like iont (I don’t in Stan-\ndard American English (SAE)), or talmbout corresponding to SAE talking about,\nSAE\nboth examples that inﬂuence word segmentation (Blodgett et al. 2016, Jones 2015).\nIt’s also quite common for speakers or writers to use multiple languages in a\nsingle communicative act, a phenomenon called code switching.\nCode switch-\ncode switching\ning is enormously common across the world; here are examples showing Spanish\nand (transliterated) Hindi code switching with English (Solorio et al. 2014, Jurgens\net al. 2017):\n(2.2) Por primera vez veo a @username actually being hateful! it was beautiful:)\n[For the ﬁrst time I get to see @username actually being hateful! it was\nbeautiful:]\n(2.3) dost tha or ra- hega ... dont wory ... but dherya rakhe\n[“he was and will remain a friend ... don’t worry ... but have faith”]\nAnother dimension of variation is the genre. The text that our algorithms must\nprocess might come from newswire, ﬁction or non-ﬁction books, scientiﬁc articles,\nWikipedia, or religious texts. It might come from spoken genres like telephone\nconversations, business meetings, police body-worn cameras, medical interviews,\nor transcripts of television shows or movies. It might come from work situations\nlike doctors’ notes, legal text, or parliamentary or congressional proceedings.\nText also reﬂects the demographic characteristics of the writer (or speaker): their\nage, gender, race, socio-economic class can all inﬂuence the linguistic properties of\nthe text we are processing.\nAnd ﬁnally, time matters too. Language changes over time, and for some lan-\nguages we have good corpora of texts from different historical periods.\nBecause language is so situated, when developing computational models for lan-",
  "27": "22\nCHAPTER 2\n•\nREGULAR EXPRESSIONS, TEXT NORMALIZATION, EDIT DISTANCE\nguage processing, it’s important to consider who produced the language, in what\ncontext, for what purpose, and make sure that the models are ﬁt to the data.\n2.4\nText Normalization\nBefore almost any natural language processing of a text, the text has to be normal-\nized. At least three tasks are commonly applied as part of any normalization process:\n1. Segmenting/tokenizing words from running text\n2. Normalizing word formats\n3. Segmenting sentences in running text.\nIn the next sections we walk through each of these tasks.\n2.4.1\nUnix tools for crude tokenization and normalization\nLet’s begin with an easy, if somewhat naive version of word tokenization and nor-\nmalization (and frequency computation) that can be accomplished for English solely\nin a single UNIX command-line, inspired by Church (1994). We’ll make use of some\nUnix commands: tr, used to systematically change particular characters in the in-\nput; sort, which sorts input lines in alphabetical order; and uniq, which collapses\nand counts adjacent identical lines.\nFor example let’s begin with the ‘complete words’ of Shakespeare in one textﬁle,\nsh.txt. We can use tr to tokenize the words by changing every sequence of non-\nalphabetic characters to a newline (’A-Za-z’ means alphabetic, the -c option com-\nplements to non-alphabet, and the -s option squeezes all sequences into a single\ncharacter):\ntr -sc ’A-Za-z’ ’\\n’ < sh.txt\nThe output of this command will be:\nTHE\nSONNETS\nby\nWilliam\nShakespeare\nFrom\nfairest\ncreatures\nWe\n...\nNow that there is one word per line, we can sort the lines, and pass them to uniq\n-c which will collapse and count them:\ntr -sc ’A-Za-z’ ’\\n’ < sh.txt | sort | uniq -c\nwith the following output:\n1945 A\n72 AARON\n19 ABBESS\n25 Aaron",
  "28": "2.4\n•\nTEXT NORMALIZATION\n23\n6 Abate\n1 Abates\n5 Abbess\n6 Abbey\n3 Abbot\n...\nAlternatively, we can collapse all the upper case to lower case:\ntr -sc ’A-Za-z’ ’\\n’ < sh.txt | tr A-Z a-z | sort | uniq -c\nwhose output is\n14725 a\n97 aaron\n1 abaissiez\n10 abandon\n2 abandoned\n2 abase\n1 abash\n14 abate\n3 abated\n3 abatement\n...\nNow we can sort again to ﬁnd the frequent words. The -n option to sort means\nto sort numerically rather than alphabetically, and the -r option means to sort in\nreverse order (highest-to-lowest):\ntr -sc ’A-Za-z’ ’\\n’ < sh.txt | tr A-Z a-z | sort | uniq -c | sort -n -r\nThe results show that the most frequent words in Shakespeare, as in any other\ncorpus, are the short function words like articles, pronouns, prepositions:\n27378 the\n26084 and\n22538 i\n19771 to\n17481 of\n14725 a\n13826 you\n12489 my\n11318 that\n11112 in\n...\nUnix tools of this sort can be very handy in building quick word count statistics\nfor any corpus.\n2.4.2\nWord Tokenization and Normalization\nThe simple UNIX tools above were ﬁne for getting rough word statistics but more\nsophisticated algorithms are generally necessary for tokenization, the task of seg-\ntokenization\nmenting running text into words, and normalization, the task of putting words/to-\nnormalization\nkens in a standard format.\nWhile the Unix command sequence just removed all the numbers and punctu-\nation, for most NLP applications we’ll need to keep these in our tokenization. We",
  "29": "24\nCHAPTER 2\n•\nREGULAR EXPRESSIONS, TEXT NORMALIZATION, EDIT DISTANCE\noften want to break off punctuation as a separate token; commas are a useful piece of\ninformation for parsers, periods help indicate sentence boundaries. But we’ll often\nwant to keep the punctuation that occurs word internally, in examples like m.p.h,,\nPh.D., AT&T, cap’n. Special characters and numbers will need to be kept in prices\n($45.55) and dates (01/02/06); we don’t want to segment that price into separate to-\nkens of “45” and “55”. And there are URLs (http://www.stanford.edu), Twitter\nhashtags (#nlproc), or email addresses (someone@cs.colorado.edu).\nNumber expressions introduce other complications as well; while commas nor-\nmally appear at word boundaries, commas are used inside numbers in English, every\nthree digits: 555,500.50. Languages, and hence tokenization requirements, differ\non this; many continental European languages like Spanish, French, and German, by\ncontrast, use a comma to mark the decimal point, and spaces (or sometimes periods)\nwhere English puts commas, for example, 555 500,50.\nA tokenizer can also be used to expand clitic contractions that are marked by\nclitic\napostrophes, for example, converting what’re to the two tokens what are, and\nwe’re to we are. A clitic is a part of a word that can’t stand on its own, and can only\noccur when it is attached to another word. Some such contractions occur in other\nalphabetic languages, including articles and pronouns in French (j’ai, l’homme).\nDepending on the application, tokenization algorithms may also tokenize mul-\ntiword expressions like New York or rock ’n’ roll as a single token, which re-\nquires a multiword expression dictionary of some sort. Tokenization is thus inti-\nmately tied up with named entity detection, the task of detecting names, dates, and\norganizations (Chapter 17).\nOne commonly used tokenization standard is known as the Penn Treebank to-\nkenization standard, used for the parsed corpora (treebanks) released by the Lin-\nPenn Treebank\ntokenization\nguistic Data Consortium (LDC), the source of many useful datasets. This standard\nseparates out clitics (doesn’t becomes does plus n’t), keeps hyphenated words to-\ngether, and separates out all punctuation:\nInput:\n“The San Francisco-based restaurant,” they said, “doesn’t charge $10”.\nOutput:\n“\nThe\nSan\nFrancisco-based\nrestaurant\n,\n”\nthey\nsaid\n,\n“\ndoes\nn’t\ncharge\n$\n10\n”\n.\nTokens can also be normalized, in which a single normalized form is chosen for\nwords with multiple forms like USA and US or uh-huh and uhhuh. This standard-\nization may be valuable, despite the spelling information that is lost in the normal-\nization process. For information retrieval, we might want a query for US to match a\ndocument that has USA; for information extraction we might want to extract coherent\ninformation that is consistent across differently-spelled instances.\nCase folding is another kind of normalization. For tasks like speech recognition\ncase folding\nand information retrieval, everything is mapped to lower case. For sentiment anal-\nysis and other text classiﬁcation tasks, information extraction, and machine transla-\ntion, by contrast, case is quite helpful and case folding is generally not done (losing\nthe difference, for example, between US the country and us the pronoun can out-\nweigh the advantage in generality that case folding provides).\nIn practice, since tokenization needs to be run before any other language process-\ning, it is important for it to be very fast. The standard method for tokenization/nor-\nmalization is therefore to use deterministic algorithms based on regular expressions\ncompiled into very efﬁcient ﬁnite state automata. Carefully designed deterministic\nalgorithms can deal with the ambiguities that arise, such as the fact that the apos-\ntrophe needs to be tokenized differently when used as a genitive marker (as in the",
  "30": "2.4\n•\nTEXT NORMALIZATION\n25\nbook’s cover), a quotative as in ‘The other class’, she said, or in clitics like they’re.\n2.4.3\nWord Segmentation in Chinese: the MaxMatch algorithm\nSome languages, including written Chinese, Japanese, and Thai, do not use spaces to\nmark potential word-boundaries, and so require alternative segmentation methods.\nIn Chinese, for example, words are composed of characters known as hanzi. Each\nhanzi\ncharacter generally represents a single morpheme and is pronounceable as a single\nsyllable. Words are about 2.4 characters long on average. A simple algorithm that\ndoes remarkably well for segmenting Chinese, and often used as a baseline com-\nparison for more advanced methods, is a version of greedy search called maximum\nmatching or sometimes MaxMatch. The algorithm requires a dictionary (wordlist)\nmaximum\nmatching\nof the language.\nThe maximum matching algorithm starts by pointing at the beginning of a string.\nIt chooses the longest word in the dictionary that matches the input at the current\nposition. The pointer is then advanced to the end of that word in the string. If\nno word matches, the pointer is instead advanced one character (creating a one-\ncharacter word). The algorithm is then iteratively applied again starting from the\nnew pointer position. Fig. 2.11 shows a version of the algorithm.\nfunction MAXMATCH(sentence, dictionary) returns word sequence W\nif sentence is empty\nreturn empty list\nfor i←length(sentence) downto 1\nﬁrstword = ﬁrst i chars of sentence\nremainder = rest of sentence\nif InDictionary(ﬁrstword, dictionary)\nreturn list(ﬁrstword, MaxMatch(remainder,dictionary) )\n# no word was found, so make a one-character word\nﬁrstword = ﬁrst char of sentence\nremainder = rest of sentence\nreturn list(ﬁrstword, MaxMatch(remainder,dictionary) )\nFigure 2.11\nThe MaxMatch algorithm for word segmentation.\nMaxMatch works very well on Chinese; the following example shows an appli-\ncation to a simple Chinese sentence using a simple Chinese lexicon available from\nthe Linguistic Data Consortium:\nInput:\n他特别喜欢北京烤鸭\n“He especially likes Peking duck”\nOutput: 他特别\n喜欢北京烤鸭\nHe especially likes Peking duck\nMaxMatch doesn’t work as well on English. To make the intuition clear, we’ll\ncreate an example by removing the spaces from the beginning of Turing’s famous\nquote “We can only see a short distance ahead”, producing “wecanonlyseeashortdis-\ntanceahead”. The MaxMatch results are shown below.\nInput:\nwecanonlyseeashortdistanceahead\nOutput: we canon l y see ash ort distance ahead\nOn English the algorithm incorrectly chose canon instead of stopping at can,\nwhich left the algorithm confused and having to create single-character words l and",
  "31": "26\nCHAPTER 2\n•\nREGULAR EXPRESSIONS, TEXT NORMALIZATION, EDIT DISTANCE\ny and use the very rare word ort.\nThe algorithm works better in Chinese than English, because Chinese has much\nshorter words than English. We can quantify how well a segmenter works using a\nmetric called word error rate. We compare our output segmentation with a perfect\nword error rate\nhand-segmented (‘gold’) sentence, seeing how many words differ. The word error\nrate is then the normalized minimum edit distance in words between our output and\nthe gold: the number of word insertions, deletions, and substitutions divided by the\nlength of the gold sentence in words; we’ll see in Section 2.5 how to compute edit\ndistance. Even in Chinese, however, MaxMatch has problems, for example dealing\nwith unknown words (words not in the dictionary) or genres that differ a lot from\nthe assumptions made by the dictionary builder.\nThe most accurate Chinese segmentation algorithms generally use statistical se-\nquence models trained via supervised machine learning on hand-segmented training\nsets; we’ll introduce sequence models in Chapter 8.\n2.4.4\nCollapsing words: Lemmatization and Stemming\nFor many natural language processing situations we want two different forms of\na word to behave similarly. For example in web search, someone may type the\nstring woodchucks but a useful system might want to also return pages that mention\nwoodchuck with no s. This is especially common in morphologically complex lan-\nguages like Russian, where for example the word Moscow has different endings in\nthe phrases Moscow, of Moscow, from Moscow, and so on.\nLemmatization is the task of determining that two words have the same root,\ndespite their surface differences. The words am, are, and is have the shared lemma\nbe; the words dinner and dinners both have the lemma dinner.\nLemmatizing each of these forms to the same lemma will let us ﬁnd all mentions\nof words like Moscow. The the lemmatized form of a sentence like He is reading\ndetective stories would thus be He be read detective story.\nHow is lemmatization done? The most sophisticated methods for lemmatization\ninvolve complete morphological parsing of the word. Morphology is the study of\nthe way words are built up from smaller meaning-bearing units called morphemes.\nmorpheme\nTwo broad classes of morphemes can be distinguished: stems—the central mor-\nstem\npheme of the word, supplying the main meaning— and afﬁxes—adding “additional”\nafﬁx\nmeanings of various kinds. So, for example, the word fox consists of one morpheme\n(the morpheme fox) and the word cats consists of two: the morpheme cat and the\nmorpheme -s. A morphological parser takes a word like cats and parses it into the\ntwo morphemes cat and s, or a Spanish word like amaren (‘if in the future they\nwould love’) into the morphemes amar ‘to love’, 3PL, and future subjunctive.\nThe Porter Stemmer\nLemmatization algorithms can be complex. For this reason we sometimes make use\nof a simpler but cruder method, which mainly consists of chopping off word-ﬁnal\nafﬁxes. This naive version of morphological analysis is called stemming. One of\nstemming\nthe most widely used stemming algorithms is the Porter (1980). The Porter stemmer\nPorter stemmer\napplied to the following paragraph:\nThis was not the map we found in Billy Bones’s chest, but\nan accurate copy, complete in all things-names and heights\nand soundings-with the single exception of the red crosses\nand the written notes.",
  "32": "2.4\n•\nTEXT NORMALIZATION\n27\nproduces the following stemmed output:\nThi wa not the map we found in Billi Bone s chest but an\naccur copi complet in all thing name and height and sound\nwith the singl except of the red cross and the written note\nThe algorithm is based on series of rewrite rules run in series, as a cascade, in\ncascade\nwhich the output of each pass is fed as input to the next pass; here is a sampling of\nthe rules:\nATIONAL →ATE (e.g., relational →relate)\nING →ϵ\nif stem contains vowel (e.g., motoring →motor)\nSSES →SS\n(e.g., grasses →grass)\nDetailed rule lists for the Porter stemmer, as well as code (in Java, Python, etc.)\ncan be found on Martin Porter’s homepage; see also the original paper (Porter, 1980).\nSimple stemmers can be useful in cases where we need to collapse across differ-\nent variants of the same lemma. Nonetheless, they do tend to commit errors of both\nover- and under-generalizing, as shown in the table below (Krovetz, 1993):\nErrors of Commission\nErrors of Omission\norganization organ\nEuropean Europe\ndoing\ndoe\nanalysis\nanalyzes\nnumerical\nnumerous\nnoise\nnoisy\npolicy\npolice\nsparse\nsparsity\n2.4.5\nByte-Pair Encoding\nStemming or lemmatizing has another side-beneﬁt. By treating two similar words\nidentically, these normalization methods help deal with the problem of unknown\nwords, words that a system has not seen before.\nunknown\nwords\nUnknown words are particularly relevant for machine learning systems. As we\nwill see in the next chapter, machine learning systems often learn some facts about\nwords in one corpus (a training corpus) and then use these facts to make decisions\nabout a separate test corpus and its words. Thus if our training corpus contains, say\nthe words low, and lowest, but not lower, but then the word lower appears in our\ntest corpus, our system will not know what to do with it. Stemming or lemmatizing\neverything to low can solve the problem, but has the disadvantage that sometimes\nwe don’t want words to be completely collapsed. For some purposes (for example\npart-of-speech tagging) the words low and lower need to remain distinct.\nA solution to this problem is to use a different kind of tokenization in which\nmost tokens are words, but some tokens are frequent word parts like -er, so that an\nunseen word can be represented by combining the parts.\nThe simplest such algorithm is byte-pair encoding, or BPE (Sennrich et al.,\nbyte-pair\nencoding\nBPE\n2016). Byte-pair encoding is based on a method for text compression (Gage, 1994),\nbut here we use it for tokenization instead. The intuition of the algorithm is to\niteratively merge frequent pairs of characters,\nThe algorithm begins with the set of symbols equal to the set of characters. Each\nword is represented as a sequence of characters plus a special end-of-word symbol\n·. At each step of the algorithm, we count the number of symbol pairs, ﬁnd the\nmost frequent pair (‘A’, ‘B’), and replace it with the new merged symbol (‘AB’). We\ncontinue to count and merge, creating new longer and longer character strings, until",
  "33": "28\nCHAPTER 2\n•\nREGULAR EXPRESSIONS, TEXT NORMALIZATION, EDIT DISTANCE\nwe’ve done k merges; k is a parameter of the algorithm. The resulting symbol set\nwill consist of the original set of characters plus k new symbols.\nThe algorithm is run inside words (we don’t merge across word boundaries).\nFor this reason, the algorithm can take as input a dictionary of words together with\ncounts. For example, consider the following tiny input dictionary:\nword\nfrequency\nl o w ·\n5\nl o w e s t · 2\nn e w e r ·\n6\nw i d e r ·\n3\nn e w ·\n2\nWe ﬁrst count all pairs of symbols: the most frequent is the pair r · because it\noccurs in newer (frequency of 6) and wider (frequency of 3) for a total of 9 occur-\nrences. We then merge these symbols, treating r· as one symbol, and count again:\nword\nfrequency\nl o w ·\n5\nl o w e s t · 2\nn e w e r·\n6\nw i d e r·\n3\nn e w ·\n2\nNow the most frequent pair is e r·, which we merge:\nword\nfrequency\nl o w ·\n5\nl o w e s t · 2\nn e w er·\n6\nw i d er·\n3\nn e w ·\n2\nOur system has learned that there should be a token for word-ﬁnal er, repre-\nsented as er·. If we continue, the next merges are\n(’e’, ’w’)\n(’n’, ’ew’)\n(’l’, ’o’)\n(’lo’, ’w’)\n(’new’, ’er·’)\n(’low’, ’·’)\nThe current set of symbols is thus {·, d, e, i, l, n, o, r, s, t, w,\nr·, er·, ew, new, lo, low, newer·, low·}\nWhen we need to tokenize a test sentence, we just run the merges we have\nlearned, greedily, in the order we learned them, on the test data. (Thus the fre-\nquencies in the test data don’t play a role, just the frequencies in the training data).\nSo ﬁrst we segment each test sentence word into characters. Then we apply the ﬁrst\nrule: replace every instance of r · in the test corpus with r·, and then the second\nrule: replace every instance of e r· in the test corpus with er·, and so on. By the\nend, if the test corpus contained the word n e w e r ·, it would be tokenized as a\nfull word. But a new (unknown) word like l o w e r · would be merged into the\ntwo tokens low er·.\nOf course in real algorithms BPE is run with many thousands of merges on a\nvery large input dictionary. The result is that most words will be represented as",
  "34": "2.4\n•\nTEXT NORMALIZATION\n29\nfull symbols, and only the very rare words (and unknown words) will have to be\nrepresented by their parts.\nThe full BPE learning algorithm is given in Fig. 2.12.\nimport\nre ,\nc o l l e c t i o n s\ndef\ng e t s t a t s ( vocab ) :\np a i r s =\nc o l l e c t i o n s . d e f a u l t d i c t ( i n t )\nf o r\nword ,\nf r e q\nin\nvocab . items ( ) :\nsymbols = word . s p l i t ( )\nf o r\ni\nin\nrange ( len ( symbols ) −1):\np a i r s [ symbols [ i ] , symbols [ i +1]] += f r e q\nr e t u r n\np a i r s\ndef\nmerge vocab ( pair ,\nv i n ) :\nv out = {}\nbigram = re . escape ( ’\n’ . j o i n ( p a i r ) )\np = re . compile ( r ’ (? <!\\S ) ’ + bigram + r ’ ( ? ! \\ S ) ’ )\nf o r\nword\nin\nv i n :\nw out = p . sub ( ’ ’ . j o i n ( p a i r ) ,\nword )\nv out [ w out ] = v i n [ word ]\nr e t u r n\nv out\nvocab = { ’ l\no w </w>’\n:\n5 ,\n’ l\no w e s\nt\n</w>’\n:\n2 ,\n’n e w e\nr </w>’ : 6 ,\n’w i\nd e\nr </w>’ : 3 ,\n’n e w </w>’ :2}\nnum merges = 8\nf o r\ni\nin\nrange ( num merges ) :\np a i r s =\ng e t s t a t s ( vocab )\nb e s t = max ( p a i r s ,\nkey= p a i r s . get )\nvocab = merge vocab ( best ,\nvocab )\np r i n t ( b e s t )\nFigure 2.12\nPython code for BPE learning algorithm from Sennrich et al. (2016).\n2.4.6\nSentence Segmentation\nSentence segmentation is another important step in text processing. The most use-\nSentence\nsegmentation\nful cues for segmenting a text into sentences are punctuation, like periods, question\nmarks, exclamation points. Question marks and exclamation points are relatively\nunambiguous markers of sentence boundaries. Periods, on the other hand, are more\nambiguous. The period character “.” is ambiguous between a sentence boundary\nmarker and a marker of abbreviations like Mr. or Inc. The previous sentence that\nyou just read showed an even more complex case of this ambiguity, in which the ﬁnal\nperiod of Inc. marked both an abbreviation and the sentence boundary marker. For\nthis reason, sentence tokenization and word tokenization may be addressed jointly.\nIn general, sentence tokenization methods work by building a binary classiﬁer\n(based on a sequence of rules or on machine learning) that decides if a period is part\nof the word or is a sentence-boundary marker. In making this decision, it helps to\nknow if the period is attached to a commonly used abbreviation; thus, an abbrevia-\ntion dictionary is useful.\nState-of-the-art methods for sentence tokenization are based on machine learning\nand are introduced in later chapters.",
  "35": "30\nCHAPTER 2\n•\nREGULAR EXPRESSIONS, TEXT NORMALIZATION, EDIT DISTANCE\n2.5\nMinimum Edit Distance\nMuch of natural language processing is concerned with measuring how similar two\nstrings are.\nFor example in spelling correction, the user typed some erroneous\nstring—let’s say graffe–and we want to know what the user meant. The user prob-\nably intended a word that is similar to graffe. Among candidate similar words,\nthe word giraffe, which differs by only one letter from graffe, seems intuitively\nto be more similar than, say grail or graf, which differ in more letters. Another\nexample comes from coreference, the task of deciding whether two strings such as\nthe following refer to the same entity:\nStanford President John Hennessy\nStanford University President John Hennessy\nAgain, the fact that these two strings are very similar (differing by only one word)\nseems like useful evidence for deciding that they might be coreferent.\nEdit distance gives us a way to quantify both of these intuitions about string sim-\nilarity. More formally, the minimum edit distance between two strings is deﬁned\nminimum edit\ndistance\nas the minimum number of editing operations (operations like insertion, deletion,\nsubstitution) needed to transform one string into another.\nThe gap between intention and execution, for example, is 5 (delete an i, substi-\ntute e for n, substitute x for t, insert c, substitute u for n). It’s much easier to see\nthis by looking at the most important visualization for string distances, an alignment\nalignment\nbetween the two strings, shown in Fig. 2.13. Given two sequences, an alignment is\na correspondence between substrings of the two sequences. Thus, we say I aligns\nwith the empty string, N with E, and so on. Beneath the aligned strings is another\nrepresentation; a series of symbols expressing an operation list for converting the\ntop string into the bottom string: d for deletion, s for substitution, i for insertion.\nI N T E * N T I O N\n| | | | | | | | | |\n* E X E C U T I O N\nd s s\ni s\nFigure 2.13\nRepresenting the minimum edit distance between two strings as an alignment.\nThe ﬁnal row gives the operation list for converting the top string into the bottom string: d for\ndeletion, s for substitution, i for insertion.\nWe can also assign a particular cost or weight to each of these operations. The\nLevenshtein distance between two sequences is the simplest weighting factor in\nwhich each of the three operations has a cost of 1 (Levenshtein, 1966)—we assume\nthat the substitution of a letter for itself, for example, t for t, has zero cost. The Lev-\nenshtein distance between intention and execution is 5. Levenshtein also proposed\nan alternative version of his metric in which each insertion or deletion has a cost of\n1 and substitutions are not allowed. (This is equivalent to allowing substitution, but\ngiving each substitution a cost of 2 since any substitution can be represented by one\ninsertion and one deletion). Using this version, the Levenshtein distance between\nintention and execution is 8.",
  "36": "2.5\n•\nMINIMUM EDIT DISTANCE\n31\n2.5.1\nThe Minimum Edit Distance Algorithm\nHow do we ﬁnd the minimum edit distance? We can think of this as a search task, in\nwhich we are searching for the shortest path—a sequence of edits—from one string\nto another.\nn t e n t i o n\ni n t e c n t i o n\ni n x e n t i o n\ndel\nins\nsubst\ni n t e n t i o n\nFigure 2.14\nFinding the edit distance viewed as a search problem\nThe space of all possible edits is enormous, so we can’t search naively. However,\nlots of distinct edit paths will end up in the same state (string), so rather than recom-\nputing all those paths, we could just remember the shortest path to a state each time\nwe saw it. We can do this by using dynamic programming. Dynamic programming\ndynamic\nprogramming\nis the name for a class of algorithms, ﬁrst introduced by Bellman (1957), that apply\na table-driven method to solve problems by combining solutions to sub-problems.\nSome of the most commonly used algorithms in natural language processing make\nuse of dynamic programming, such as the Viterbi algorithm (Chapter 8) and the\nCKY algorithm for parsing (Chapter 11).\nThe intuition of a dynamic programming problem is that a large problem can\nbe solved by properly combining the solutions to various sub-problems. Consider\nthe shortest path of transformed words that represents the minimum edit distance\nbetween the strings intention and execution shown in Fig. 2.15.\nn t e n t i o n\ni n t e n t i o n\ne t e n t i o n\ne x e n t i o n\ne x e n u t i o n\ne x e c u t i o n\ndelete i\nsubstitute n by e\nsubstitute t by x\ninsert u\nsubstitute n by c\nFigure 2.15\nPath from intention to execution.\nImagine some string (perhaps it is exention) that is in this optimal path (whatever\nit is). The intuition of dynamic programming is that if exention is in the optimal\noperation list, then the optimal sequence must also include the optimal path from\nintention to exention. Why? If there were a shorter path from intention to exention,\nthen we could use it instead, resulting in a shorter overall path, and the optimal\nsequence wouldn’t be optimal, thus leading to a contradiction.\nThe minimum edit distance algorithm was named by Wagner and Fischer (1974)\nminimum edit\ndistance\nbut independently discovered by many people (see the Historical Notes section of\nChapter 8).\nLet’s ﬁrst deﬁne the minimum edit distance between two strings. Given two\nstrings, the source string X of length n, and target string Y of length m, we’ll deﬁne\nD(i, j) as the edit distance between X[1..i] and Y[1..j], i.e., the ﬁrst i characters of X\nand the ﬁrst j characters of Y. The edit distance between X and Y is thus D(n,m).",
  "37": "32\nCHAPTER 2\n•\nREGULAR EXPRESSIONS, TEXT NORMALIZATION, EDIT DISTANCE\nWe’ll use dynamic programming to compute D(n,m) bottom up, combining so-\nlutions to subproblems. In the base case, with a source substring of length i but an\nempty target string, going from i characters to 0 requires i deletes. With a target\nsubstring of length j but an empty source going from 0 characters to j characters\nrequires j inserts. Having computed D(i, j) for small i, j we then compute larger\nD(i, j) based on previously computed smaller values. The value of D(i, j) is com-\nputed by taking the minimum of the three possible paths through the matrix which\narrive there:\nD[i, j] = min\n\n\n\nD[i−1, j]+del-cost(source[i])\nD[i, j −1]+ins-cost(target[j])\nD[i−1, j −1]+sub-cost(source[i],target[j])\nIf we assume the version of Levenshtein distance in which the insertions and\ndeletions each have a cost of 1 (ins-cost(·) = del-cost(·) = 1), and substitutions have\na cost of 2 (except substitution of identical letters have zero cost), the computation\nfor D(i, j) becomes:\nD[i, j] = min\n\n\n\n\n\n\n\nD[i−1, j]+1\nD[i, j −1]+1\nD[i−1, j −1]+\n\u001a\n2; if source[i] ̸= target[j]\n0; if source[i] = target[j]\n(2.4)\nThe algorithm is summarized in Fig. 2.16; Fig. 2.17 shows the results of applying\nthe algorithm to the distance between intention and execution with the version of\nLevenshtein in Eq. 2.4.\nKnowing the minimum edit distance is useful for algorithms like ﬁnding poten-\ntial spelling error corrections. But the edit distance algorithm is important in another\nway; with a small change, it can also provide the minimum cost alignment between\ntwo strings. Aligning two strings is useful throughout speech and language process-\ning. In speech recognition, minimum edit distance alignment is used to compute\nthe word error rate (Chapter 26). Alignment plays a role in machine translation, in\nwhich sentences in a parallel corpus (a corpus with a text in two languages) need to\nbe matched to each other.\nTo extend the edit distance algorithm to produce an alignment, we can start by\nvisualizing an alignment as a path through the edit distance matrix. Figure 2.18\nshows this path with the boldfaced cell. Each boldfaced cell represents an alignment\nof a pair of letters in the two strings. If two boldfaced cells occur in the same row,\nthere will be an insertion in going from the source to the target; two boldfaced cells\nin the same column indicate a deletion.\nFigure 2.18 also shows the intuition of how to compute this alignment path. The\ncomputation proceeds in two steps. In the ﬁrst step, we augment the minimum edit\ndistance algorithm to store backpointers in each cell. The backpointer from a cell\npoints to the previous cell (or cells) that we came from in entering the current cell.\nWe’ve shown a schematic of these backpointers in Fig. 2.18. Some cells have mul-\ntiple backpointers because the minimum extension could have come from multiple\nprevious cells. In the second step, we perform a backtrace. In a backtrace, we start\nbacktrace\nfrom the last cell (at the ﬁnal row and column), and follow the pointers back through\nthe dynamic programming matrix. Each complete path between the ﬁnal cell and the\ninitial cell is a minimum distance alignment. Exercise 2.7 asks you to modify the",
  "38": "2.5\n•\nMINIMUM EDIT DISTANCE\n33\nfunction MIN-EDIT-DISTANCE(source, target) returns min-distance\nn←LENGTH(source)\nm←LENGTH(target)\nCreate a distance matrix distance[n+1,m+1]\n# Initialization: the zeroth row and column is the distance from the empty string\nD[0,0] = 0\nfor each row i from 1 to n do\nD[i,0]←D[i-1,0] + del-cost(source[i])\nfor each column j from 1 to m do\nD[0,j]←D[0, j-1] + ins-cost(target[j])\n# Recurrence relation:\nfor each row i from 1 to n do\nfor each column j from 1 to m do\nD[i,j]←MIN( D[i−1, j] + del-cost(source[i]),\nD[i−1, j−1] + sub-cost(source[i],target[j]),\nD[i, j−1] + ins-cost(target[j]))\n# Termination\nreturn D[n,m]\nFigure 2.16\nThe minimum edit distance algorithm, an example of the class of dynamic\nprogramming algorithms. The various costs can either be ﬁxed (e.g., ∀x,ins-cost(x) = 1)\nor can be speciﬁc to the letter (to model the fact that some letters are more likely to be in-\nserted than others). We assume that there is no cost for substituting a letter for itself (i.e.,\nsub-cost(x,x) = 0).\nSrc\\Tar\n#\ne\nx\ne\nc\nu\nt\ni\no\nn\n#\n0\n1\n2\n3\n4\n5\n6\n7\n8\n9\ni\n1\n2\n3\n4\n5\n6\n7\n6\n7\n8\nn\n2\n3\n4\n5\n6\n7\n8\n7\n8\n7\nt\n3\n4\n5\n6\n7\n8\n7\n8\n9\n8\ne\n4\n3\n4\n5\n6\n7\n8\n9\n10\n9\nn\n5\n4\n5\n6\n7\n8\n9\n10\n11\n10\nt\n6\n5\n6\n7\n8\n9\n8\n9\n10\n11\ni\n7\n6\n7\n8\n9\n10\n9\n8\n9\n10\no\n8\n7\n8\n9\n10\n11\n10\n9\n8\n9\nn\n9\n8\n9\n10\n11\n12\n11\n10\n9\n8\nFigure 2.17\nComputation of minimum edit distance between intention and execution with\nthe algorithm of Fig. 2.16, using Levenshtein distance with cost of 1 for insertions or dele-\ntions, 2 for substitutions.\nminimum edit distance algorithm to store the pointers and compute the backtrace to\noutput an alignment.\nWhile we worked our example with simple Levenshtein distance, the algorithm\nin Fig. 2.16 allows arbitrary weights on the operations. For spelling correction, for\nexample, substitutions are more likely to happen between letters that are next to\neach other on the keyboard. The Viterbi algorithm is a probabilistic extension of\nminimum edit distance. Instead of computing the “minimum edit distance” between\ntwo strings, Viterbi computes the “maximum probability alignment” of one string\nwith another. We’ll discuss this more in Chapter 8.",
  "39": "34\nCHAPTER 2\n•\nREGULAR EXPRESSIONS, TEXT NORMALIZATION, EDIT DISTANCE\n#\ne\nx\ne\nc\nu\nt\ni\no\nn\n#\n0\n←1\n←2\n←3\n←4\n←5\n←6\n←7\n←8\n←9\ni\n↑1\n↖←↑2\n↖←↑3\n↖←↑4\n↖←↑5\n↖←↑6\n↖←↑7\n↖6\n←7\n←8\nn\n↑2\n↖←↑3\n↖←↑4\n↖←↑5\n↖←↑6\n↖←↑7\n↖←↑8\n↑7\n↖←↑8\n↖7\nt\n↑3\n↖←↑4\n↖←↑5\n↖←↑6\n↖←↑7\n↖←↑8\n↖7\n←↑8\n↖←↑9\n↑8\ne\n↑4\n↖3\n←4\n↖←5\n←6\n←7\n←↑8\n↖←↑9\n↖←↑10\n↑9\nn\n↑5\n↑4\n↖←↑5\n↖←↑6\n↖←↑7\n↖←↑8\n↖←↑9\n↖←↑10\n↖←↑11 ↖↑10\nt\n↑6\n↑5\n↖←↑6\n↖←↑7\n↖←↑8\n↖←↑9\n↖8\n←9\n←10 ←↑11\ni\n↑7\n↑6\n↖←↑7\n↖←↑8\n↖←↑9\n↖←↑10\n↑9\n↖8\n←9\n←10\no\n↑8\n↑7\n↖←↑8\n↖←↑9\n↖←↑10\n↖←↑11\n↑10\n↑9\n↖8\n←9\nn\n↑9\n↑8\n↖←↑9\n↖←↑10\n↖←↑11\n↖←↑12\n↑11\n↑10\n↑9\n↖8\nFigure 2.18\nWhen entering a value in each cell, we mark which of the three neighboring\ncells we came from with up to three arrows. After the table is full we compute an alignment\n(minimum edit path) by using a backtrace, starting at the 8 in the lower-right corner and\nfollowing the arrows back. The sequence of bold cells represents one possible minimum cost\nalignment between the two strings. Diagram design after Gusﬁeld (1997).\n2.6\nSummary\nThis chapter introduced a fundamental tool in language processing, the regular ex-\npression, and showed how to perform basic text normalization tasks including\nword segmentation and normalization, sentence segmentation, and stemming.\nWe also introduce the important minimum edit distance algorithm for comparing\nstrings. Here’s a summary of the main points we covered about these ideas:\n• The regular expression language is a powerful tool for pattern-matching.\n• Basic operations in regular expressions include concatenation of symbols,\ndisjunction of symbols ([], |, and .), counters (*, +, and {n,m}), anchors\n(ˆ, $) and precedence operators ((,)).\n• Word tokenization and normalization are generally done by cascades of\nsimple regular expressions substitutions or ﬁnite automata.\n• The Porter algorithm is a simple and efﬁcient way to do stemming, stripping\noff afﬁxes. It does not have high accuracy but may be useful for some tasks.\n• The minimum edit distance between two strings is the minimum number of\noperations it takes to edit one into the other. Minimum edit distance can be\ncomputed by dynamic programming, which also results in an alignment of\nthe two strings.\nBibliographical and Historical Notes\nKleene (1951) and (1956) ﬁrst deﬁned regular expressions and the ﬁnite automaton,\nbased on the McCulloch-Pitts neuron. Ken Thompson was one of the ﬁrst to build\nregular expressions compilers into editors for text searching (Thompson, 1968). His\neditor ed included a command “g/regular expression/p”, or Global Regular Expres-\nsion Print, which later became the Unix grep utility.\nText normalization algorithms has been applied since the beginning of the ﬁeld.\nOne of the earliest widely-used stemmers was Lovins (1968). Stemming was also",
  "40": "regular expressions compilers into editors for text searching (Thompson, 1968). His\neditor ed included a command “g/regular expression/p”, or Global Regular Expres-\nsion Print, which later became the Unix grep utility.\nText normalization algorithms has been applied since the beginning of the ﬁeld.\nOne of the earliest widely-used stemmers was Lovins (1968). Stemming was also\napplied early to the digital humanities, by Packard (1973), who built an afﬁx-stripping\nmorphological parser for Ancient Greek. Currently a wide variety of code for tok-",
  "41": "EXERCISES\n35\nenization and normalization is available, such as the Stanford Tokenizer (http://\nnlp.stanford.edu/software/tokenizer.shtml) or specialized tokenizers for\nTwitter (O’Connor et al., 2010), or for sentiment (http://sentiment.christopherpotts.\nnet/tokenizing.html). See Palmer (2012) for a survey of text preprocessing.\nWhile the max-match algorithm we describe is commonly used as a segmentation\nbaseline in languages like Chinese, higher accuracy algorithms like the Stanford\nCRF segmenter, are based on sequence models; see Tseng et al. (2005a) and Chang\net al. (2008). NLTK is an essential tool that offers both useful Python libraries\n(http://www.nltk.org) and textbook descriptions (Bird et al., 2009) of many al-\ngorithms including text normalization and corpus interfaces.\nFor more on Herdan’s law and Heaps’ Law, see Herdan (1960, p. 28), Heaps\n(1978), Egghe (2007) and Baayen (2001); Yasseri et al. (2012) discuss the relation-\nship with other measures of linguistic complexity. For more on edit distance, see the\nexcellent Gusﬁeld (1997). Our example measuring the edit distance from ‘intention’\nto ‘execution’ was adapted from Kruskal (1983). There are various publicly avail-\nable packages to compute edit distance, including Unix diff and the NIST sclite\nprogram (NIST, 2005).\nIn his autobiography Bellman (1984) explains how he originally came up with\nthe term dynamic programming:\n“...The 1950s were not good years for mathematical research. [the]\nSecretary of Defense ...had a pathological fear and hatred of the word,\nresearch...\nI decided therefore to use the word, “programming”.\nI\nwanted to get across the idea that this was dynamic, this was multi-\nstage... I thought, let’s ... take a word that has an absolutely precise\nmeaning, namely dynamic... it’s impossible to use the word, dynamic,\nin a pejorative sense. Try thinking of some combination that will pos-\nsibly give it a pejorative meaning. It’s impossible. Thus, I thought\ndynamic programming was a good name. It was something not even a\nCongressman could object to.”\nExercises\n2.1\nWrite regular expressions for the following languages.\n1. the set of all alphabetic strings;\n2. the set of all lower case alphabetic strings ending in a b;\n3. the set of all strings from the alphabet a,b such that each a is immedi-\nately preceded by and immediately followed by a b;\n2.2\nWrite regular expressions for the following languages. By “word”, we mean\nan alphabetic string separated from other words by whitespace, any relevant\npunctuation, line breaks, and so forth.\n1. the set of all strings with two consecutive repeated words (e.g., “Hum-\nbert Humbert” and “the the” but not “the bug” or “the big bug”);\n2. all strings that start at the beginning of the line with an integer and that\nend at the end of the line with a word;\n3. all strings that have both the word grotto and the word raven in them\n(but not, e.g., words like grottos that merely contain the word grotto);",
  "42": "36\nCHAPTER 2\n•\nREGULAR EXPRESSIONS, TEXT NORMALIZATION, EDIT DISTANCE\n4. write a pattern that places the ﬁrst word of an English sentence in a\nregister. Deal with punctuation.\n2.3\nImplement an ELIZA-like program, using substitutions such as those described\non page 18. You might want to choose a different domain than a Rogerian psy-\nchologist, although keep in mind that you would need a domain in which your\nprogram can legitimately engage in a lot of simple repetition.\n2.4\nCompute the edit distance (using insertion cost 1, deletion cost 1, substitution\ncost 1) of “leda” to “deal”. Show your work (using the edit distance grid).\n2.5\nFigure out whether drive is closer to brief or to divers and what the edit dis-\ntance is to each. You may use any version of distance that you like.\n2.6\nNow implement a minimum edit distance algorithm and use your hand-computed\nresults to check your code.\n2.7\nAugment the minimum edit distance algorithm to output an alignment; you\nwill need to store pointers and add a stage to compute the backtrace.\n2.8\nImplement the MaxMatch algorithm.\n2.9\nTo test how well your MaxMatch algorithm works, create a test set by remov-\ning spaces from a set of sentences. Implement the Word Error Rate metric (the\nnumber of word insertions + deletions + substitutions, divided by the length\nin words of the correct string) and compute the WER for your test set.",
  "43": "CHAPTER\n3\nN-gram Language Models\n“You are uniformly charming!” cried he, with a smile of associating and now\nand then I bowed and they perceived a chaise and four to wish for.\nRandom sentence generated from a Jane Austen trigram model\nBeing able to predict the future is not always a good thing. Cassandra of Troy\nhad the gift of foreseeing but was cursed by Apollo that no one would believe her\npredictions. Her warnings of the destruction of Troy were ignored and—well, let’s\njust say that things didn’t turn out great for her.\nIn this chapter we take up the somewhat less fraught topic of predicting words.\nWhat word, for example, is likely to follow\nPlease turn your homework ...\nHopefully, most of you concluded that a very likely word is in, or possibly over,\nbut probably not refrigerator or the. In the following sections we will formalize\nthis intuition by introducing models that assign a probability to each possible next\nword. The same models will also serve to assign a probability to an entire sentence.\nSuch a model, for example, could predict that the following sequence has a much\nhigher probability of appearing in a text:\nall of a sudden I notice three guys standing on the sidewalk\nthan does this same set of words in a different order:\non guys all I of notice sidewalk three a sudden standing the\nWhy would you want to predict upcoming words, or assign probabilities to sen-\ntences? Probabilities are essential in any task in which we have to identify words\nin noisy, ambiguous input, like speech recognition or handwriting recognition. In\nthe movie Take the Money and Run, Woody Allen tries to rob a bank with a sloppily\nwritten hold-up note that the teller incorrectly reads as “I have a gub”. As Rus-\nsell and Norvig (2002) point out, a language processing system could avoid making\nthis mistake by using the knowledge that the sequence “I have a gun” is far more\nprobable than the non-word “I have a gub” or even “I have a gull”.\nIn spelling correction, we need to ﬁnd and correct spelling errors like Their\nare two midterms in this class, in which There was mistyped as Their. A sentence\nstarting with the phrase There are will be much more probable than one starting with\nTheir are, allowing a spellchecker to both detect and correct these errors.\nAssigning probabilities to sequences of words is also essential in machine trans-\nlation. Suppose we are translating a Chinese source sentence:\n他向记者\n介绍了\n主要内容\nHe to\nreporters introduced main content",
  "44": "38\nCHAPTER 3\n•\nN-GRAM LANGUAGE MODELS\nAs part of the process we might have built the following set of potential rough\nEnglish translations:\nhe introduced reporters to the main contents of the statement\nhe briefed to reporters the main contents of the statement\nhe briefed reporters on the main contents of the statement\nA probabilistic model of word sequences could suggest that briefed reporters on\nis a more probable English phrase than briefed to reporters (which has an awkward\nto after briefed) or introduced reporters to (which uses a verb that is less ﬂuent\nEnglish in this context), allowing us to correctly select the boldfaced sentence above.\nProbabilities are also important for augmentative communication (Newell et al.,\n1998) systems. People like the late physicist Stephen Hawking who are unable to\nphysically talk or sign can instead use simple movements to select words from a\nmenu to be spoken by the system. Word prediction can be used to suggest likely\nwords for the menu.\nModels that assign probabilities to sequences of words are called language mod-\nels or LMs. In this chapter we introduce the simplest model that assigns probabilities\nlanguage model\nLM\nto sentences and sequences of words, the n-gram. An n-gram is a sequence of N\nn-gram\nwords: a 2-gram (or bigram) is a two-word sequence of words like “please turn”,\n“turn your”, or ”your homework”, and a 3-gram (or trigram) is a three-word se-\nquence of words like “please turn your”, or “turn your homework”. We’ll see how\nto use n-gram models to estimate the probability of the last word of an n-gram given\nthe previous words, and also to assign probabilities to entire sequences. In a bit of\nterminological ambiguity, we usually drop the word “model”, and thus the term n-\ngram is used to mean either the word sequence itself or the predictive model that\nassigns it a probability.\n3.1\nN-Grams\nLet’s begin with the task of computing P(w|h), the probability of a word w given\nsome history h. Suppose the history h is “its water is so transparent that” and we\nwant to know the probability that the next word is the:\nP(the|its water is so transparent that).\n(3.1)\nOne way to estimate this probability is from relative frequency counts: take a\nvery large corpus, count the number of times we see its water is so transparent that,\nand count the number of times this is followed by the. This would be answering the\nquestion “Out of the times we saw the history h, how many times was it followed by\nthe word w”, as follows:\nP(the|its water is so transparent that) =\nC(its water is so transparent that the)\nC(its water is so transparent that)\n(3.2)\nWith a large enough corpus, such as the web, we can compute these counts and\nestimate the probability from Eq. 3.2. You should pause now, go to the web, and\ncompute this estimate for yourself.\nWhile this method of estimating probabilities directly from counts works ﬁne in\nmany cases, it turns out that even the web isn’t big enough to give us good estimates",
  "45": "3.1\n•\nN-GRAMS\n39\nin most cases. This is because language is creative; new sentences are created all the\ntime, and we won’t always be able to count entire sentences. Even simple extensions\nof the example sentence may have counts of zero on the web (such as “Walden\nPond’s water is so transparent that the”).\nSimilarly, if we wanted to know the joint probability of an entire sequence of\nwords like its water is so transparent, we could do it by asking “out of all possible\nsequences of ﬁve words, how many of them are its water is so transparent?” We\nwould have to get the count of its water is so transparent and divide by the sum of\nthe counts of all possible ﬁve word sequences. That seems rather a lot to estimate!\nFor this reason, we’ll need to introduce cleverer ways of estimating the proba-\nbility of a word w given a history h, or the probability of an entire word sequence W.\nLet’s start with a little formalizing of notation. To represent the probability of a par-\nticular random variable Xi taking on the value “the”, or P(Xi = “the”), we will use\nthe simpliﬁcation P(the). We’ll represent a sequence of N words either as w1 ...wn\nor wn\n1 (so the expression wn−1\n1\nmeans the string w1,w2,...,wn−1). For the joint prob-\nability of each word in a sequence having a particular value P(X = w1,Y = w2,Z =\nw3,...,W = wn) we’ll use P(w1,w2,...,wn).\nNow how can we compute probabilities of entire sequences like P(w1,w2,...,wn)?\nOne thing we can do is decompose this probability using the chain rule of proba-\nbility:\nP(X1...Xn) = P(X1)P(X2|X1)P(X3|X2\n1 )...P(Xn|Xn−1\n1\n)\n=\nn\nY\nk=1\nP(Xk|Xk−1\n1\n)\n(3.3)\nApplying the chain rule to words, we get\nP(wn\n1) = P(w1)P(w2|w1)P(w3|w2\n1)...P(wn|wn−1\n1\n)\n=\nn\nY\nk=1\nP(wk|wk−1\n1\n)\n(3.4)\nThe chain rule shows the link between computing the joint probability of a se-\nquence and computing the conditional probability of a word given previous words.\nEquation 3.4 suggests that we could estimate the joint probability of an entire se-\nquence of words by multiplying together a number of conditional probabilities. But\nusing the chain rule doesn’t really seem to help us! We don’t know any way to\ncompute the exact probability of a word given a long sequence of preceding words,\nP(wn|wn−1\n1\n). As we said above, we can’t just estimate by counting the number of\ntimes every word occurs following every long string, because language is creative\nand any particular context might have never occurred before!\nThe intuition of the n-gram model is that instead of computing the probability of\na word given its entire history, we can approximate the history by just the last few\nwords.\nThe bigram model, for example, approximates the probability of a word given\nbigram\nall the previous words P(wn|wn−1\n1\n) by using only the conditional probability of the\npreceding word P(wn|wn−1). In other words, instead of computing the probability\nP(the|Walden Pond’s water is so transparent that)\n(3.5)",
  "46": "40\nCHAPTER 3\n•\nN-GRAM LANGUAGE MODELS\nwe approximate it with the probability\nP(the|that)\n(3.6)\nWhen we use a bigram model to predict the conditional probability of the next\nword, we are thus making the following approximation:\nP(wn|wn−1\n1\n) ≈P(wn|wn−1)\n(3.7)\nThe assumption that the probability of a word depends only on the previous word\nis called a Markov assumption. Markov models are the class of probabilistic models\nMarkov\nthat assume we can predict the probability of some future unit without looking too\nfar into the past. We can generalize the bigram (which looks one word into the past)\nto the trigram (which looks two words into the past) and thus to the n-gram (which\nn-gram\nlooks n−1 words into the past).\nThus, the general equation for this n-gram approximation to the conditional\nprobability of the next word in a sequence is\nP(wn|wn−1\n1\n) ≈P(wn|wn−1\nn−N+1)\n(3.8)\nGiven the bigram assumption for the probability of an individual word, we can\ncompute the probability of a complete word sequence by substituting Eq. 3.7 into\nEq. 3.4:\nP(wn\n1) ≈\nn\nY\nk=1\nP(wk|wk−1)\n(3.9)\nHow do we estimate these bigram or n-gram probabilities? An intuitive way to\nestimate probabilities is called maximum likelihood estimation or MLE. We get\nmaximum\nlikelihood\nestimation\nthe MLE estimate for the parameters of an n-gram model by getting counts from a\ncorpus, and normalizing the counts so that they lie between 0 and 1.1\nnormalize\nFor example, to compute a particular bigram probability of a word y given a\nprevious word x, we’ll compute the count of the bigram C(xy) and normalize by the\nsum of all the bigrams that share the same ﬁrst word x:\nP(wn|wn−1) =\nC(wn−1wn)\nP\nwC(wn−1w)\n(3.10)\nWe can simplify this equation, since the sum of all bigram counts that start with\na given word wn−1 must be equal to the unigram count for that word wn−1 (the reader\nshould take a moment to be convinced of this):\nP(wn|wn−1) = C(wn−1wn)\nC(wn−1)\n(3.11)\nLet’s work through an example using a mini-corpus of three sentences. We’ll\nﬁrst need to augment each sentence with a special symbol <s> at the beginning\nof the sentence, to give us the bigram context of the ﬁrst word. We’ll also need a\nspecial end-symbol. </s>2\n1\nFor probabilistic models, normalizing means dividing by some total count so that the resulting prob-\nabilities fall legally between 0 and 1.\n2\nWe need the end-symbol to make the bigram grammar a true probability distribution. Without an\nend-symbol, the sentence probabilities for all sentences of a given length would sum to one. This model\nwould deﬁne an inﬁnite set of probability distributions, with one distribution per sentence length. See\nExercise 3.5.",
  "47": "3.1\n•\nN-GRAMS\n41\n<s> I am Sam </s>\n<s> Sam I am </s>\n<s> I do not like green eggs and ham </s>\nHere are the calculations for some of the bigram probabilities from this corpus\nP(I|<s>) = 2\n3 = .67\nP(Sam|<s>) = 1\n3 = .33\nP(am|I) = 2\n3 = .67\nP(</s>|Sam) = 1\n2 = 0.5\nP(Sam|am) = 1\n2 = .5\nP(do|I) = 1\n3 = .33\nFor the general case of MLE n-gram parameter estimation:\nP(wn|wn−1\nn−N+1) = C(wn−1\nn−N+1wn)\nC(wn−1\nn−N+1)\n(3.12)\nEquation 3.12 (like Eq. 3.11) estimates the n-gram probability by dividing the\nobserved frequency of a particular sequence by the observed frequency of a preﬁx.\nThis ratio is called a relative frequency. We said above that this use of relative\nrelative\nfrequency\nfrequencies as a way to estimate probabilities is an example of maximum likelihood\nestimation or MLE. In MLE, the resulting parameter set maximizes the likelihood\nof the training set T given the model M (i.e., P(T|M)). For example, suppose the\nword Chinese occurs 400 times in a corpus of a million words like the Brown corpus.\nWhat is the probability that a random word selected from some other text of, say,\na million words will be the word Chinese? The MLE of its probability is\n400\n1000000\nor .0004. Now .0004 is not the best possible estimate of the probability of Chinese\noccurring in all situations; it might turn out that in some other corpus or context\nChinese is a very unlikely word. But it is the probability that makes it most likely\nthat Chinese will occur 400 times in a million-word corpus. We present ways to\nmodify the MLE estimates slightly to get better probability estimates in Section 3.4.\nLet’s move on to some examples from a slightly larger corpus than our 14-word\nexample above. We’ll use data from the now-defunct Berkeley Restaurant Project,\na dialogue system from the last century that answered questions about a database\nof restaurants in Berkeley, California (Jurafsky et al., 1994). Here are some text-\nnormalized sample user queries (a sample of 9332 sentences is on the website):\ncan you tell me about any good cantonese restaurants close by\nmid priced thai food is what i’m looking for\ntell me about chez panisse\ncan you give me a listing of the kinds of food that are available\ni’m looking for a good place to eat breakfast\nwhen is caffe venezia open during the day\nFigure 3.1 shows the bigram counts from a piece of a bigram grammar from the\nBerkeley Restaurant Project. Note that the majority of the values are zero. In fact,\nwe have chosen the sample words to cohere with each other; a matrix selected from\na random set of seven words would be even more sparse.\nFigure 3.2 shows the bigram probabilities after normalization (dividing each cell\nin Fig. 3.1 by the appropriate unigram for its row, taken from the following set of\nunigram probabilities):\ni\nwant to\neat\nchinese food lunch spend\n2533 927\n2417 746 158\n1093 341\n278\nHere are a few other useful probabilities:",
  "48": "42\nCHAPTER 3\n•\nN-GRAM LANGUAGE MODELS\ni\nwant\nto\neat\nchinese\nfood\nlunch\nspend\ni\n5\n827\n0\n9\n0\n0\n0\n2\nwant\n2\n0\n608\n1\n6\n6\n5\n1\nto\n2\n0\n4\n686\n2\n0\n6\n211\neat\n0\n0\n2\n0\n16\n2\n42\n0\nchinese\n1\n0\n0\n0\n0\n82\n1\n0\nfood\n15\n0\n15\n0\n1\n4\n0\n0\nlunch\n2\n0\n0\n0\n0\n1\n0\n0\nspend\n1\n0\n1\n0\n0\n0\n0\n0\nFigure 3.1\nBigram counts for eight of the words (out of V = 1446) in the Berkeley Restau-\nrant Project corpus of 9332 sentences. Zero counts are in gray.\ni\nwant\nto\neat\nchinese\nfood\nlunch\nspend\ni\n0.002\n0.33\n0\n0.0036\n0\n0\n0\n0.00079\nwant\n0.0022\n0\n0.66\n0.0011\n0.0065\n0.0065\n0.0054\n0.0011\nto\n0.00083\n0\n0.0017\n0.28\n0.00083\n0\n0.0025\n0.087\neat\n0\n0\n0.0027\n0\n0.021\n0.0027\n0.056\n0\nchinese\n0.0063\n0\n0\n0\n0\n0.52\n0.0063\n0\nfood\n0.014\n0\n0.014\n0\n0.00092\n0.0037\n0\n0\nlunch\n0.0059\n0\n0\n0\n0\n0.0029\n0\n0\nspend\n0.0036\n0\n0.0036\n0\n0\n0\n0\n0\nFigure 3.2\nBigram probabilities for eight words in the Berkeley Restaurant Project corpus\nof 9332 sentences. Zero probabilities are in gray.\nP(i|<s>) = 0.25\nP(english|want) = 0.0011\nP(food|english) = 0.5\nP(</s>|food) = 0.68\nNow we can compute the probability of sentences like I want English food or\nI want Chinese food by simply multiplying the appropriate bigram probabilities to-\ngether, as follows:\nP(<s> i want english food </s>)\n= P(i|<s>)P(want|i)P(english|want)\nP(food|english)P(</s>|food)\n= .25×.33×.0011×0.5×0.68\n= .000031\nWe leave it as Exercise 3.2 to compute the probability of i want chinese food.\nWhat kinds of linguistic phenomena are captured in these bigram statistics?\nSome of the bigram probabilities above encode some facts that we think of as strictly\nsyntactic in nature, like the fact that what comes after eat is usually a noun or an\nadjective, or that what comes after to is usually a verb. Others might be a fact about\nthe personal assistant task, like the high probability of sentences beginning with\nthe words I. And some might even be cultural rather than linguistic, like the higher\nprobability that people are looking for Chinese versus English food.\nSome practical issues:\nAlthough for pedagogical purposes we have only described\nbigram models, in practice it’s more common to use trigram models, which con-\ntrigram\ndition on the previous two words rather than the previous word, or 4-gram or even\n4-gram\n5-gram models, when there is sufﬁcient training data. Note that for these larger n-\n5-gram\ngrams, we’ll need to assume extra context for the contexts to the left and right of the",
  "49": "3.2\n•\nEVALUATING LANGUAGE MODELS\n43\nsentence end. For example, to compute trigram probabilities at the very beginning of\nthe sentence, we can use two pseudo-words for the ﬁrst trigram (i.e., P(I|<s><s>).\nWe always represent and compute language model probabilities in log format\nas log probabilities. Since probabilities are (by deﬁnition) less than or equal to\nlog\nprobabilities\n1, the more probabilities we multiply together, the smaller the product becomes.\nMultiplying enough n-grams together would result in numerical underﬂow. By using\nlog probabilities instead of raw probabilities, we get numbers that are not as small.\nAdding in log space is equivalent to multiplying in linear space, so we combine log\nprobabilities by adding them. The result of doing all computation and storage in log\nspace is that we only need to convert back into probabilities if we need to report\nthem at the end; then we can just take the exp of the logprob:\np1 × p2 × p3 × p4 = exp(log p1 +log p2 +log p3 +log p4)\n(3.13)\n3.2\nEvaluating Language Models\nThe best way to evaluate the performance of a language model is to embed it in\nan application and measure how much the application improves. Such end-to-end\nevaluation is called extrinsic evaluation. Extrinsic evaluation is the only way to\nextrinsic\nevaluation\nknow if a particular improvement in a component is really going to help the task\nat hand. Thus, for speech recognition, we can compare the performance of two\nlanguage models by running the speech recognizer twice, once with each language\nmodel, and seeing which gives the more accurate transcription.\nUnfortunately, running big NLP systems end-to-end is often very expensive. In-\nstead, it would be nice to have a metric that can be used to quickly evaluate potential\nimprovements in a language model. An intrinsic evaluation metric is one that mea-\nintrinsic\nevaluation\nsures the quality of a model independent of any application.\nFor an intrinsic evaluation of a language model we need a test set. As with many\nof the statistical models in our ﬁeld, the probabilities of an n-gram model come from\nthe corpus it is trained on, the training set or training corpus. We can then measure\ntraining set\nthe quality of an n-gram model by its performance on some unseen data called the\ntest set or test corpus. We will also sometimes call test sets and other datasets that\ntest set\nare not in our training sets held out corpora because we hold them out from the\nheld out\ntraining data.\nSo if we are given a corpus of text and want to compare two different n-gram\nmodels, we divide the data into training and test sets, train the parameters of both\nmodels on the training set, and then compare how well the two trained models ﬁt the\ntest set.\nBut what does it mean to “ﬁt the test set”? The answer is simple: whichever\nmodel assigns a higher probability to the test set—meaning it more accurately\npredicts the test set—is a better model. Given two probabilistic models, the better\nmodel is the one that has a tighter ﬁt to the test data or that better predicts the details\nof the test data, and hence will assign a higher probability to the test data.\nSince our evaluation metric is based on test set probability, it’s important not to\nlet the test sentences into the training set. Suppose we are trying to compute the\nprobability of a particular “test” sentence. If our test sentence is part of the training\ncorpus, we will mistakenly assign it an artiﬁcially high probability when it occurs\nin the test set. We call this situation training on the test set. Training on the test\nset introduces a bias that makes the probabilities all look too high, and causes huge",
  "50": "44\nCHAPTER 3\n•\nN-GRAM LANGUAGE MODELS\ninaccuracies in perplexity, the probability-based metric we introduce below.\nSometimes we use a particular test set so often that we implicitly tune to its\ncharacteristics. We then need a fresh test set that is truly unseen. In such cases, we\ncall the initial test set the development test set or, devset. How do we divide our\ndevelopment\ntest\ndata into training, development, and test sets? We want our test set to be as large\nas possible, since a small test set may be accidentally unrepresentative, but we also\nwant as much training data as possible. At the minimum, we would want to pick\nthe smallest test set that gives us enough statistical power to measure a statistically\nsigniﬁcant difference between two potential models. In practice, we often just divide\nour data into 80% training, 10% development, and 10% test. Given a large corpus\nthat we want to divide into training and test, test data can either be taken from some\ncontinuous sequence of text inside the corpus, or we can remove smaller “stripes”\nof text from randomly selected parts of our corpus and combine them into a test set.\n3.2.1\nPerplexity\nIn practice we don’t use raw probability as our metric for evaluating language mod-\nels, but a variant called perplexity. The perplexity (sometimes called PP for short)\nperplexity\nof a language model on a test set is the inverse probability of the test set, normalized\nby the number of words. For a test set W = w1w2 ...wN,:\nPP(W) = P(w1w2 ...wN)−1\nN\n(3.14)\n=\nN\ns\n1\nP(w1w2 ...wN)\nWe can use the chain rule to expand the probability of W:\nPP(W) =\nN\nv\nu\nu\nt\nN\nY\ni=1\n1\nP(wi|w1 ...wi−1)\n(3.15)\nThus, if we are computing the perplexity of W with a bigram language model,\nwe get:\nPP(W) =\nN\nv\nu\nu\nt\nN\nY\ni=1\n1\nP(wi|wi−1)\n(3.16)\nNote that because of the inverse in Eq. 3.15, the higher the conditional probabil-\nity of the word sequence, the lower the perplexity. Thus, minimizing perplexity is\nequivalent to maximizing the test set probability according to the language model.\nWhat we generally use for word sequence in Eq. 3.15 or Eq. 3.16 is the entire se-\nquence of words in some test set. Since this sequence will cross many sentence\nboundaries, we need to include the begin- and end-sentence markers <s> and </s>\nin the probability computation. We also need to include the end-of-sentence marker\n</s> (but not the beginning-of-sentence marker <s>) in the total count of word to-\nkens N.\nThere is another way to think about perplexity: as the weighted average branch-\ning factor of a language. The branching factor of a language is the number of possi-\nble next words that can follow any word. Consider the task of recognizing the digits",
  "51": "3.3\n•\nGENERALIZATION AND ZEROS\n45\nin English (zero, one, two,..., nine), given that each of the 10 digits occurs with equal\nprobability P = 1\n10. The perplexity of this mini-language is in fact 10. To see that,\nimagine a string of digits of length N. By Eq. 3.15, the perplexity will be\nPP(W) = P(w1w2 ...wN)−1\nN\n= ( 1\n10\nN\n)−1\nN\n=\n1\n10\n−1\n= 10\n(3.17)\nBut suppose that the number zero is really frequent and occurs 10 times more\noften than other numbers. Now we should expect the perplexity to be lower since\nmost of the time the next number will be zero. Thus, although the branching factor\nis still 10, the perplexity or weighted branching factor is smaller. We leave this\ncalculation as an exercise to the reader.\nWe see in Section 3.7 that perplexity is also closely related to the information-\ntheoretic notion of entropy.\nFinally, let’s look at an example of how perplexity can be used to compare dif-\nferent n-gram models. We trained unigram, bigram, and trigram grammars on 38\nmillion words (including start-of-sentence tokens) from the Wall Street Journal, us-\ning a 19,979 word vocabulary. We then computed the perplexity of each of these\nmodels on a test set of 1.5 million words with Eq. 3.16. The table below shows the\nperplexity of a 1.5 million word WSJ test set according to each of these grammars.\nUnigram Bigram Trigram\nPerplexity 962\n170\n109\nAs we see above, the more information the n-gram gives us about the word\nsequence, the lower the perplexity (since as Eq. 3.15 showed, perplexity is related\ninversely to the likelihood of the test sequence according to the model).\nNote that in computing perplexities, the n-gram model P must be constructed\nwithout any knowledge of the test set or any prior knowledge of the vocabulary of\nthe test set. Any kind of knowledge of the test set can cause the perplexity to be\nartiﬁcially low. The perplexity of two language models is only comparable if they\nuse identical vocabularies.\nAn (intrinsic) improvement in perplexity does not guarantee an (extrinsic) im-\nprovement in the performance of a language processing task like speech recognition\nor machine translation. Nonetheless, because perplexity often correlates with such\nimprovements, it is commonly used as a quick check on an algorithm. But a model’s\nimprovement in perplexity should always be conﬁrmed by an end-to-end evaluation\nof a real task before concluding the evaluation of the model.\n3.3\nGeneralization and Zeros\nThe n-gram model, like many statistical models, is dependent on the training corpus.\nOne implication of this is that the probabilities often encode speciﬁc facts about a\ngiven training corpus. Another implication is that n-grams do a better and better job\nof modeling the training corpus as we increase the value of N.",
  "52": "46\nCHAPTER 3\n•\nN-GRAM LANGUAGE MODELS\nWe can visualize both of these facts by borrowing the technique of Shannon\n(1951) and Miller and Selfridge (1950) of generating random sentences from dif-\nferent n-gram models. It’s simplest to visualize how this works for the unigram\ncase. Imagine all the words of the English language covering the probability space\nbetween 0 and 1, each word covering an interval proportional to its frequency. We\nchoose a random value between 0 and 1 and print the word whose interval includes\nthis chosen value. We continue choosing random numbers and generating words\nuntil we randomly generate the sentence-ﬁnal token </s>. We can use the same\ntechnique to generate bigrams by ﬁrst generating a random bigram that starts with\n<s> (according to its bigram probability). Let’s say the second word of that bigram\nis w. We next chose a random bigram starting with w (again, drawn according to its\nbigram probability), and so on.\nTo give an intuition for the increasing power of higher-order n-grams, Fig. 3.3\nshows random sentences generated from unigram, bigram, trigram, and 4-gram\nmodels trained on Shakespeare’s works.\n1\n–To him swallowed confess hear both. Which. Of save on trail for are ay device and\nrote life have\ngram\n–Hill he late speaks; or! a more to leg less ﬁrst you enter\n2\n–Why dost stand forth thy canopy, forsooth; he is this palpable hit the King Henry. Live\nking. Follow.\ngram\n–What means, sir. I confess she? then all sorts, he is trim, captain.\n3\n–Fly, and will rid me these news of price. Therefore the sadness of parting, as they say,\n’tis done.\ngram\n–This shall forbid it should be branded, if renown made it empty.\n4\n–King Henry. What! I will go seek the traitor Gloucester. Exeunt some of the watch. A\ngreat banquet serv’d in;\ngram\n–It cannot be but so.\nFigure 3.3\nEight sentences randomly generated from four n-grams computed from Shakespeare’s works. All\ncharacters were mapped to lower-case and punctuation marks were treated as words. Output is hand-corrected\nfor capitalization to improve readability.\nThe longer the context on which we train the model, the more coherent the sen-\ntences. In the unigram sentences, there is no coherent relation between words or any\nsentence-ﬁnal punctuation. The bigram sentences have some local word-to-word\ncoherence (especially if we consider that punctuation counts as a word). The tri-\ngram and 4-gram sentences are beginning to look a lot like Shakespeare. Indeed, a\ncareful investigation of the 4-gram sentences shows that they look a little too much\nlike Shakespeare. The words It cannot be but so are directly from King John. This is\nbecause, not to put the knock on Shakespeare, his oeuvre is not very large as corpora\ngo (N = 884,647,V = 29,066), and our n-gram probability matrices are ridiculously\nsparse. There are V 2 = 844,000,000 possible bigrams alone, and the number of pos-\nsible 4-grams is V 4 = 7×1017. Thus, once the generator has chosen the ﬁrst 4-gram\n(It cannot be but), there are only ﬁve possible continuations (that, I, he, thou, and\nso); indeed, for many 4-grams, there is only one continuation.\nTo get an idea of the dependence of a grammar on its training set, let’s look at an\nn-gram grammar trained on a completely different corpus: the Wall Street Journal\n(WSJ) newspaper. Shakespeare and the Wall Street Journal are both English, so\nwe might expect some overlap between our n-grams for the two genres. Fig. 3.4",
  "53": "3.3\n•\nGENERALIZATION AND ZEROS\n47\nshows sentences generated by unigram, bigram, and trigram grammars trained on\n40 million words from WSJ.\n1\nMonths the my and issue of year foreign new exchange’s september\nwere recession exchange new endorsed a acquire to six executives\ngram\n2\nLast December through the way to preserve the Hudson corporation N.\nB. E. C. Taylor would seem to complete the major central planners one\ngram\npoint ﬁve percent of U. S. E. has already old M. X. corporation of living\non information such as more frequently ﬁshing to keep her\n3\nThey also point to ninety nine point six billion dollars from two hundred\nfour oh six three percent of the rates of interest stores as Mexico and\ngram\nBrazil on market conditions\nFigure 3.4\nThree sentences randomly generated from three n-gram models computed from\n40 million words of the Wall Street Journal, lower-casing all characters and treating punctua-\ntion as words. Output was then hand-corrected for capitalization to improve readability.\nCompare these examples to the pseudo-Shakespeare in Fig. 3.3. While they both\nmodel “English-like sentences”, there is clearly no overlap in generated sentences,\nand little overlap even in small phrases. Statistical models are likely to be pretty use-\nless as predictors if the training sets and the test sets are as different as Shakespeare\nand WSJ.\nHow should we deal with this problem when we build n-gram models? One step\nis to be sure to use a training corpus that has a similar genre to whatever task we are\ntrying to accomplish. To build a language model for translating legal documents,\nwe need a training corpus of legal documents. To build a language model for a\nquestion-answering system, we need a training corpus of questions.\nIt is equally important to get training data in the appropriate dialect, especially\nwhen processing social media posts or spoken transcripts. Thus tweets in AAVE\n(African American Vernacular English) often use words like ﬁnna—an auxiliary\nverb that markes immediate future tense —that don’t occur in other dialects, or\nspellings like den for then, in tweets like this one (Blodgett and O’Connor, 2017):\n(3.18) Bored af den my phone ﬁnna die!!!\nwhile tweets from varieties like Nigerian English have markedly different vocabu-\nlary and n-gram patterns from American English (Jurgens et al., 2017):\n(3.19) @username R u a wizard or wat gan sef: in d mornin - u tweet, afternoon - u\ntweet, nyt gan u dey tweet. beta get ur IT placement wiv twitter\nMatching genres and dialects is still not sufﬁcient. Our models may still be\nsubject to the problem of sparsity. For any n-gram that occurred a sufﬁcient number\nof times, we might have a good estimate of its probability. But because any corpus is\nlimited, some perfectly acceptable English word sequences are bound to be missing\nfrom it. That is, we’ll have many cases of putative “zero probability n-grams” that\nshould really have some non-zero probability. Consider the words that follow the\nbigram denied the in the WSJ Treebank3 corpus, together with their counts:\ndenied the allegations:\n5\ndenied the speculation: 2\ndenied the rumors:\n1\ndenied the report:\n1\nBut suppose our test set has phrases like:",
  "54": "48\nCHAPTER 3\n•\nN-GRAM LANGUAGE MODELS\ndenied the offer\ndenied the loan\nOur model will incorrectly estimate that the P(offer|denied the) is 0!\nThese zeros— things that don’t ever occur in the training set but do occur in\nzeros\nthe test set—are a problem for two reasons. First, their presence means we are\nunderestimating the probability of all sorts of words that might occur, which will\nhurt the performance of any application we want to run on this data.\nSecond, if the probability of any word in the test set is 0, the entire probability\nof the test set is 0. By deﬁnition, perplexity is based on the inverse probability of the\ntest set. Thus if some words have zero probability, we can’t compute perplexity at\nall, since we can’t divide by 0!\n3.3.1\nUnknown Words\nThe previous section discussed the problem of words whose bigram probability is\nzero. But what about words we simply have never seen before?\nSometimes we have a language task in which this can’t happen because we know\nall the words that can occur. In such a closed vocabulary system the test set can\nclosed\nvocabulary\nonly contain words from this lexicon, and there will be no unknown words. This is\na reasonable assumption in some domains, such as speech recognition or machine\ntranslation, where we have a pronunciation dictionary or a phrase table that are ﬁxed\nin advance, and so the language model can only use the words in that dictionary or\nphrase table.\nIn other cases we have to deal with words we haven’t seen before, which we’ll\ncall unknown words, or out of vocabulary (OOV) words. The percentage of OOV\nOOV\nwords that appear in the test set is called the OOV rate. An open vocabulary system\nopen\nvocabulary\nis one in which we model these potential unknown words in the test set by adding a\npseudo-word called <UNK>.\nThere are two common ways to train the probabilities of the unknown word\nmodel <UNK>. The ﬁrst one is to turn the problem back into a closed vocabulary one\nby choosing a ﬁxed vocabulary in advance:\n1. Choose a vocabulary (word list) that is ﬁxed in advance.\n2. Convert in the training set any word that is not in this set (any OOV word) to\nthe unknown word token <UNK> in a text normalization step.\n3. Estimate the probabilities for <UNK> from its counts just like any other regular\nword in the training set.\nThe second alternative, in situations where we don’t have a prior vocabulary in ad-\nvance, is to create such a vocabulary implicitly, replacing words in the training data\nby <UNK> based on their frequency. For example we can replace by <UNK> all words\nthat occur fewer than n times in the training set, where n is some small number, or\nequivalently select a vocabulary size V in advance (say 50,000) and choose the top\nV words by frequency and replace the rest by UNK. In either case we then proceed\nto train the language model as before, treating <UNK> like a regular word.\nThe exact choice of <UNK> model does have an effect on metrics like perplexity.\nA language model can achieve low perplexity by choosing a small vocabulary and\nassigning the unknown word a high probability. For this reason, perplexities should\nonly be compared across language models with the same vocabularies (Buck et al.,\n2014).",
  "55": "3.4\n•\nSMOOTHING\n49\n3.4\nSmoothing\nWhat do we do with words that are in our vocabulary (they are not unknown words)\nbut appear in a test set in an unseen context (for example they appear after a word\nthey never appeared after in training)? To keep a language model from assigning\nzero probability to these unseen events, we’ll have to shave off a bit of probability\nmass from some more frequent events and give it to the events we’ve never seen.\nThis modiﬁcation is called smoothing or discounting. In this section and the fol-\nsmoothing\ndiscounting\nlowing ones we’ll introduce a variety of ways to do smoothing: add-1 smoothing,\nadd-k smoothing, stupid backoff, and Kneser-Ney smoothing.\n3.4.1\nLaplace Smoothing\nThe simplest way to do smoothing is to add one to all the bigram counts, before\nwe normalize them into probabilities. All the counts that used to be zero will now\nhave a count of 1, the counts of 1 will be 2, and so on. This algorithm is called\nLaplace smoothing. Laplace smoothing does not perform well enough to be used\nLaplace\nsmoothing\nin modern n-gram models, but it usefully introduces many of the concepts that we\nsee in other smoothing algorithms, gives a useful baseline, and is also a practical\nsmoothing algorithm for other tasks like text classiﬁcation (Chapter 4).\nLet’s start with the application of Laplace smoothing to unigram probabilities.\nRecall that the unsmoothed maximum likelihood estimate of the unigram probability\nof the word wi is its count ci normalized by the total number of word tokens N:\nP(wi) = ci\nN\nLaplace smoothing merely adds one to each count (hence its alternate name add-\none smoothing). Since there are V words in the vocabulary and each one was incre-\nadd-one\nmented, we also need to adjust the denominator to take into account the extra V\nobservations. (What happens to our P values if we don’t increase the denominator?)\nPLaplace(wi) = ci +1\nN +V\n(3.20)\nInstead of changing both the numerator and denominator, it is convenient to\ndescribe how a smoothing algorithm affects the numerator, by deﬁning an adjusted\ncount c∗. This adjusted count is easier to compare directly with the MLE counts and\ncan be turned into a probability like an MLE count by normalizing by N. To deﬁne\nthis count, since we are only changing the numerator in addition to adding 1 we’ll\nalso need to multiply by a normalization factor\nN\nN+V :\nc∗\ni = (ci +1)\nN\nN +V\n(3.21)\nWe can now turn c∗\ni into a probability P∗\ni by normalizing by N.\nA related way to view smoothing is as discounting (lowering) some non-zero\ndiscounting\ncounts in order to get the probability mass that will be assigned to the zero counts.\nThus, instead of referring to the discounted counts c∗, we might describe a smooth-\ning algorithm in terms of a relative discount dc, the ratio of the discounted counts to\ndiscount\nthe original counts:",
  "56": "50\nCHAPTER 3\n•\nN-GRAM LANGUAGE MODELS\ndc = c∗\nc\nNow that we have the intuition for the unigram case, let’s smooth our Berkeley\nRestaurant Project bigrams. Figure 3.5 shows the add-one smoothed counts for the\nbigrams in Fig. 3.1.\ni\nwant\nto\neat\nchinese\nfood\nlunch\nspend\ni\n6\n828\n1\n10\n1\n1\n1\n3\nwant\n3\n1\n609\n2\n7\n7\n6\n2\nto\n3\n1\n5\n687\n3\n1\n7\n212\neat\n1\n1\n3\n1\n17\n3\n43\n1\nchinese\n2\n1\n1\n1\n1\n83\n2\n1\nfood\n16\n1\n16\n1\n2\n5\n1\n1\nlunch\n3\n1\n1\n1\n1\n2\n1\n1\nspend\n2\n1\n2\n1\n1\n1\n1\n1\nFigure 3.5\nAdd-one smoothed bigram counts for eight of the words (out of V = 1446) in\nthe Berkeley Restaurant Project corpus of 9332 sentences. Previously-zero counts are in gray.\nFigure 3.6 shows the add-one smoothed probabilities for the bigrams in Fig. 3.2.\nRecall that normal bigram probabilities are computed by normalizing each row of\ncounts by the unigram count:\nP(wn|wn−1) = C(wn−1wn)\nC(wn−1)\n(3.22)\nFor add-one smoothed bigram counts, we need to augment the unigram count by\nthe number of total word types in the vocabulary V:\nP∗\nLaplace(wn|wn−1) =\nC(wn−1wn)+1\nP\nw (C(wn−1w)+1) = C(wn−1wn)+1\nC(wn−1)+V\n(3.23)\nThus, each of the unigram counts given in the previous section will need to be\naugmented by V = 1446. The result is the smoothed bigram probabilities in Fig. 3.6.\ni\nwant\nto\neat\nchinese\nfood\nlunch\nspend\ni\n0.0015\n0.21\n0.00025\n0.0025\n0.00025\n0.00025\n0.00025\n0.00075\nwant\n0.0013\n0.00042\n0.26\n0.00084\n0.0029\n0.0029\n0.0025\n0.00084\nto\n0.00078\n0.00026\n0.0013\n0.18\n0.00078\n0.00026\n0.0018\n0.055\neat\n0.00046\n0.00046\n0.0014\n0.00046\n0.0078\n0.0014\n0.02\n0.00046\nchinese\n0.0012\n0.00062\n0.00062\n0.00062\n0.00062\n0.052\n0.0012\n0.00062\nfood\n0.0063\n0.00039\n0.0063\n0.00039\n0.00079\n0.002\n0.00039\n0.00039\nlunch\n0.0017\n0.00056\n0.00056\n0.00056\n0.00056\n0.0011\n0.00056\n0.00056\nspend\n0.0012\n0.00058\n0.0012\n0.00058\n0.00058\n0.00058\n0.00058\n0.00058\nFigure 3.6\nAdd-one smoothed bigram probabilities for eight of the words (out of V = 1446) in the BeRP\ncorpus of 9332 sentences. Previously-zero probabilities are in gray.\nIt is often convenient to reconstruct the count matrix so we can see how much a\nsmoothing algorithm has changed the original counts. These adjusted counts can be\ncomputed by Eq. 3.24. Figure 3.7 shows the reconstructed counts.\nc∗(wn−1wn) = [C(wn−1wn)+1]×C(wn−1)\nC(wn−1)+V\n(3.24)",
  "57": "3.4\n•\nSMOOTHING\n51\ni\nwant\nto\neat\nchinese\nfood\nlunch\nspend\ni\n3.8\n527\n0.64\n6.4\n0.64\n0.64\n0.64\n1.9\nwant\n1.2\n0.39\n238\n0.78\n2.7\n2.7\n2.3\n0.78\nto\n1.9\n0.63\n3.1\n430\n1.9\n0.63\n4.4\n133\neat\n0.34\n0.34\n1\n0.34\n5.8\n1\n15\n0.34\nchinese\n0.2\n0.098\n0.098\n0.098\n0.098\n8.2\n0.2\n0.098\nfood\n6.9\n0.43\n6.9\n0.43\n0.86\n2.2\n0.43\n0.43\nlunch\n0.57\n0.19\n0.19\n0.19\n0.19\n0.38\n0.19\n0.19\nspend\n0.32\n0.16\n0.32\n0.16\n0.16\n0.16\n0.16\n0.16\nFigure 3.7\nAdd-one reconstituted counts for eight words (of V = 1446) in the BeRP corpus\nof 9332 sentences. Previously-zero counts are in gray.\nNote that add-one smoothing has made a very big change to the counts. C(want to)\nchanged from 608 to 238! We can see this in probability space as well: P(to|want)\ndecreases from .66 in the unsmoothed case to .26 in the smoothed case. Looking at\nthe discount d (the ratio between new and old counts) shows us how strikingly the\ncounts for each preﬁx word have been reduced; the discount for the bigram want to\nis .39, while the discount for Chinese food is .10, a factor of 10!\nThe sharp change in counts and probabilities occurs because too much probabil-\nity mass is moved to all the zeros.\n3.4.2\nAdd-k smoothing\nOne alternative to add-one smoothing is to move a bit less of the probability mass\nfrom the seen to the unseen events. Instead of adding 1 to each count, we add a frac-\ntional count k (.5? .05? .01?). This algorithm is therefore called add-k smoothing.\nadd-k\nP∗\nAdd-k(wn|wn−1) = C(wn−1wn)+k\nC(wn−1)+kV\n(3.25)\nAdd-k smoothing requires that we have a method for choosing k; this can be\ndone, for example, by optimizing on a devset. Although add-k is useful for some\ntasks (including text classiﬁcation), it turns out that it still doesn’t work well for\nlanguage modeling, generating counts with poor variances and often inappropriate\ndiscounts (Gale and Church, 1994).\n3.4.3\nBackoff and Interpolation\nThe discounting we have been discussing so far can help solve the problem of zero\nfrequency n-grams. But there is an additional source of knowledge we can draw on.\nIf we are trying to compute P(wn|wn−2wn−1) but we have no examples of a particular\ntrigram wn−2wn−1wn, we can instead estimate its probability by using the bigram\nprobability P(wn|wn−1). Similarly, if we don’t have counts to compute P(wn|wn−1),\nwe can look to the unigram P(wn).\nIn other words, sometimes using less context is a good thing, helping to general-\nize more for contexts that the model hasn’t learned much about. There are two ways\nto use this n-gram “hierarchy”. In backoff, we use the trigram if the evidence is\nbackoff\nsufﬁcient, otherwise we use the bigram, otherwise the unigram. In other words, we\nonly “back off” to a lower-order n-gram if we have zero evidence for a higher-order\nn-gram. By contrast, in interpolation, we always mix the probability estimates from\ninterpolation\nall the n-gram estimators, weighing and combining the trigram, bigram, and unigram\ncounts.",
  "58": "52\nCHAPTER 3\n•\nN-GRAM LANGUAGE MODELS\nIn simple linear interpolation, we combine different order n-grams by linearly in-\nterpolating all the models. Thus, we estimate the trigram probability P(wn|wn−2wn−1)\nby mixing together the unigram, bigram, and trigram probabilities, each weighted\nby a λ:\nˆP(wn|wn−2wn−1) = λ1P(wn|wn−2wn−1)\n+λ2P(wn|wn−1)\n+λ3P(wn)\n(3.26)\nsuch that the λs sum to 1:\nX\ni\nλi = 1\n(3.27)\nIn a slightly more sophisticated version of linear interpolation, each λ weight is\ncomputed by conditioning on the context. This way, if we have particularly accurate\ncounts for a particular bigram, we assume that the counts of the trigrams based on\nthis bigram will be more trustworthy, so we can make the λs for those trigrams\nhigher and thus give that trigram more weight in the interpolation. Equation 3.28\nshows the equation for interpolation with context-conditioned weights:\nˆP(wn|wn−2wn−1) = λ1(wn−1\nn−2)P(wn|wn−2wn−1)\n+λ2(wn−1\nn−2)P(wn|wn−1)\n+λ3(wn−1\nn−2)P(wn)\n(3.28)\nHow are these λ values set? Both the simple interpolation and conditional inter-\npolation λs are learned from a held-out corpus. A held-out corpus is an additional\nheld-out\ntraining corpus that we use to set hyperparameters like these λ values, by choosing\nthe λ values that maximize the likelihood of the held-out corpus. That is, we ﬁx\nthe n-gram probabilities and then search for the λ values that—when plugged into\nEq. 3.26—give us the highest probability of the held-out set. There are various ways\nto ﬁnd this optimal set of λs. One way is to use the EM algorithm, an iterative\nlearning algorithm that converges on locally optimal λs (Jelinek and Mercer, 1980).\nIn a backoff n-gram model, if the n-gram we need has zero counts, we approxi-\nmate it by backing off to the (N-1)-gram. We continue backing off until we reach a\nhistory that has some counts.\nIn order for a backoff model to give a correct probability distribution, we have\nto discount the higher-order n-grams to save some probability mass for the lower\ndiscount\norder n-grams. Just as with add-one smoothing, if the higher-order n-grams aren’t\ndiscounted and we just used the undiscounted MLE probability, then as soon as we\nreplaced an n-gram which has zero probability with a lower-order n-gram, we would\nbe adding probability mass, and the total probability assigned to all possible strings\nby the language model would be greater than 1! In addition to this explicit discount\nfactor, we’ll need a function α to distribute this probability mass to the lower order\nn-grams.\nThis kind of backoff with discounting is also called Katz backoff. In Katz back-\nKatz backoff\noff we rely on a discounted probability P∗if we’ve seen this n-gram before (i.e., if\nwe have non-zero counts). Otherwise, we recursively back off to the Katz probabil-\nity for the shorter-history (N-1)-gram. The probability for a backoff n-gram PBO is",
  "59": "3.5\n•\nKNESER-NEY SMOOTHING\n53\nthus computed as follows:\nPBO(wn|wn−1\nn−N+1) =\n\n\n\nP∗(wn|wn−1\nn−N+1),\nif C(wn\nn−N+1) > 0\nα(wn−1\nn−N+1)PBO(wn|wn−1\nn−N+2),\notherwise.\n(3.29)\nKatz backoff is often combined with a smoothing method called Good-Turing.\nGood-Turing\nThe combined Good-Turing backoff algorithm involves quite detailed computation\nfor estimating the Good-Turing smoothing and the P∗and α values.\n3.5\nKneser-Ney Smoothing\nOne of the most commonly used and best performing n-gram smoothing methods\nis the interpolated Kneser-Ney algorithm (Kneser and Ney 1995, Chen and Good-\nKneser-Ney\nman 1998).\nKneser-Ney has its roots in a method called absolute discounting. Recall that\ndiscounting of the counts for frequent n-grams is necessary to save some probability\nmass for the smoothing algorithm to distribute to the unseen n-grams.\nTo see this, we can use a clever idea from Church and Gale (1991). Consider\nan n-gram that has count 4. We need to discount this count by some amount. But\nhow much should we discount it? Church and Gale’s clever idea was to look at a\nheld-out corpus and just see what the count is for all those bigrams that had count\n4 in the training set. They computed a bigram grammar from 22 million words of\nAP newswire and then checked the counts of each of these bigrams in another 22\nmillion words. On average, a bigram that occurred 4 times in the ﬁrst 22 million\nwords occurred 3.23 times in the next 22 million words. The following table from\nChurch and Gale (1991) shows these counts for bigrams with c from 0 to 9:\nBigram count in Bigram count in\ntraining set heldout set\n0 0.0000270\n1 0.448\n2 1.25\n3 2.24\n4 3.23\n5 4.21\n6 5.23\n7 6.21\n8 7.21\n9 8.26\nFigure 3.8\nFor all bigrams in 22 million words of AP newswire of count 0, 1, 2,...,9, the\ncounts of these bigrams in a held-out corpus also of 22 million words.\nThe astute reader may have noticed that except for the held-out counts for 0\nand 1, all the other bigram counts in the held-out set could be estimated pretty well\nby just subtracting 0.75 from the count in the training set! Absolute discounting\nAbsolute\ndiscounting\nformalizes this intuition by subtracting a ﬁxed (absolute) discount d from each count.\nThe intuition is that since we have good estimates already for the very high counts, a\nsmall discount d won’t affect them much. It will mainly modify the smaller counts,",
  "60": "54\nCHAPTER 3\n•\nN-GRAM LANGUAGE MODELS\nfor which we don’t necessarily trust the estimate anyway, and Fig. 3.8 suggests that\nin practice this discount is actually a good one for bigrams with counts 2 through 9.\nThe equation for interpolated absolute discounting applied to bigrams:\nPAbsoluteDiscounting(wi|wi−1) = C(wi−1wi)−d\nP\nvC(wi−1 v) +λ(wi−1)P(wi)\n(3.30)\nThe ﬁrst term is the discounted bigram, and the second term is the unigram with\nan interpolation weight λ. We could just set all the d values to .75, or we could keep\na separate discount value of 0.5 for the bigrams with counts of 1.\nKneser-Ney discounting (Kneser and Ney, 1995) augments absolute discount-\ning with a more sophisticated way to handle the lower-order unigram distribution.\nConsider the job of predicting the next word in this sentence, assuming we are inter-\npolating a bigram and a unigram model.\nI can’t see without my reading\n.\nThe word glasses seems much more likely to follow here than, say, the word\nKong, so we’d like our unigram model to prefer glasses. But in fact it’s Kong that is\nmore common, since Hong Kong is a very frequent word. A standard unigram model\nwill assign Kong a higher probability than glasses. We would like to capture the\nintuition that although Kong is frequent, it is mainly only frequent in the phrase Hong\nKong, that is, after the word Hong. The word glasses has a much wider distribution.\nIn other words, instead of P(w), which answers the question “How likely is\nw?”, we’d like to create a unigram model that we might call PCONTINUATION, which\nanswers the question “How likely is w to appear as a novel continuation?”. How can\nwe estimate this probability of seeing the word w as a novel continuation, in a new\nunseen context? The Kneser-Ney intuition is to base our estimate of PCONTINUATION\non the number of different contexts word w has appeared in, that is, the number of\nbigram types it completes. Every bigram type was a novel continuation the ﬁrst time\nit was seen. We hypothesize that words that have appeared in more contexts in the\npast are more likely to appear in some new context as well. The number of times a\nword w appears as a novel continuation can be expressed as:\nPCONTINUATION(w) ∝|{v : C(vw) > 0}|\n(3.31)\nTo turn this count into a probability, we normalize by the total number of word\nbigram types. In summary:\nPCONTINUATION(w) =\n|{v : C(vw) > 0}|\n|{(u′,w′) : C(u′w′) > 0}|\n(3.32)\nAn alternative metaphor for an equivalent formulation is to use the number of\nword types seen to precede w (Eq. 3.31 repeated):\nPCONTINUATION(w) ∝|{v : C(vw) > 0}|\n(3.33)\nnormalized by the number of words preceding all words, as follows:\nPCONTINUATION(w) =\n|{v : C(vw) > 0}|\nP\nw′ |{v : C(vw′) > 0}|\n(3.34)\nA frequent word (Kong) occurring in only one context (Hong) will have a low\ncontinuation probability.",
  "61": "3.6\n•\nTHE WEB AND STUPID BACKOFF\n55\nThe ﬁnal equation for Interpolated Kneser-Ney smoothing for bigrams is then:\nInterpolated\nKneser-Ney\nPKN(wi|wi−1) = max(C(wi−1wi)−d,0)\nC(wi−1)\n+λ(wi−1)PCONTINUATION(wi)\n(3.35)\nThe λ is a normalizing constant that is used to distribute the probability mass\nwe’ve discounted.:\nλ(wi−1) =\nd\nP\nvC(wi−1v)|{w : C(wi−1w) > 0}|\n(3.36)\nThe ﬁrst term\nd\nP\nvC(wi−1v) is the normalized discount. The second term |{w : C(wi−1w) > 0}|\nis the number of word types that can follow wi−1 or, equivalently, the number of\nword types that we discounted; in other words, the number of times we applied the\nnormalized discount.\nThe general recursive formulation is as follows:\nPKN(wi|wi−1\ni−n+1) = max(cKN(wi\ni−n+1)−d,0)\nP\nv cKN(wi−1\ni−n+1v)\n+λ(wi−1\ni−n+1)PKN(wi|wi−1\ni−n+2) (3.37)\nwhere the deﬁnition of the count cKN depends on whether we are counting the\nhighest-order n-gram being interpolated (for example trigram if we are interpolating\ntrigram, bigram, and unigram) or one of the lower-order n-grams (bigram or unigram\nif we are interpolating trigram, bigram, and unigram):\ncKN(·) =\n\u001a\ncount(·)\nfor the highest order\ncontinuationcount(·)\nfor lower orders\n(3.38)\nThe continuation count is the number of unique single word contexts for ·.\nAt the termination of the recursion, unigrams are interpolated with the uniform\ndistribution, where the parameter ϵ is the empty string:\nPKN(w) = max(cKN(w)−d,0)\nP\nw′ cKN(w′)\n+λ(ϵ) 1\nV\n(3.39)\nIf we want to include an unknown word <UNK>, it’s just included as a regular vo-\ncabulary entry with count zero, and hence its probability will be a lambda-weighted\nuniform distribution λ(ϵ)\nV .\nThe best-performing version of Kneser-Ney smoothing is called modiﬁed Kneser-\nNey smoothing, and is due to Chen and Goodman (1998). Rather than use a single\nmodiﬁed\nKneser-Ney\nﬁxed discount d, modiﬁed Kneser-Ney uses three different discounts d1, d2, and\nd3+ for n-grams with counts of 1, 2 and three or more, respectively. See Chen and\nGoodman (1998, p. 19) or Heaﬁeld et al. (2013) for the details.\n3.6\nThe Web and Stupid Backoff\nBy using text from the web, it is possible to build extremely large language mod-\nels. In 2006 Google released a very large set of n-gram counts, including n-grams\n(1-grams through 5-grams) from all the ﬁve-word sequences that appear at least\n40 times from 1,024,908,267,229 words of running text on the web; this includes",
  "62": "56\nCHAPTER 3\n•\nN-GRAM LANGUAGE MODELS\n1,176,470,663 ﬁve-word sequences using over 13 million unique words types (Franz\nand Brants, 2006). Some examples:\n4-gram\nCount\nserve as the incoming\n92\nserve as the incubator\n99\nserve as the independent\n794\nserve as the index\n223\nserve as the indication\n72\nserve as the indicator\n120\nserve as the indicators\n45\nserve as the indispensable\n111\nserve as the indispensible\n40\nserve as the individual\n234\nEfﬁciency considerations are important when building language models that use\nsuch large sets of n-grams. Rather than store each word as a string, it is generally\nrepresented in memory as a 64-bit hash number, with the words themselves stored\non disk. Probabilities are generally quantized using only 4-8 bits (instead of 8-byte\nﬂoats), and n-grams are stored in reverse tries.\nN-grams can also be shrunk by pruning, for example only storing n-grams with\ncounts greater than some threshold (such as the count threshold of 40 used for the\nGoogle n-gram release) or using entropy to prune less-important n-grams (Stolcke,\n1998). Another option is to build approximate language models using techniques\nlike Bloom ﬁlters (Talbot and Osborne 2007, Church et al. 2007). Finally, efﬁ-\nBloom ﬁlters\ncient language model toolkits like KenLM (Heaﬁeld 2011, Heaﬁeld et al. 2013) use\nsorted arrays, efﬁciently combine probabilities and backoffs in a single value, and\nuse merge sorts to efﬁciently build the probability tables in a minimal number of\npasses through a large corpus.\nAlthough with these toolkits it is possible to build web-scale language models\nusing full Kneser-Ney smoothing, Brants et al. (2007) show that with very large lan-\nguage models a much simpler algorithm may be sufﬁcient. The algorithm is called\nstupid backoff. Stupid backoff gives up the idea of trying to make the language\nstupid backoff\nmodel a true probability distribution. There is no discounting of the higher-order\nprobabilities. If a higher-order n-gram has a zero count, we simply backoff to a\nlower order n-gram, weighed by a ﬁxed (context-independent) weight. This algo-\nrithm does not produce a probability distribution, so we’ll follow Brants et al. (2007)\nin referring to it as S:\nS(wi|wi−1\ni−k+1) =\n\n\n\ncount(wi\ni−k+1)\ncount(wi−1\ni−k+1)\nif count(wi\ni−k+1) > 0\nλS(wi|wi−1\ni−k+2)\notherwise\n(3.40)\nThe backoff terminates in the unigram, which has probability S(w) = count(w)\nN\n. Brants\net al. (2007) ﬁnd that a value of 0.4 worked well for λ.\n3.7\nAdvanced: Perplexity’s Relation to Entropy\nWe introduced perplexity in Section 3.2.1 as a way to evaluate n-gram models on\na test set. A better n-gram model is one that assigns a higher probability to the",
  "63": "3.7\n•\nADVANCED: PERPLEXITY’S RELATION TO ENTROPY\n57\ntest data, and perplexity is a normalized version of the probability of the test set.\nThe perplexity measure actually arises from the information-theoretic concept of\ncross-entropy, which explains otherwise mysterious properties of perplexity (why\nthe inverse probability, for example?) and its relationship to entropy. Entropy is a\nEntropy\nmeasure of information. Given a random variable X ranging over whatever we are\npredicting (words, letters, parts of speech, the set of which we’ll call χ) and with a\nparticular probability function, call it p(x), the entropy of the random variable X is:\nH(X) = −\nX\nx∈χ\np(x)log2 p(x)\n(3.41)\nThe log can, in principle, be computed in any base. If we use log base 2, the\nresulting value of entropy will be measured in bits.\nOne intuitive way to think about entropy is as a lower bound on the number of\nbits it would take to encode a certain decision or piece of information in the optimal\ncoding scheme.\nConsider an example from the standard information theory textbook Cover and\nThomas (1991). Imagine that we want to place a bet on a horse race but it is too\nfar to go all the way to Yonkers Racetrack, so we’d like to send a short message to\nthe bookie to tell him which of the eight horses to bet on. One way to encode this\nmessage is just to use the binary representation of the horse’s number as the code;\nthus, horse 1 would be 001, horse 2 010, horse 3 011, and so on, with horse 8 coded\nas 000. If we spend the whole day betting and each horse is coded with 3 bits, on\naverage we would be sending 3 bits per race.\nCan we do better? Suppose that the spread is the actual distribution of the bets\nplaced and that we represent it as the prior probability of each horse as follows:\nHorse 1\n1\n2\nHorse 5\n1\n64\nHorse 2\n1\n4\nHorse 6\n1\n64\nHorse 3\n1\n8\nHorse 7\n1\n64\nHorse 4\n1\n16\nHorse 8\n1\n64\nThe entropy of the random variable X that ranges over horses gives us a lower\nbound on the number of bits and is\nH(X) = −\ni=8\nX\ni=1\np(i)log p(i)\n=\n−1\n2 log 1\n2 −1\n4 log 1\n4 −1\n8 log 1\n8 −1\n16 log 1\n16 −4( 1\n64 log 1\n64 )\n= 2 bits\n(3.42)\nA code that averages 2 bits per race can be built with short encodings for more\nprobable horses, and longer encodings for less probable horses. For example, we\ncould encode the most likely horse with the code 0, and the remaining horses as 10,\nthen 110, 1110, 111100, 111101, 111110, and 111111.\nWhat if the horses are equally likely? We saw above that if we used an equal-\nlength binary code for the horse numbers, each horse took 3 bits to code, so the\naverage was 3. Is the entropy the same? In this case each horse would have a\nprobability of 1\n8. The entropy of the choice of horses is then\nH(X) = −\ni=8\nX\ni=1\n1\n8 log 1\n8 = −log 1\n8 = 3 bits\n(3.43)",
  "64": "58\nCHAPTER 3\n•\nN-GRAM LANGUAGE MODELS\nUntil now we have been computing the entropy of a single variable. But most of\nwhat we will use entropy for involves sequences. For a grammar, for example, we\nwill be computing the entropy of some sequence of words W = {w0,w1,w2,...,wn}.\nOne way to do this is to have a variable that ranges over sequences of words. For\nexample we can compute the entropy of a random variable that ranges over all ﬁnite\nsequences of words of length n in some language L as follows:\nH(w1,w2,...,wn) = −\nX\nW n\n1 ∈L\np(W n\n1 )log p(W n\n1 )\n(3.44)\nWe could deﬁne the entropy rate (we could also think of this as the per-word\nentropy rate\nentropy) as the entropy of this sequence divided by the number of words:\n1\nnH(W n\n1 ) = −1\nn\nX\nW n\n1 ∈L\np(W n\n1 )log p(W n\n1 )\n(3.45)\nBut to measure the true entropy of a language, we need to consider sequences of\ninﬁnite length. If we think of a language as a stochastic process L that produces a\nsequence of words, and allow W to represent the sequence of words w1,...,wn, then\nL’s entropy rate H(L) is deﬁned as\nH(L) = lim\nn→∞\n1\nnH(w1,w2,...,wn)\n= −lim\nn→∞\n1\nn\nX\nW∈L\np(w1,...,wn)log p(w1,...,wn)\n(3.46)\nThe Shannon-McMillan-Breiman theorem (Algoet and Cover 1988, Cover and\nThomas 1991) states that if the language is regular in certain ways (to be exact, if it\nis both stationary and ergodic),\nH(L) = lim\nn→∞−1\nn log p(w1w2 ...wn)\n(3.47)\nThat is, we can take a single sequence that is long enough instead of summing\nover all possible sequences. The intuition of the Shannon-McMillan-Breiman the-\norem is that a long-enough sequence of words will contain in it many other shorter\nsequences and that each of these shorter sequences will reoccur in the longer se-\nquence according to their probabilities.\nA stochastic process is said to be stationary if the probabilities it assigns to a\nStationary\nsequence are invariant with respect to shifts in the time index. In other words, the\nprobability distribution for words at time t is the same as the probability distribution\nat time t + 1. Markov models, and hence n-grams, are stationary. For example, in\na bigram, Pi is dependent only on Pi−1. So if we shift our time index by x, Pi+x is\nstill dependent on Pi+x−1. But natural language is not stationary, since as we show\nin Chapter 10, the probability of upcoming words can be dependent on events that\nwere arbitrarily distant and time dependent. Thus, our statistical models only give\nan approximation to the correct distributions and entropies of natural language.\nTo summarize, by making some incorrect but convenient simplifying assump-\ntions, we can compute the entropy of some stochastic process by taking a very long\nsample of the output and computing its average log probability.\nNow we are ready to introduce cross-entropy. The cross-entropy is useful when\ncross-entropy\nwe don’t know the actual probability distribution p that generated some data. It",
  "65": "3.7\n•\nADVANCED: PERPLEXITY’S RELATION TO ENTROPY\n59\nallows us to use some m, which is a model of p (i.e., an approximation to p). The\ncross-entropy of m on p is deﬁned by\nH(p,m) = lim\nn→∞−1\nn\nX\nW∈L\np(w1,...,wn)logm(w1,...,wn)\n(3.48)\nThat is, we draw sequences according to the probability distribution p, but sum\nthe log of their probabilities according to m.\nAgain, following the Shannon-McMillan-Breiman theorem, for a stationary er-\ngodic process:\nH(p,m) = lim\nn→∞−1\nn logm(w1w2 ...wn)\n(3.49)\nThis means that, as for entropy, we can estimate the cross-entropy of a model\nm on some distribution p by taking a single sequence that is long enough instead of\nsumming over all possible sequences.\nWhat makes the cross-entropy useful is that the cross-entropy H(p,m) is an up-\nper bound on the entropy H(p). For any model m:\nH(p) ≤H(p,m)\n(3.50)\nThis means that we can use some simpliﬁed model m to help estimate the true en-\ntropy of a sequence of symbols drawn according to probability p. The more accurate\nm is, the closer the cross-entropy H(p,m) will be to the true entropy H(p). Thus,\nthe difference between H(p,m) and H(p) is a measure of how accurate a model is.\nBetween two models m1 and m2, the more accurate model will be the one with the\nlower cross-entropy. (The cross-entropy can never be lower than the true entropy, so\na model cannot err by underestimating the true entropy.)\nWe are ﬁnally ready to see the relation between perplexity and cross-entropy as\nwe saw it in Eq. 3.49. Cross-entropy is deﬁned in the limit, as the length of the\nobserved word sequence goes to inﬁnity. We will need an approximation to cross-\nentropy, relying on a (sufﬁciently long) sequence of ﬁxed length. This approxima-\ntion to the cross-entropy of a model M = P(wi|wi−N+1...wi−1) on a sequence of\nwords W is\nH(W) = −1\nN logP(w1w2 ...wN)\n(3.51)\nThe perplexity of a model P on a sequence of words W is now formally deﬁned as\nperplexity\nthe exp of this cross-entropy:\nPerplexity(W) = 2H(W)\n= P(w1w2 ...wN)−1\nN\n=\nN\ns\n1\nP(w1w2 ...wN)\n=\nN\nv\nu\nu\nt\nN\nY\ni=1\n1\nP(wi|w1 ...wi−1)\n(3.52)",
  "66": "60\nCHAPTER 3\n•\nN-GRAM LANGUAGE MODELS\n3.8\nSummary\nThis chapter introduced language modeling and the n-gram, one of the most widely\nused tools in language processing.\n• Language models offer a way to assign a probability to a sentence or other\nsequence of words, and to predict a word from preceding words.\n• n-grams are Markov models that estimate words from a ﬁxed window of pre-\nvious words. n-gram probabilities can be estimated by counting in a corpus\nand normalizing (the maximum likelihood estimate).\n• n-gram language models are evaluated extrinsically in some task, or intrinsi-\ncally using perplexity.\n• The perplexity of a test set according to a language model is the geometric\nmean of the inverse test set probability computed by the model.\n• Smoothing algorithms provide a more sophisticated way to estimate the prob-\nability of n-grams. Commonly used smoothing algorithms for n-grams rely on\nlower-order n-gram counts through backoff or interpolation.\n• Both backoff and interpolation require discounting to create a probability dis-\ntribution.\n• Kneser-Ney smoothing makes use of the probability of a word being a novel\ncontinuation. The interpolated Kneser-Ney smoothing algorithm mixes a\ndiscounted probability with a lower-order continuation probability.\nBibliographical and Historical Notes\nThe underlying mathematics of the n-gram was ﬁrst proposed by Markov (1913),\nwho used what are now called Markov chains (bigrams and trigrams) to predict\nwhether an upcoming letter in Pushkin’s Eugene Onegin would be a vowel or a con-\nsonant. Markov classiﬁed 20,000 letters as V or C and computed the bigram and\ntrigram probability that a given letter would be a vowel given the previous one or\ntwo letters. Shannon (1948) applied n-grams to compute approximations to English\nword sequences. Based on Shannon’s work, Markov models were commonly used in\nengineering, linguistic, and psychological work on modeling word sequences by the\n1950s. In a series of extremely inﬂuential papers starting with Chomsky (1956) and\nincluding Chomsky (1957) and Miller and Chomsky (1963), Noam Chomsky argued\nthat “ﬁnite-state Markov processes”, while a possibly useful engineering heuristic,\nwere incapable of being a complete cognitive model of human grammatical knowl-\nedge. These arguments led many linguists and computational linguists to ignore\nwork in statistical modeling for decades.\nThe resurgence of n-gram models came from Jelinek and colleagues at the IBM\nThomas J. Watson Research Center, who were inﬂuenced by Shannon, and Baker\nat CMU, who was inﬂuenced by the work of Baum and colleagues. Independently\nthese two labs successfully used n-grams in their speech recognition systems (Baker 1990,\nJelinek 1976, Baker 1975, Bahl et al. 1983, Jelinek 1990). A trigram model was used\nin the IBM TANGORA speech recognition system in the 1970s, but the idea was not\nwritten up until later.\nAdd-one smoothing derives from Laplace’s 1812 law of succession and was ﬁrst\napplied as an engineering solution to the zero-frequency problem by Jeffreys (1948)",
  "67": "EXERCISES\n61\nbased on an earlier Add-K suggestion by Johnson (1932). Problems with the add-\none algorithm are summarized in Gale and Church (1994).\nA wide variety of different language modeling and smoothing techniques were\nproposed in the 80s and 90s, including Good-Turing discounting—ﬁrst applied to\nthe n-gram smoothing at IBM by Katz (N´adas 1984, Church and Gale 1991)—\nWitten-Bell discounting (Witten and Bell, 1991), and varieties of class-based n-\ngram models that used information about word classes.\nclass-based\nn-gram\nStarting in the late 1990s, Chen and Goodman produced a highly inﬂuential\nseries of papers with a comparison of different language models (Chen and Good-\nman 1996, Chen and Goodman 1998, Chen and Goodman 1999, Goodman 2006).\nThey performed a number of carefully controlled experiments comparing differ-\nent discounting algorithms, cache models, class-based models, and other language\nmodel parameters. They showed the advantages of Modiﬁed Interpolated Kneser-\nNey, which has since become the standard baseline for language modeling, espe-\ncially because they showed that caches and class-based models provided only minor\nadditional improvement. These papers are recommended for any reader with further\ninterest in language modeling.\nTwo commonly used toolkits for building language models are SRILM (Stolcke,\n2002) and KenLM (Heaﬁeld 2011, Heaﬁeld et al. 2013). Both are publicly available.\nSRILM offers a wider range of options and types of discounting, while KenLM is\noptimized for speed and memory size, making it possible to build web-scale lan-\nguage models.\nThe highest accuracy language models at the time of this writing make use of\nneural nets. The problem with standard language models is that the number of pa-\nneural nets\nrameters increases exponentially as the n-gram order increases, and n-grams have no\nway to generalize from training to test set. Neural networks instead project words\ninto a continuous space in which words with similar contexts have similar represen-\ntations. Both feedforward nets Bengio et al. 2006, Schwenk 2007 and recurrent\nnets (Mikolov, 2012) are used.\nOther important classes of language models are maximum entropy language\nmaximum\nentropy\nmodels (Rosenfeld, 1996), based on logistic regression classiﬁers that use lots of\nfeatures to help predict upcoming words. These classiﬁers can use the standard\nfeatures presented in this chapter (i.e., the previous words) but also lots of other\nuseful predictors, as can other kinds of discriminative language models (Roark et al.,\n2007). We’ll introduce logistic regression language modeling when we introduce\nclassiﬁcation in Chapter 4.\nAnother important technique is language model adaptation, where we want to\nadaptation\ncombine data from multiple domains (for example we might have less in-domain\ntraining data but more general data that we then need to adapt) (Bulyko et al. 2003,\nBacchiani et al. 2004, Bellegarda 2004, Bacchiani et al. 2006, Hsu 2007, Liu et al. 2013).\nExercises\n3.1\nWrite out the equation for trigram probability estimation (modifying Eq. 3.11).\nNow write out all the non-zero trigram probabilities for the I am Sam corpus\non page 41.\n3.2\nCalculate the probability of the sentence i want chinese food. Give two\nprobabilities, one using Fig. 3.2, and another using the add-1 smoothed table\nin Fig. 3.6.",
  "68": "62\nCHAPTER 3\n•\nN-GRAM LANGUAGE MODELS\n3.3\nWhich of the two probabilities you computed in the previous exercise is higher,\nunsmoothed or smoothed? Explain why.\n3.4\nWe are given the following corpus, modiﬁed from the one in the chapter:\n<s> I am Sam </s>\n<s> Sam I am </s>\n<s> I am Sam </s>\n<s> I do not like green eggs and Sam </s>\nUsing a bigram language model with add-one smoothing, what is P(Sam |\nam)? Include <s> and </s> in your counts just like any other token.\n3.5\nSuppose we didn’t use the end-symbol </s>. Train an unsmoothed bigram\ngrammar on the following training corpus without using the end-symbol </s>:\n<s> a b\n<s> b b\n<s> b a\n<s> a a\nDemonstrate that your bigram model does not assign a single probability dis-\ntribution across all sentence lengths by showing that the sum of the probability\nof the four possible 2 word sentences over the alphabet {a,b} is 1.0, and the\nsum of the probability of all possible 3 word sentences over the alphabet {a,b}\nis also 1.0.\n3.6\nSuppose we train a trigram language model with add-one smoothing on a\ngiven corpus. The corpus contains V word types. Express a formula for esti-\nmating P(w3|w1,w2), where w3 is a word which follows the bigram (w1,w2),\nin terms of various N-gram counts and V. Use the notation c(w1,w2,w3) to\ndenote the number of times that trigram (w1,w2,w3) occurs in the corpus, and\nso on for bigrams and unigrams.\n3.7\nWe are given the following corpus, modiﬁed from the one in the chapter:\n<s> I am Sam </s>\n<s> Sam I am </s>\n<s> I am Sam </s>\n<s> I do not like green eggs and Sam </s>\nIf we use linear interpolation smoothing between a maximum-likelihood bi-\ngram model and a maximum-likelihood unigram model with λ1 = 1\n2 and λ2 =\n1\n2, what is P(Sam|am)? Include <s> and </s>\\verb in your counts just like\nany other token.\n3.8\nWrite a program to compute unsmoothed unigrams and bigrams.\n3.9\nRun your n-gram program on two different small corpora of your choice (you\nmight use email text or newsgroups). Now compare the statistics of the two\ncorpora. What are the differences in the most common unigrams between the\ntwo? How about interesting differences in bigrams?\n3.10 Add an option to your program to generate random sentences.\n3.11 Add an option to your program to compute the perplexity of a test set.",
  "69": "CHAPTER\n4\nNaive Bayes and Sentiment\nClassiﬁcation\nClassiﬁcation lies at the heart of both human and machine intelligence. Deciding\nwhat letter, word, or image has been presented to our senses, recognizing faces\nor voices, sorting mail, assigning grades to homeworks; these are all examples of\nassigning a category to an input. The potential challenges of this task are highlighted\nby the fabulist Jorge Luis Borges (1964), who imagined classifying animals into:\n(a) those that belong to the Emperor, (b) embalmed ones, (c) those that\nare trained, (d) suckling pigs, (e) mermaids, (f) fabulous ones, (g) stray\ndogs, (h) those that are included in this classiﬁcation, (i) those that\ntremble as if they were mad, (j) innumerable ones, (k) those drawn with\na very ﬁne camel’s hair brush, (l) others, (m) those that have just broken\na ﬂower vase, (n) those that resemble ﬂies from a distance.\nMany language processing tasks involve classiﬁcation, although luckily our classes\nare much easier to deﬁne than those of Borges. In this chapter we introduce the naive\nBayes algorithm and apply it to text categorization, the task of assigning a label or\ntext\ncategorization\ncategory to an entire text or document.\nWe focus on one common text categorization task, sentiment analysis, the ex-\nsentiment\nanalysis\ntraction of sentiment, the positive or negative orientation that a writer expresses\ntoward some object. A review of a movie, book, or product on the web expresses the\nauthor’s sentiment toward the product, while an editorial or political text expresses\nsentiment toward a candidate or political action. Extracting consumer or public sen-\ntiment is thus relevant for ﬁelds from marketing to politics.\nThe simplest version of sentiment analysis is a binary classiﬁcation task, and the\nwords of the review provide excellent cues. Consider, for example, the following\nphrases extracted from positive and negative reviews of movies and restaurants,.\nWords like great, richly, awesome, and pathetic, and awful and ridiculously are very\ninformative cues:\n+ ...zany characters and richly applied satire, and some great plot twists\n−It was pathetic. The worst part about it was the boxing scenes...\n+ ...awesome caramel sauce and sweet toasty almonds. I love this place!\n−...awful pizza and ridiculously overpriced...\nSpam detection is another important commercial application, the binary clas-\nspam detection\nsiﬁcation task of assigning an email to one of the two classes spam or not-spam.\nMany lexical and other features can be used to perform this classiﬁcation. For ex-\nample you might quite reasonably be suspicious of an email containing phrases like\n“online pharmaceutical” or “WITHOUT ANY COST” or “Dear Winner”.\nAnother thing we might want to know about a text is the language it’s written\nin. Texts on social media, for example, can be in any number of languages and we’ll\nneed to apply different processing. The task of language id is thus the ﬁrst step\nlanguage id\nin most language processing pipelines. Related tasks like determining a text’s au-\nthor, (authorship attribution), or author characteristics like gender, age, and native\nauthorship\nattribution",
  "70": "64\nCHAPTER 4\n•\nNAIVE BAYES AND SENTIMENT CLASSIFICATION\nlanguage are text classiﬁcation tasks that are also relevant to the digital humanities,\nsocial sciences, and forensic linguistics.\nFinally, one of the oldest tasks in text classiﬁcation is assigning a library sub-\nject category or topic label to a text. Deciding whether a research paper concerns\nepidemiology or instead, perhaps, embryology, is an important component of infor-\nmation retrieval. Various sets of subject categories exist, such as the MeSH (Medical\nSubject Headings) thesaurus. In fact, as we will see, subject category classiﬁcation\nis the task for which the naive Bayes algorithm was invented in 1961.\nClassiﬁcation is essential for tasks below the level of the document as well.\nWe’ve already seen period disambiguation (deciding if a period is the end of a sen-\ntence or part of a word), and word tokenization (deciding if a character should be\na word boundary). Even language modeling can be viewed as classiﬁcation: each\nword can be thought of as a class, and so predicting the next word is classifying the\ncontext-so-far into a class for each next word. A part-of-speech tagger (Chapter 8)\nclassiﬁes each occurrence of a word in a sentence as, e.g., a noun or a verb.\nThe goal of classiﬁcation is to take a single observation, extract some useful\nfeatures, and thereby classify the observation into one of a set of discrete classes.\nOne method for classifying text is to use hand-written rules. There are many areas\nof language processing where hand-written rule-based classiﬁers constitute a state-\nof-the-art system, or at least part of it.\nRules can be fragile, however, as situations or data change over time, and for\nsome tasks humans aren’t necessarily good at coming up with the rules. Most cases\nof classiﬁcation in language processing are instead done via supervised machine\nlearning, and this will be the subject of the remainder of this chapter. In supervised\nsupervised\nmachine\nlearning\nlearning, we have a data set of input observations, each associated with some correct\noutput (a ‘supervision signal’). The goal of the algorithm is to learn how to map\nfrom a new observation to a correct output.\nFormally, the task of supervised classiﬁcation is to take an input x and a ﬁxed\nset of output classes Y = y1,y2,...,yM and return a predicted class y ∈Y. For text\nclassiﬁcation, we’ll sometimes talk about c (for “class”) instead of y as our output\nvariable, and d (for “document”) instead of x as our input variable. In the supervised\nsituation we have a training set of N documents that have each been hand-labeled\nwith a class: (d1,c1),....,(dN,cN). Our goal is to learn a classiﬁer that is capable of\nmapping from a new document d to its correct class c ∈C. A probabilistic classiﬁer\nadditionally will tell us the probability of the observation being in the class. This\nfull distribution over the classes can be useful information for downstream decisions;\navoiding making discrete decisions early on can be useful when combining systems.\nMany kinds of machine learning algorithms are used to build classiﬁers. This\nchapter introduces naive Bayes; the following one introduces logistic regression.\nThese exemplify two ways of doing classiﬁcation. Generative classiﬁers like naive\nBayes build a model of how a class could generate some input data. Given an ob-\nservation, they return the class most likely to have generated the observation. Dis-\ncriminative classiﬁers like logistic regression instead learn what features from the\ninput are most useful to discriminate between the different possible classes. While\ndiscriminative systems are often more accurate and hence more commonly used,\ngenerative classiﬁers still have a role.",
  "71": "4.1\n•\nNAIVE BAYES CLASSIFIERS\n65\n4.1\nNaive Bayes Classiﬁers\nIn this section we introduce the multinomial naive Bayes classiﬁer, so called be-\nnaive Bayes\nclassiﬁer\ncause it is a Bayesian classiﬁer that makes a simplifying (naive) assumption about\nhow the features interact.\nThe intuition of the classiﬁer is shown in Fig. 4.1. We represent a text document\nas if it were a bag-of-words, that is, an unordered set of words with their position\nbag-of-words\nignored, keeping only their frequency in the document. In the example in the ﬁgure,\ninstead of representing the word order in all the phrases like “I love this movie” and\n“I would recommend it”, we simply note that the word I occurred 5 times in the\nentire excerpt, the word it 6 times, the words love, recommend, and movie once, and\nso on.\nit\nit\nit\nit\nit\nit\nI\nI\nI\nI\nI\nlove\nrecommend\nmovie\nthe\nthe\nthe\nthe\nto\nto\nto\nand\nand\nand\nseen\nseen\nyet\nwould\nwith\nwho\nwhimsical\nwhile\nwhenever\ntimes\nsweet\nseveral\nscenes\nsatirical\nromantic\nof\nmanages\nhumor\nhave\nhappy\nfun\nfriend\nfairy\ndialogue\nbut\nconventions\nareanyone\nadventure\nalways\nagain\nabout\nI love this movie! It's sweet, \nbut with satirical humor. The \ndialogue is great and the \nadventure scenes are fun... \nIt manages to be whimsical \nand romantic while laughing \nat the conventions of the \nfairy tale genre. I would \nrecommend it to just about \nanyone. I've seen it several \ntimes, and I'm always happy \nto see it again whenever I \nhave a friend who hasn't \nseen it yet!\nit \nI\nthe\nto\nand\nseen\nyet\nwould\nwhimsical\ntimes\nsweet\nsatirical\nadventure\ngenre\nfairy\nhumor\nhave\ngreat\n…\n6 \n5\n4\n3\n3\n2\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n…\nFigure 4.1\nIntuition of the multinomial naive Bayes classiﬁer applied to a movie review. The position of the\nwords is ignored (the bag of words assumption) and we make use of the frequency of each word.\nNaive Bayes is a probabilistic classiﬁer, meaning that for a document d, out of\nall classes c ∈C the classiﬁer returns the class ˆc which has the maximum posterior\nprobability given the document. In Eq. 4.1 we use the hat notation ˆ to mean “our\nˆ\nestimate of the correct class”.\nˆc = argmax\nc∈C\nP(c|d)\n(4.1)\nThis idea of Bayesian inference has been known since the work of Bayes (1763),\nBayesian\ninference\nand was ﬁrst applied to text classiﬁcation by Mosteller and Wallace (1964). The in-\ntuition of Bayesian classiﬁcation is to use Bayes’ rule to transform Eq. 4.1 into other\nprobabilities that have some useful properties. Bayes’ rule is presented in Eq. 4.2;\nit gives us a way to break down any conditional probability P(x|y) into three other",
  "72": "66\nCHAPTER 4\n•\nNAIVE BAYES AND SENTIMENT CLASSIFICATION\nprobabilities:\nP(x|y) = P(y|x)P(x)\nP(y)\n(4.2)\nWe can then substitute Eq. 4.2 into Eq. 4.1 to get Eq. 4.3:\nˆc = argmax\nc∈C\nP(c|d) = argmax\nc∈C\nP(d|c)P(c)\nP(d)\n(4.3)\nWe can conveniently simplify Eq. 4.3 by dropping the denominator P(d). This\nis possible because we will be computing P(d|c)P(c)\nP(d)\nfor each possible class. But P(d)\ndoesn’t change for each class; we are always asking about the most likely class for\nthe same document d, which must have the same probability P(d). Thus, we can\nchoose the class that maximizes this simpler formula:\nˆc = argmax\nc∈C\nP(c|d) = argmax\nc∈C\nP(d|c)P(c)\n(4.4)\nWe thus compute the most probable class ˆc given some document d by choosing\nthe class which has the highest product of two probabilities: the prior probability\nprior\nprobability\nof the class P(c) and the likelihood of the document P(d|c):\nlikelihood\nˆc = argmax\nc∈C\nlikelihood\nz }| {\nP(d|c)\nprior\nz}|{\nP(c)\n(4.5)\nWithout loss of generalization, we can represent a document d as a set of features\nf1, f2,..., fn:\nˆc = argmax\nc∈C\nlikelihood\nz\n}|\n{\nP(f1, f2,...., fn|c)\nprior\nz}|{\nP(c)\n(4.6)\nUnfortunately, Eq. 4.6 is still too hard to compute directly: without some sim-\nplifying assumptions, estimating the probability of every possible combination of\nfeatures (for example, every possible set of words and positions) would require huge\nnumbers of parameters and impossibly large training sets. Naive Bayes classiﬁers\ntherefore make two simplifying assumptions.\nThe ﬁrst is the bag of words assumption discussed intuitively above: we assume\nposition doesn’t matter, and that the word “love” has the same effect on classiﬁcation\nwhether it occurs as the 1st, 20th, or last word in the document. Thus we assume\nthat the features f1, f2,..., fn only encode word identity and not position.\nThe second is commonly called the naive Bayes assumption: this is the condi-\nnaive Bayes\nassumption\ntional independence assumption that the probabilities P(fi|c) are independent given\nthe class c and hence can be ‘naively’ multiplied as follows:\nP(f1, f2,...., fn|c) = P(f1|c)·P(f2|c)·...·P(fn|c)\n(4.7)\nThe ﬁnal equation for the class chosen by a naive Bayes classiﬁer is thus:\ncNB = argmax\nc∈C\nP(c)\nY\nf∈F\nP(f|c)\n(4.8)\nTo apply the naive Bayes classiﬁer to text, we need to consider word positions,\nby simply walking an index through every word position in the document:",
  "73": "4.2\n•\nTRAINING THE NAIVE BAYES CLASSIFIER\n67\npositions ←all word positions in test document\ncNB = argmax\nc∈C\nP(c)\nY\ni∈positions\nP(wi|c)\n(4.9)\nNaive Bayes calculations, like calculations for language modeling, are done in\nlog space, to avoid underﬂow and increase speed. Thus Eq. 4.9 is generally instead\nexpressed as\ncNB = argmax\nc∈C\nlogP(c)+\nX\ni∈positions\nlogP(wi|c)\n(4.10)\nBy considering features in log space Eq. 4.10 computes the predicted class as\na linear function of input features.\nClassiﬁers that use a linear combination of\nthe inputs to make a classiﬁcation decision —like naive Bayes and also logistic\nregression— are called linear classiﬁers.\nlinear\nclassiﬁers\n4.2\nTraining the Naive Bayes Classiﬁer\nHow can we learn the probabilities P(c) and P(fi|c)? Let’s ﬁrst consider the max-\nimum likelihood estimate. We’ll simply use the frequencies in the data. For the\ndocument prior P(c) we ask what percentage of the documents in our training set\nare in each class c. Let Nc be the number of documents in our training data with\nclass c and Ndoc be the total number of documents. Then:\nˆP(c) = Nc\nNdoc\n(4.11)\nTo learn the probability P(fi|c), we’ll assume a feature is just the existence of a\nword in the document’s bag of words, and so we’ll want P(wi|c), which we compute\nas the fraction of times the word wi appears among all words in all documents of\ntopic c. We ﬁrst concatenate all documents with category c into one big “category\nc” text. Then we use the frequency of wi in this concatenated document to give a\nmaximum likelihood estimate of the probability:\nˆP(wi|c) =\ncount(wi,c)\nP\nw∈V count(w,c)\n(4.12)\nHere the vocabulary V consists of the union of all the word types in all classes,\nnot just the words in one class c.\nThere is a problem, however, with maximum likelihood training. Imagine we\nare trying to estimate the likelihood of the word “fantastic” given class positive, but\nsuppose there are no training documents that both contain the word “fantastic” and\nare classiﬁed as positive. Perhaps the word “fantastic” happens to occur (sarcasti-\ncally?) in the class negative. In such a case the probability for this feature will be\nzero:",
  "74": "68\nCHAPTER 4\n•\nNAIVE BAYES AND SENTIMENT CLASSIFICATION\nˆP(“fantastic”|positive) = count(“fantastic”,positive)\nP\nw∈V count(w,positive)\n= 0\n(4.13)\nBut since naive Bayes naively multiplies all the feature likelihoods together, zero\nprobabilities in the likelihood term for any class will cause the probability of the\nclass to be zero, no matter the other evidence!\nThe simplest solution is the add-one (Laplace) smoothing introduced in Chap-\nter 3. While Laplace smoothing is usually replaced by more sophisticated smoothing\nalgorithms in language modeling, it is commonly used in naive Bayes text catego-\nrization:\nˆP(wi|c) =\ncount(wi,c)+1\nP\nw∈V (count(w,c)+1) =\ncount(wi,c)+1\n\u0000P\nw∈V count(w,c)\n\u0001\n+|V|\n(4.14)\nNote once again that it is crucial that the vocabulary V consists of the union of\nall the word types in all classes, not just the words in one class c (try to convince\nyourself why this must be true; see the exercise at the end of the chapter).\nWhat do we do about words that occur in our test data but are not in our vocab-\nulary at all because they did not occur in any training document in any class? The\nsolution for such unknown words is to ignore them—remove them from the test\nunknown word\ndocument and not include any probability for them at all.\nFinally, some systems choose to completely ignore another class of words: stop\nwords, very frequent words like the and a. This can be done by sorting the vocabu-\nstop words\nlary by frequency in the training set, and deﬁning the top 10–100 vocabulary entries\nas stop words, or alternatively by using one of the many pre-deﬁned stop word list\navailable online. Then every instance of these stop words are simply removed from\nboth training and test documents as if they had never occurred. In most text classi-\nﬁcation applications, however, using a stop word list doesn’t improve performance,\nand so it is more common to make use of the entire vocabulary and not use a stop\nword list.\nFig. 4.2 shows the ﬁnal algorithm.\n4.3\nWorked example\nLet’s walk through an example of training and testing naive Bayes with add-one\nsmoothing. We’ll use a sentiment analysis domain with the two classes positive\n(+) and negative (-), and take the following miniature training and test documents\nsimpliﬁed from actual movie reviews.\nCat\nDocuments\nTraining -\njust plain boring\n-\nentirely predictable and lacks energy\n-\nno surprises and very few laughs\n+\nvery powerful\n+\nthe most fun ﬁlm of the summer\nTest\n?\npredictable with no fun\nThe prior P(c) for the two classes is computed via Eq. 4.11 as\nNc\nNdoc :",
  "75": "4.3\n•\nWORKED EXAMPLE\n69\nfunction TRAIN NAIVE BAYES(D, C) returns log P(c) and log P(w|c)\nfor each class c ∈C\n# Calculate P(c) terms\nNdoc = number of documents in D\nNc = number of documents from D in class c\nlogprior[c]←log Nc\nNdoc\nV←vocabulary of D\nbigdoc[c]←append(d) for d ∈D with class c\nfor each word w in V\n# Calculate P(w|c) terms\ncount(w,c)←# of occurrences of w in bigdoc[c]\nloglikelihood[w,c]←log\ncount(w,c) + 1\nP\nw′ in V (count (w′,c) + 1)\nreturn logprior, loglikelihood, V\nfunction TEST NAIVE BAYES(testdoc,logprior, loglikelihood, C, V) returns best c\nfor each class c ∈C\nsum[c]←logprior[c]\nfor each position i in testdoc\nword←testdoc[i]\nif word ∈V\nsum[c]←sum[c]+ loglikelihood[word,c]\nreturn argmaxc sum[c]\nFigure 4.2\nThe naive Bayes algorithm, using add-1 smoothing. To use add-α smoothing\ninstead, change the +1 to +α for loglikelihood counts in training.\nP(−) = 3\n5\nP(+) = 2\n5\nThe word with doesn’t occur in the training set, so we drop it completely (as\nmentioned above, we don’t use unknown word models for naive Bayes). The like-\nlihoods from the training set for the remaining three words “predictable”, “no”, and\n“fun”, are as follows, from Eq. 4.14 (computing the probabilities for the remainder\nof the words in the training set is left as Exercise 4.?? (TBD)).\nP(“predictable”|−) =\n1+1\n14+20\nP(“predictable”|+) = 0+1\n9+20\nP(“no”|−) =\n1+1\n14+20\nP(“no”|+) = 0+1\n9+20\nP(“fun”|−) =\n0+1\n14+20\nP(“fun”|+) = 1+1\n9+20\nFor the test sentence S = “predictable with no fun”, after removing the word\n‘with’, the chosen class, via Eq. 4.9, is therefore computed as follows:\nP(−)P(S|−) = 3\n5 × 2×2×1\n343\n= 6.1×10−5\nP(+)P(S|+) = 2\n5 × 1×1×2\n293\n= 3.2×10−5",
  "76": "70\nCHAPTER 4\n•\nNAIVE BAYES AND SENTIMENT CLASSIFICATION\nThe model thus predicts the class negative for the test sentence.\n4.4\nOptimizing for Sentiment Analysis\nWhile standard naive Bayes text classiﬁcation can work well for sentiment analysis,\nsome small changes are generally employed that improve performance.\nFirst, for sentiment classiﬁcation and a number of other text classiﬁcation tasks,\nwhether a word occurs or not seems to matter more than its frequency. Thus it\noften improves performance to clip the word counts in each document at 1 (see\nthe end of the chapter for pointers to these results). This variant is called binary\nmultinomial naive Bayes or binary NB. The variant uses the same Eq. 4.10 except\nbinary NB\nthat for each document we remove all duplicate words before concatenating them\ninto the single big document. Fig. 4.3 shows an example in which a set of four\ndocuments (shortened and text-normalized for this example) are remapped to binary,\nwith the modiﬁed counts shown in the table on the right. The example is worked\nwithout add-1 smoothing to make the differences clearer. Note that the results counts\nneed not be 1; the word great has a count of 2 even for Binary NB, because it appears\nin multiple documents.\nFour original documents:\n−it was pathetic the worst part was the\nboxing scenes\n−no plot twists or great scenes\n+ and satire and great plot twists\n+ great scenes great ﬁlm\nAfter per-document binarization:\n−it was pathetic the worst part boxing\nscenes\n−no plot twists or great scenes\n+ and satire great plot twists\n+ great scenes ﬁlm\nNB\nBinary\nCounts\nCounts\n+\n−\n+\n−\nand\n2\n0\n1\n0\nboxing\n0\n1\n0\n1\nﬁlm\n1\n0\n1\n0\ngreat\n3\n1\n2\n1\nit\n0\n1\n0\n1\nno\n0\n1\n0\n1\nor\n0\n1\n0\n1\npart\n0\n1\n0\n1\npathetic\n0\n1\n0\n1\nplot\n1\n1\n1\n1\nsatire\n1\n0\n1\n0\nscenes\n1\n2\n1\n2\nthe\n0\n2\n0\n1\ntwists\n1\n1\n1\n1\nwas\n0\n2\n0\n1\nworst\n0\n1\n0\n1\nFigure 4.3\nAn example of binarization for the binary naive Bayes algorithm.\nA second important addition commonly made when doing text classiﬁcation for\nsentiment is to deal with negation. Consider the difference between I really like this\nmovie (positive) and I didn’t like this movie (negative). The negation expressed by\ndidn’t completely alters the inferences we draw from the predicate like. Similarly,\nnegation can modify a negative word to produce a positive review (don’t dismiss this\nﬁlm, doesn’t let us get bored).\nA very simple baseline that is commonly used in sentiment to deal with negation\nis during text normalization to prepend the preﬁx NOT to every word after a token\nof logical negation (n’t, not, no, never) until the next punctuation mark. Thus the\nphrase\ndidn’t like this movie , but I",
  "77": "4.5\n•\nNAIVE BAYES FOR OTHER TEXT CLASSIFICATION TASKS\n71\nbecomes\ndidn’t NOT_like NOT_this NOT_movie , but I\nNewly formed ‘words’ like NOT like, NOT recommend will thus occur more of-\nten in negative document and act as cues for negative sentiment, while words like\nNOT bored, NOT dismiss will acquire positive associations. We will return in Chap-\nter 15 to the use of parsing to deal more accurately with the scope relationship be-\ntween these negation words and the predicates they modify, but this simple baseline\nworks quite well in practice.\nFinally, in some situations we might have insufﬁcient labeled training data to\ntrain accurate naive Bayes classiﬁers using all words in the training set to estimate\npositive and negative sentiment. In such cases we can instead derive the positive\nand negative word features from sentiment lexicons, lists of words that are pre-\nsentiment\nlexicons\nannotated with positive or negative sentiment. Four popular lexicons are the General\nInquirer (Stone et al., 1966), LIWC (Pennebaker et al., 2007), the opinion lexicon\nGeneral\nInquirer\nLIWC\nof Hu and Liu (2004a) and the MPQA Subjectivity Lexicon (Wilson et al., 2005).\nFor example the MPQA subjectivity lexicon has 6885 words, 2718 positive and\n4912 negative, each marked for whether it is strongly or weakly biased. (Chapter 19\nwill discuss how these lexicons can be learned automatically.) Some samples of\npositive and negative words from the MPQA lexicon include:\n+ : admirable, beautiful, conﬁdent, dazzling, ecstatic, favor, glee, great\n−: awful, bad, bias, catastrophe, cheat, deny, envious, foul, harsh, hate\nA common way to use lexicons in a naive Bayes classiﬁer is to add a feature\nthat is counted whenever a word from that lexicon occurs. Thus we might add a\nfeature called ‘this word occurs in the positive lexicon’, and treat all instances of\nwords in the lexicon as counts for that one feature, instead of counting each word\nseparately. Similarly, we might add as a second feature ‘this word occurs in the\nnegative lexicon’ of words in the negative lexicon. If we have lots of training data,\nand if the test data matches the training data, using just two features won’t work as\nwell as using all the words. But when training data is sparse or not representative of\nthe test set, using dense lexicon features instead of sparse individual-word features\nmay generalize better.\n4.5\nNaive Bayes for other text classiﬁcation tasks\nIn the previous section we pointed out that naive Bayes doesn’t require that our\nclassiﬁer use all the words in the training data as features. In fact features in naive\nBayes can express any property of the input text we want.\nConsider the task of spam detection, deciding if a particular piece of email is\nspam detection\nan example of spam (unsolicited bulk email) — and one of the ﬁrst applications of\nnaive Bayes to text classiﬁcation (Sahami et al., 1998).\nA common solution here, rather than using all the words as individual features, is\nto predeﬁne likely sets of words or phrases as features, combined these with features\nthat are not purely linguistic. For example the open-source SpamAssassin tool1\npredeﬁnes features like the phrase “one hundred percent guaranteed”, or the feature\nmentions millions of dollars, which is a regular expression that matches suspiciously\nlarge sums of money. But it also includes features like HTML has a low ratio of\n1\nhttps://spamassassin.apache.org",
  "78": "72\nCHAPTER 4\n•\nNAIVE BAYES AND SENTIMENT CLASSIFICATION\ntext to image area, that isn’t purely linguistic and might require some sophisticated\ncomputation, or totally non-linguistic features about, say, the path that the email\ntook to arrive. More sample SpamAssassin features:\n• Email subject line is all capital letters\n• Contains phrases of urgency like “urgent reply”\n• Email subject line contains “online pharmaceutical”\n• HTML has unbalanced ”head” tags\n• Claims you can be removed from the list\nFor other tasks, like language ID—determining what language a given piece of\nlanguage ID\ntext is written in—the most effective naive Bayes features are not words at all, but\nbyte n-grams, 2-grams (‘zw’) 3-grams (‘nya’, ‘ Vo’), or 4-grams (‘ie z’, ‘thei’).\nBecause spaces count as a byte, byte n-grams can model statistics about the begin-\nning or ending of words. 2 A widely used naive Bayes system, langid.py (Lui\nand Baldwin, 2012) begins with all possible n-grams of lengths 1-4, using feature\nselection to winnow down to the most informative 7000 ﬁnal features.\nLanguage ID systems are trained on multilingual text, such as Wikipedia (Wikipedia\ntext in 68 different languages were used in (Lui and Baldwin, 2011)), or newswire.\nTo make sure that this multilingual text correctly reﬂects different regions, dialects,\nand socio-economic classes, systems also add Twitter text in many languages geo-\ntagged to many regions (important for getting world English dialects from countries\nwith large Anglophone populations like Nigeria or India), Bible and Quran transla-\ntions, slang websites like Urban Dictionary, corpora of African American Vernacular\nEnglish (Blodgett et al., 2016), and so on (Jurgens et al., 2017).\n4.6\nNaive Bayes as a Language Model\nAs we saw in the previous section, naive Bayes classiﬁers can use any sort of fea-\nture: dictionaries, URLs, email addresses, network features, phrases, and so on. But\nif, as in the previous section, we use only individual word features, and we use all\nof the words in the text (not a subset), then naive Bayes has an important similar-\nity to language modeling. Speciﬁcally, a naive Bayes model can be viewed as a\nset of class-speciﬁc unigram language models, in which the model for each class\ninstantiates a unigram language model.\nSince the likelihood features from the naive Bayes model assign a probability to\neach word P(word|c), the model also assigns a probability to each sentence:\nP(s|c) =\nY\ni∈positions\nP(wi|c)\n(4.15)\nThus consider a naive Bayes model with the classes positive (+) and negative (-)\nand the following model parameters:\n2\nIt’s also possible to use codepoints, which are multi-byte Unicode representations of characters in\ncharacter sets, but simply using bytes seems to work better.",
  "79": "4.7\n•\nEVALUATION: PRECISION, RECALL, F-MEASURE\n73\nw\nP(w|+) P(w|-)\nI\n0.1\n0.2\nlove 0.1\n0.001\nthis\n0.01\n0.01\nfun\n0.05\n0.005\nﬁlm 0.1\n0.1\n...\n...\n...\nEach of the two columns above instantiates a language model that can assign a\nprobability to the sentence “I love this fun ﬁlm”:\nP(”I love this fun ﬁlm”|+) = 0.1×0.1×0.01×0.05×0.1 = 0.0000005\nP(”I love this fun ﬁlm”|−) = 0.2×0.001×0.01×0.005×0.1 = .0000000010\nAs it happens, the positive model assigns a higher probability to the sentence:\nP(s|pos) > P(s|neg). Note that this is just the likelihood part of the naive Bayes\nmodel; once we multiply in the prior a full naive Bayes model might well make a\ndifferent classiﬁcation decision.\n4.7\nEvaluation: Precision, Recall, F-measure\nTo introduce the methods for evaluating text classiﬁcation, let’s ﬁrst consider some\nsimple binary detection tasks. For example, in spam detection, our goal is to label\nevery text as being in the spam category (“positive”) or not in the spam category\n(“negative”). For each item (email document) we therefore need to know whether\nour system called it spam or not. We also need to know whether the email is actually\nspam or not, i.e. the human-deﬁned labels for each document that we are trying to\nmatch. We will refer to these human labels as the gold labels.\ngold labels\nOr imagine you’re the CEO of the Delicious Pie Company and you need to know\nwhat people are saying about your pies on social media, so you build a system that\ndetects tweets concerning Delicious Pie. Here the positive class is tweets about\nDelicious Pie and the negative class is all other tweets.\nIn both cases, we need a metric for knowing how well our spam detector (or\npie-tweet-detector) is doing. To evaluate any system for detecting things, we start\nby building a contingency table like the one shown in Fig. 4.4. Each cell labels a\ncontingency\ntable\nset of possible outcomes. In the spam detection case, for example, true positives are\ndocuments that are indeed spam (indicated by human-created gold labels) and our\nsystem said they were spam. False negatives are documents that are indeed spam\nbut our system labeled as non-spam.\nTo the bottom right of the table is the equation for accuracy, which asks what\npercentage of all the observations (for the spam or pie examples that means all emails\nor tweets) our system labeled correctly. Although accuracy might seem a natural\nmetric, we generally don’t use it. That’s because accuracy doesn’t work well when\nthe classes are unbalanced (as indeed they are with spam, which is a large majority\nof email, or with tweets, which are mainly not about pie).\nTo make this more explicit, imagine that we looked at a million tweets, and\nlet’s say that only 100 of them are discussing their love (or hatred) for our pie,\nwhile the other 999,900 are tweets about something completely unrelated. Imagine a",
  "80": "74\nCHAPTER 4\n•\nNAIVE BAYES AND SENTIMENT CLASSIFICATION\ntrue positive\nfalse negative\nfalse positive\ntrue negative\ngold positive\ngold negative\nsystem\npositive\nsystem\nnegative\ngold standard labels\nsystem\noutput\nlabels\nrecall = tp\ntp+fn\nprecision = \ntp\ntp+fp\naccuracy = \ntp+tn\ntp+fp+tn+fn\nFigure 4.4\nContingency table\nsimple classiﬁer that stupidly classiﬁed every tweet as “not about pie”. This classiﬁer\nwould have 999,900 true negatives and only 100 false negatives for an accuracy of\n999,900/1,000,000 or 99.99%! What an amazing accuracy level! Surely we should\nbe happy with this classiﬁer? But of course this fabulous ‘no pie’ classiﬁer would\nbe completely useless, since it wouldn’t ﬁnd a single one of the customer comments\nwe are looking for. In other words, accuracy is not a good metric when the goal is\nto discover something that is rare, or at least not completely balanced in frequency,\nwhich is a very common situation in the world.\nThat’s why instead of accuracy we generally turn to two other metrics: precision\nand recall. Precision measures the percentage of the items that the system detected\nprecision\n(i.e., the system labeled as positive) that are in fact positive (i.e., are positive accord-\ning to the human gold labels). Precision is deﬁned as\nPrecision =\ntrue positives\ntrue positives + false positives\nRecall measures the percentage of items actually present in the input that were\nrecall\ncorrectly identiﬁed by the system. Recall is deﬁned as\nRecall =\ntrue positives\ntrue positives + false negatives\nPrecision and recall will help solve the problem with the useless “nothing is\npie” classiﬁer. This classiﬁer, despite having a fabulous accuracy of 99.99%, has\na terrible recall of 0 (since there are no true positives, and 100 false negatives, the\nrecall is 0/100). You should convince yourself that the precision at ﬁnding relevant\ntweets is equally problematic. Thus precision and recall, unlike accuracy, emphasize\ntrue positives: ﬁnding the things that we are supposed to be looking for.\nThere are many ways to deﬁne a single metric that incorporates aspects of both\nprecision and recall. The simplest of these combinations is the F-measure (van\nF-measure\nRijsbergen, 1975) , deﬁned as:\nFβ = (β 2 +1)PR\nβ 2P+R\nThe β parameter differentially weights the importance of recall and precision,\nbased perhaps on the needs of an application. Values of β > 1 favor recall, while\nvalues of β < 1 favor precision. When β = 1, precision and recall are equally bal-\nanced; this is the most frequently used metric, and is called Fβ=1 or just F1:\nF1",
  "81": "4.7\n•\nEVALUATION: PRECISION, RECALL, F-MEASURE\n75\nF1 = 2PR\nP+R\n(4.16)\nF-measure comes from a weighted harmonic mean of precision and recall. The\nharmonic mean of a set of numbers is the reciprocal of the arithmetic mean of recip-\nrocals:\nHarmonicMean(a1,a2,a3,a4,...,an) =\nn\n1\na1 + 1\na2 + 1\na3 +...+ 1\nan\n(4.17)\nand hence F-measure is\nF =\n1\nα 1\nP +(1−α) 1\nR\nor\n\u0012\nwith β 2 = 1−α\nα\n\u0013\nF = (β 2 +1)PR\nβ 2P+R\n(4.18)\nHarmonic mean is used because it is a conservative metric; the harmonic mean of\ntwo values is closer to the minimum of the two values than the arithmetic mean is.\nThus it weighs the lower of the two numbers more heavily.\n4.7.1\nMore than two classes\nUp to now we have been assuming text classiﬁcation tasks with only two classes.\nBut lots of classiﬁcation tasks in language processing have more than two classes.\nFor sentiment analysis we generally have 3 classes (positive, negative, neutral) and\neven more classes are common for tasks like part-of-speech tagging, word sense\ndisambiguation, semantic role labeling, emotion detection, and so on.\nThere are two kinds of multi-class classiﬁcation tasks. In any-of or multi-label\nany-of\nclassiﬁcation, each document or item can be assigned more than one label. We can\nsolve any-of classiﬁcation by building separate binary classiﬁers for each class c,\ntrained on positive examples labeled c and negative examples not labeled c. Given\na test document or item d, then each classiﬁer makes their decision independently,\nand we may assign multiple labels to d.\nMore common in language processing is one-of or multinomial classiﬁcation,\none-of\nmultinomial\nclassiﬁcation\nin which the classes are mutually exclusive and each document or item appears in\nexactly one class. Here we again build a separate binary classiﬁer trained on positive\nexamples from c and negative examples from all other classes. Now given a test\ndocument or item d, we run all the classiﬁers and choose the label from the classiﬁer\nwith the highest score. Consider the sample confusion matrix for a hypothetical 3-\nway one-of email categorization decision (urgent, normal, spam) shown in Fig. 4.5.\nThe matrix shows, for example, that the system mistakenly labeled 1 spam doc-\nument as urgent, and we have shown how to compute a distinct precision and recall\nvalue for each class. In order to derive a single metric that tells us how well the\nsystem is doing, we can combine these values in two ways. In macroaveraging, we\nmacroaveraging\ncompute the performance for each class, and then average over classes. In microav-\neraging, we collect the decisions for all classes into a single contingency table, and\nmicroaveraging\nthen compute precision and recall from that table. Fig. 4.6 shows the contingency\ntable for each class separately, and shows the computation of microaveraged and\nmacroaveraged precision.\nAs the ﬁgure shows, a microaverage is dominated by the more frequent class (in\nthis case spam), since the counts are pooled. The macroaverage better reﬂects the\nstatistics of the smaller classes, and so is more appropriate when performance on all\nthe classes is equally important.",
  "82": "76\nCHAPTER 4\n•\nNAIVE BAYES AND SENTIMENT CLASSIFICATION\n8\n5\n10\n60\nurgent\nnormal\ngold labels\nsystem\noutput\nrecallu = \n8\n8+5+3\nprecisionu= \n8\n8+10+1\n1\n50\n30\n200\nspam\nurgent\nnormal\nspam\n3\nrecalln = recalls = \nprecisionn= \n60\n5+60+50\nprecisions= \n200\n3+30+200\n60\n10+60+30\n200\n1+50+200\nFigure 4.5\nConfusion matrix for a three-class categorization task, showing for each pair of\nclasses (c1,c2), how many documents from c1 were (in)correctly assigned to c2\n8\n8\n11\n340\ntrue\nurgent\ntrue\nnot\nsystem\nurgent\nsystem\nnot\n60\n40\n55\n212\ntrue\nnormal\ntrue\nnot\nsystem\nnormal\nsystem\nnot\n200\n51\n33\n83\ntrue\nspam\ntrue\nnot\nsystem\nspam\nsystem\nnot\n268\n99\n99\n635\ntrue\nyes\ntrue\nno\nsystem\nyes\nsystem\nno\nprecision = 8+11\n8\n= .42\nprecision = 200+33\n200\n= .86\nprecision = 60+55\n60\n= .52\nmicroaverage\nprecision\n268+99\n268\n= .73\n=\nmacroaverage\nprecision\n3\n.42+.52+.86\n= .60\n=\nPooled\nClass 3: Spam\nClass 2: Normal\nClass 1: Urgent\nFigure 4.6\nSeparate contingency tables for the 3 classes from the previous ﬁgure, showing the pooled contin-\ngency table and the microaveraged and macroaveraged precision.\n4.8\nTest sets and Cross-validation\nThe training and testing procedure for text classiﬁcation follows what we saw with\nlanguage modeling (Section 3.2): we use the training set to train the model, then use\nthe development test set (also called a devset) to perhaps tune some parameters,\ndevelopment\ntest set\ndevset\nand in general decide what the best model is. Once we come up with what we think\nis the best model, we run it on the (hitherto unseen) test set to report its performance.\nWhile the use of a devset avoids overﬁtting the test set, having a ﬁxed training\nset, devset, and test set creates another problem: in order to save lots of data for\ntraining, the test set (or devset) might not be large enough to be representative. It\nwould be better if we could somehow use all our data both for training and test. We\ndo this by cross-validation: we randomly choose a training and test set division of\ncross-validation\nour data, train our classiﬁer, and then compute the error rate on the test set. Then\nwe repeat with a different randomly selected training set and test set. We do this\nsampling process 10 times and average these 10 runs to get an average error rate.\nThis is called 10-fold cross-validation.\n10-fold\ncross-validation\nThe only problem with cross-validation is that because all the data is used for\ntesting, we need the whole corpus to be blind; we can’t examine any of the data\nto suggest possible features and in general see what’s going on. But looking at the\ncorpus is often important for designing the system. For this reason, it is common",
  "83": "4.9\n•\nSTATISTICAL SIGNIFICANCE TESTING\n77\nto create a ﬁxed training set and test set, then do 10-fold cross-validation inside\nthe training set, but compute error rate the normal way in the test set, as shown in\nFig. 4.7.\nTraining Iterations\n1\n3\n4\n5\n2\n6\n7\n8\n9\n10\nDev\nDev\nDev\nDev\nDev\nDev\nDev\nDev\nDev\nDev\nTraining\nTraining\nTraining\nTraining\nTraining\nTraining\nTraining\nTraining\nTraining\nTraining\nTraining\nTest \nSet\nTesting\nFigure 4.7\n10-fold cross-validation\n4.9\nStatistical Signiﬁcance Testing\nIn building systems we are constantly comparing the performance of systems. Often\nwe have added some new bells and whistles to our algorithm and want to compare\nthe new version of the system to the unaugmented version. Or we want to compare\nour algorithm to a previously published one to know which is better.\nWe might imagine that to compare the performance of two classiﬁers A and B\nall we have to do is look at A and B’s score on the same test set—for example we\nmight choose to compare macro-averaged F1— and see whether it’s A or B that has\nthe higher score. But just looking at this one difference isn’t good enough, because\nA might have a better performance than B on a particular test set just by chance.\nLet’s say we have a test set x of n observations x = x1,x2,..,xn on which A’s\nperformance is better than B by δ(x). How can we know if A is really better than B?\nTo do so we’d need to reject the null hypothesis that A isn’t really better than B and\nnull hypothesis\nthis difference δ(x) occurred purely by chance. If the null hypothesis was correct,\nwe would expect that if we had many test sets of size n and we measured A and B’s\nperformance on all of them, that on average A might accidentally still be better than\nB by this amount δ(x) just by chance.\nMore formally, if we had a random variable X ranging over test sets, the null\nhypothesis H0 expects P(δ(X) > δ(x)|H0), the probability that we’ll see similarly\nbig differences just by chance, to be high.\nIf we had all these test sets we could just measure all the δ(x′) for all the x′. If we\nfound that those deltas didn’t seem to be bigger than δ(x), that is, that p-value(x) was\nsufﬁciently small, less than the standard thresholds of 0.05 or 0.01, then we might\nreject the null hypothesis and agree that δ(x) was a sufﬁciently surprising difference\nand A is really a better algorithm than B. Following Berg-Kirkpatrick et al. (2012)\nwe’ll refer to P(δ(X) > δ(x)|H0) as p-value(x).\nIn language processing we don’t generally use traditional statistical approaches\nlike paired t-tests to compare system outputs because most metrics are not normally",
  "84": "78\nCHAPTER 4\n•\nNAIVE BAYES AND SENTIMENT CLASSIFICATION\ndistributed, violating the assumptions of the tests. The standard approach to comput-\ning p-value(x) in natural language processing is to use non-parametric tests like the\nbootstrap test (Efron and Tibshirani, 1993)— which we will describe below—or a\nbootstrap test\nsimilar test, approximate randomization (Noreen, 1989). The advantage of these\napproximate\nrandomization\ntests is that they can apply to any metric; from precision, recall, or F1 to the BLEU\nmetric used in machine translation.\nThe word bootstrapping refers to repeatedly drawing large numbers of smaller\nbootstrapping\nsamples with replacement (called bootstrap samples) from an original larger sam-\nple. The intuition of the bootstrap test is that we can create many virtual test sets\nfrom an observed test set by repeatedly sampling from it. The method only makes\nthe assumption that the sample is representative of the population.\nConsider a tiny text classiﬁcation example with a test set x of 10 documents. The\nﬁrst row of Fig. 4.8 shows the results of two classiﬁers (A and B) on this test set,\nwith each document labeled by one of the four possibilities: (A and B both right,\nboth wrong, A right and B wrong, A wrong and B right); a slash through a letter\n(\u0013B) means that that classiﬁer got the answer wrong. On the ﬁrst document both A\nand B get the correct class (AB), while on the second document A got it right but B\ngot it wrong (A\u0013B). If we assume for simplicity that our metric is accuracy, A has an\naccuracy of .70 and B of .50, so δ(x) is .20. To create each virtual test set of size\nN = 10, we repeatedly (10 times) select a cell from row x with replacement. Fig. 4.8\nshows a few examples.\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10 A% B% δ()\nx\nAB A\u0013B AB \u0000AB A\u0013B \u0000AB A\u0013B AB \u0000A\u0013B A\u0013B\n.70 .50\n.20\nx∗(1) A\u0013B AB A\u0013B \u0000AB \u0000AB A\u0013B \u0000AB AB \u0000A\u0013B AB\n.60 .60\n.00\nx∗(2) A\u0013B AB \u0000A\u0013B \u0000AB \u0000AB AB \u0000AB A\u0013B AB AB\n.60 .70 -.10\n...\nx∗(b)\nFigure 4.8\nThe bootstrap: Examples of b pseudo test sets being created from an initial true\ntest set x. Each pseudo test set is created by sampling n = 10 times with replacement; thus an\nindividual sample is a single cell, a document with its gold label and the correct or incorrect\nperformance of classiﬁers A and B.\nNow that we have a sampling distribution, we can do statistics on how how often\nA has an accidental advantage. There are various ways to compute this advantage;\nhere we follow the version laid out in Berg-Kirkpatrick et al. (2012). We might\nthink that we should just ask, for each bootstrap sample x∗(i), whether A beats B\nby more than δ(x). But there’s a problem: we didn’t draw these samples from a\ndistribution with 0 mean. The x∗(i) were sampled from x, and so the expected value\nof δ(x∗(i)) lies very close to δ(x). That is, about half the time A will be better than\nB, so we expect A to beat B by δ(x). Instead, we want to know how often A beats\nthese expectations by more than δ(x). To correct for the expected success, we need\nto zero-center, subtracting δ(x) from each pseudo test set. Thus we’ll be comparing\nfor each x∗(i) whether δ(x∗(i)) > 2δ(x). The full algorithm for the bootstrap is shown\nin Fig. 4.9. It is given a test set x, a number of samples b, and counts the percentage\nof the b bootstrap test sets in which delta(x∗(i)) > 2δ(x). This percentage then\nacts as a one-sided empirical p-value (more sophisticated ways to get p-values from\nconﬁdence intervals also exist).",
  "85": "4.10\n•\nADVANCED: FEATURE SELECTION\n79\nfunction BOOTSTRAP(test set x, num of samples b) returns p-value(x)\nCalculate δ(x) # how much better does algorithm A do than B on x\nfor i = 1 to b do\nfor j = 1 to n do\n# Draw a bootstrap sample x∗(i) of size n\nSelect a member of x at random and add it to x∗(i)\nCalculate δ(x∗(i))\n# how much better does algorithm A do than B on x∗(i)\nfor each x∗(i)\ns←s + 1 if δ(x∗(i)) > 2δ(x)\np-value(x) ≈s\nb\n# on what % of the b samples did algorithm A beat expectations?\nreturn p-value(x)\nFigure 4.9\nA version of the bootstrap algorithm after Berg-Kirkpatrick et al. (2012).\n4.10\nAdvanced: Feature Selection\nThe regularization technique introduced in the previous section is feature selection\nFeature\nselection\nis a method of removing features that are unlikely to generalize well. The basis\nof feature selection is to assign some metric of goodness to each feature, rank the\nfeatures, and keep the best ones. The number of features to keep is a meta-parameter\nthat can be optimized on a dev set.\nFeatures are generally ranked by how informative they are about the classiﬁca-\ntion decision. A very common metric is information gain. Information gain tells\ninformation\ngain\nus how many bits of information the presence of the word gives us for guessing the\nclass, and can be computed as follows (where ci is the ith class and ¯w means that a\ndocument does not contain the word w):\nG(w) =\n−\nC\nX\ni=1\nP(ci)logP(ci)\n+P(w)\nC\nX\ni=1\nP(ci|w)logP(ci|w)\n+P( ¯w)\nC\nX\ni=1\nP(ci| ¯w)logP(ci| ¯w)\n(4.19)\n4.11\nSummary\nThis chapter introduced the naive Bayes model for classiﬁcation and applied it to\nthe text categorization task of sentiment analysis.\n• Many language processing tasks can be viewed as tasks of classiﬁcation.\nlearn to model the class given the observation.\n• Text categorization, in which an entire text is assigned a class from a ﬁnite set,\nincludes such tasks as sentiment analysis, spam detection, language identi-\nﬁcation, and authorship attribution.\n• Sentiment analysis classiﬁes a text as reﬂecting the positive or negative orien-\ntation (sentiment) that a writer expresses toward some object.",
  "86": "80\nCHAPTER 4\n•\nNAIVE BAYES AND SENTIMENT CLASSIFICATION\n• Naive Bayes is a generative model that make the bag of words assumption\n(position doesn’t matter) and the conditional independence assumption (words\nare conditionally independent of each other given the class)\n• Naive Bayes with binarized features seems to work better for many text clas-\nsiﬁcation tasks.\n• Feature selection can be used to automatically remove features that aren’t\nhelpful.\n• Classiﬁers are evaluated based on precision and recall.\n• Classiﬁers are trained using distinct training, dev, and test sets, including the\nuse of cross-validation in the training set.\nBibliographical and Historical Notes\nMultinomial naive Bayes text classiﬁcation was proposed by Maron (1961) at the\nRAND Corporation for the task of assigning subject categories to journal abstracts.\nHis model introduced most of the features of the modern form presented here, ap-\nproximating the classiﬁcation task with one-of categorization, and implementing\nadd-δ smoothing and information-based feature selection.\nThe conditional independence assumptions of naive Bayes and the idea of Bayes-\nian analysis of text seem to have been arisen multiple times. The same year as\nMaron’s paper, Minsky (1961) proposed a naive Bayes classiﬁer for vision and other\nartiﬁcial intelligence problems, and Bayesian techniques were also applied to the\ntext classiﬁcation task of authorship attribution by Mosteller and Wallace (1963). It\nhad long been known that Alexander Hamilton, John Jay, and James Madison wrote\nthe anonymously-published Federalist papers. in 1787–1788 to persuade New York\nto ratify the United States Constitution. Yet although some of the 85 essays were\nclearly attributable to one author or another, the authorship of 12 were in dispute\nbetween Hamilton and Madison. Mosteller and Wallace (1963) trained a Bayesian\nprobabilistic model of the writing of Hamilton and another model on the writings\nof Madison, then computed the maximum-likelihood author for each of the disputed\nessays. Naive Bayes was ﬁrst applied to spam detection in Heckerman et al. (1998).\nMetsis et al. (2006), Pang et al. (2002), and Wang and Manning (2012) show\nthat using boolean attributes with multinomial naive Bayes works better than full\ncounts. Binary multinomial naive Bayes is sometimes confused with another variant\nof naive Bayes that also use a binary representation of whether a term occurs in\na document: Multivariate Bernoulli naive Bayes. The Bernoulli variant instead\nestimates P(w|c) as the fraction of documents that contain a term, and includes a\nprobability for whether a term is not in a document. McCallum and Nigam (1998)\nand Wang and Manning (2012) show that the multivariate Bernoulli variant of naive\nBayes doesn’t work as well as the multinomial algorithm for sentiment or other text\ntasks.\nThere are a variety of sources covering the many kinds of text classiﬁcation\ntasks. For sentiment analysis see Pang and Lee (2008), and Liu and Zhang (2012).\nStamatatos (2009) surveys authorship attribute algorithms. On language identiﬁca-\ntion see Jauhiainen et al. (2018); Jaech et al. (2016) is an important early neural\nsystem. The task of newswire indexing was often used as a test case for text classi-\nﬁcation algorithms, based on the Reuters-21578 collection of newswire articles.\nSee Manning et al. (2008) and Aggarwal and Zhai (2012) on text classiﬁcation;\nclassiﬁcation in general is covered in machine learning textbooks (Hastie et al. 2001,",
  "87": "EXERCISES\n81\nWitten and Frank 2005, Bishop 2006, Murphy 2012).\nNon-parametric methods for computing statistical signiﬁcance were used ﬁrst in\nNLP in the MUC competition (Chinchor et al., 1993), and even earlier in speech\nrecognition (Gillick and Cox 1989, Bisani and Ney 2004). Our description of the\nbootstrap draws on the description in Berg-Kirkpatrick et al. (2012). Recent work\nhas focused on issues including multiple test sets and multiple metrics (Søgaard\net al. 2014, Dror et al. 2017).\nMetrics besides information gain for feature selection include χ2, pointwise mu-\ntual information, and GINI index; see Yang and Pedersen (1997) for a comparison\nand Guyon and Elisseeff (2003) for a broad introduction survey of feature selection.\nExercises\n4.1\nAssume the following likelihoods for each word being part of a positive or\nnegative movie review, and equal prior probabilities for each class.\npos\nneg\nI\n0.09 0.16\nalways 0.07 0.06\nlike\n0.29 0.06\nforeign 0.04 0.15\nﬁlms\n0.08 0.11\nWhat class will Naive bayes assign to the sentence “I always like foreign\nﬁlms.”?\n4.2\nGiven the following short movie reviews, each labeled with a genre, either\ncomedy or action:\n1. fun, couple, love, love\ncomedy\n2. fast, furious, shoot\naction\n3. couple, ﬂy, fast, fun, fun\ncomedy\n4. furious, shoot, shoot, fun\naction\n5. ﬂy, fast, shoot, love\naction\nand a new document D:\nfast, couple, shoot, ﬂy\ncompute the most likely class for D. Assume a naive Bayes classiﬁer and use\nadd-1 smoothing for the likelihoods.\n4.3\nTrain two models, multinominal naive Bayes and binarized naive Bayes, both\nwith add-1 smoothing, on the following document counts for key sentiment\nwords, with positive or negative class assigned as noted.\ndoc “good” “poor” “great” (class)\nd1. 3\n0\n3\npos\nd2. 0\n1\n2\npos\nd3. 1\n3\n0\nneg\nd4. 1\n5\n2\nneg\nd5. 0\n2\n0\nneg\nUse both naive Bayes models to assign a class (pos or neg) to this sentence:\nA good, good plot and great characters, but poor acting.\nDo the two models agree or disagree?",
  "88": "82\nCHAPTER 5\n•\nLOGISTIC REGRESSION\nCHAPTER\n5\nLogistic Regression\n”And how do you know that these ﬁne begonias are not of equal importance?”\nHercule Poirot, in Agatha Christie’s The Mysterious Affair at Styles\nDetective stories are as littered with clues as texts are with words. Yet for the\npoor reader it can be challenging to know how to weigh the author’s clues in order\nto make the crucial classiﬁcation task: deciding whodunnit.\nIn this chapter we introduce an algorithm that is admirably suited for discovering\nthe link between features or cues and some particular outcome: logistic regression.\nlogistic\nregression\nIndeed, logistic regression is one of the most important analytic tool in the social and\nnatural sciences. In natural language processing, logistic regression is the baseline\nsupervised machine learning algorithm for classiﬁcation, and also has a very close\nrelationship with neural networks. As we will see in Chapter 7, a neural network can\nbe viewed as a series of logistic regression classiﬁers stacked on top of each other.\nThus the classiﬁcation and machine learning techniques introduced here will play\nan important role throughout the book.\nLogistic regression can be used to classify an observation into one of two classes\n(like ‘positive sentiment’ and ‘negative sentiment’), or into one of many classes.\nBecause the mathematics for the two-class case is simpler, we’ll describe this special\ncase of logistic regression ﬁrst in the next few sections, and then brieﬂy summarize\nthe use of multinomial logistic regression for more than two classes in Section 5.6.\nWe’ll introduce the mathematics of logistic regression in the next few sections.\nBut let’s begin with some high-level issues.\nGenerative and Discriminative Classiﬁers:\nThe most important difference be-\ntween naive Bayes and logistic regression is that logistic regression is a discrimina-\ntive classiﬁer while naive Bayes is a generative classiﬁer.\nThese are two very different frameworks for how\nto build a machine learning model. Consider a visual\nmetaphor: imagine we’re trying to distinguish dog\nimages from cat images. A generative model would\nhave the goal of understanding what dogs look like\nand what cats look like. You might literally ask such\na model to ‘generate’, i.e. draw, a dog. Given a test\nimage, the system then asks whether it’s the cat model or the dog model that better\nﬁts (is less surprised by) the image, and chooses that as its label.\nA discriminative model, by contrast, is only try-\ning to learn to distinguish the classes (perhaps with-\nout learning much about them). So maybe all the\ndogs in the training data are wearing collars and the\ncats aren’t. If that one feature neatly separates the\nclasses, the model is satisﬁed.\nIf you ask such a\nmodel what it knows about cats all it can say is that\nthey don’t wear collars.",
  "89": "5.1\n•\nCLASSIFICATION: THE SIGMOID\n83\nMore formally, recall that the naive Bayes assigns a class c to a document d not\nby directly computing P(c|d) but by computing a likelihood and a prior\nˆc = argmax\nc∈C\nlikelihood\nz }| {\nP(d|c)\nprior\nz}|{\nP(c)\n(5.1)\nA generative model like naive Bayes makes use of this likelihood term, which\ngenerative\nmodel\nexpresses how to generate the features of a document if we knew it was of class c.\nBy contrast a discriminative model in this text categorization scenario attempts\ndiscriminative\nmodel\nto directly compute P(c|d). Perhaps it will learn to assign high weight to document\nfeatures that directly improve its ability to discriminate between possible classes,\neven if it couldn’t generate an example of one of the classes.\nComponents of a probabilistic machine learning classiﬁer:\nLike naive Bayes,\nlogistic regression is a probabilistic classiﬁer that makes use of supervised machine\nlearning. Machine learning classiﬁers require a training corpus of M observations\ninput/output pairs (x(i),y(i)). (We’ll use superscripts in parentheses to refer to indi-\nvidual instances in the training set—for sentiment classiﬁcation each instance might\nbe an individual document to be classiﬁed). A machine learning system for classiﬁ-\ncation then has four components:\n1. A feature representation of the input. For each input observation x(i), this\nwill be a vector of features [x1,x2,...,xn]. We will generally refer to feature\ni for input x(j) as x(j)\ni , sometimes simpliﬁed as xi, but we will also see the\nnotation fi, fi(x), or, for multiclass classiﬁcation, fi(c,x).\n2. A classiﬁcation function that computes ˆy, the estimated class, via p(y|x). In\nthe next section we will introduce the sigmoid and softmax tools for classiﬁ-\ncation.\n3. An objective function for learning, usually involving minimizing error on\ntraining examples. We will introduce the cross-entropy loss function\n4. An algorithm for optimizing the objective function. We introduce the stochas-\ntic gradient descent algorithm.\nLogistic regression has two phases:\ntraining: we train the system (speciﬁcally the weights w and b) using stochastic\ngradient descent and the cross-entropy loss.\ntest: Given a test example x we compute p(y|x) and return the higher probability\nlabel y = 1 or y = 0.\n5.1\nClassiﬁcation: the sigmoid\nThe goal of binary logistic regression is to train a classiﬁer that can make a binary\ndecision about the class of a new input observation. Here we introduce the sigmoid\nclassiﬁer that will help us make this decision.\nConsider a single input observation x, which we will represent by a vector of\nfeatures [x1,x2,...,xn] (we’ll show sample features in the next subsection). The clas-\nsiﬁer output y can be 1 (meaning the observation is a member of the class) or 0\n(the observation is not a member of the class). We want to know the probability\nP(y = 1|x) that this observation is a member of the class. So perhaps the decision",
  "90": "84\nCHAPTER 5\n•\nLOGISTIC REGRESSION\nis “positive sentiment” versus “negative sentiment”, the features represent counts\nof words in a document, and P(y = 1|x) is the probability that the document has\npositive sentiment, while and P(y = 0|x) is the probability that the document has\nnegative sentiment.\nLogistic regression solves this task by learning, from a training set, a vector of\nweights and a bias term. Each weight wi is a real number, and is associated with one\nof the input features xi. The weight wi represents how important that input feature is\nto the classiﬁcation decision, and can be positive (meaning the feature is associated\nwith the class) or negative (meaning the feature is not associated with the class).\nThus we might expect in a sentiment task the word awesome to have a high positive\nweight, and abysmal to have a very negative weight. The bias term, also called the\nbias term\nintercept, is another real number that’s added to the weighted inputs.\nintercept\nTo make a decision on a test instance— after we’ve learned the weights in\ntraining— the classiﬁer ﬁrst multiplies each xi by its weight wi, sums up the weighted\nfeatures, and adds the bias term b. The resulting single number z expresses the\nweighted sum of the evidence for the class.\nz =\n n\nX\ni=1\nwixi\n!\n+b\n(5.2)\nIn the rest of the book we’ll represent such sums using the dot product notation from\ndot product\nlinear algebra. The dot product of two vectors a and b, written as a·b is the sum of\nthe products of the corresponding elements of each vector. Thus the following is an\nequivalent formation to Eq. 5.2:\nz = w·x+b\n(5.3)\nBut note that nothing in Eq. 5.3 forces z to be a legal probability, that is, to lie\nbetween 0 and 1. In fact, since weights are real-valued, the output might even be\nnegative; z ranges from −∞to ∞.\nFigure 5.1\nThe sigmoid function y =\n1\n1+e−z takes a real value and maps it to the range [0,1].\nBecause it is nearly linear around 0 but has a sharp slope toward the ends, it tends to squash\noutlier values toward 0 or 1.\nTo create a probability, we’ll pass z through the sigmoid function, σ(z). The\nsigmoid\nsigmoid function (named because it looks like an s) is also called the logistic func-\ntion, and gives logistic regression its name. The sigmoid has the following equation,\nlogistic\nfunction\nshown graphically in Fig. 5.1:\ny = σ(z) =\n1\n1+e−z\n(5.4)",
  "91": "5.1\n•\nCLASSIFICATION: THE SIGMOID\n85\nThe sigmoid has a number of advantages; it take a real-valued number and maps\nit into the range [0,1], which is just what we want for a probability. Because it is\nnearly linear around 0 but has a sharp slope toward the ends, it tends to squash outlier\nvalues toward 0 or 1. And it’s differentiable, which as we’ll see in Section 5.8 will\nbe handy for learning.\nWe’re almost there. If we apply the sigmoid to the sum of the weighted features,\nwe get a number between 0 and 1. To make it a probability, we just need to make\nsure that the two cases, p(y = 1) and p(y = 0), sum to 1. We can do this as follows:\nP(y = 1) = σ(w·x+b)\n=\n1\n1+e−(w·x+b)\nP(y = 0) = 1−σ(w·x+b)\n= 1−\n1\n1+e−(w·x+b)\n=\ne−(w·x+b)\n1+e−(w·x+b)\n(5.5)\nNow we have an algorithm that given an instance x computes the probability\nP(y = 1|x). How do we make a decision? For a test instance x, we say yes if the\nprobability P(y = 1|x) is more than .5, and no otherwise. We call .5 the decision\nboundary:\ndecision\nboundary\nˆy =\n\u001a 1 if P(y = 1|x) > 0.5\n0 otherwise\n5.1.1\nExample: sentiment classiﬁcation\nLet’s have an example. Suppose we are doing binary sentiment classiﬁcation on\nmovie review text, and we would like to know whether to assign the sentiment class\n+ or −to a review document doc. We’ll represent each input observation by the\nfollowing 6 features x1...x6 of the input; Fig. 5.2 shows the features in a sample mini\ntest document.\nVar\nDeﬁnition\nValue in Fig. 5.2\nx1\ncount(positive lexicon) ∈doc)\n3\nx2\ncount(negative lexicon) ∈doc)\n2\nx3\n\u001a 1 if “no” ∈doc\n0 otherwise\n1\nx4\ncount(1st and 2nd pronouns ∈doc)\n3\nx5\n\u001a\n1 if “!” ∈doc\n0 otherwise\n0\nx6\nlog(word count of doc)\nln(64) = 4.15\nLet’s assume for the moment that we’ve already learned a real-valued weight\nfor each of these features, and that the 6 weights corresponding to the 6 features\nare [2.5,−5.0,−1.2,0.5,2.0,0.7], while b = 0.1. (We’ll discuss in the next section\nhow the weights are learned.) The weight w1, for example indicates how important",
  "92": "86\nCHAPTER 5\n•\nLOGISTIC REGRESSION\nIt's hokey. There are virtually no surprises , and the writing is second-rate . \nSo why was it so enjoyable? For one thing , the cast is\ngreat . Another nice touch is the music . I was overcome with the urge to get off\nthe couch and start dancing .  It sucked me in , and it'll do the same to you .\nx1=3\nx6=4.15\nx3=1\nx4=3\nx5=0\nx2=2\nFigure 5.2\nA sample mini test document showing the extracted features in the vector x.\na feature the number of positive lexicon words (great, nice, enjoyable, etc.) is to\na positive sentiment decision, while w2 tells us the importance of negative lexicon\nwords. Note that w1 = 2.5 is positive, while w2 = −5.0, meaning that negative words\nare negatively associated with a positive sentiment decision, and are about twice as\nimportant as positive words.\nGiven these 6 features and the input review x, P(+|x) and P(−|x) can be com-\nputed using Eq. 5.5:\np(+|x) = P(Y = 1|x) = σ(w·x+b)\n= σ([2.5,−5.0,−1.2,0.5,2.0,0.7]·[3,2,1,3,0,4.15]+0.1)\n= σ(1.805)\n= 0.86\np(−|x) = P(Y = 0|x) = 1−σ(w·x+b)\n= 0.14\nLogistic regression is commonly applied to all sorts of NLP tasks, and any prop-\nerty of the input can be a feature. Consider the task of period disambiguation:\ndeciding if a period is the end of a sentence or part of a word, by classifying each\nperiod into one of two classes EOS (end-of-sentence) and not-EOS. We might use\nfeatures like x1 below expressing that the current word is lower case and the class\nis EOS (perhaps with a positive weight), or that the current word is in our abbrevia-\ntions dictionary (“Prof.”) and the class is EOS (perhaps with a negative weight). A\nfeature can also express a quite complex combination of properties. For example a\nperiod following a upper cased word is a likely to be an EOS, but if the word itself is\nSt. and the previous word is capitalized, then the period is likely part of a shortening\nof the word street.\nx1 =\n\u001a 1 if “Case(wi) = Lower”\n0 otherwise\nx2 =\n\u001a 1 if “wi ∈AcronymDict”\n0 otherwise\nx3 =\n\u001a 1 if “wi = St. & Case(wi−1) = Cap”\n0 otherwise\nDesigning features:\nFeatures are generally designed by examining the training\nset with an eye to linguistic intuitions and the linguistic literature on the domain. A\ncareful error analysis on the training or dev set. of an early version of a system often\nprovides insights into features.",
  "93": "5.2\n•\nLEARNING IN LOGISTIC REGRESSION\n87\nFor some tasks it is especially helpful to build complex features that are combi-\nnations of more primitive features. We saw such a feature for period disambiguation\nabove, where a period on the word St. was less likely to be the end of sentence if\nthe previous word was capitalized. For logistic regression and naive Bayes these\ncombination features or feature interactions have to be designed by hand.\nfeature\ninteractions\nFor many tasks (especially when feature values can reference speciﬁc words)\nwe’ll need large numbers of features. Often these are created automatically via fea-\nture templates, abstract speciﬁcations of features. For example a bigram template\nfeature\ntemplates\nfor period disambiguation might create a feature for every pair of words that occurs\nbefore a period in the training set. Thus the feature space is sparse, since we only\nhave to create a feature if that n-gram exists in that position in the training set. The\nfeature is generally created as a hash from the string descriptions. A user description\nof a feature as, “bigram(American breakfast)” is hashed into a unique integer i that\nbecomes the feature number fi.\nIn order to avoid the extensive human effort of feature design, recent research in\nNLP has focused on representation learning: ways to learn features automatically\nin an unsupervised way from the input. We’ll introduce methods for representation\nlearning in Chapter 6 and Chapter 7.\nChoosing a classiﬁer\nLogistic regression has a number of advantages over naive\nBayes. Naive Bayes has overly strong conditional independence assumptions. Con-\nsider two features which are strongly correlated; in fact, imagine that we just add the\nsame feature f1 twice. Naive Bayes will treat both copies of f1 as if they were sep-\narate, multiplying them both in, overestimating the evidence. By contrast, logistic\nregression is much more robust to correlated features; if two features f1 and f2 are\nperfectly correlated, regression will simply assign part of the weight to w1 and part\nto w2. Thus when there are many correlated features, logistic regression will assign\na more accurate probability than naive Bayes. So logistic regression generally works\nbetter on larger documents or datasets and is a common default.\nDespite the less accurate probabilities, naive Bayes still often makes the correct\nclassiﬁcation decision. Furthermore, naive Bayes works extremely well (even bet-\nter than logistic regression) on very small datasets (Ng and Jordan, 2002) or short\ndocuments (Wang and Manning, 2012). Furthermore, naive Bayes is easy to imple-\nment and very fast to train (there’s no optimization step). So it’s still a reasonable\napproach to use in some situations.\n5.2\nLearning in Logistic Regression\nHow are the parameters of the model, the weights w and bias b, learned?\nLogistic regression is an instance of supervised classiﬁcation in which we know\nthe correct label y (either 0 or 1) for each observation x. What the system produces,\nvia Eq. 5.5 is ˆy, the system’s estimate of the true y. We want to learn parameters\n(meaning w and b) that make ˆy for each training observation as close as possible to\nthe true y .\nThis requires 2 components that we foreshadowed in the introduction to the\nchapter. The ﬁrst is a metric for how close the current label (ˆy) is to the true gold\nlabel y. Rather than measure similarity, we usually talk about the opposite of this:\nthe distance between the system output and the gold output, and we call this distance\nthe loss function or the cost function. In the next section we’ll introduce the loss\nloss",
  "94": "88\nCHAPTER 5\n•\nLOGISTIC REGRESSION\nfunction that is commonly used for logistic regression and also for neural networks,\nthe cross-entropy loss.\nThe second thing we need is an optimization algorithm for iteratively updating\nthe weights so as to minimize this loss function. The standard algorithm for this is\ngradient descent; we’ll introduce the stochastic gradient descent algorithm in the\nfollowing section.\n5.3\nThe cross-entropy loss function\nWe need a loss function that expresses, for an observation x, how close the classiﬁer\noutput (ˆy = σ(w·x+b)) is to the correct output (y, which is 0 or 1). We’ll call this:\nL(ˆy,y) = How much ˆy differs from the true y\n(5.6)\nYou could imagine using a simple loss function that just takes the mean squared\nerror between ˆy and y.\nLMSE(ˆy,y) = 1\n2(ˆy−y)2\n(5.7)\nIt turns out that this MSE loss, which is very useful for some algorithms like\nlinear regression, becomes harder to optimize (technically, non-convex), when it’s\napplied to probabilistic classiﬁcation.\nInstead, we use a loss function that prefers the correct class labels of the training\nexample to be more likely. This is called conditional maximum likelihood estima-\ntion: we choose the parameters w,b that maximize the log probability of the true\ny labels in the training data given the observations x. The resulting loss function\nis the negative log likelihood loss, generally called the cross entropy loss.\ncross entropy\nloss\nLet’s derive this loss function, applied to a single observation x. We’d like to\nlearn weights that maximize the probability of the correct label p(y|x). Since there\nare only two discrete outcomes (1 or 0), this is a Bernoulli distribution, and we can\nexpress the probability p(y|x) that our classiﬁer produces for one observation as\nthe following (keeping in mind that if y=1, Eq. 5.8 simpliﬁes to ˆy; if y=0, Eq. 5.8\nsimpliﬁes to 1−ˆy):\np(y|x) = ˆyy (1−ˆy)1−y\n(5.8)\nNow we take the log of both sides. This will turn out to be handy mathematically,\nand doesn’t hurt us; whatever values maximize a probability will also maximize the\nlog of the probability:\nlog p(y|x) = log\n\u0002\nˆyy (1−ˆy)1−y\u0003\n= ylog ˆy+(1−y)log(1−ˆy)\n(5.9)\nEq. 5.9 describes a log likelihood that should be maximized. In order to turn this\ninto loss function (something that we need to minimize), we’ll just ﬂip the sign on\nEq. 5.9. The result is the cross-entropy loss LCE:\nLCE(ˆy,y) = −log p(y|x) = −[ylog ˆy+(1−y)log(1−ˆy)]\n(5.10)\nFinally, we can plug in the deﬁnition of ˆy = σ(w·x)+b:\nLCE(w,b) = −[ylogσ(w·x+b)+(1−y)log(1−σ(w·x+b))]\n(5.11)",
  "95": "5.4\n•\nGRADIENT DESCENT\n89\nWhy does minimizing this negative log probability do what we want? A perfect\nclassiﬁer would assign probability 1 to the correct outcome (y=1 or y=0) and prob-\nability 0 to the incorrect outcome. That means the higher ˆy (the closer it is to 1), the\nbetter the classiﬁer; the lower ˆy is (the closer it is to 0), the worse the classiﬁer. The\nnegative log of this probability is a convenient loss metric since it goes from 0 (neg-\native log of 1, no loss) to inﬁnity (negative log of 0, inﬁnite loss). This loss function\nalso insures that as probability of the correct answer is maximized, the probability\nof the incorrect answer is minimized; since the two sum to one, any increase in the\nprobability of the correct answer is coming at the expense of the incorrect answer.\nIt’s called the cross-entropy loss, because Eq. 5.9 is also the formula for the cross-\nentropy between the true probability distribution y and our estimated distribution\nˆy.\nLet’s now extend Eq. 5.10 from one example to the whole training set: we’ll con-\ntinue to use the notation that x(i) and y(i) mean the ith training features and training\nlabel, respectively. We make the assumption that the training examples are indepen-\ndent:\nlog p(training labels) = log\nm\nY\ni=1\np(y(i)|x(i))\n(5.12)\n=\nm\nX\ni=1\nlog p(y(i)|x(i))\n(5.13)\n= −\nm\nX\ni=1\nLCE(ˆy(i),y(i))\n(5.14)\nWe’ll deﬁne the cost function for the whole dataset as the average loss for each\nexample:\nCost(w,b) =\n1\nm\nm\nX\ni=1\nLCE(ˆy(i),y(i))\n= −1\nm\nm\nX\ni=1\ny(i) logσ(w·x(i) +b)+(1−y(i))log\n\u0010\n1−σ(w·x(i) +b)\n\u0011\n(5.15)\nNow we know what we want to minimize; in the next section, we’ll see how to\nﬁnd the minimum.\n5.4\nGradient Descent\nOur goal with gradient descent is to ﬁnd the optimal weights: minimize the loss\nfunction we’ve deﬁned for the model. In Eq. 5.16 below, we’ll explicitly represent\nthe fact that the loss function L is parameterized by the weights, which we’ll refer to\nin machine learning in general as θ (in the case of logistic regression θ = w,b):\nˆθ = argmin\nθ\n1\nm\nm\nX\ni=1\nLCE(y(i),x(i);θ)\n(5.16)",
  "96": "90\nCHAPTER 5\n•\nLOGISTIC REGRESSION\nHow shall we ﬁnd the minimum of this (or any) loss function? Gradient descent\nis a method that ﬁnds a minimum of a function by ﬁguring out in which direction\n(in the space of the parameters θ) the function’s slope is rising the most steeply,\nand moving in the opposite direction. The intuition is that if you are hiking in a\ncanyon and trying to descend most quickly down to the river at the bottom, you might\nlook around yourself 360 degrees, ﬁnd the direction where the ground is sloping the\nsteepest, and walk downhill in that direction.\nFor logistic regression, this loss function is conveniently convex. A convex func-\nconvex\ntion has just one minimum; there are no local minima to get stuck in, so gradient\ndescent starting from any point is guaranteed to ﬁnd the minimum.\nAlthough the algorithm (and the concept of gradient) are designed for direction\nvectors, let’s ﬁrst consider a visualization of the the case where the parameter of our\nsystem, is just a single scalar w, shown in Fig. 5.3.\nGiven a random initialization of w at some value w1, and assuming the loss\nfunction L happened to have the shape in Fig. 5.3, we need the algorithm to tell us\nwhether at the next iteration, we should move left (making w2 smaller than w1) or\nright (making w2 bigger than w1) to reach the minimum.\nw\nLoss\n0\nw1\nwmin\nslope of loss at w1 \nis negative\n(goal)\none step\nof gradient\ndescent\nFigure 5.3\nThe ﬁrst step in iteratively ﬁnding the minimum of this loss function, by moving\nw in the reverse direction from the slope of the function. Since the slope is negative, we need\nto move w in a positive direction, to the right. Here superscripts are used for learning steps,\nso w1 means the initial value of w (which is 0), w2 at the second step, and so on.\nThe gradient descent algorithm answers this question by ﬁnding the gradient\ngradient\nof the loss function at the current point and moving in the opposite direction. The\ngradient of a function of many variables is a vector pointing in the direction the\ngreatest increase in a function. The gradient is a multi-variable generalization of the\nslope, so for a function of one variable like the one in Fig. 5.3, we can informally\nthink of the gradient as the slope. The dotted line in Fig. 5.3 shows the slope of this\nhypothetical loss function at point w = w1. You can see that the slope of this dotted\nline is negative. Thus to ﬁnd the minimum, gradient descent tells us to go in the\nopposite direction: moving w in a positive direction.\nThe magnitude of the amount to move in gradient descent is the value of the slope\nd\ndw f(x;w) weighted by a learning rate η. A higher (faster) learning rate means that\nlearning rate\nwe should move w more on each step. The change we make in our parameter is the\nlearning rate times the gradient (or the slope, in our single-variable example):\nwt+1 = wt −η d\ndw f(x;w)\n(5.17)\nNow let’s extend the intuition from a function of one scalar variable w to many",
  "97": "5.4\n•\nGRADIENT DESCENT\n91\nvariables, because we don’t just want to move left or right, we want to know where\nin the N-dimensional space (of the N parameters that make up θ) we should move.\nThe gradient is just such a vector; it expresses the directional components of the\nsharpest slope along each of those N dimensions. If we’re just imagining two weight\ndimension (say for one weight w and one bias b), the gradient might be a vector with\ntwo orthogonal components, each of which tells us how much the ground slopes in\nthe w dimension and in the b dimension. Fig. 5.4 shows a visualization:\nCost(w,b)\nw\nb\nFigure 5.4\nVisualization of the gradient vector in two dimensions w and b.\nIn an actual logistic regression, the parameter vector w is much longer than 1 or\n2, since the input feature vector x can be quite long, and we need a weight wi for\neach xi For each dimension/variable wi in w (plus the bias b), the gradient will have\na component that tells us the slope with respect to that variable. Essentially we’re\nasking: “How much would a small change in that variable wi inﬂuence the total loss\nfunction L?”\nIn each dimension wi, we express the slope as a partial derivative\n∂\n∂wi of the loss\nfunction. The gradient is then deﬁned as a vector of these partials. We’ll represent ˆy\nas f(x;θ) to make the dependence on θ more obvious:\n∇θL(f(x;θ),y)) =\n\n\n∂\n∂w1 L(f(x;θ),y)\n∂\n∂w2 L(f(x;θ),y)\n...\n∂\n∂wn L(f(x;θ),y)\n\n\n(5.18)\nThe ﬁnal equation for updating θ based on the gradient is thus\nθt+1 = θt −η∇L(f(x;θ),y)\n(5.19)\n5.4.1\nThe Gradient for Logistic Regression\nIn order to update θ, we need a deﬁnition for the gradient ∇L(f(x;θ),y). Recall that\nfor logistic regression, the cross-entropy loss function is:\nLCE(w,b) = −[ylogσ(w·x+b)+(1−y)log(1−σ(w·x+b))]\n(5.20)\nIt turns out that the derivative of this function for one observation vector x is\nEq. 5.21 (the interested reader can see Section 5.8 for the derivation of this equation):\n∂LCE(w,b)\n∂w j\n= [σ(w·x+b)−y]xj\n(5.21)",
  "98": "92\nCHAPTER 5\n•\nLOGISTIC REGRESSION\nNote in Eq. 5.21 that the gradient with respect to a single weight w j represents a\nvery intuitive value: the difference between the true y and our estimated ˆy = σ(w ·\nx+b) for that observation, multiplied by the corresponding input value xj.\nThe loss for a batch of data or an entire dataset is just the average loss over the\nm examples:\nCost(w,b) = −1\nm\nm\nX\ni=1\ny(i) logσ(w·x(i) +b)+(1−y(i))log\n\u0010\n1−σ(w·x(i) +b)\n\u0011\n(5.22)\nAnd the gradient for multiple data points is the sum of the individual gradients::\n∂Cost(w,b)\n∂wj\n=\nm\nX\ni=1\nh\nσ(w·x(i) +b)−y(i)i\nx(i)\nj\n(5.23)\n5.4.2\nThe Stochastic Gradient Descent Algorithm\nStochastic gradient descent is an online algorithm that minimizes the loss function\nby computing its gradient after each training example, and nudging θ in the right\ndirection (the opposite direction of the gradient). Fig. 5.5 shows the algorithm.\nfunction STOCHASTIC GRADIENT DESCENT(L(), f(), x, y) returns θ\n# where: L is the loss function\n#\nf is a function parameterized by θ\n#\nx is the set of training inputs x(1), x(2),..., x(n)\n#\ny is the set of training outputs (labels) y(1), y(2),..., y(n)\nθ ←0\nrepeat T times\nFor each training tuple (x(i), y(i)) (in random order)\nCompute ˆy(i) =\nf(x(i);θ)\n# What is our estimated output ˆy?\nCompute the loss L(ˆy(i),y(i)) # How far off is ˆy(i)) from the true output y(i)?\ng←∇θL(f(x(i);θ),y(i))\n# How should we move θ to maximize loss ?\nθ ←θ −η g\n# go the other way instead\nreturn θ\nFigure 5.5\nThe stochastic gradient descent algorithm\nStochastic gradient descent is called stochastic because it chooses a single ran-\ndom example at a time, moving the weights so as to improve performance on that\nsingle example. That can result in very choppy movements, so it’s also common to\ndo minibatch gradient descent, which computes the gradient over batches of train-\nminibatch\ning instances rather than a single instance.\nThe learning rate η is a parameter that must be adjusted. If it’s too high, the\nlearner will take steps that are too large, overshooting the minimum of the loss func-\ntion. If it’s too low, the learner will take steps that are too small, and take too long to\nget to the minimum. It is most common to begin the learning rate at a higher value,\nand then slowly decrease it, so that it is a function of the iteration k of training; you\nwill sometimes see the notation ηk to mean the value of the learning rate at iteration\nk.",
  "99": "5.5\n•\nREGULARIZATION\n93\n5.4.3\nWorking through an example\nLet’s walk though a single step of the gradient descent algorithm. We’ll use a sim-\npliﬁed version of the example in Fig. 5.2 as it sees a single observation x, whose\ncorrect value is y = 1 (this is a positive review), and with only two features:\nx1 = 3\n(count of positive lexicon words)\nx2 = 2\n(count of negative lexicon words)\nLet’s assume the initial weights and bias in θ 0 are all set to 0, and the initial learning\nrate η is 0.1:\nw1 = w2 = b = 0\nη = 0.1\nThe single update step requires that we compute the gradient, multiplied by the\nlearning rate\nθt+1 = θt −η∇θL(f(x(i);θ),y(i))\nIn our mini example there are three parameters, so the gradient vector has 3 dimen-\nsions, for w1, w2, and b. We can compute the ﬁrst gradient as follows:\n∇w,b =\n\n\n∂LCE(w,b)\n∂w1\n∂LCE(w,b)\n∂w2\n∂LCE(w,b)\n∂b\n\n=\n\n\n(σ(w·x+b)−y)x1\n(σ(w·x+b)−y)x2\nσ(w·x+b)−y\n\n=\n\n\n(σ(0)−1)x1\n(σ(0)−1)x2\nσ(0)−1\n\n=\n\n\n−0.5x1\n−0.5x2\n−0.5\n\n=\n\n\n−1.5\n−1.0\n−0.5\n\n\nNow that we have a gradient, we compute the new parameter vector θ 2 by mov-\ning θ 1 in the opposite direction from the gradient:\nθ 2 =\n\n\nw1\nw2\nb\n\n−η\n\n\n−1.5\n−1.0\n−0.5\n\n=\n\n\n.15\n.1\n.05\n\n\nSo after one step of gradient descent, the weights have shifted to be: w1 = .15,\nw2 = .1, and b = .05.\nNote that this observation x happened to be a positive example. We would expect\nthat after seeing more negative examples with high counts of negative words, that\nthe weight w2 would shift to have a negative value.\n5.5\nRegularization\nNumquam ponenda est pluralitas sine necessitate\n‘Plurality should never be proposed unless needed’\nWilliam of Occam",
  "100": "94\nCHAPTER 5\n•\nLOGISTIC REGRESSION\nThere is a problem with learning weights that make the model perfectly match\nthe training data. If a feature is perfectly predictive of the outcome because it hap-\npens to only occur in one class, it will be assigned a very high weight. The weights\nfor features will attempt to perfectly ﬁt details of the training set, in fact too per-\nfectly, modeling noisy factors that just accidentally correlate with the class. This\nproblem is called overﬁtting. A good model should be able to generalize well from\noverﬁtting\ngeneralize\nthe training data to the unseen test set, but a model that overﬁts will have poor gen-\neralization.\nTo avoid overﬁtting, a regularization term is added to the objective function in\nregularization\nEq. 5.16, resulting in the following objective:\nˆw = argmax\nw\nm\nX\n1=1\nlogP(y(i)|x(i))−αR(w)\n(5.24)\nThe new component, R(w) is called a regularization term, and is used to penalize\nlarge weights. Thus a setting of the weights that matches the training data perfectly,\nbut uses many weights with high values to do so, will be penalized more than a\nsetting that matches the data a little less well, but does so using smaller weights.\nThere are two common regularization terms R(w). L2 regularization is a quad-\nL2\nregularization\nratic function of the weight values, named because it uses the (square of the) L2\nnorm of the weight values. The L2 norm, ||W||2, is the same as the Euclidean\ndistance:\nR(W) = ||W||2\n2 =\nN\nX\nj=1\nw2\nj\n(5.25)\nThe L2 regularized objective function becomes:\nˆw = argmax\nw\n\" m\nX\n1=i\nlogP(y(i)|x(i))\n#\n−α\nn\nX\nj=1\nw2\nj\n(5.26)\nL1 regularization is a linear function of the weight values, named after the L1\nL1\nregularization\nnorm ||W||1, the sum of the absolute values of the weights, or Manhattan distance\n(the Manhattan distance is the distance you’d have to walk between two points in a\ncity with a street grid like New York):\nR(W) = ||W||1 =\nN\nX\ni=1\n|wi|\n(5.27)\nThe L1 regularized objective function becomes:\nˆw = argmax\nw\n\" m\nX\n1=i\nlogP(y(i)|x(i))\n#\n−α\nn\nX\nj=1\n|w j|\n(5.28)\nThese kinds of regularization come from statistics, where L1 regularization is\ncalled the ‘lasso’ or lasso regression (Tibshirani, 1996) and L2 regression is called",
  "101": "5.6\n•\nMULTINOMIAL LOGISTIC REGRESSION\n95\nridge regression, and both are commonly used in language processing. L2 regu-\nlarization is easier to optimize because of its simple derivative (the derivative of w2\nis just 2w), while L1 regularization is more complex (the derivative of |w| is non-\ncontinuous at zero). But where L2 prefers weight vectors with many small weights,\nL1 prefers sparse solutions with some larger weights but many more weights set to\nzero. Thus L1 regularization leads to much sparser weight vectors, that is, far fewer\nfeatures.\nBoth L1 and L2 regularization have Bayesian interpretations as constraints on\nthe prior of how weights should look. L1 regularization can be viewed as a Laplace\nprior on the weights. L2 regularization corresponds to assuming that weights are\ndistributed according to a gaussian distribution with mean µ = 0. In a gaussian\nor normal distribution, the further away a value is from the mean, the lower its\nprobability (scaled by the variance σ). By using a gaussian prior on the weights, we\nare saying that weights prefer to have the value 0. A gaussian for a weight wj is\n1\nq\n2πσ2\nj\nexp\n \n−(w j −µj)2\n2σ2\nj\n!\n(5.29)\nIf we multiply each weight by a gaussian prior on the weight, we are thus maxi-\nmizing the following constraint:\nˆw = argmax\nw\nM\nY\ni=1\nP(y(i)|x(i))×\nn\nY\nj=1\n1\nq\n2πσ2\nj\nexp\n \n−(w j −µj)2\n2σ2\nj\n!\n(5.30)\nwhich in log space, with µ = 0, and assuming 2σ2 = 1, corresponds to\nˆw = argmax\nw\nm\nX\ni=1\nlogP(y(i)|x(i))−α\nn\nX\nj=1\nw2\nj\n(5.31)\nwhich is in the same form as Eq. 5.26.\n5.6\nMultinomial logistic regression\nSometimes we need more than two classes. Perhaps we might want to do 3-way\nsentiment classiﬁcation (positive, negative, or neutral). Or we could be classifying\nthe part of speech of a word (choosing from 10, 30, or even 50 different parts of\nspeech), or assigning semantic labels like the named entities or semantic relations\nwe will introduce in Chapter 17.\nIn such cases we use multinominal logistic regression, also called softmax re-\nmultinominal\nlogistic\nregression\ngression (or, historically, the maxent classiﬁer). In multinominal logistic regression\nthe target y is a variable that ranges over more than two classes; we want to know\nthe probability of y being in each potential class c ∈C, p(y = c|x).\nThe multinominal logistic classiﬁer uses a generalization of the sigmoid, called\nthe softmax function, to compute the probability p(y = c|x). The softmax function\nsoftmax\ntakes a vector z = [z1,z2,...,zk] of k arbitrary values and maps them to a probability\ndistribution, with each value in the range (0,1], and all the values summing to 1.\nLike the sigmoid, it is an exponential function;",
  "102": "96\nCHAPTER 5\n•\nLOGISTIC REGRESSION\nFor a vector z of dimensionality k, the softmax is deﬁned as:\nsoftmax(zi) =\nezi\nPk\nj=1 ezj 1 ≤i ≤k\n(5.32)\nThe softmax of an input vector z = [z1,z2,...,zk] is thus a vector itself:\nsoftmax(z) =\n\"\nez1\nPk\ni=1 ezi ,\nez2\nPk\ni=1 ezi ,...,\nezk\nPk\ni=1 ezi\n#\n(5.33)\nThe denominator Pk\ni=1 ezi is used to normalize all the values into probabilities.\nThus for example given a vector:\nz = [0.6,1.1,−1.5,1.2,3.2,−1.1]\nthe result softmax(z) is\n[0.055,0.090,0.0067,0.10,0.74,0.010]\nAgain like the sigmoid, the input to the softmax will be the dot product between\na weight vector w and an input vector x (plus a bias). But now we’ll need separate\nweight vectors (and bias) for each of the K classes.\np(y = c|x) =\newc ·x+bc\nk\nX\nj=1\new j ·x+b j\n(5.34)\nLike the sigmoid, the softmax has the property of squashing values toward 0 or\n1. thus if one of the inputs is larger than the others, will tend to push its probability\ntoward 1, and suppress the probabilities of the smaller inputs.\n5.6.1\nFeatures in Multinomial Logistic Regression\nFor multiclass classiﬁcation the input features need to be a function of both the\nobservation x and the candidate output class c. Thus instead of the notation xi, fi\nor fi(x), when we’re discussing features we will use the notation fi(c,x), meaning\nfeature i for a particular class c for a given observation x.\nIn binary classiﬁcation, a positive weight on a feature pointed toward y=1 and\na negative weight toward y=0... but in multiclass a feature could be evidence for or\nagainst an individual class.\nLet’s look at some sample features for a few NLP tasks to help understand this\nperhaps unintuitive use of features that are functions of both the observation x and\nthe class c,\nSuppose we are doing text classiﬁcation, and instead of binary classiﬁcation our\ntask is to assign one of the 3 classes +, −, or 0 (neutral) to a document. Now a\nfeature related to exclamation marks might have a negative weight for 0 documents,\nand a positive weight for + or −documents:",
  "103": "5.7\n•\nINTERPRETING MODELS\n97\nVar\nDeﬁnition\nWt\nf1(0,x)\n\u001a 1 if “!” ∈doc\n0 otherwise\n−4.5\nf1(+,x)\n\u001a 1 if “!” ∈doc\n0 otherwise\n2.6\nf1(0,x)\n\u001a\n1 if “!” ∈doc\n0 otherwise\n1.3\n5.6.2\nLearning in Multinomial Logistic Regression\nMultinomial logistic regression has a slightly different loss function than binary lo-\ngistic regression because it uses the softmax rather than sigmoid classiﬁer, The loss\nfunction for a single example x is the sum of the logs of the K output classes:\nLCE(ˆy,y) = −\nK\nX\nk=1\n1{y = k}log p(y = k|x)\n= −\nK\nX\nk=1\n1{y = k}log\newk·x+bk\nPK\nj=1 ewj·x+b j\n(5.35)\nThis makes use of the function 1{} which evaluates to 1 if the condition in the\nbrackets is true and to 0 otherwise.\nThe gradient for a single example turns out to be very similar to the gradient for\nlogistic regression, although we don’t show the derivation here. It is the different\nbetween the value for the true class k (which is 1) and the probability the classiﬁer\noutputs for class k, weighted by the value of the input xk:\n∂LCE\n∂wk\n= (1{y = k}−p(y = k|x))xk\n=\n \n1{y = k}−\newk·x+bk\nPK\nj=1 ew j·x+b j\n!\nxk\n(5.36)\n5.7\nInterpreting models\nOften we want to know more than just the correct classiﬁcation of an observation.\nWe want to know why the classiﬁer made the decision it did. That is, we want our\ndecision to be interpretable. Interpretability can be hard to deﬁne strictly, but the\ninterpretable\ncore idea is that as humans we should know why our algorithms reach the conclu-\nsions they do. Because the features to logistic regression are often human-designed,\none way to understand a classiﬁer’s decision is to understand the role each feature it\nplays in the decision. Logistic regression can be combined with statistical tests (the\nlikelihood ratio test, or the Wald test); investigating whether a particular feature is\nsigniﬁcant by one of these tests, or inspecting its magnitude (how large is the weight\nw associated with the feature?) can help us interpret why the classiﬁer made the\ndecision it makes. This is enormously important for building transparent models.\nFurthermore, in addition to its use as a classiﬁer, logistic regression in NLP and\nmany other ﬁelds is widely used as an analytic tool for testing hypotheses about the",
  "104": "98\nCHAPTER 5\n•\nLOGISTIC REGRESSION\neffect of various explanatory variables (features). In text classiﬁcation, perhaps we\nwant to know if logically negative words (no, not, never) are more likely to be asso-\nciated with negative sentiment, or if negative reviews of movies are more likely to\ndiscuss the cinematography. However, in doing so it’s necessary to control for po-\ntential confounds: other factors that might inﬂuence sentiment (the movie genre, the\nyear it was made, perhaps the length of the review in words). Or we might be study-\ning the relationship between NLP-extracted linguistic features and non-linguistic\noutcomes (hospital readmissions, political outcomes, or product sales), but need to\ncontrol for confounds (the age of the patient, the county of voting, the brand of the\nproduct). In such cases, logistic regression allows us to test whether some feature is\nassociated with some outcome above and beyond the effect of other features.\n5.8\nAdvanced: Deriving the Gradient Equation\nIn this section we give the derivation of the gradient of the cross-entropy loss func-\ntion LCE for logistic regression. Let’s start with some quick calculus refreshers.\nFirst, the derivative of ln(x):\nd\ndx ln(x) = 1\nx\n(5.37)\nSecond, the (very elegant) derivative of the sigmoid:\ndσ(z)\ndz\n= σ(z)(1−σ(z))\n(5.38)\nFinally, the chain rule of derivatives. Suppose we are computing the derivative\nchain rule\nof a composite function f(x) = u(v(x)). The derivative of f(x) is the derivative of\nu(x) with respect to v(x) times the derivative of v(x) with respect to x:\nd f\ndx = du\ndv · dv\ndx\n(5.39)\nFirst, we want to know the derivative of the loss function with respect to a single\nweight w j (we’ll need to compute it for each weight, and for the bias):\n∂LL(w,b)\n∂w j\n=\n∂\n∂w j\n−[ylogσ(w·x+b)+(1−y)log(1−σ(w·x+b))]\n= −\n\u0014 ∂\n∂w j\nylogσ(w·x+b)+ ∂\n∂w j\n(1−y)log[1−σ(w·x+b)]\n\u0015\n(5.40)\nNext, using the chain rule, and relying on the derivative of log:\n∂LL(w,b)\n∂w j\n= −\ny\nσ(w·x+b)\n∂\n∂w j\nσ(w·x+b)−\n1−y\n1−σ(w·x+b)\n∂\n∂w j\n1−σ(w·x+b)\n(5.41)\nRearranging terms:\n∂LL(w,b)\n∂w j\n= −\n\u0014\ny\nσ(w·x+b) −\n1−y\n1−σ(w·x+b)\n\u0015 ∂\n∂w j\nσ(w·x+b)\n(5.42)",
  "105": "5.9\n•\nSUMMARY\n99\nAnd now plugging in the derivative of the sigmoid, and using the chain rule one\nmore time, we end up with Eq. 5.43:\n∂LL(w,b)\n∂wj\n= −\n\u0014\ny−σ(w·x+b)\nσ(w·x+b)[1−σ(w·x+b)]\n\u0015\nσ(w·x+b)[1−σ(w·x+b)]∂(w·x+b)\n∂wj\n= −\n\u0014\ny−σ(w·x+b)\nσ(w·x+b)[1−σ(w·x+b)]\n\u0015\nσ(w·x+b)[1−σ(w·x+b)]xj\n= −[y−σ(w·x+b)]xj\n= [σ(w·x+b)−y]xj\n(5.43)\n5.9\nSummary\nThis chapter introduced the logistic regression model of classiﬁcation.\n• Logistic regression is a supervised machine learning classiﬁer that extracts\nreal-valued features from the input, multiplies each by a weight, sums them,\nand passes the sum through a sigmoid function to generate a probability. A\nthreshold is used to make a decision.\n• Logistic regression can be used with two classes (e.g., positive and negative\nsentiment) or with multiple classes (multinomial logistic regression, for ex-\nample for n-ary text classiﬁcation, part-of-speech labeling, etc.).\n• Multinomial logistic regression uses the softmax function to compute proba-\nbilities.\n• The weights (vector w and bias b) are learned from a labeled training set via a\nloss function, such as the cross-entropy loss, that must be minimized.\n• Minimizing this loss function is a convex optimization problem, and iterative\nalgorithms like gradient descent are used to ﬁnd the optimal weights.\n• Regularization is used to avoid overﬁtting.\n• Logistic regression is also one of the most useful analytic tools, because of its\nability to transparently study the importance of individual features.\nBibliographical and Historical Notes\nLogistic regression was developed in the ﬁeld of statistics, where it was used for\nthe analysis of binary data by the 1960s, and was particularly common in medicine\n(Cox, 1969). Starting in the late 1970s it became widely used in linguistics as one\nof the formal foundations of the study of linguistic variation (Sankoff and Labov,\n1979).\nNonetheless, logistic regression didn’t become common in natural language pro-\ncessing until the 1990s, when it seems to have appeared simultaneously from two\ndirections. The ﬁrst source was the neighboring ﬁelds of information retrieval and\nspeech processing, both of which had made use of regression, and both of which\nlent many other statistical techniques to NLP. Indeed a very early use of logistic\nregression for document routing was one of the ﬁrst NLP applications to use (LSI)\nembeddings as word representations (Sch¨utze et al., 1995).\nAt the same time in the early 1990s logistic regression was developed and ap-\nplied to NLP at IBM Research under the name maximum entropy modeling or\nmaximum\nentropy",
  "106": "100\nCHAPTER 5\n•\nLOGISTIC REGRESSION\nmaxent (Berger et al., 1996), seemingly independent of the statistical literature. Un-\nder that name it was applied to language modeling (Rosenfeld, 1996), part-of-speech\ntagging ((Ratnaparkhi, 1996)), parsing (Ratnaparkhi, 1997), and text classiﬁcation\n(Nigam et al., 1999).\nMore on classiﬁcation can be found in machine learning textbooks (Hastie et al. 2001,\nWitten and Frank 2005, Bishop 2006, Murphy 2012).\nExercises",
  "107": "CHAPTER\n6\nVector Semantics\nThe asphalt that Los Angeles is famous for occurs mainly on its freeways. But in the\nmiddle of the city is another patch of asphalt, the La Brea tar pits, and this asphalt\npreserves millions of fossil bones from the last of the Ice Ages of the Pleistocene\nEpoch. One of these fossils is the Smilodon, or sabre-toothed tiger, instantly rec-\nognizable by its long canines. Five million years ago or so, a completely different\nsabre-tooth tiger called Thylacosmilus lived\nin Argentina and other parts of South Amer-\nica. Thylacosmilus was a marsupial whereas\nSmilodon was a placental mammal, but Thy-\nlacosmilus had the same long upper canines\nand, like Smilodon, had a protective bone\nﬂange on the lower jaw.\nThe similarity of\nthese two mammals is one of many examples\nof parallel or convergent evolution, in which particular contexts or environments\nlead to the evolution of very similar structures in different species (Gould, 1980).\nThe role of context is also important in the similarity of a less biological kind\nof organism: the word. Words that occur in similar contexts tend to have similar\nmeanings. This link between similarity in how words are distributed and similarity\nin what they mean is called the distributional hypothesis. The hypothesis was\ndistributional\nhypothesis\nﬁrst formulated in the 1950s by linguists like Joos (1950), Harris (1954), and Firth\n(1957), who noticed that words which are synonyms (like oculist and eye-doctor)\ntended to occur in the same environment (e.g., near words like eye or examined)\nwith the amount of meaning difference between two words “corresponding roughly\nto the amount of difference in their environments” (Harris, 1954, 157).\nIn this chapter we introduce a model known as vector semantics, which instan-\nvector\nsemantics\ntiates this linguistic hypothesis by learning representations of the meaning of words\ndirectly from their distributions in texts. These representations are used in every\nnatural language processing application that makes use of meaning. These word\nrepresentations are also the ﬁrst example we will see in the book of representation\nlearning, automatically learning useful representations of the input text. Finding\nrepresentation\nlearning\nsuch unsupervised ways to learn representations of the input, instead of creating\nrepresentations by hand via feature engineering, is an important focus of recent\nNLP research (Bengio et al., 2013).\nWe’ll begin, however, by introducing some basic principles of word meaning,\nwhich will motivate the vector semantic models of this chapter as well as extensions\nthat we’ll return to in Appendix C, Chapter 19, and Chapter 18.",
  "108": "102\nCHAPTER 6\n•\nVECTOR SEMANTICS\n6.1\nLexical Semantics\nHow should we represent the meaning of a word? In the N-gram models we saw in\nChapter 3, and in many traditional NLP applications, our only representation of a\nword is as a string of letters, or perhaps as an index in a vocabulary list. This repre-\nsentation is not that different from a tradition in philosophy, perhaps you’ve seen it\nin introductory logic classes, in which the meaning of words is often represented by\njust spelling the word with small capital letters; representing the meaning of “dog”\nas DOG, and “cat” as CAT).\nRepresenting the meaning of a word by capitalizing it is a pretty unsatisfactory\nmodel. You might have seen the old philosophy joke:\nQ: What’s the meaning of life?\nA: LIFE\nSurely we can do better than this! After all, we’ll want a model of word meaning\nto do all sorts of things for us. It should tell us that some words have similar mean-\nings (cat is similar to dog), other words are antonyms (cold is the opposite of hot). It\nshould know that some words have positive connotations (happy) while others have\nnegative connotations (sad). It should represent the fact that the meanings of buy,\nsell, and pay offer differing perspectives on the same underlying purchasing event\n(If I buy something from you, you’ve probably sold it to me, and I likely paid you).\nMore generally, a model of word meaning should allow us to draw useful infer-\nences that will help us solve meaning-related tasks like question-answering, sum-\nmarization, paraphrase or plagiarism detection, and dialogue.\nIn this section we summarize some of these desiderata, drawing on results in the\nlinguistic study of word meaning, which is called lexical semantics.\nlexical\nsemantics\nLemmas and Senses\nLet’s start by looking at how one word (we’ll choose mouse)\nmight be deﬁned in a dictionary: 1\nmouse (N)\n1.\nany of numerous small rodents...\n2.\na hand-operated device that controls a cursor...\nHere the form mouse is the lemma, also called the citation form. The form\nlemma\ncitation form\nmouse would also be the lemma for the word mice; dictionaries don’t have separate\ndeﬁnitions for inﬂected forms like mice. Similarly sing is the lemma for sing, sang,\nsung. In many languages the inﬁnitive form is used as the lemma for the verb, so\nSpanish dormir “to sleep” is the lemma for duermes “you sleep”. The speciﬁc forms\nsung or carpets or sing or duermes are called wordforms.\nwordform\nAs the example above shows, each lemma can have multiple meanings; the\nlemma mouse can refer to the rodent or the cursor control device. We call each\nof these aspects of the meaning of mouse a word sense. The fact that lemmas can be\nhomonymous (have multiple senses) can make interpretation difﬁcult (is someone\nwho types “mouse info” to a search engine looking for a pet or a tool?). Appendix C\nwill discuss the problem of homonymy, and introduce word sense disambiguation,\nthe task of determining which sense of a word is being used in a particular context.\nRelationships between words or senses\nOne important component of word mean-\ning is the relationship between word senses. For example when one word has a sense\n1\nThis example shortened from the online dictionary WordNet, discussed in Appendix C.",
  "109": "6.1\n•\nLEXICAL SEMANTICS\n103\nwhose meaning is identical to a sense of another word, or nearly identical, we say\nthe two senses of those two words are synonyms. Synonyms include such pairs as\nsynonym\ncouch/sofa vomit/throw up ﬁlbert/hazelnut car/automobile\nA more formal deﬁnition of synonymy (between words rather than senses) is that\ntwo words are synonymous if they are substitutable one for the other in any sentence\nwithout changing the truth conditions of the sentence, the situations in which the\nsentence would be true. We often say in this case that the two words have the same\npropositional meaning.\npropositional\nmeaning\nWhile substitutions between some pairs of words like car / automobile or water /\nH2O are truth preserving, the words are still not identical in meaning. Indeed, proba-\nbly no two words are absolutely identical in meaning. One of the fundamental tenets\nof semantics, called the principle of contrast (Br´eal 1897, ?, Clark 1987), is the as-\nprinciple of\ncontrast\nsumption that a difference in linguistic form is always associated with at least some\ndifference in meaning. For example, the word H2O is used in scientiﬁc contexts and\nwould be inappropriate in a hiking guide—water would be more appropriate— and\nthis difference in genre is part of the meaning of the word. In practice, the word\nsynonym is therefore commonly used to describe a relationship of approximate or\nrough synonymy.\nWhere synonyms are words with identical or similar meanings, Antonyms are\nantonym\nwords with an opposite meaning, like:\nlong/short big/little fast/slow cold/hot dark/light\nrise/fall\nup/down in/out\nTwo senses can be antonyms if they deﬁne a binary opposition or are at opposite\nends of some scale. This is the case for long/short, fast/slow, or big/little, which are\nat opposite ends of the length or size scale. Another group of antonyms, reversives,\nreversives\ndescribe change or movement in opposite directions, such as rise/fall or up/down.\nAntonyms thus differ completely with respect to one aspect of their meaning—\ntheir position on a scale or their direction—but are otherwise very similar, sharing\nalmost all other aspects of meaning. Thus, automatically distinguishing synonyms\nfrom antonyms can be difﬁcult.\nWord Similarity:\nWhile words don’t have many synonyms, most words do have\nlots of similar words. Cat is not a synonym of dog, but cats and dogs are certainly\nsimilar words. In moving from synonymy to similarity, it will be useful to shift from\ntalking about relations between word senses (like synonymy) to relations between\nwords (like similarity). Dealing with words avoids having to commit to a particular\nrepresentation of word senses, which will turn out to simplify our task.\nThe notion of word similarity is very useful in larger semantic tasks. For exam-\nsimilarity\nple knowing how similar two words are is helpful if we are trying to decide if two\nphrases or sentences mean similar things. Phrase or sentence similarity is useful in\nsuch natural language understanding tasks as question answering, paraphrasing, and\nsummarization.\nOne way of getting values for word similarity is to ask humans to judge how\nsimilar one word is to another. A number of datasets have resulted from such ex-\nperiments. For example the SimLex-999 dataset (Hill et al., 2015) gives values on\na scale from 0 to 10, like the examples below, which range from near-synonyms\n(vanish, disappear) to pairs that scarcely seem to have anything in common (hole,\nagreement):",
  "110": "104\nCHAPTER 6\n•\nVECTOR SEMANTICS\nvanish\ndisappear\n9.8\nbehave obey\n7.3\nbelief\nimpression 5.95\nmuscle bone\n3.65\nmodest ﬂexible\n0.98\nhole\nagreement\n0.3\nWord Relatedness:\nThe meaning of two words can be related in ways others than\nsimilarity. One such class of connections is called word relatedness (Budanitsky\nrelatedness\nand Hirst, 2006), also traditionally called word association in psychology.\nassociation\nConsider the meanings of the words coffee and cup; Coffee is not similar to cup;\nthey share practically no features (coffee is a plant or a beverage, while a cup is an\nmanufactured object with a particular shape).\nBut coffee and cup are clearly related; they are associated in the world by com-\nmonly co-participating in a shared event (the event of drinking coffee out of a cup).\nSimilarly the nouns scalpel and surgeon are not similar but are related eventively (a\nsurgeon tends to make use of a scalpel).\nOne common kind of relatedness between words is if they belong to the same\nsemantic ﬁeld. A semantic ﬁeld is a set of words which cover a particular semantic\nsemantic ﬁeld\ndomain and bear structured relations with each other.\nFor example, words might be related by being in the semantic ﬁeld of hospitals\n(surgeon, scalpel, nurse, anaesthetic, hospital), restaurants (waiter, menu, plate,\nfood, chef), or houses (door, roof, kitchen, family, bed).\nSemantic ﬁelds are also related to topic models, like Latent Dirichlet Alloca-\ntopic models\ntion, LDA, which apply unsupervised learning on large sets of texts to induce sets\nof associated words from text. Semantic ﬁelds and topic models are a very useful\ntool for discovering topical structure in documents.\nSemantic Frames and Roles:\nClosely related to semantic ﬁelds is the idea of a\nsemantic frame. A semantic frame is a set of words that denote perspectives or\nsemantic frame\nparticipants in a particular type of event. A commercial transaction, for example,\nis a kind of event in which one entity trades money to another entity in return for\nsome good or service, after which the good changes hands or perhaps the service\nis performed. This event can be encoded lexically by using verbs like buy (the\nevent from the perspective of the buyer) sell (from the perspective of the seller), pay\n(focusing on the monetary aspect), or nouns like buyer. Frames have semantic roles\n(like buyer, seller, goods, money), and words in a sentence can take on these roles.\nKnowing that buy and sell have this relation makes it possible for a system to\nknow that a sentence like Sam bought the book from Ling could be paraphrased as\nLing sold the book to Sam, and that Sam has the role of the buyer in the frame and\nLing the seller. Being able to recognize such paraphrases is important for question\nanswering, and can help in shifting perspective for machine translation.\nTaxonomic Relations:\nAnother way word senses can be related is taxonomically.\nA word (or sense) is a hyponym of another word or sense if the ﬁrst is more speciﬁc,\nhyponym\ndenoting a subclass of the other. For example, car is a hyponym of vehicle; dog is\na hyponym of animal, and mango is a hyponym of fruit. Conversely, we say that\nvehicle is a hypernym of car, and animal is a hypernym of dog. It is unfortunate that\nhypernym\nthe two words (hypernym and hyponym) are very similar and hence easily confused;\nfor this reason, the word superordinate is often used instead of hypernym.\nsuperordinate\nSuperordinate vehicle fruit\nfurniture mammal\nSubordinate\ncar\nmango chair\ndog",
  "111": "6.1\n•\nLEXICAL SEMANTICS\n105\nWe can deﬁne hypernymy more formally by saying that the class denoted by the\nsuperordinate extensionally includes the class denoted by the hyponym. Thus, the\nclass of animals includes as members all dogs, and the class of moving actions in-\ncludes all walking actions. Hypernymy can also be deﬁned in terms of entailment.\nUnder this deﬁnition, a sense A is a hyponym of a sense B if everything that is A is\nalso B, and hence being an A entails being a B, or ∀x A(x) ⇒B(x). Hyponymy/hy-\npernymy is usually a transitive relation; if A is a hyponym of B and B is a hyponym\nof C, then A is a hyponym of C. Another name for the hypernym/hyponym structure\nis the IS-A hierarchy, in which we say A IS-A B, or B subsumes A.\nIS-A\nHypernymy is useful for tasks like textual entailment or question answering;\nknowing that leukemia is a type of cancer, for example, would certainly be useful in\nanswering questions about leukemia.\nConnotation:\nFinally, words have affective meanings or connotations. The word\nconnotations\nconnotation has different meanings in different ﬁelds, but here we use it to mean\nthe aspects of a word’s meaning that are related to a writer or reader’s emotions,\nsentiment, opinions, or evaluations. For example some words have positive conno-\ntations (happy) while others have negative connotations (sad). Some words describe\npositive evaluation (great, love) and others negative evaluation (terrible, hate). Pos-\nitive or negative evaluation expressed through language is called sentiment, as we\nsentiment\nsaw in Chapter 4, and word sentiment plays a role in important tasks like sentiment\nanalysis, stance detection, and many aspects of natural language processing to the\nlanguage of politics and consumer reviews.\nEarly work on affective meaning (Osgood et al., 1957) found that words varied\nalong three important dimensions of affective meaning. These are now generally\ncalled valence, arousal, and dominance, deﬁned as follows:\nvalence: the pleasantness of the stimulus\narousal: the intensity of emotion provoked by the stimulus\ndominance: the degree of control exerted by the stimulus\nThus words like happy or satisﬁed are high on valence, while unhappy or an-\nnoyed are low on valence. Excited or frenzied are high on arousal, while relaxed\nor calm are low on arousal. Important or controlling are high on dominance, while\nawed or inﬂuenced are low on dominance. Each word is thus represented by three\nnumbers, corresponding to its value on each of the three dimensions, like the exam-\nples below:\nValence Arousal Dominance\ncourageous 8.05\n5.5\n7.38\nmusic\n7.67\n5.57\n6.5\nheartbreak\n2.45\n5.65\n3.58\ncub\n6.71\n3.95\n4.24\nlife\n6.68\n5.59\n5.89\nOsgood et al. (1957) noticed that in using these 3 numbers to represent the\nmeaning of a word, the model was representing each word as a point in a three-\ndimensional space, a vector whose three dimensions corresponded to the word’s\nrating on the three scales. This revolutionary idea that word meaning word could\nbe represented as a point in space (e.g., that part of the meaning of heartbreak can\nbe represented as the point [2.45,5.65,3.58]) was the ﬁrst expression of the vector\nsemantics models that we introduce next.",
  "112": "106\nCHAPTER 6\n•\nVECTOR SEMANTICS\n6.2\nVector Semantics\nHow can we build a computational model that successfully deals with the different\naspects of word meaning we saw in the previous section (word senses, word simi-\nlarity and relatedness, lexical ﬁelds and frames, connotation)?\nA perfect model that completely deals with each of these aspects of word mean-\ning turns out to be elusive. But the current best model, called vector semantics,\nvector\nsemantics\ndraws its inspiration from linguistic and philosophical work of the 1950’s.\nDuring that period, the philosopher Ludwig Wittgenstein, skeptical of the possi-\nbility of building a completely formal theory of meaning deﬁnitions for each word,\nsuggested instead that “the meaning of a word is its use in the language” (Wittgen-\nstein, 1953, PI 43). That is, instead of using some logical language to deﬁne each\nword, we should deﬁne words by some representation of how the word was used by\nactual people in speaking and understanding.\nLinguists of the period like Joos (1950), Harris (1954), and Firth (1957) (the\nlinguistic distributionalists), came up with a speciﬁc idea for realizing Wittgenstein’s\nintuition: deﬁne a word by the environment or distribution it occurs in in language\nuse. A word’s distribution is the set of contexts in which it occurs, the neighboring\nwords or grammatical environments. The idea is that two words that occur in very\nsimilar distributions (that occur together with very similar words) are likely to have\nthe same meaning.\nLet’s see an example illustrating this distributionalist approach. Suppose you\ndidn’t know what the Cantonese word ongchoi meant, but you do see it in the fol-\nlowing sentences or contexts:\n(6.1) Ongchoi is delicious sauteed with garlic.\n(6.2) Ongchoi is superb over rice.\n(6.3) ...ongchoi leaves with salty sauces...\nAnd furthermore let’s suppose that you had seen many of these context words\noccurring in contexts like:\n(6.4) ...spinach sauteed with garlic over rice...\n(6.5) ...chard stems and leaves are delicious...\n(6.6) ...collard greens and other salty leafy greens\nThe fact that ongchoi occurs with words like rice and garlic and delicious and\nsalty, as do words like spinach, chard, and collard greens might suggest to the reader\nthat ongchoi is a leafy green similar to these other leafy greens.2\nWe can do the same thing computationally by just counting words in the context\nof ongchoi; we’ll tend to see words like sauteed and eaten and garlic. The fact that\nthese words and other similar context words also occur around the word spinach or\ncollard greens can help us discover the similarity between these words and ongchoi.\nVector semantics thus combines two intuitions: the distributionalist intuition\n(deﬁning a word by counting what other words occur in its environment), and the\nvector intuition of Osgood et al. (1957) we saw in the last section on connotation:\ndeﬁning the meaning of a word w as a vector, a list of numbers, a point in N-\ndimensional space. There are various versions of vector semantics, each deﬁning\nthe numbers in the vector somewhat differently, but in each case the numbers are\nbased in some way on counts of neighboring words.\n2\nIt’s in fact Ipomoea aquatica, a relative of morning glory sometimes called water spinach in English.",
  "113": "6.2\n•\nVECTOR SEMANTICS\n107\ngood\nnice\nbad\nworst\nnot good\nwonderful\namazing\nterrific\ndislike\nworse\nvery good\nincredibly good\nfantastic\nincredibly bad\nnow\nyou\ni\nthat\nwith\nby\nto\n‘s\nare\nis\na\nthan\nFigure 6.1\nA two-dimensional (t-SNE) projection of embeddings for some words and\nphrases, showing that words with similar meanings are nearby in space. The original 60-\ndimensional embeddings were trained for a sentiment analysis task. Simpliﬁed from Li et al.\n(2015).\nThe idea of vector semantics is thus to represent a word as a point in some multi-\ndimensional semantic space. Vectors for representing words are generally called\nembeddings, because the word is embedded in a particular vector space. Fig. 6.1\nembeddings\ndisplays a visualization of embeddings that were learned for a sentiment analysis\ntask, showing the location of some selected words projected down from the original\n60-dimensional space into a two dimensional space.\nNotice that positive and negative words seem to be located in distinct portions of\nthe space (and different also from the neutral function words). This suggests one of\nthe great advantages of vector semantics: it offers a ﬁne-grained model of meaning\nthat lets us also implement word similarity (and phrase similarity). For example,\nthe sentiment analysis classiﬁer we saw in Chapter 4 only works if enough of the\nimportant sentimental words that appear in the test set also appeared in the training\nset. But if words were represented as embeddings, we could assign sentiment as\nlong as words with similar meanings as the test set words occurred in the training\nset. Vector semantic models are also extremely practical because they can be learned\nautomatically from text without any complex labeling or supervision.\nAs a result of these advantages, vector models of meaning are now the standard\nway to represent the meaning of words in NLP. In this chapter we’ll introduce the\ntwo most commonly used models. . First is the tf-idf model, often used a a baseline,\nin which the meaning of a word is deﬁned by a simple function of the counts of\nnearby words. We will see that this method results in very long vectors that are\nsparse, i.e. contain mostly zeros (since most words simply never occur in the context\nof others).\nThen we’ll introduce the word2vec model, one of a family of models that are\nways of constructing short, dense vectors that have useful semantic properties.\nWe’ll also introduce the cosine, the standard way to use embeddings (vectors)\nto compute functions like semantic similarity, the similarity between two words,\ntwo sentences, or two documents, an important tool in practical applications like\nquestion answering, summarization, or automatic essay grading.",
  "114": "108\nCHAPTER 6\n•\nVECTOR SEMANTICS\n6.3\nWords and Vectors\nVector or distributional models of meaning are generally based on a co-occurrence\nmatrix, a way of representing how often words co-occur. This matrix can be con-\nstructed in various ways; let’s s begin by looking at one such co-occurrence matrix,\na term-document matrix.\n6.3.1\nVectors and documents\nIn a term-document matrix, each row represents a word in the vocabulary and each\nterm-document\nmatrix\ncolumn represents a document from some collection of documents. Fig. 6.2 shows a\nsmall selection from a term-document matrix showing the occurrence of four words\nin four plays by Shakespeare. Each cell in this matrix represents the number of times\na particular word (deﬁned by the row) occurs in a particular document (deﬁned by\nthe column). Thus fool appeared 58 times in Twelfth Night.\nAs You Like It\nTwelfth Night\nJulius Caesar\nHenry V\nbattle\n1\n0\n7\n13\ngood\n114\n80\n62\n89\nfool\n36\n58\n1\n4\nwit\n20\n15\n2\n3\nFigure 6.2\nThe term-document matrix for four words in four Shakespeare plays. Each cell\ncontains the number of times the (row) word occurs in the (column) document.\nThe term-document matrix of Fig. 6.2 was ﬁrst deﬁned as part of the vector\nspace model of information retrieval (Salton, 1971). In this model, a document is\nvector space\nmodel\nrepresented as a count vector, a column in Fig. 6.3.\nTo review some basic linear algebra, a vector is, at heart, just a list or array\nvector\nof numbers. So As You Like It is represented as the list [1,114,36,20] and Julius\nCaesar is represented as the list [7,62,1,2]. A vector space is a collection of vectors,\nvector space\ncharacterized by their dimension. In the example in Fig. 6.3, the vectors are of\ndimension\ndimension 4, just so they ﬁt on the page; in real term-document matrices, the vectors\nrepresenting each document would have dimensionality |V|, the vocabulary size.\nThe ordering of the numbers in a vector space is not arbitrary; each position\nindicates a meaningful dimension on which the documents can vary. Thus the ﬁrst\ndimension for both these vectors corresponds to the number of times the word battle\noccurs, and we can compare each dimension, noting for example that the vectors for\nAs You Like It and Twelfth Night have similar values (1 and 0, respectively) for the\nﬁrst dimension.\nAs You Like It\nTwelfth Night\nJulius Caesar\nHenry V\nbattle\n1\n0\n7\n13\ngood\n114\n80\n62\n89\nfool\n36\n58\n1\n4\nwit\n20\n15\n2\n3\nFigure 6.3\nThe term-document matrix for four words in four Shakespeare plays. The red\nboxes show that each document is represented as a column vector of length four.\nWe can think of the vector for a document as identifying a point in |V|-dimensional\nspace; thus the documents in Fig. 6.3 are points in 4-dimensional space. Since 4-\ndimensional spaces are hard to draw in textbooks, Fig. 6.4 shows a visualization in",
  "115": "6.3\n•\nWORDS AND VECTORS\n109\ntwo dimensions; we’ve arbitrarily chosen the dimensions corresponding to the words\nbattle and fool.\n5\n10\n15\n20\n25\n30\n5\n10\nHenry V [4,13]\nAs You Like It [36,1]\nJulius Caesar [1,7]\nbattle\n fool\nTwelfth Night [58,0]\n15\n40\n35\n40\n45\n50\n55\n60\nFigure 6.4\nA spatial visualization of the document vectors for the four Shakespeare play\ndocuments, showing just two of the dimensions, corresponding to the words battle and fool.\nThe comedies have high values for the fool dimension and low values for the battle dimension.\nTerm-document matrices were originally deﬁned as a means of ﬁnding similar\ndocuments for the task of document information retrieval. Two documents that are\nsimilar will tend to have similar words, and if two documents have similar words\ntheir column vectors will tend to be similar. The vectors for the comedies As You\nLike It [1,114,36,20] and Twelfth Night [0,80,58,15] look a lot more like each other\n(more fools and wit than battles) than they do like Julius Caesar [7,62,1,2] or Henry\nV [13,89,4,3]. We can see the intuition with the raw numbers; in the ﬁrst dimension\n(battle) the comedies have low numbers and the others have high numbers, and we\ncan see it visually in Fig. 6.4; we’ll see very shortly how to quantify this intuition\nmore formally.\nA real term-document matrix, of course, wouldn’t just have 4 rows and columns,\nlet alone 2. More generally, the term-document matrix X has |V| rows (one for each\nword type in the vocabulary) and D columns (one for each document in the collec-\ntion); as we’ll see, vocabulary sizes are generally at least in the tens of thousands,\nand the number of documents can be enormous (think about all the pages on the\nweb).\nInformation retrieval (IR) is the task of ﬁnding the document d from the D\ninformation\nretrieval\ndocuments in some collection that best matches a query q. For IR we’ll therefore also\nrepresent a query by a vector, also of length |V|, and we’ll need a way to compare\ntwo vectors to ﬁnd how similar they are. (Doing IR will also require efﬁcient ways\nto store and manipulate these vectors, which is accomplished by making use of the\nconvenient fact that these vectors are sparse, i.e., mostly zeros).\nLater in the chapter we’ll introduce some of the components of this vector com-\nparison process: the tf-idf term weighting, and the cosine similarity metric.\n6.3.2\nWords as vectors\nWe’ve seen that documents can be represented as vectors in a vector space. But\nvector semantics can also be used to represent the meaning of words, by associating\neach word with a vector.\nThe word vector is now a row vector rather than a column vector, and hence the\nrow vector\ndimensions of the vector are different. The four dimensions of the vector for fool,",
  "116": "110\nCHAPTER 6\n•\nVECTOR SEMANTICS\n[36,58,1,4], correspond to the four Shakespeare plays. The same four dimensions are\nused to form the vectors for the other 3 words: wit, [20, 15, 2, 3]; battle, [1,0,7,13];\nand good [114,80,62,89]. Each entry in the vector thus represents the counts of the\nword’s occurrence in the document corresponding to that dimension.\nFor documents, we saw that similar documents had similar vectors, because sim-\nilar documents tend to have similar words. This same principle applies to words:\nsimilar words have similar vectors because they tend to occur in similar documents.\nThe term-document matrix thus lets us represent the meaning of a word by the doc-\numents it tends to occur in.\nHowever, it is most common to use a different kind of context for the dimensions\nof a word’s vector representation. Rather than the term-document matrix we use the\nterm-term matrix, more commonly called the word-word matrix or the term-\nterm-term\nmatrix\nword-word\nmatrix\ncontext matrix, in which the columns are labeled by words rather than documents.\nThis matrix is thus of dimensionality |V|×|V| and each cell records the number of\ntimes the row (target) word and the column (context) word co-occur in some context\nin some training corpus. The context could be the document, in which case the cell\nrepresents the number of times the two words appear in the same document. It is\nmost common, however, to use smaller contexts, generally a window around the\nword, for example of 4 words to the left and 4 words to the right, in which case\nthe cell represents the number of times (in some training corpus) the column word\noccurs in such a ±4 word window around the row word.\nFor example here are 7-word windows surrounding four sample words from the\nBrown corpus (just one example of each word):\nsugar, a sliced lemon, a tablespoonful of apricot\njam, a pinch each of,\ntheir enjoyment. Cautiously she sampled her ﬁrst pineapple\nand another fruit whose taste she likened\nwell suited to programming on the digital computer.\nIn ﬁnding the optimal R-stage policy from\nfor the purpose of gathering data and information necessary for the study authorized in the\nFor each word we collect the counts (from the windows around each occurrence)\nof the occurrences of context words. Fig. 6.5 shows a selection from the word-word\nco-occurrence matrix computed from the Brown corpus for these four words.\naardvark\n...\ncomputer\ndata\npinch\nresult\nsugar\n...\napricot\n0\n...\n0\n0\n1\n0\n1\npineapple\n0\n...\n0\n0\n1\n0\n1\ndigital\n0\n...\n2\n1\n0\n1\n0\ninformation\n0\n...\n1\n6\n0\n4\n0\nFigure 6.5\nCo-occurrence vectors for four words, computed from the Brown corpus, show-\ning only six of the dimensions (hand-picked for pedagogical purposes). The vector for the\nword digital is outlined in red. Note that a real vector would have vastly more dimensions\nand thus be much sparser.\nNote in Fig. 6.5 that the two words apricot and pineapple are more similar to\neach other (both pinch and sugar tend to occur in their window) than they are to\nother words like digital; conversely, digital and information are more similar to each\nother than, say, to apricot. Fig. 6.6 shows a spatial visualization.\nNote that |V|, the length of the vector, is generally the size of the vocabulary,\nusually between 10,000 and 50,000 words (using the most frequent words in the\ntraining corpus; keeping words after about the most frequent 50,000 or so is gener-\nally not helpful). But of course since most of these numbers are zero these are sparse\nvector representations, and there are efﬁcient algorithms for storing and computing\nwith sparse matrices.",
  "117": "6.4\n•\nCOSINE FOR MEASURING SIMILARITY\n111\n1\n2\n3\n4\n5\n6\n1\n2\ndigital\n [1,1]\nresult\n data\ninformation\n [6,4] \n3\n4\nFigure 6.6\nA spatial visualization of word vectors for digital and information, showing just\ntwo of the dimensions, corresponding to the words data and result.\nNow that we have some intuitions, let’s move on to examine the details of com-\nputing word similarity. Afterwards we’ll discuss the tf-idf method of weighting\ncells.\n6.4\nCosine for measuring similarity\nTo deﬁne similarity between two target words v and w, we need a measure for taking\ntwo such vectors and giving a measure of vector similarity. By far the most common\nsimilarity metric is the cosine of the angle between the vectors.\nThe cosine—like most measures for vector similarity used in NLP—is based on\nthe dot product operator from linear algebra, also called the inner product:\ndot product\ninner product\ndot-product(⃗v,⃗w) =⃗v·⃗w =\nN\nX\ni=1\nviwi = v1w1 +v2w2 +...+vNwN\n(6.7)\nAs we will see, most metrics for similarity between vectors are based on the dot\nproduct. The dot product acts as a similarity metric because it will tend to be high\njust when the two vectors have large values in the same dimensions. Alternatively,\nvectors that have zeros in different dimensions—orthogonal vectors—will have a\ndot product of 0, representing their strong dissimilarity.\nThis raw dot-product, however, has a problem as a similarity metric: it favors\nlong vectors. The vector length is deﬁned as\nvector length\n|⃗v| =\nv\nu\nu\nt\nN\nX\ni=1\nv2\ni\n(6.8)\nThe dot product is higher if a vector is longer, with higher values in each dimension.\nMore frequent words have longer vectors, since they tend to co-occur with more\nwords and have higher co-occurrence values with each of them. The raw dot product\nthus will be higher for frequent words. But this is a problem; we’d like a similarity\nmetric that tells us how similar two words are regardless of their frequency.\nThe simplest way to modify the dot product to normalize for the vector length is\nto divide the dot product by the lengths of each of the two vectors. This normalized\ndot product turns out to be the same as the cosine of the angle between the two",
  "118": "112\nCHAPTER 6\n•\nVECTOR SEMANTICS\nvectors, following from the deﬁnition of the dot product between two vectors ⃗a and\n⃗b:\n⃗a·⃗b = |⃗a||⃗b|cosθ\n⃗a·⃗b\n|⃗a||⃗b|\n= cosθ\n(6.9)\nThe cosine similarity metric between two vectors⃗v and ⃗w thus can be computed as:\ncosine\ncosine(⃗v,⃗w) = ⃗v·⃗w\n|⃗v||⃗w| =\nN\nX\ni=1\nviwi\nv\nu\nu\nt\nN\nX\ni=1\nv2\ni\nv\nu\nu\nt\nN\nX\ni=1\nw2\ni\n(6.10)\nFor some applications we pre-normalize each vector, by dividing it by its length,\ncreating a unit vector of length 1. Thus we could compute a unit vector from ⃗a by\nunit vector\ndividing it by |⃗a|. For unit vectors, the dot product is the same as the cosine.\nThe cosine value ranges from 1 for vectors pointing in the same direction, through\n0 for vectors that are orthogonal, to -1 for vectors pointing in opposite directions.\nBut raw frequency values are non-negative, so the cosine for these vectors ranges\nfrom 0–1.\nLet’s see how the cosine computes which of the words apricot or digital is closer\nin meaning to information, just using raw counts from the following simpliﬁed table:\nlarge data computer\napricot\n2\n0\n0\ndigital\n0\n1\n2\ninformation\n1\n6\n1\ncos(apricot,information) =\n2+0+0\n√4+0+0\n√\n1+36+1 =\n2\n2\n√\n38 = .16\ncos(digital,information) =\n0+6+2\n√0+1+4\n√\n1+36+1 =\n8\n√\n38\n√\n5\n= .58\n(6.11)\nThe model decides that information is closer to digital than it is to apricot, a\nresult that seems sensible. Fig. 6.7 shows a visualization.\n6.5\nTF-IDF: Weighing terms in the vector\nThe co-occurrence matrix in Fig. 6.5 represented each cell by the raw frequency of\nthe co-occurrence of two words.\nIt turns out, however, that simple frequency isn’t the best measure of association\nbetween words. One problem is that raw frequency is very skewed and not very\ndiscriminative. If we want to know what kinds of contexts are shared by apricot\nand pineapple but not by digital and information, we’re not going to get good dis-\ncrimination from words like the, it, or they, which occur frequently with all sorts of\nwords and aren’t informative about any particular word. We saw this also in Fig. 6.3\nfor the Shakespeare corpus; the dimension for the word good is not very discrimina-\ntive between plays; good is simply a frequent word and has roughly equivalent high\nfrequencies in each of the plays.",
  "119": "6.5\n•\nTF-IDF: WEIGHING TERMS IN THE VECTOR\n113\n1\n2\n3\n4\n5\n6\n7\n1\n2\n3\ndigital\napricot\ninformation\nDimension 1: ‘large’\nDimension 2: ‘data’\nFigure 6.7\nA graphical demonstration of cosine similarity, showing vectors for three words\n(apricot, digital, and information) in the two dimensional space deﬁned by counts of the\nwords data and large in the neighborhood. Note that the angle between digital and informa-\ntion is smaller than the angle between apricot and information. When two vectors are more\nsimilar, the cosine is larger but the angle is smaller; the cosine has its maximum (1) when the\nangle between two vectors is smallest (0◦); the cosine of all other angles is less than 1.\nIt’s a bit of a paradox. Word that occur nearby frequently (maybe sugar appears\noften in our corpus near apricot) are more important than words that only appear\nonce or twice. Yet words that are too frequent—ubiquitous, like the or good— are\nunimportant. How can we balance these two conﬂicting constraints?\nThe tf-idf algorithm (the ‘-’ here is a hyphen, not a minus sign) algorithm is the\nproduct of two terms, each term capturing one of these two intuitions:\n1. The ﬁrst is the term frequency (Luhn, 1957): the frequency of the word in the\nterm frequency\ndocument. Normally we want to downweight the raw frequency a bit, since\na word appearing 100 times in a document doesn’t make that word 100 times\nmore likely to be relevant to the meaning of the document. So we generally\nuse the log10 of the frequency, resulting in the following deﬁnition for the term\nfrequency weight:\ntft,d =\n\u001a 1+log10 count(t,d)\nif count(t,d) > 0\n0\notherwise\nThus terms which occur 10 times in a document would have a tf=2, 100 times\nin a document tf=3, 1000 times tf=4, and so on.\n2. The second factor is used to give a higher weight to words that occur only\nin a few documents. Terms that are limited to a few documents are useful\nfor discriminating those documents from the rest of the collection; terms that\noccur frequently across the entire collection aren’t as helpful. The document\nfrequency dft of a term t is simply the number of documents it occurs in. By\ndocument\nfrequency\ncontrast, the collection frequency of a term is the total number of times the\nword appears in the whole collection in any document. Consider in the col-\nlection Shakespeare’s 37 plays the two words Romeo and action. The words\nhave identical collection frequencies of 113 (they both occur 113 times in all\nthe plays) but very different document frequencies, since Romeo only occurs\nin a single play. If our goal is ﬁnd documents about the romantic tribulations\nof Romeo, the word Romeo should be highly weighted:",
  "120": "114\nCHAPTER 6\n•\nVECTOR SEMANTICS\nCollection Frequency Document Frequency\nRomeo 113\n1\naction\n113\n31\nWe assign importance to these more discriminative words like Romeo via\nthe inverse document frequency or idf term weight (Sparck Jones, 1972).\nidf\nThe idf is deﬁned using the fraction N/dft, where N is the total number of\ndocuments in the collection, and dft is the number of documents in which\nterm t occurs. The fewer documents in which a term occurs, the higher this\nweight. The lowest weight of 1 is assigned to terms that occur in all the\ndocuments. It’s usually clear what counts as a document: in Shakespeare\nwe would use a play; when processing a collection of encyclopedia articles\nlike Wikipedia, the document is a Wikipedia page; in processing newspaper\narticles, the document is a single article. Occasionally your corpus might\nnot have appropriate document divisions and you might need to break up the\ncorpus into documents yourself for the purposes of computing idf.\nBecause of the large number of documents in many collections, this mea-\nsure is usually squashed with a log function. The resulting deﬁnition for in-\nverse document frequency (idf) is thus\nidft = log10\n\u0012 N\ndft\n\u0013\n(6.12)\nHere are some idf values for some words in the Shakespeare corpus, ranging\nfrom extremely informative words which occur in only one play like Romeo, to\nthose that occur in a few like salad or Falstaff, to those which are very common like\nfool or so common as to be completely non-discriminative since they occur in all 37\nplays like good or sweet.3\nWord\ndf\nidf\nRomeo\n1\n1.57\nsalad\n2\n1.27\nFalstaff\n4\n0.967\nforest\n12\n0.489\nbattle\n21\n0.074\nfool\n36\n0.012\ngood\n37\n0\nsweet\n37\n0\nThe tf-idf weighting of the value for word t in document d, wt,d thus combines\ntf-idf\nterm frequency with idf:\nwt,d = tft,d ×idft\n(6.13)\nFig. 6.8 applies tf-idf weighting to the Shakespeare term-document matrix in Fig. 6.2.\nNote that the tf-idf values for the dimension corresponding to the word good have\nnow all become 0; since this word appears in every document, the tf-idf algorithm\nleads it to be ignored in any comparison of the plays. Similarly, the word fool, which\nappears in 36 out of the 37 plays, has a much lower weight.\nThe tf-idf weighting is by far the dominant way of weighting co-occurrence ma-\ntrices in information retrieval, but also plays a role in many other aspects of natural\n3\nSweet was one of Shakespeare’s favorite adjectives, a fact probably related to the increased use of\nsugar in European recipes around the turn of the 16th century (Jurafsky, 2014, p. 175).",
  "121": "6.6\n•\nAPPLICATIONS OF THE TF-IDF VECTOR MODEL\n115\nAs You Like It\nTwelfth Night\nJulius Caesar\nHenry V\nbattle\n0.074\n0\n0.22\n0.28\ngood\n0\n0\n0\n0\nfool\n0.019\n0.021\n0.0036\n0.0083\nwit\n0.049\n0.044\n0.018\n0.022\nFigure 6.8\nA tf-idf weighted term-document matrix for four words in four Shakespeare\nplays, using the counts in Fig. 6.2. Note that the idf weighting has eliminated the importance\nof the ubiquitous word good and vastly reduced the impact of the almost-ubiquitous word\nfool.\nlanguage processing. It’s also a great baseline, the simple thing to try ﬁrst. We’ll\nlook at other weightings like PPMI (Positive Pointwise Mutual Information) in Sec-\ntion 6.7.\n6.6\nApplications of the tf-idf vector model\nIn summary, the vector semantics model we’ve described so far represents a target\nword as a vector with dimensions corresponding to all the words in the vocabulary\n(length |V|, with vocabularies of 20,000 to 50,000), which is also sparse (most values\nare zero). The values in each dimension are the frequency with which the target\nword co-occurs with each neighboring context word, weighted by tf-idf. The model\ncomputes the similarity between two words x and y by taking the cosine of their\ntf-idf vectors; high cosine, high similarity. This entire model is sometimes referred\nto for short as the tf-idf model, after the weighting function.\nOne common use for a tf-idf model is to compute word similarity, a useful tool\nfor tasks like ﬁnding word paraphrases, tracking changes in word meaning, or au-\ntomatically discovering meanings of words in different corpora. For example, we\ncan ﬁnd the 10 most similar words to any target word w by computing the cosines\nbetween w and each of the V −1 other words, sorting, and looking at the top 10.\nThe tf-idf vector model can also be used to decide if two documents are similar.\nWe represent a document by taking the vectors of all the words in the document, and\ncomputing the centroid of all those vectors. The centroid is the multidimensional\ncentroid\nversion of the mean; the centroid of a set of vectors is a single vector that has the\nminimum sum of squared distances to each of the vectors in the set. Given k word\nvectors w1,w2,...,wk, the centroid document vector d is:\ndocument\nvector\nd = w1 +w2 +...+wk\nk\n(6.14)\nGiven two documents, we can then compute their document vectors d1 and d2,\nand estimate the similarity between the two documents by cos(d1,d2).\nDocument similarity is also useful for all sorts of applications; information re-\ntrieval, plagiarism detection, news recommender systems, and even for digital hu-\nmanities tasks like comparing different versions of a text to see which are similar to\neach other.",
  "122": "116\nCHAPTER 6\n•\nVECTOR SEMANTICS\n6.7\nOptional: Pointwise Mutual Information (PMI)\nAn alternative weighting function to tf-idf is called PPMI (positive pointwise mutual\ninformation). PPMI draws on the intuition that best way to weigh the association\nbetween two words is to ask how much more the two words co-occur in our corpus\nthan we would have a priori expected them to appear by chance.\nPointwise mutual information (Fano, 1961)4 is one of the most important con-\npointwise\nmutual\ninformation\ncepts in NLP. It is a measure of how often two events x and y occur, compared with\nwhat we would expect if they were independent:\nI(x,y) = log2\nP(x,y)\nP(x)P(y)\n(6.16)\nThe pointwise mutual information between a target word w and a context word\nc (Church and Hanks 1989, Church and Hanks 1990) is then deﬁned as:\nPMI(w,c) = log2\nP(w,c)\nP(w)P(c)\n(6.17)\nThe numerator tells us how often we observed the two words together (assuming\nwe compute probability by using the MLE). The denominator tells us how often\nwe would expect the two words to co-occur assuming they each occurred indepen-\ndently; recall that the probability of two independent events both occurring is just\nthe product of the probabilities of the two events. Thus, the ratio gives us an esti-\nmate of how much more the two words co-occur than we expect by chance. PMI is\na useful tool whenever we need to ﬁnd words that are strongly associated.\nPMI values range from negative to positive inﬁnity. But negative PMI values\n(which imply things are co-occurring less often than we would expect by chance)\ntend to be unreliable unless our corpora are enormous. To distinguish whether two\nwords whose individual probability is each 10−6 occur together more often than\nchance, we would need to be certain that the probability of the two occurring to-\ngether is signiﬁcantly different than 10−12, and this kind of granularity would require\nan enormous corpus. Furthermore it’s not clear whether it’s even possible to evalu-\nate such scores of ‘unrelatedness’ with human judgments. For this reason it is more\ncommon to use Positive PMI (called PPMI) which replaces all negative PMI values\nPPMI\nwith zero (Church and Hanks 1989, Dagan et al. 1993, Niwa and Nitta 1994)5:\nPPMI(w,c) = max(log2\nP(w,c)\nP(w)P(c),0)\n(6.18)\nMore formally, let’s assume we have a co-occurrence matrix F with W rows (words)\nand C columns (contexts), where fij gives the number of times word wi occurs in\n4\nPointwise mutual information is based on the mutual information between two random variables X\nand Y, which is deﬁned as:\nI(X,Y) =\nX\nx\nX\ny\nP(x,y)log2\nP(x,y)\nP(x)P(y)\n(6.15)\nIn a confusion of terminology, Fano used the phrase mutual information to refer to what we now call\npointwise mutual information and the phrase expectation of the mutual information for what we now call\nmutual information\n5\nPositive PMI also cleanly solves the problem of what to do with zero counts, using 0 to replace the\n−∞from log(0).",
  "123": "6.7\n•\nOPTIONAL: POINTWISE MUTUAL INFORMATION (PMI)\n117\ncontext cj. This can be turned into a PPMI matrix where ppmiij gives the PPMI\nvalue of word wi with context cj as follows:\npij =\nfij\nPW\ni=1\nPC\nj=1 fi j pi∗=\nPC\nj=1 fi j\nPW\ni=1\nPC\nj=1 fi j p∗j =\nPW\ni=1 fi j\nPW\ni=1\nPC\nj=1 fi j\n(6.19)\nPPMIij = max(log2\npij\npi∗p∗j\n,0)\n(6.20)\nThus for example we could compute PPMI(w=information,c=data), assuming we\npretended that Fig. 6.5 encompassed all the relevant word contexts/dimensions, as\nfollows:\nP(w=information,c=data) =\n6\n19 = .316\nP(w=information) = 11\n19 = .579\nP(c=data) =\n7\n19 = .368\nppmi(information,data) = log2(.316/(.368∗.579)) = .568\nFig. 6.9 shows the joint probabilities computed from the counts in Fig. 6.5, and\nFig. 6.10 shows the PPMI values.\np(w,context)\np(w)\ncomputer\ndata\npinch\nresult\nsugar\np(w)\napricot\n0\n0\n0.05\n0\n0.05\n0.11\npineapple\n0\n0\n0.05\n0\n0.05\n0.11\ndigital\n0.11\n0.05\n0\n0.05\n0\n0.21\ninformation\n0.05\n.32\n0\n0.21\n0\n0.58\np(context)\n0.16\n0.37\n0.11\n0.26\n0.11\nFigure 6.9\nReplacing the counts in Fig. 6.5 with joint probabilities, showing the marginals\naround the outside.\ncomputer\ndata\npinch\nresult\nsugar\napricot\n0\n0\n2.25\n0\n2.25\npineapple\n0\n0\n2.25\n0\n2.25\ndigital\n1.66\n0\n0\n0\n0\ninformation\n0\n0.57\n0\n0.47\n0\nFigure 6.10\nThe PPMI matrix showing the association between words and context words,\ncomputed from the counts in Fig. 6.5 again showing ﬁve dimensions.\nNote that the 0\nppmi values are ones that had a negative pmi; for example pmi(information,computer) =\nlog2(.05/(.16 ∗.58)) = −0.618, meaning that information and computer co-occur in this\nmini-corpus slightly less often than we would expect by chance, and with ppmi we re-\nplace negative values by zero.\nMany of the zero ppmi values had a pmi of −∞, like\npmi(apricot,computer) = log2(0/(0.16∗0.11)) = log2(0) = −∞.\nPMI has the problem of being biased toward infrequent events; very rare words\ntend to have very high PMI values. One way to reduce this bias toward low frequency\nevents is to slightly change the computation for P(c), using a different function Pα(c)\nthat raises contexts to the power of α:\nPPMIα(w,c) = max(log2\nP(w,c)\nP(w)Pα(c),0)\n(6.21)",
  "124": "118\nCHAPTER 6\n•\nVECTOR SEMANTICS\nPα(c) =\ncount(c)α\nP\nc count(c)α\n(6.22)\nLevy et al. (2015) found that a setting of α = 0.75 improved performance of\nembeddings on a wide range of tasks (drawing on a similar weighting used for skip-\ngrams described below in Eq. 6.31). This works because raising the probability to\nα = 0.75 increases the probability assigned to rare contexts, and hence lowers their\nPMI (Pα(c) > P(c) when c is rare).\nAnother possible solution is Laplace smoothing: Before computing PMI, a small\nconstant k (values of 0.1-3 are common) is added to each of the counts, shrinking\n(discounting) all the non-zero values. The larger the k, the more the non-zero counts\nare discounted.\ncomputer\ndata\npinch\nresult\nsugar\napricot\n2\n2\n3\n2\n3\npineapple\n2\n2\n3\n2\n3\ndigital\n4\n3\n2\n3\n2\ninformation\n3\n8\n2\n6\n2\nFigure 6.11\nLaplace (add-2) smoothing of the counts in Fig. 6.5.\ncomputer\ndata\npinch\nresult\nsugar\napricot\n0\n0\n0.56\n0\n0.56\npineapple\n0\n0\n0.56\n0\n0.56\ndigital\n0.62\n0\n0\n0\n0\ninformation\n0\n0.58\n0\n0.37\n0\nFigure 6.12\nThe Add-2 Laplace smoothed PPMI matrix from the add-2 smoothing counts\nin Fig. 6.11.\n6.8\nWord2vec\nIn the previous sections we saw how to represent a word as a sparse, long vector with\ndimensions corresponding to the words in the vocabulary, and whose values were tf-\nidf or PPMI functions of the count of the word co-occurring with each neighboring\nword. In this section we turn to an alternative method for representing a word: the\nuse of vectors that are short (of length perhaps 50-500) and dense (most values are\nnon-zero).\nIt turns out that dense vectors work better in every NLP task than sparse vec-\ntors. While we don’t complete understand all the reasons for this, we have some\nintuitions. First, dense vectors may be more successfully included as features in\nmachine learning systems; for example if we use 100-dimensional word embed-\ndings as features, a classiﬁer can just learn 100 weights to represent a function of\nword meaning; if we instead put in a 50,000 dimensional vector, a classiﬁer would\nhave to learn tens of thousands of weights for each of the sparse dimensions. Sec-\nond, because they contain fewer parameters than sparse vectors of explicit counts,\ndense vectors may generalize better and help avoid overﬁtting. Finally, dense vec-\ntors may do a better job of capturing synonymy than sparse vectors. For example,\ncar and automobile are synonyms; but in a typical sparse vector representation, the\ncar dimension and the automobile dimension are distinct dimensions. Because the",
  "125": "6.8\n•\nWORD2VEC\n119\nrelationship between these two dimensions is not modeled, sparse vectors may fail\nto capture the similarity between a word with car as a neighbor and a word with\nautomobile as a neighbor.\nIn this section we introduce one method for very dense, short vectors, skip-\ngram with negative sampling, sometimes called SGNS. The skip-gram algorithm\nskip-gram\nSGNS\nis one of two algorithms in a software package called word2vec, and so sometimes\nword2vec\nthe algorithm is loosely referred to as word2vec (Mikolov et al. 2013, Mikolov\net al. 2013a). The word2vec methods are fast, efﬁcient to train, and easily avail-\nable online with code and pretrained embeddings. We point to other embedding\nmethods, like the equally popular GloVe (Pennington et al., 2014), at the end of the\nchapter.\nThe intuition of word2vec is that instead of counting how often each word w oc-\ncurs near, say, apricot, we’ll instead train a classiﬁer on a binary prediction task: “Is\nword w likely to show up near apricot?” We don’t actually care about this prediction\ntask; instead we’ll take the learned classiﬁer weights as the word embeddings.\nThe revolutionary intuition here is that we can just use running text as implicitly\nsupervised training data for such a classiﬁer; a word s that occurs near the target\nword apricot acts as gold ‘correct answer’ to the question “Is word w likely to show\nup near apricot?” This avoids the need for any sort of hand-labeled supervision\nsignal. This idea was ﬁrst proposed in the task of neural language modeling, when\nBengio et al. (2003) and Collobert et al. (2011) showed that a neural language model\n(a neural network that learned to predict the next word from prior words) could just\nuse the next word in running text as its supervision signal, and could be used to learn\nan embedding representation for each word as part of doing this prediction task.\nWe’ll see how to do neural networks in the next chapter, but word2vec is a\nmuch simpler model than the neural network language model, in two ways. First,\nword2vec simpliﬁes the task (making it binary classiﬁcation instead of word pre-\ndiction). Second, word2vec simpliﬁes the architecture (training a logistic regression\nclassiﬁer instead of a multi-layer neural network with hidden layers that demand\nmore sophisticated training algorithms). The intuition of skip-gram is:\n1. Treat the target word and a neighboring context word as positive examples.\n2. Randomly sample other words in the lexicon to get negative samples\n3. Use logistic regression to train a classiﬁer to distinguish those two cases\n4. Use the regression weights as the embeddings\n6.8.1\nThe classiﬁer\nLet’s start by thinking about the classiﬁcation task, and then turn to how to train.\nImagine a sentence like the following, with a target word apricot and assume we’re\nusing a window of ±2 context words:\n... lemon,\na [tablespoon of apricot jam,\na] pinch ...\nc1\nc2\nt\nc3\nc4\nOur goal is to train a classiﬁer such that, given a tuple (t,c) of a target word\nt paired with a candidate context word c (for example (apricot, jam), or perhaps\n(apricot, aardvark) it will return the probability that c is a real context word (true\nfor jam, false for aardvark):\nP(+|t,c)\n(6.23)",
  "126": "120\nCHAPTER 6\n•\nVECTOR SEMANTICS\nThe probability that word c is not a real context word for t is just 1 minus\nEq. 6.23:\nP(−|t,c) = 1−P(+|t,c)\n(6.24)\nHow does the classiﬁer compute the probability P? The intuition of the skip-\ngram model is to base this probability on similarity: a word is likely to occur near\nthe target if its embedding is similar to the target embedding. How can we compute\nsimilarity between embeddings? Recall that two vectors are similar if they have a\nhigh dot product (cosine, the most popular similarity metric, is just a normalized dot\nproduct). In other words:\nSimilarity(t,c) ≈t ·c\n(6.25)\nOf course, the dot product t · c is not a probability, it’s just a number ranging from\n0 to ∞. (Recall, for that matter, that cosine isn’t a probability either). To turn the\ndot product into a probability, we’ll use the logistic or sigmoid function σ(x), the\nfundamental core of logistic regression:\nσ(x) =\n1\n1+e−x\n(6.26)\nThe probability that word c is a real context word for target word t is thus computed\nas:\nP(+|t,c) =\n1\n1+e−t·c\n(6.27)\nThe sigmoid function just returns a number between 0 and 1, so to make it a proba-\nbility we’ll need to make sure that the total probability of the two possible events (c\nbeing a context word, and c not being a context word) sum to 1.\nThe probability that word c is not a real context word for t is thus:\nP(−|t,c) = 1−P(+|t,c)\n=\ne−t·c\n1+e−t·c\n(6.28)\nEquation 6.27 give us the probability for one word, but we need to take account of\nthe multiple context words in the window. Skip-gram makes the strong but very\nuseful simplifying assumption that all context words are independent, allowing us to\njust multiply their probabilities:\nP(+|t,c1:k) =\nkY\ni=1\n1\n1+e−t·ci\n(6.29)\nlogP(+|t,c1:k) =\nk\nX\ni=1\nlog\n1\n1+e−t·ci\n(6.30)\nIn summary, skip-gram trains a probabilistic classiﬁer that, given a test target word\nt and its context window of k words c1:k, assigns a probability based on how similar\nthis context window is to the target word. The probability is based on applying the\nlogistic (sigmoid) function to the dot product of the embeddings of the target word\nwith each context word. We could thus compute this probability if only we had\nembeddings for each word target and context word in the vocabulary. Let’s now turn\nto learning these embeddings (which is the real goal of training this classiﬁer in the\nﬁrst place).",
  "127": "6.8\n•\nWORD2VEC\n121\n6.8.2\nLearning skip-gram embeddings\nWord2vec learns embeddings by starting with an initial set of embedding vectors\nand then iteratively shifting the embedding of each word w to be more like the em-\nbeddings of words that occur nearby in texts, and less like the embeddings of words\nthat don’t occur nearby.\nLet’s start by considering a single piece of the training data, from the sentence\nabove:\n... lemon,\na [tablespoon of apricot jam,\na] pinch ...\nc1\nc2\nt\nc3\nc4\nThis example has a target word t (apricot), and 4 context words in the L = ±2\nwindow, resulting in 4 positive training instances (on the left below):\npositive examples +\nt\nc\napricot tablespoon\napricot of\napricot preserves\napricot or\nnegative examples -\nt\nc\nt\nc\napricot aardvark apricot twelve\napricot puddle\napricot hello\napricot where\napricot dear\napricot coaxial\napricot forever\nFor training a binary classiﬁer we also need negative examples, and in fact skip-\ngram uses more negative examples than positive examples, the ratio set by a param-\neter k. So for each of these (t,c) training instances we’ll create k negative samples,\neach consisting of the target t plus a ‘noise word’. A noise word is a random word\nfrom the lexicon, constrained not to be the target word t. The right above shows the\nsetting where k = 2, so we’ll have 2 negative examples in the negative training set\n−for each positive example t,c.\nThe noise words are chosen according to their weighted unigram frequency\npα(w), where α is a weight. If we were sampling according to unweighted fre-\nquency p(w), it would mean that with unigram probability p(“the”) we would choose\nthe word the as a noise word, with unigram probability p(“aardvark”) we would\nchoose aardvark, and so on. But in practice it is common to set α = .75, i.e. use the\nweighting p\n3\n4 (w):\nPα(w) =\ncount(w)α\nP\nw′ count(w′)α\n(6.31)\nSetting α = .75 gives better performance because it gives rare noise words slightly\nhigher probability: for rare words, Pα(w) > P(w). To visualize this intuition, it\nmight help to work out the probabilities for an example with two events, P(a) = .99\nand P(b) = .01:\nPα(a) =\n.99.75\n.99.75 +.01.75 = .97\nPα(b) =\n.01.75\n.99.75 +.01.75 = .03\n(6.32)\nGiven the set of positive and negative training instances, and an initial set of\nembeddings, the goal of the learning algorithm is to adjust those embeddings such\nthat we\n• Maximize the similarity of the target word, context word pairs (t,c) drawn\nfrom the positive examples",
  "128": "122\nCHAPTER 6\n•\nVECTOR SEMANTICS\n• Minimize the similarity of the (t,c) pairs drawn from the negative examples.\nWe can express this formally over the whole training set as:\nL(θ) =\nX\n(t,c)∈+\nlogP(+|t,c)+\nX\n(t,c)∈−\nlogP(−|t,c)\n(6.33)\nOr, focusing in on one word/context pair (t,c) with its k noise words n1...nk, the\nlearning objective L is:\nL(θ) = logP(+|t,c)+\nk\nX\ni=1\nlogP(−|t,ni)\n= logσ(c·t)+\nk\nX\ni=1\nlogσ(−ni ·t)\n= log\n1\n1+e−c·t +\nk\nX\ni=1\nlog\n1\n1+eni·t\n(6.34)\nThat is, we want to maximize the dot product of the word with the actual context\nwords, and minimize the dot products of the word with the k negative sampled non-\nneighbor words.\nWe can then use stochastic gradient descent to train to this objective, iteratively\nmodifying the parameters (the embeddings for each target word t and each context\nword or noise word c in the vocabulary) to maximize the objective.\nNote that the skip-gram model thus actually learns two separate embeddings\nfor each word w: the target embedding t and the context embedding c. These\ntarget\nembedding\ncontext\nembedding\nembeddings are stored in two matrices, the target matrix T and the context matrix\nC. So each row i of the target matrix T is the 1 × d vector embedding ti for word\ni in the vocabulary V, and each column i of the context matrix C is a d × 1 vector\nembedding ci for word i in V. Fig. 6.13 shows an intuition of the learning task for\nthe embeddings encoded in these two matrices.\n1\n.\nk\n.\nn\n.\nV\n1.2…….j………V\n1\n.\n.\n.\nd\nW\nC\n1. ..    …   d\nincrease\nsimilarity( apricot , jam)\nwj . ck\njam\napricot\naardvark\ndecrease\nsimilarity( apricot , aardvark)\nwj . cn\n“…apricot jam…”\nneighbor word\nrandom noise\nword\nFigure 6.13\nThe skip-gram model tries to shift embeddings so the target embedding (here\nfor apricot) are closer to (have a higher dot product with) context embeddings for nearby\nwords (here jam) and further from (have a lower dot product with) context embeddings for\nwords that don’t occur nearby (here aardvark).\nJust as in logistic regression, then, the learning algorithm starts with randomly\ninitialized W and C matrices, and then walks through the training corpus using gra-\ndient descent to move W and C so as to maximize the objective in Eq. 6.34. Thus\nthe matrices W and C function as the parameters θ that logistic regression is tuning.",
  "129": "6.9\n•\nVISUALIZING EMBEDDINGS\n123\nOnce the embeddings are learned, we’ll have two embeddings for each word wi:\nti and ci. We can choose to throw away the C matrix and just keep W, in which case\neach word i will be represented by the vector ti.\nAlternatively we can add the two embeddings together, using the summed em-\nbedding ti + ci as the new d-dimensional embedding, or we can concatenate them\ninto an embedding of dimensionality 2d.\nAs with the simple count-based methods like tf-idf, the context window size L\neffects the performance of skip-gram embeddings, and experiments often tune the\nparameter L on a dev set. One difference from the count-based methods is that for\nskip-grams, the larger the window size the more computation the algorithm requires\nfor training (more neighboring words must be predicted).\n6.9\nVisualizing Embeddings\nVisualizing embeddings is an important goal in helping understands, apply, and im-\nprove these models of word meaning. But how can we visualize a (for example)\n100-dimensional vector?\nThe simplest way to visualize the meaning of a word w embedded in a space\nis to list the most similar words to w sorting all words in the vocabulary by their\ncosines. For example the 7 closest words to frog using the GloVe embeddings are:\nfrogs, toad, litoria, leptodactylidae, rana, lizard, and eleutherodactylus (Pennington\net al., 2014)\nWRIST\nANKLE\nSHOULDER\nARM\nLEG\nHAND\nFOOT\nHEAD\nNOSE\nFINGER\nTOE\nFACE\nEAR\nEYE\nTOOTH\nDOG\nCAT\nPUPPY\nKITTEN\nCOW\nMOUSE\nTURTLE\nOYSTER\nLION\nBULL\nCHICAGO\nATLANTA\nMONTREAL\nNASHVILLE\nTOKYO\nCHINA\nRUSSIA\nAFRICA\nASIA\nEUROPE\nAMERICA\nBRAZIL\nMOSCOW\nFRANCE\nHAWAII\nYet another visualization method is to use a clus-\ntering algorithm to show a hierarchical representa-\ntion of which words are similar to others in the em-\nbedding space. The example on the right uses hi-\nerarchical clustering of some embedding vectors for\nnouns as a visualization method (Rohde et al., 2006).\nProbably the most common visualization method,\nhowever, is to project the 100 dimensions of a word\ndown into 2 dimensions. Fig. 6.1 showed one such\nvisualization, using a projection method called t-\nSNE (van der Maaten and Hinton, 2008).\n6.10\nSemantic properties of embeddings\nVector semantic models have a number of parameters. One parameter that is relevant\nto both sparse tf-idf vectors and dense word2vec vectors is the size of the context\nwindow used to collect counts. This is generally between 1 and 10 words on each\nside of the target word (for a total context of 3-20 words).\nThe choice depends on on the goals of the representation. Shorter context win-\ndows tend to lead to representations that are a bit more syntactic, since the infor-\nmation is coming from immediately nearby words. When the vectors are computed\nfrom short context windows, the most similar words to a target word w tend to be\nsemantically similar words with the same parts of speech. When vectors are com-\nputed from long context windows, the highest cosine words to a target word w tend\nto be words that are topically related but not similar.",
  "130": "124\nCHAPTER 6\n•\nVECTOR SEMANTICS\nFor example Levy and Goldberg (2014a) showed that using skip-gram with a\nwindow of ±2, the most similar words to the word Hogwarts (from the Harry Potter\nseries) were names of other ﬁctional schools: Sunnydale (from Buffy the Vampire\nSlayer) or Evernight (from a vampire series). With a window of ±5, the most similar\nwords to Hogwarts were other words topically related to the Harry Potter series:\nDumbledore, Malfoy, and half-blood.\nIt’s also often useful to distinguish two kinds of similarity or association between\nwords (Sch¨utze and Pedersen, 1993). Two words have ﬁrst-order co-occurrence\nﬁrst-order\nco-occurrence\n(sometimes called syntagmatic association) if they are typically nearby each other.\nThus wrote is a ﬁrst-order associate of book or poem. Two words have second-order\nco-occurrence (sometimes called paradigmatic association) if they have similar\nsecond-order\nco-occurrence\nneighbors. Thus wrote is a second-order associate of words like said or remarked.\nAnalogy\nAnother semantic property of embeddings is their ability to capture re-\nlational meanings. Mikolov et al. (2013b) and Levy and Goldberg (2014b) show\nthat the offsets between vector embeddings can capture some analogical relations\nbetween words.\nFor example, the result of the expression vector(‘king’) - vec-\ntor(‘man’) + vector(‘woman’) is a vector close to vector(‘queen’); the left panel\nin Fig. 6.14 visualizes this, again projected down into 2 dimensions. Similarly, they\nfound that the expression vector(‘Paris’) - vector(‘France’) + vector(‘Italy’) results\nin a vector that is very close to vector(‘Rome’).\n(a)\n(b)\nFigure 6.14\nRelational properties of the vector space, shown by projecting vectors onto two dimensions. (a)\n’king’ - ’man’ + ’woman’ is close to ’queen’ (b) offsets seem to capture comparative and superlative morphology\n(Pennington et al., 2014).\nEmbeddings and Historical Semantics:\nEmbeddings can also be a useful tool\nfor studying how meaning changes over time, by computing multiple embedding\nspaces, each from texts written in a particular time period. For example Fig. 6.15\nshows a visualization of changes in meaning in English words over the last two\ncenturies, computed by building separate embedding spaces for each decade from\nhistorical corpora like Google N-grams (Lin et al., 2012) and the Corpus of Histori-\ncal American English (Davies, 2012).",
  "131": "6.11\n•\nBIAS AND EMBEDDINGS\n125\nFigure 6.15\nA t-SNE visualization of the semantic change of 3 words in English using\nword2vec vectors. The modern sense of each word, and the grey context words, are com-\nputed from the most recent (modern) time-point embedding space. Earlier points are com-\nputed from earlier historical embedding spaces. The visualizations show the changes in the\nword gay from meanings related to “cheerful” or “frolicsome” to referring to homosexuality,\nthe development of the modern “transmission” sense of broadcast from its original sense of\nsowing seeds, and the pejoration of the word awful as it shifted from meaning “full of awe”\nto meaning “terrible or appalling” (Hamilton et al., 2016b).\n6.11\nBias and Embeddings\nIn addition to their ability to learn word meaning from text, embeddings, alas, also\nreproduce the implicit biases and stereotypes that were latent in the text. Recall that\nembeddings model analogical relations; ‘queen’ as the closest word to ’king’ - ’man’\n+ ’woman’ implies the analogy man:woman::king:queen. But embedding analogies\nalso exhibit gender stereotypes. For example Bolukbasi et al. (2016) ﬁnd that the\nclosest occupation to ‘man’ - ‘computer programmer’ + ‘woman’ in word2vec em-\nbeddings trained on news text is ‘homemaker’, and that the embeddings similarly\nsuggest the analogy ‘father’ is to ‘doctor’ as ‘mother’ is to ‘nurse’. Algorithms that\nused embeddings as part of an algorithm to search for potential programmers or\ndoctors might thus incorrectly downweight documents with women’s names.\nEmbeddings also encode the implicit associations that are a property of human\nreasoning. The Implicit Association Test (Greenwald et al., 1998) measures peo-\nple’s associations between concepts (like ’ﬂowers’ or ’insects’) and attributes (like\n‘pleasantness’ and ‘unpleasantness’) by measuring differences in the latency with\nwhich they label words in the various categories.6 Using such methods, people\nin the United States have been shown to associate African-American names with\nunpleasant words (more than European-American names), male names more with\nmathematics and female names with the arts, and old people’s names with unpleas-\nant words (Greenwald et al. 1998, Nosek et al. 2002a, Nosek et al. 2002b). Caliskan\net al. (2017) replicated all these ﬁndings of implicit associations using GloVe vec-\ntors and cosine similarity instead of human latencies. For example Afrian American\nnames like ‘Leroy’ and ‘Shaniqua’ had a higher GloVe cosine with unpleasant words\nwhile European American names (‘Brad’, ‘Greg’, ‘Courtney’) had a higher cosine\nwith pleasant words. Any embedding-aware algorithm that made use of word senti-\nment could thus lead to bias against African Americans.\n6\nRoughly speaking, if humans associate ‘ﬂowers’ with ’pleasantness’ and ‘insects’ with ‘unpleasant-\nness’, when they are instructed to push a red button for ‘ﬂowers’ (daisy, iris, lilac) and ’pleasant words’\n(love, laughter, pleasure) and a green button for ‘insects’ (ﬂea, spider, mosquito) and ‘unpleasant words’\n(abuse, hatred, ugly) they are faster than in an incongruous condition where they push a red button for\n‘ﬂowers’ and ‘unpleasant words’ and a green button for ‘insects’ and ‘pleasant words’.",
  "132": "126\nCHAPTER 6\n•\nVECTOR SEMANTICS\nRecent research focuses on ways to try to remove the kinds of biases, for example\nby developing a transformation of the embedding space that removes gender stereo-\ntypes but preserves deﬁnitional gender (Bolukbasi et al. 2016, Zhao et al. 2017).\nHistorical embeddings are also being used to measure biases in the past. Garg\net al. (2018) used embeddings from historical texts to measure the association be-\ntween embeddings for occupations and embeddings for names of various ethnici-\nties or genders (for example the relative cosine similarity of women’s names versus\nmen’s to occupation words like ‘librarian’ or ‘carpenter’) across the 20th century.\nThey found that the cosines correlate with the empirical historical percentages of\nwomen or ethnic groups in those occupation. Historical embeddings also replicated\nold surveys of ethnic stereotypes; the tendency of experimental participants in 1933\nto associate adjectives like ‘industrious’ or ‘superstitious’ with, e.g., Chinese eth-\nnicity, correlates with the cosine between Chinese last names and those adjectives\nusing embeddings trained on 1930s text. They also were able to document historical\ngender biases, such as the fact that embeddings for adjectives related to competence\n(‘smart’, ‘wise’, ‘thoughtful’, ’resourceful’) had a higher cosine with male than fe-\nmale words, and showed that this bias has been slowly decreasing since 1960.\nWe will return in later chapters to this question about the role of bias in natural\nlanguage processing and machine learning in general.\n6.12\nEvaluating Vector Models\nThe most important evaluation metric for vector models is extrinsic evaluation on\ntasks; adding them as features into any NLP task and seeing whether this improves\nperformance over some other model.\nNonetheless it is useful to have intrinsic evaluations. The most common metric\nis to test their performance on similarity, computing the correlation between an\nalgorithm’s word similarity scores and word similarity ratings assigned by humans.\nWordSim-353 (Finkelstein et al., 2002) is a commonly used set of ratings from 0\nto 10 for 353 noun pairs; for example (plane, car) had an average score of 5.77.\nSimLex-999 (Hill et al., 2015) is a more difﬁcult dataset that quantiﬁes similarity\n(cup, mug) rather than relatedness (cup, coffee), and including both concrete and\nabstract adjective, noun and verb pairs. The TOEFL dataset is a set of 80 questions,\neach consisting of a target word with 4 additional word choices; the task is to choose\nwhich is the correct synonym, as in the example: Levied is closest in meaning to:\nimposed, believed, requested, correlated (Landauer and Dumais, 1997). All of these\ndatasets present words without context.\nSlightly more realistic are intrinsic similarity tasks that include context. The\nStanford Contextual Word Similarity (SCWS) dataset (Huang et al., 2012) offers a\nricher evaluation scenario, giving human judgments on 2,003 pairs of words in their\nsentential context, including nouns, verbs, and adjectives. This dataset enables the\nevaluation of word similarity algorithms that can make use of context words. The\nsemantic textual similarity task (Agirre et al. 2012, Agirre et al. 2015) evaluates the\nperformance of sentence-level similarity algorithms, consisting of a set of pairs of\nsentences, each pair with human-labeled similarity scores.\nAnother task used for evaluate is an analogy task, where the system has to solve\nproblems of the form a is to b as c is to d, given a, b, and c and having to ﬁnd d.\nThus given Athens is to Greece as Oslo is to\n, the system must ﬁll in the word\nNorway. Or more syntactically-oriented examples: given mouse, mice, and dollar",
  "133": "6.13\n•\nSUMMARY\n127\nthe system must return dollars. Large sets of such tuples have been created (Mikolov\net al. 2013, Mikolov et al. 2013b).\n6.13\nSummary\n• In vector semantics, a word is modeled as a vector—a point in high-dimensional\nspace, also called an embedding.\n• Vector semantic models fall into two classes: sparse and dense. In sparse\nmodels like tf-idf each dimension corresponds to a word in the vocabulary V;\n• Cell in sparse models are functions of co-occurrence counts. The term-\ndocument matrix has rows for each word (term) in the vocabulary and a\ncolumn for each document.\n• The word-context matrix has a row for each (target) word in the vocabulary\nand a column for each context term in the vocabulary.\n• A common sparse weighting is tf-idf, which weights each cell by its term\nfrequency and inverse document frequency.\n• Word and document similarity is computed by computing the dot product\nbetween vectors. The cosine of two vectors—a normalized dot product—is\nthe most popular such metric.\n• PPMI (pointwise positive mutual information) is an alternative weighting\nscheme to tf-idf.\n• Dense vector models have dimensionality 50-300 and the dimensions are harder\nto interpret.\n• The word2vec family of models, including skip-gram and CBOW, is a pop-\nular efﬁcient way to compute dense embeddings.\n• Skip-gram trains a logistic regression classiﬁer to compute the probability that\ntwo words are ‘likely to occur nearby in text’. This probability is computed\nfrom the dot product between the embeddings for the two words,\n• Skip-gram uses stochastic gradient descent to train the classiﬁer, by learning\nembeddings that have a high dot-product with embeddings of words that occur\nnearby and a low dot-product with noise words.\n• Other important embedding algorithms include GloVe, a method based on ra-\ntios of word co-occurrence probabilities, and fasttext, an open-source library\nfor computing word embeddings by summing embeddings of the bag of char-\nacter n-grams that make up a word.\nBibliographical and Historical Notes\nThe idea of vector semantics arose out of research in the 1950s in three distinct\nﬁelds: linguistics, psychology, and computer science, each of which contributed a\nfundamental aspect of the model.\nThe idea that meaning was related to distribution of words in context was widespread\nin linguistic theory of the 1950s, among distributionalists like Zellig Harris, Martin\nJoos, and J. R. Firth, and semioticians like Thomas Sebeok. As Joos (1950) put it,",
  "134": "128\nCHAPTER 6\n•\nVECTOR SEMANTICS\nthe linguist’s “meaning” of a morpheme. . . is by deﬁnition the set of conditional\nprobabilities of its occurrence in context with all other morphemes.\nThe idea that the meaning of a word might be modeled as a point in a multi-\ndimensional semantic space came from psychologists like Charles E. Osgood, who\nhad been studying how people responded to the meaning of words by assigning val-\nues along scales like happy/sad, or hard/soft. Osgood et al. (1957) proposed that\nthe meaning of a word in general could be modeled as a point in a multidimensional\nEuclidean space, and that the similarity of meaning between two words could be\nmodeled as the distance between these points in the space.\nA ﬁnal intellectual source in the 1950s and early 1960s was the ﬁeld then called\nmechanical indexing, now known as information retrieval. In what became known\nmechanical\nindexing\nas the vector space model for information retrieval (Salton 1971,Sparck Jones 1986),\nresearchers demonstrated new ways to deﬁne the meaning of words in terms of vec-\ntors (Switzer, 1965), and reﬁned methods for word similarity based on measures\nof statistical association between words like mutual information (Giuliano, 1965)\nand idf (Sparck Jones, 1972), and showed that the meaning of documents could be\nrepresented in the same vector spaces used for words.\nMore distantly related is the idea of deﬁning words by a vector of discrete fea-\ntures, which has a venerable history in our ﬁeld, with roots at least as far back as\nDescartes and Leibniz (Wierzbicka 1992, Wierzbicka 1996). By the middle of the\n20th century, beginning with the work of Hjelmslev (Hjelmslev, 1969) and ﬂeshed\nout in early models of generative grammar (Katz and Fodor, 1963), the idea arose of\nrepresenting meaning with semantic features, symbols that represent some sort of\nsemantic\nfeature\nprimitive meaning. For example words like hen, rooster, or chick, have something\nin common (they all describe chickens) and something different (their age and sex),\nrepresentable as:\nhen\n+female, +chicken, +adult\nrooster -female, +chicken, +adult\nchick\n+chicken, -adult\nThe dimensions used by vector models of meaning to deﬁne words, however, are\nonly abstractly related to this idea of a small ﬁxed number of hand-built dimensions.\nNonetheless, there has been some attempt to show that certain dimensions of em-\nbedding models do contribute some speciﬁc compositional aspect of meaning like\nthese early semantic features.\nThe ﬁrst use of dense vectors to model word meaning was the latent seman-\ntic indexing (LSI) model (Deerwester et al., 1988) recast as LSA (latent semantic\nanalysis) (Deerwester et al., 1990). In LSA SVD is applied to a term-document ma-\ntrix (each cell weighted by log frequency and normalized by entropy), and then using\nthe ﬁrst 300 dimensions as the embedding. LSA was then quickly widely applied:\nas a cognitive model Landauer and Dumais (1997), and tasks like spell checking\n(Jones and Martin, 1997), language modeling (Bellegarda 1997, Coccaro and Ju-\nrafsky 1998, Bellegarda 2000) morphology induction (Schone and Jurafsky 2000,\nSchone and Jurafsky 2001), and essay grading (Rehder et al., 1998). Related mod-\nels were simultaneously developed and applied to word sense disambiguation by\nSch¨utze (1992b). LSA also led to the earliest use of embeddings to represent words\nin a probabilistic classiﬁer, in the logistic regression document router of Sch¨utze\net al. (1995). The idea of SVD on the term-term matrix (rather than the term-\ndocument matrix) as a model of meaning for NLP was proposed soon after LSA\nby Sch¨utze (1992b). Sch¨utze applied the low-rank (97-dimensional) embeddings\nproduced by SVD to the task of word sense disambiguation, analyzed the result-",
  "135": "BIBLIOGRAPHICAL AND HISTORICAL NOTES\n129\ning semantic space, and also suggested possible techniques like dropping high-order\ndimensions. See Sch¨utze (1997a).\nA number of alternative matrix models followed on from the early SVD work,\nincluding Probabilistic Latent Semantic Indexing (PLSI) (Hofmann, 1999) Latent\nDirichlet Allocation (LDA) (Blei et al., 2003). Nonnegative Matrix Factorization\n(NMF) (Lee and Seung, 1999).\nBy the next decade, Bengio et al. (2003) and Bengio et al. (2006) showed that\nneural language models could also be used to develop embeddings as part of the task\nof word prediction. Collobert and Weston (2007), Collobert and Weston (2008), and\nCollobert et al. (2011) then demonstrated that embeddings could play a role for rep-\nresenting word meanings for a number of NLP tasks. Turian et al. (2010) compared\nthe value of different kinds of embeddings for different NLP tasks. Mikolov et al.\n(2011) showed that recurrent neural nets could be used as language models. The\nidea of simplifying the hidden layer of these neural net language models to create\nthe skip-gram and CBOW algorithms was proposed by Mikolov et al. (2013). The\nnegative sampling training algorithm was proposed in Mikolov et al. (2013a).\nStudies of embeddings include results showing an elegant mathematical relation-\nship between sparse and dense embeddings (Levy and Goldberg, 2014c), as well\nas numerous surveys of embeddings and their parameterizations. (Bullinaria and\nLevy 2007, Bullinaria and Levy 2012, Lapesa and Evert 2014, Kiela and Clark 2014,\nLevy et al. 2015).\nThere are many other embedding algorithms, using methods like non-negative\nmatrix factorization (Fyshe et al., 2015), or by converting sparse PPMI embeddings\nto dense vectors by using SVD (Levy and Goldberg, 2014c). The most widely-\nused embedding model besides word2vec is GloVe (Pennington et al., 2014). The\nname stands for Global Vectors, because the model is based on capturing global\ncorpus statistics. GloVe is based on ratios of probabilities from the word-word co-\noccurrence matrix, combining the intuitions of count-based models like PPMI while\nalso capturing the linear structures used by methods like word2vec.\nAn extension of word2vec, fasttext (Bojanowski et al., 2017), deals with un-\nfasttext\nknown words and sparsity in languages with rich morphology, by using subword\nmodels. Each word in fasttext is represented as itself plus a bag of constituent n-\ngrams, with special boundary symbols < and > added to each word. For example,\nwith n = 3 the word where would be represented by the character n-grams:\n<wh, whe, her, ere, re>\nplus the sequence\n<where>\nThen a skipgram embedding is learned for each constituent n-gram, and the word\nwhere is represented by the sum of all of the embeddings of its constituent n-grams.\nA fasttext open-source library, including pretrained embeddings for 157 languages,\nis available at https://fasttext.cc.\nSee Manning et al. (2008) for a deeper understanding of the role of vectors in in-\nformation retrieval, including how to compare queries with documents, more details\non tf-idf, and issues of scaling to very large datasets.\nCruse (2004) is a useful introductory linguistic text on lexical semantics.",
  "136": "130\nCHAPTER 6\n•\nVECTOR SEMANTICS\nExercises",
  "137": "CHAPTER\n7\nNeural Networks and Neural\nLanguage Models\n“[M]achines of this character can behave in a very complicated manner when\nthe number of units is large.”\nAlan Turing (1948) “Intelligent Machines”, page 6\nNeural networks are an essential computational tool for language processing, and\na very old one. They are called neural because their origins lie in the McCulloch-\nPitts neuron (McCulloch and Pitts, 1943), a simpliﬁed model of the human neuron\nas a kind of computing element that could be described in terms of propositional\nlogic. But the modern use in language processing no longer draws on these early\nbiological inspirations.\nInstead, a modern neural network is a network of small computing units, each\nof which takes a vector of input values and produces a single output value. In this\nchapter we introduce the neural net applied to classiﬁcation. The architecture we\nintroduce is called a feed-forward network because the computation proceeds iter-\natively from one layer of units to the next. The use of modern neural nets is often\ncalled deep learning, because modern networks are often deep (have many layers).\ndeep learning\ndeep\nNeural networks share much of the same mathematics as logistic regression. But\nneural networks are a more powerful classiﬁer than logistic regression, and indeed a\nminimal neural network (technically one with a single ‘hidden layer’) can be shown\nto learn any function.\nNeural net classiﬁers are different from logistic regression in another way. With\nlogistic regression, we applied the regression classiﬁer to many different tasks by\ndeveloping many rich kinds of feature templates based on domain knowledge. When\nworking with neural networks, it is more common to avoid the use of rich hand-\nderived features, instead building neural networks that take raw words as inputs\nand learn to induce features as part of the process of learning to classify. We saw\nexamples of this kind of representation learning for embeddings in Chapter 6. Nets\nthat are very deep are particularly good at representation learning for that reason\ndeep neural nets are the right tool for large scale problems that offer sufﬁcient data\nto learn features automatically.\nIn this chapter we’ll see feedforward networks as classiﬁers, and apply them to\nthe simple task of language modeling: assigning probabilities to word sequences and\npredicting upcoming words. In later chapters we’ll introduce many other aspects of\nneural models, such as the recurrent neural network and the encoder-decoder\nmodel.",
  "138": "132\nCHAPTER 7\n•\nNEURAL NETWORKS AND NEURAL LANGUAGE MODELS\n7.1\nUnits\nThe building block of a neural network is a single computational unit. A unit takes\na set of real valued numbers as input, performs some computation on them, and\nproduces an output.\nAt its heart, a neural unit is taking a weighted sum of its inputs, with one addi-\ntional term in the sum called a bias term. Thus given a set of inputs x1...xn, a unit\nbias term\nhas a set of corresponding weights w1...wn and a bias b, so the weighted sum z can\nbe represented as:\nz = b+\nX\ni\nwixi\n(7.1)\nOften it’s more convenient to express this weighted sum using vector notation;\nrecall from linear algebra that a vector is, at heart, just a list or array of numbers.\nvector\nThus we’ll talk about z in terms of a weight vector w, a scalar bias b, and an input\nvector x, and we’ll replace the sum with the convenient dot product:\nz = w·x+b\n(7.2)\nAs deﬁned in Eq. 7.2, z is just a real valued number.\nFinally, instead of using z, a linear function of x, as the output, neural units\napply a non-linear function f to z. We will refer to the output of this function as\nthe activation value for the unit, a. Since we are just modeling a single unit, the\nactivation\nactivation for the node is in fact the ﬁnal output of the network, which we’ll generally\ncall y. So the value y is deﬁned as:\ny = a = f(z)\n(7.3)\nWe’ll discuss three popular non-linear functions f() below (the sigmoid, the\ntanh, and the rectiﬁed linear ReLU) but it’s pedagogically convenient to start with\nthe sigmoid function since we saw it in Chapter 5:\nsigmoid\ny = σ(z) =\n1\n1+e−z\n(7.4)\nThe sigmoid (shown in Fig. 7.1) has a number of advantages; it maps the output\ninto the range [0,1], which is useful in squashing outliers toward 0 or 1. And it’s\ndifferentiable, which as we saw in Section 5.8 will be handy for learning.\nSubstituting the sigmoid equation into Eq. 7.2 gives us the ﬁnal value for the\noutput of a neural unit:\ny = σ(w·x+b) =\n1\n1+exp(−(w·x+b))\n(7.5)\nFig. 7.2 shows a ﬁnal schematic of a basic neural unit. In this example the unit\ntakes 3 input values x1,x2, and x3, and computes a weighted sum, multiplying each\nvalue by a weight (w1, w2, and w3, respectively), adds them to a bias term b, and then\npasses the resulting sum through a sigmoid function to result in a number between 0\nand 1.\nLet’s walk through an example just to get an intuition. Let’s suppose we have a\nunit with the following weight vector and bias:",
  "139": "7.1\n•\nUNITS\n133\nFigure 7.1\nThe sigmoid function takes a real value and maps it to the range [0,1]. Because\nit is nearly linear around 0 but has a sharp slope toward the ends, it tends to squash outlier\nvalues toward 0 or 1.\nx1\nx2\nx3\ny\nw1\nw2\nw3\n∑\nb\nσ\n+1\nz\na\nFigure 7.2\nA neural unit, taking 3 inputs x1, x2, and x3 (and a bias b that we represent as a\nweight for an input clamped at +1) and producing an output y. We include some convenient\nintermediate variables: the output of the summation, z, and the output of the sigmoid, a. In\nthis case the output of the unit y is the same as a, but in deeper networks we’ll reserve y to\nmean the ﬁnal output of the entire network, leaving a as the activation of an individual node.\nw = [0.2,0.3,0.9]\nb = 0.5\nWhat would this unit do with the following input vector:\nx = [0.5,0.6,0.1]\nThe resulting output y would be:\ny = σ(w·x+b) =\n1\n1+e−(w·x+b) =\n1\n1+e−(.5∗.2+.6∗.3+.1∗.9+.5) = e−0.87 = .70\nIn practice, the sigmoid is not commonly used as an activation function. A\nfunction that is very similar but almost always better is the tanh function shown\ntanh\nin Fig. 7.3a; tanh is a variant of the sigmoid that ranges from -1 to +1:\ny = ez −e−z\nez +e−z\n(7.6)\nThe simplest activation function, and perhaps the most commonly used, is the\nrectiﬁed linear unit, also called the ReLU, shown in Fig. 7.3b. It’s just the same as x\nReLU",
  "140": "134\nCHAPTER 7\n•\nNEURAL NETWORKS AND NEURAL LANGUAGE MODELS\nwhen x is positive, and 0 otherwise:\ny = max(x,0)\n(7.7)\n(a)\n(b)\nFigure 7.3\nThe tanh and ReLU activation functions.\nThese activation functions have different properties that make them useful for\ndifferent language applications or network architectures. For example the rectiﬁer\nfunction has nice properties that result from it being very close to linear. In the sig-\nmoid or tanh functions, very high values of z result in values of y that are saturated,\nsaturated\ni.e., extremely close to 1, which causes problems for learning. Rectiﬁers don’t have\nthis problem, since the output of values close to 1 also approaches 1 in a nice gentle\nlinear way. By contrast, the tanh function has the nice properties of being smoothly\ndifferentiable and mapping outlier values toward the mean.\n7.2\nThe XOR problem\nEarly in the history of neural networks it was realized that the power of neural net-\nworks, as with the real neurons that inspired them, comes from combining these\nunits into larger networks.\nOne of the most clever demonstrations of the need for multi-layer networks was\nthe proof by Minsky and Papert (1969) that a single neural unit cannot compute\nsome very simple functions of its input. Consider the very simple task of computing\nsimple logical functions of two inputs, like AND, OR, and XOR. As a reminder,\nhere are the truth tables for those functions:\nAND\nOR\nXOR\nx1 x2 y\nx1 x2 y\nx1 x2 y\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n1\n0\n0\n1\n1\n0\n1\n1\n1\n0\n0\n1\n0\n1\n1\n0\n1\n1\n1\n1\n1\n1\n1\n1\n1\n0\nThis example was ﬁrst shown for the perceptron, which is a very simple neural\nperceptron\nunit that has a binary output and no non-linear activation function. The output y of",
  "141": "7.2\n•\nTHE XOR PROBLEM\n135\na perceptron is 0 or 1, and just computed as follows (using the same weight w, input\nx, and bias b as in Eq. 7.2):\ny =\n\u001a\n0, if w·x+b ≤0\n1, if w·x+b > 0\n(7.8)\nIt’s very easy to build a perceptron that can compute the logical AND and OR\nfunctions of its binary inputs; Fig. 7.4 shows the necessary weights.\nx1\nx2\n+1\n-1\n1\n1\nx1\nx2\n+1\n0\n1\n1\n(a)\n(b)\nFigure 7.4\nThe weights w and bias b for perceptrons for computing logical functions. The\ninputs are shown as x1 and x2 and the bias as a special node with value +1 which is multiplied\nwith the bias weight b. (a) logical AND, showing weights w1 = 1 and w2 = 1 and bias weight\nb = −1. (b) logical OR, showing weights w1 = 1 and w2 = 1 and bias weight b = 0. These\nweights/biases are just one from an inﬁnite number of possible sets of weights and biases that\nwould implement the functions.\nIt turns out, however, that it’s not possible to build a perceptron to compute\nlogical XOR! (It’s worth spending a moment to give it a try!)\nThe intuition behind this important result relies on understanding that a percep-\ntron is a linear classiﬁer. For a two-dimensional input x0 and x1, the perception\nequation, w1x1 + w2x2 + b = 0 is the equation of a line (we can see this by putting\nit in the standard linear format: x2 = −(w1/w2)x1 −b.) This line acts as a decision\nboundary in two-dimensional space in which the output 0 is assigned to all inputs\ndecision\nboundary\nlying on one side of the line, and the output 1 to all input points lying on the other\nside of the line. If we had more than 2 inputs, the decision boundary becomes a\nhyperplane instead of a line, but the idea is the same, separating the space into two\ncategories.\nFig. 7.5 shows the possible logical inputs (00, 01, 10, and 11) and the line drawn\nby one possible set of parameters for an AND and an OR classiﬁer. Notice that there\nis simply no way to draw a line that separates the positive cases of XOR (01 and 10)\nfrom the negative cases (00 and 11). We say that XOR is not a linearly separable\nlinearly\nseparable\nfunction. Of course we could draw a boundary with a curve, or some other function,\nbut not a single line.\n7.2.1\nThe solution: neural networks\nWhile the XOR function cannot be calculated by a single perceptron, it can be cal-\nculated by a layered network of units. Let’s see an example of how to do this from\nGoodfellow et al. (2016) that computes XOR using two layers of ReLU-based units.\nFig. 7.6 shows a ﬁgure with the input being processed by two layers of neural units.\nThe middle layer (called h) has two units, and the output layer (called y) has one\nunit. A set of weights and biases are shown for each ReLU that correctly computes\nthe XOR function\nLet’s walk through what happens with the input x = [0 0]. If we multiply each\ninput value by the appropriate weight, sum, and then add the bias b, we get the vector",
  "142": "136\nCHAPTER 7\n•\nNEURAL NETWORKS AND NEURAL LANGUAGE MODELS\n0\n0\n1\n1\nx1\nx2\n0\n0\n1\n1\nx1\nx2\n0\n0\n1\n1\nx1\nx2\na)  x1 AND x2\nb)  x1 OR x2\nc)  x1 XOR x2\n?\nFigure 7.5\nThe functions AND, OR, and XOR, represented with input x0 on the x-axis and input x1 on the\ny axis, Filled circles represent perceptron outputs of 1, and white circles perceptron outputs of 0. There is no\nway to draw a line that correctly separates the two categories for XOR. Figure styled after Russell and Norvig\n(2002).\nx1\nx2\nh1\nh2\ny1\n+1\n1\n-1\n1\n1\n1\n-2\n0\n1\n+1\n0\nFigure 7.6\nXOR solution after Goodfellow et al. (2016). There are three ReLU units, in\ntwo layers; we’ve called them h1, h2 (h for “hidden layer”) and y1. As before, the numbers\non the arrows represent the weights w for each unit, and we represent the bias b as a weight\non a unit clamped to +1, with the bias weights/units in gray.\n[0 -1], and we then we apply the rectiﬁed linear transformation to give the output\nof the h layer as [0 0]. Now we once again multiply by the weights, sum, and add\nthe bias (0 in this case) resulting in the value 0. The reader should work through the\ncomputation of the remaining 3 possible input pairs to see that the resulting y values\ncorrectly are 1 for the inputs [0 1] and [1 0] and 0 for [0 0] and [1 1].\nIt’s also instructive to look at the intermediate results, the outputs of the two\nhidden nodes h0 and h1. We showed in the previous paragraph that the h vector for\nthe inputs x = [0 0] was [0 0]. Fig. 7.7b shows the values of the h layer for all 4\ninputs. Notice that hidden representations of the two input points x = [0 1] and x\n= [1 0] (the two cases with XOR output = 1) are merged to the single point h = [1\n0]. The merger makes it easy to linearly separate the positive and negative cases\nof XOR. In other words, we can view the hidden layer of the network is forming a\nrepresentation for the input.\nIn this example we just stipulated the weights in Fig. 7.6. But for real exam-\nples the weights for neural networks are learned automatically using the error back-\npropagation algorithm to be introduced in Section 7.4. That means the hidden layers\nwill learn to form useful representations. This intuition, that neural networks can au-\ntomatically learn useful representations of the input, is one of their key advantages,",
  "143": "7.3\n•\nFEED-FORWARD NEURAL NETWORKS\n137\n0\n0\n1\n1\nx0\nx1\na) The original x space\n0\n0\n1\n1\nh0\nh1\n2\nb) The new h space\nFigure 7.7\nThe hidden layer forming a new representation of the input. Here is the rep-\nresentation of the hidden layer, h, compared to the original input representation x. Notice\nthat the input point [0 1] has been collapsed with the input point [1 0], making it possible to\nlinearly separate the positive and negative cases of XOR. After Goodfellow et al. (2016).\nand one that we will return to again and again in later chapters.\nNote that the solution to the XOR problem requires a network of units with non-\nlinear activation functions. A network made up of simple linear (perceptron) units\ncannot solve the XOR problem. This is because a network formed by many layers\nof purely linear units can always be reduced (shown to be computationally identical\nto) a single layer of linear units with appropriate weights, and we’ve already shown\n(visually, in Fig. 7.5) that a single unit cannot solve the XOR problem.\n7.3\nFeed-Forward Neural Networks\nLet’s now walk through a slightly more formal presentation of the simplest kind of\nneural network, the feed-forward network. A feed-forward network is a multilayer\nfeed-forward\nnetwork\nnetwork in which the units are connected with no cycles; the outputs from units in\neach layer are passed to units in the next higher layer, and no outputs are passed\nback to lower layers. (In Chapter 9 we’ll introduce networks with cycles, called\nrecurrent neural networks.)\nFor historical reasons multilayer networks, especially feedforward networks, are\nsometimes called multi-layer perceptrons (or MLPs); this is a technical misnomer,\nmulti-layer\nperceptrons\nMLP\nsince the units in modern multilayer networks aren’t perceptrons (perceptrons are\npurely linear, but modern networks are made up of units with non-linearities like\nsigmoids), but at some point the name stuck.\nSimple feed-forward networks have three kinds of nodes: input units, hidden\nunits, and output units. Fig. 7.8 shows a picture.\nThe input units are simply scalar values just as we saw in Fig. 7.2.\nThe core of the neural network is the hidden layer formed of hidden units,\nhidden layer\neach of which is a neural unit as described in Section 7.1, taking a weighted sum of\nits inputs and then applying a non-linearity. In the standard architecture, each layer\nis fully-connected, meaning that each unit in each layer takes as input the outputs\nfully-connected\nfrom all the units in the previous layer, and there is a link between every pair of units\nfrom two adjacent layers. Thus each hidden unit sums over all the input units.",
  "144": "138\nCHAPTER 7\n•\nNEURAL NETWORKS AND NEURAL LANGUAGE MODELS\nx1\nx2\nh1\nh2\ny1\nxn0\n…\nh3\nhn1\n…\n+1\nb\n…\nU\nW\ny2\nyn2\nFigure 7.8\nA simple 2-layer feed-forward network, with one hidden layer, one output layer,\nand one input layer (the input layer is usually not counted when enumerating layers).\nRecall that a single hidden unit has parameters w (the weight vector) and b (the\nbias scalar). We represent the parameters for the entire hidden layer by combining\nthe weight vector wi and bias bi for each unit i into a single weight matrix W and\na single bias vector b for the whole layer (see Fig. 7.8). Each element Wij of the\nweight matrix W represents the weight of the connection from the ith input unit xi to\nthe the jth hidden unit hj.\nThe advantage of using a single matrix W for the weights of the entire layer is\nthat now that hidden layer computation for a feedforward network can be done very\nefﬁciently with simple matrix operations. In fact, the computation only has three\nsteps: multiplying the weight matrix by the input vector x, adding the bias vector b,\nand applying the activation function g (such as the sigmoid, tanh, or relu activation\nfunction deﬁned above).\nThe output of the hidden layer, the vector h, is thus the following, using the\nsigmoid function σ:\nh = σ(Wx+b)\n(7.9)\nNotice that we’re applying the σ function here to a vector, while in Eq. 7.4 it was\napplied to a scalar. We’re thus allowing σ(·), and indeed any activation function\ng(·), to apply to a vector element-wise, so g[z1,z2,z3] = [g(z1),g(z2),g(z3)].\nLet’s introduce some constants to represent the dimensionalities of these vectors\nand matrices. We’ll refer to the input layer as layer 0 of the network, and use have\nn0 represent the number of inputs, so x is a vector of real numbers of dimension\nn0, or more formally x ∈Rn0. Let’s call the hidden layer layer 1 and the output\nlayer layer 2. The hidden layer has dimensionality n1, so h ∈Rn1 and also b ∈Rn1\n(since each hidden unit can take a different bias value). And the weight matrix W\nhas dimensionality W ∈Rn1×n0.\nTake a moment to convince yourself that the matrix multiplication in Eq. 7.9 will\ncompute the value of each hij as Pnx\ni=1 wijxi +bj.\nAs we saw in Section 7.2, the resulting value h (for hidden but also for hypoth-\nesis) forms a representation of the input. The role of the output layer is to take\nthis new representation h and compute a ﬁnal output. This output could be a real-\nvalued number, but in many cases the goal of the network is to make some sort of\nclassiﬁcation decision, and so we will focus on the case of classiﬁcation.\nIf we are doing a binary task like sentiment classiﬁcation, we might have a single",
  "145": "7.3\n•\nFEED-FORWARD NEURAL NETWORKS\n139\noutput node, and its value y is the probability of positive versus negative sentiment.\nIf we are doing multinomial classiﬁcation, such as assigning a part-of-speech tag, we\nmight have one output node for each potential part-of-speech, whose output value\nis the probability of that part-of-speech, and the values of all the output nodes must\nsum to one. The output layer thus gives a probability distribution across the output\nnodes.\nLet’s see how this happens. Like the hidden layer, the output layer has a weight\nmatrix (let’s call it U), but output layers may not t have a bias vector b, so we’ll sim-\nplify by eliminating the bias vector in this example. The weight matrix is multiplied\nby its input vector (h) to produce the intermediate output z.\nz = Uh\nThere are n2 output nodes, so z ∈Rn2, weight matrix U has dimensionality U ∈\nRn2×n1, and element Ui j is the weight from unit j in the hidden layer to unit i in the\noutput layer.\nHowever, z can’t be the output of the classiﬁer, since it’s a vector of real-valued\nnumbers, while what we need for classiﬁcation is a vector of probabilities. There is\na convenient function for normalizing a vector of real values, by which we mean\nnormalizing\nconverting it to a vector that encodes a probability distribution (all the numbers lie\nbetween 0 and 1 and sum to 1): the softmax function that we saw on page 96 of\nsoftmax\nChapter 5. For a vector z of dimensionality d, the softmax is deﬁned as:\nsoftmax(zi) =\nezi\nPd\nj=1 ezj 1 ≤i ≤d\n(7.10)\nThus for example given a vector z=[0.6 1.1 -1.5 1.2 3.2 -1.1], softmax(z) is [ 0.055\n0.090 0.0067 0.10 0.74 0.010].\nYou may recall that softmax was exactly what is used to create a probability\ndistribution from a vector of real-valued numbers (computed from summing weights\ntimes features) in logistic regression in Chapter 5.\nThat means we can think of a neural network classiﬁer with one hidden layer\nas building a vector h which is a hidden layer representation of the input, and then\nrunning standard logistic regression on the features that the network develops in h.\nBy contrast, in Chapter 5 the features were mainly designed by hand via feature\ntemplates. So a neural network is like logistic regression, but (a) with many layers,\nsince a deep neural network is like layer after layer of logistic regression classiﬁers,\nand (b) rather than forming the features by feature templates, the prior layers of the\nnetwork induce the feature representations themselves.\nHere are the ﬁnal equations for a feed-forward network with a single hidden\nlayer, which takes an input vector x, outputs a probability distribution y, and is pa-\nrameterized by weight matrices W and U and a bias vector b:\nh = σ(Wx+b)\nz = Uh\ny = softmax(z)\n(7.11)\nWe’ll call this network a 2-layer network (we traditionally don’t count the input\nlayer when numbering layers, but do count the output layer). So by this terminology\nlogistic regression is a 1-layer network.\nLet’s now set up some notation to make it easier to talk about deeper networks\nof depth more than 2. We’ll use superscripts in square brackets to mean layer num-\nbers, starting at 0 for the input layer. So W [1] will mean the weight matrix for the",
  "146": "140\nCHAPTER 7\n•\nNEURAL NETWORKS AND NEURAL LANGUAGE MODELS\n(ﬁrst) hidden layer, and b[1] will mean the bias vector for the (ﬁrst) hidden layer. nj\nwill mean the number of units at layer j. We’ll use g(·) to stand for the activation\nfunction, which will tend to be ReLU or tanh for intermediate layers and softmax\nfor output layers. We’ll use a[i] to mean the output from layer i, and z[i] to mean the\ncombination of weights and biases W [i]a[i−1] +b[i]. The 0th layer is for inputs, so the\ninputs x we’ll refer to more generally as a[0].\nThus we’ll represent a 3-layer net as follows:\nz[1] = W [1]a[0] +b[1]\na[1] = g[1](z[1])\nz[2] = W [2]a[1] +b[2]\na[2] = g[2](z[2])\nˆy = a[2]\n(7.12)\nNote that with this notation, the equations for the computation done at each layer are\nthe same. The algorithm for computing the forward step in an n-layer feed-forward\nnetwork, given the input vector a[0] is thus simply:\nfor i in 1..n\nz[i] = W [i] a[i−1] + b[i]\na[i] = g[i](z[i])\nˆy = a[n]\nThe\nactivation functions g(·) are generally different at the ﬁnal layer. Thus g[2] might\nbe softmax for multinomial classiﬁcation or sigmoid for binary classiﬁcation, while\nReLU or tanh might be the activation function g() at the internal layers.\n7.4\nTraining Neural Nets\nA feedforward neural net is an instance of supervised machine learning in which we\nknow the correct output y for each observation x. What the system produces, via\nEq. 7.12, is ˆy, the system’s estimate of the true y. The goal of the training procedure\nis to learn parameters W [i] and b[i] for each layer i that make ˆy for each training\nobservation as close as possible to the true y .\nIn general, we do all this by drawing on the methods we introduced in Chapter 5\nfor logistic regression, so the reader should be comfortable with that chapter before\nproceeding.\nFirst, we’ll need a loss function that models the distance between the system\noutput and the gold output, and it’s common to use the loss used for logistic regres-\nsion, the cross-entropy loss.\nSecond, to ﬁnd the parameters that minimize this loss function, we’ll use the\ngradient descent optimization algorithm introduced in Chapter 5. There are some\ndifferences\nThird, gradient descent requires knowing the gradient of the loss function, the\nvector that contains the partial derivative of the loss function with respect to each of\nthe parameters. Here is one part where learning for neural networks is more complex\nthan for logistic logistic regression. In logistic regression, for each observation we\ncould directly compute the derivative of the loss function with respect to an individ-\nual w or b. But for neural networks, with millions of parameters in many layers, it’s",
  "147": "7.4\n•\nTRAINING NEURAL NETS\n141\nmuch harder to see how to compute the partial derivative of some weight in layer 1\nwhen the loss is attached to some much later layer. How do we partial out the loss\nover all those intermediate layers?\nThe answer is the algorithm called error back-propagation or reverse differ-\nentiation.\n7.4.1\nLoss function\nThe cross entropy loss, that is used in neural networks is the same one we saw for\ncross entropy\nloss\nlogistic regression.\nIn fact, if the neural network is being used as a binary classiﬁer, with the sig-\nmoid at the ﬁnal layer, the loss function is exactly the same as we saw with logistic\nregression in Eq. 5.10:\nLCE(ˆy,y) = −log p(y|x) = −[ylog ˆy+(1−y)log(1−ˆy)]\n(7.13)\nWhat about if the neural network is being used as a multinomial classiﬁer? Let\ny be a vector over the C classes representing the true output probability distribution.\nThe cross entropy loss here is\nLCE(ˆy,y) = −\nC\nX\ni=1\nyi log ˆyi\n(7.14)\nWe can simplify this equation further. Assume this is a hard classiﬁcation task,\nmeaning that only one class is the correct one, and that there is one output unit in y\nfor each class. If the true class is i, then y is a vector where yi = 1 and y j = 0 ∀j ̸= i.\nA vector like this, with one value=1 and the rest 0, is called a one-hot vector. Now\nlet ˆy be the vector output from the network. The sum in Eq. 7.14 will be 0 except\nfor the true class. Hence the cross-entropy loss is simply the log probability of the\ncorrect class, and we therefore also call this the negative log likelihood loss:\nnegative log\nlikelihood loss\nLCE(ˆy,y) = −log ˆyi\n(7.15)\nPlugging in the softmax formula from Eq. 7.10, and with K the number of classes:\nLCE(ˆy,y) = −log\nezi\nPK\nj−1 ezj\n(7.16)\n7.4.2\nComputing the Gradient\nHow do we compute the gradient of this loss function? Computing the gradient\nrequires the partial derivative of the loss function with respect to each parameter.\nFor a network with one weight layer and sigmoid output (which is what logistic\nregression is), we could simply use the derivative of the loss that we used for logistic\nregression in: Eq. 7.17 (and derived in Section 5.8):\n∂LCE(w,b)\n∂w j\n= (ˆy−y)x j\n= (σ(w·x+b)−y)xj\n(7.17)",
  "148": "142\nCHAPTER 7\n•\nNEURAL NETWORKS AND NEURAL LANGUAGE MODELS\nOr for a network with one hidden layer and softmax output, we could use the deriva-\ntive of the softmax loss from Eq. 5.36:\n∂LCE\n∂wk\n= (1{y = k}−p(y = k|x))xk\n=\n \n1{y = k}−\newk·x+bk\nPK\nj=1 ew j·x+b j\n!\nxk\n(7.18)\nBut these derivatives only give correct updates for one weight layer: the last one!\nFor deep networks, computing the gradients for each weight is much more complex,\nsince we are computing the derivative with respect to weight parameters that appear\nall the way back in the very early layers of the network, even though the loss is\ncomputed only at the very end of the network.\nThe solution to computing this gradient is an algorithm called error backprop-\nagation or backprop (Rumelhart et al., 1986). While backprop was invented spe-\nerror back-\npropagation\ncially for neural networks, it turns out to be the same as a more general procedure\ncalled backward differentiation, which depends on the notion of computation\ngraphs. Let’s see how that works in the next subsection.\n7.4.3\nComputation Graphs\nA computation graph is a representation of the process of computing a mathematical\nexpression, in which the computation is broken down into separate operations, each\nof which is modeled as a node in a graph.\nConsider computing the function L(a,b,c) = c(a+2b). If we make each of the\ncomponent addition and multiplication operations explicit, and add names (d and e)\nfor the intermediate outputs, the resulting series of computations is:\nd = 2∗b\ne = a+d\nL = c∗e\nWe can now represent this as a graph, with nodes for each operation, and di-\nrected edges showing the outputs from each operation as the inputs to the next, as\nin Fig. 7.9. The simplest use of computation graphs is to compute the value of the\nfunction with some given inputs. In the ﬁgure, we’ve assumed the inputs a = 3,\nb = 1, c = −1, and we’ve shown the result of the forward pass to compute the re-\nsult L(3,1,−1) = 10. In the forward pass of a computation graph, we apply each\noperation left to right, passing the outputs of each computation as the input to the\nnext node.\n7.4.4\nBackward differentiation on computation graphs\nThe importance of the computation graph comes from the backward pass, which\nis used to compute the derivatives that we’ll need for the weight update. In this\nexample our goal is to compute the derivative of the output function L with respect\nto each of the input variables, i.e., ∂L\n∂a, ∂L\n∂b, and ∂L\n∂c . The derivative ∂L\n∂a, tells us how\nmuch a small change in a affects L.\nBackwards differentiation makes use of the chain rule in calculus. Suppose we\nchain rule\nare computing the derivative of a composite function f(x) = u(v(x)). The derivative",
  "149": "7.4\n•\nTRAINING NEURAL NETS\n143\ne=d+a\nd = 2b\nL=ce\n3\n1\n-2\ne=5\nd=2\nL=-10\nforward pass\na\nb\nc\nFigure 7.9\nComputation graph for the function L(a,b,c) = c(a+2b), with values for input\nnodes a = 3, b = 1, c = −1, showing the forward pass computation of L.\nof f(x) is the derivative of u(x) with respect to v(x) times the derivative of v(x) with\nrespect to x:\nd f\ndx = du\ndv · dv\ndx\n(7.19)\nThe chain rule extends to more than two functions. If computing the derivative of a\ncomposite function f(x) = u(v(w(x))), the derivative of f(x) is:\nd f\ndx = du\ndv · dv\ndw · dw\ndx\n(7.20)\nLet’s now compute the 3 derivatives we need. Since in the computation graph\nL = ce, we can directly compute the derivative ∂L\n∂c :\n∂L\n∂c = e\n(7.21)\nFor the other two, we’ll need to use the chain rule:\n∂L\n∂a = ∂L\n∂e\n∂e\n∂a\n∂L\n∂b = ∂L\n∂e\n∂e\n∂d\n∂d\n∂b\n(7.22)\nEq. 7.22 thus requires four intermediate derivatives: ∂L\n∂e , ∂e\n∂a, ∂e\n∂d , and ∂d\n∂b, which\nare as follows (making use of the fact that the derivative of a sum is the sum of the\nderivatives):\nL = ce :\n∂L\n∂e = c, ∂L\n∂c = e\ne = a+d :\n∂e\n∂a = 1, ∂e\n∂d = 1\nd = 2b :\n∂d\n∂b = 2\n(7.23)\nIn the backward pass, we compute each of these partials along each edge of\nthe graph from right to left, multiplying the necessary partials to result in the ﬁnal\nderivative we need. Thus we begin by annotating the ﬁnal node with ∂L\n∂L = 1. Moving\nto the left, we then compute ∂L\n∂c and ∂L\n∂e , and so on, until we have annotated the graph",
  "150": "144\nCHAPTER 7\n•\nNEURAL NETWORKS AND NEURAL LANGUAGE MODELS\nall the way to the input variables. The forward pass conveniently already will have\ncomputed the values of the forward intermediate variables we need (like d and e)\nto compute these derivatives. Fig. 7.10 shows the backward pass. At each node we\nneed to compute the local partial derivative with respect to the parent, multiply it by\nthe partial derivative that is being passed down from the parent, and then pass it to\nthe child.\ne=d+a\nd = 2b\nL=ce\na=3\nb=1\ne=5\nd=2\nL=-10\n \n∂L=1\n∂L\n∂L=-4\n∂b\n∂L=-2\n∂d\na\nb\nc\n∂a=-2\n∂b\n∂L=5\n∂c\n∂L =-2\n∂e\n∂L=-2\n∂e\n∂e =1\n∂d\n∂L =5\n∂c\n∂d =2\n∂b\n∂e =1\n∂a\nbackward pass\nc=-2\nFigure 7.10\nComputation graph for the function L(a,b,c) = c(a+2b), showing the back-\nward pass computation of ∂L\n∂a , ∂L\n∂b , and ∂L\n∂c .\nOf course computation graphs for real neural networks are much more complex.\nFig. 7.11 shows a sample computation graph for a 2-layer neural network with n0 =\n2, n1 = 2, and n2 = 1, assuming binary classiﬁcation and hence using a sigmoid\noutput unit for simplicity. The weights that need updating (those for which we need\nto know the partial derivative of the loss function) are shown in orange.\nz = +\na[2] = σ\n \na[1] = \nReLU\nz[1] = \n+\nb[1]\n*\n*\n*\n*\nx1\nx2\na[1] = \nReLU\nz[1] = \n+\nb[1]\n*\n*\nw[2]\n11\nw[1]\n11\nw[1]\n21\nw[1]\n12\nw[1]\n22\nb[1]\nw[2]\n21\nL (a[2],y)\nFigure 7.11\nSample computation graph for a simple 2-layer neural net (= 1 hidden layer)\nwith two input dimensions and 2 hidden dimensions.\nIn order to do the backward pass, we’ll need to know the derivatives of all the\nfunctions in the graph. We already saw in Section 5.8 the derivative of the sigmoid\nσ:\ndσ(z)\ndz\n= σ(z)(1−z)\n(7.24)\nWe’ll also need the derivatives of each of the other activation functions. The\nderivative of tanh is:\nd tanh(z)\ndz\n= 1−tanh2(z)\n(7.25)",
  "151": "7.5\n•\nNEURAL LANGUAGE MODELS\n145\nThe derivative of the ReLU is\nd ReLU(z)\ndz\n=\n\u001a 0 for x < 0\n1 for x ≥0\n(7.26)\n7.4.5\nMore details on learning\nOptimization in neural networks is a non-convex optimization problem, more com-\nplex than for logistic regression, and for that and other reasons there are many best\npractices for successful learning.\nFor logistic regression we can initialize gradient descent with all the weights and\nbiases having the value 0. In neural networks, by contrast, we need to initialize the\nweights with small random numbers. It’s also helpful to normalize the input values\nto have 0 mean and unit variance.\nVarious forms of regularization are used to prevent overﬁtting. One of the most\nimportant is dropout: randomly dropping some units and their connections from the\ndropout\nnetwork during training (Hinton et al. 2012, Srivastava et al. 2014).\nHyperparameter tuning is also important. The parameters of a neural network\nhyperparameter\nare the weights W and biases b; those are learned by gradient descent. The hyperpa-\nrameters are things that are set by the algorithm designer and not learned in the same\nway, although they must be tuned. Hyperparameters include the learning rate η, the\nminibatch size, the model architecture (the number of layers, the number of hidden\nnodes per layer, the choice of activation functions), how to regularize, and so on.\nGradient descent itself also has many architectural variants such as Adam (Kingma\nand Ba, 2015).\nFinally, most modern neural networks are built using computation graph for-\nmalisms that make all the work of gradient computation and parallelization onto\nvector-based GPUs (Graphic Processing Units) very easy and natural. Pytorch (Paszke\net al., 2017) and TensorFlow (Abadi et al., 2015) are two of the most popular. The\ninterested reader should consult a neural network textbook for further details; some\nsuggestions are at the end of the chapter.\n7.5\nNeural Language Models\nAs our ﬁrst application of neural networks, let’s consider language modeling: pre-\ndicting upcoming words from prior word context.\nNeural net-based language models turn out to have many advantages over the n-\ngram language models of Chapter 3. Among these are that neural language models\ndon’t need smoothing, they can handle much longer histories, and they can general-\nize over contexts of similar words. For a training set of a given size, a neural lan-\nguage model has much higher predictive accuracy than an n-gram language model\nFurthermore, neural language models underlie many of the models we’ll introduce\nfor tasks like machine translation, dialog, and language generation.\nOn the other hand, there is a cost for this improved performance: neural net\nlanguage models are strikingly slower to train than traditional language models, and\nso for many tasks an n-gram language model is still the right tool.\nIn this chapter we’ll describe simple feedforward neural language models, ﬁrst\nintroduced by Bengio et al. (2003). Modern neural language models are generally\nnot feedforward but recurrent, using the technology that we will introduce in Chap-\nter 9.",
  "152": "146\nCHAPTER 7\n•\nNEURAL NETWORKS AND NEURAL LANGUAGE MODELS\nA feedforward neural LM is a standard feedforward network that takes as input\nat time t a representation of some number of previous words (wt−1,wt−2, etc) and\noutputs a probability distribution over possible next words. Thus—like the n-gram\nLM—the feedforward neural LM approximates the probability of a word given the\nentire prior context P(wt|wt−1\n1\n) by approximating based on the N previous words:\nP(wt|wt−1\n1\n) ≈P(wt|wt−1\nt−N+1)\n(7.27)\nIn the following examples we’ll use a 4-gram example, so we’ll show a net to\nestimate the probability P(wt = i|wt−1,wt−2,wt−3).\n7.5.1\nEmbeddings\nIn neural language models, the prior context is represented by embeddings of the\nprevious words. Representing the prior context as embeddings, rather than by ex-\nact words as used in n-gram language models, allows neural language models to\ngeneralize to unseen data much better than n-gram language models. For example,\nsuppose we’ve seen this sentence in training:\nI have to make sure when I get home to feed the cat.\nbut we’ve never seen the word “dog” after the words ”feed the”. In our test set we\nare trying to predict what comes after the preﬁx “I forgot when I got home to feed\nthe”.\nAn n-gram language model will predict “cat”, but not “dog”. But a neural LM,\nwhich can make use of the fact that “cat” and “dog” have similar embeddings, will\nbe able to assign a reasonably high probability to “dog” as well as “cat”, merely\nbecause they have similar vectors.\nLet’s see how this works in practice. Let’s assume we have an embedding dic-\ntionary E that gives us, for each word in our vocabulary V, the embedding for that\nword, perhaps precomputed by an algorithm like word2vec from Chapter 6.\nFig. 7.12 shows a sketch of this simpliﬁed FFNNLM with N=3; we have a mov-\ning window at time t with an embedding vector representing each of the 3 previous\nwords (words wt−1, wt−2, and wt−3). These 3 vectors are concatenated together to\nproduce x, the input layer of a neural network whose output is a softmax with a\nprobability distribution over words. Thus y42, the value of output node 42 is the\nprobability of the next word wt being V42, the vocabulary word with index 42.\nThe model shown in Fig. 7.12 is quite sufﬁcient, assuming we learn the embed-\ndings separately by a method like the word2vec methods of Chapter 6. The method\nof using another algorithm to learn the embedding representations we use for input\nwords is called pretraining. If those pretrained embeddings are sufﬁcient for your\npretraining\npurposes, then this is all you need.\nHowever, often we’d like to learn the embeddings simultaneously with training\nthe network. This is true when whatever task the network is designed for (sentiment\nclassiﬁcation, or translation, or parsing) places strong constraints on what makes a\ngood representation.\nLet’s therefore show an architecture that allows the embeddings to be learned.\nTo do this, we’ll add an extra layer to the network, and propagate the error all the\nway back to the embedding vectors, starting with embeddings with random values\nand slowly moving toward sensible representations.\nFor this to work at the input layer, instead of pre-trained embeddings, we’re\ngoing to represent each of the N previous words as a one-hot vector of length |V|, i.e.,\nwith one dimension for each word in the vocabulary. A one-hot vector is a vector\none-hot vector",
  "153": "7.5\n•\nNEURAL LANGUAGE MODELS\n147\nh1\nh2\ny1\nh3\nhdh\n…\n…\nU\nW\ny42\ny|V|\nProjection layer\n1⨉3d\nconcatenated embeddings\nfor context words\nHidden layer\nOutput layer P(w|u)\n…\nin\nthe\nhole\n...\n...\nground\nthere\nlived\nword 42\nembedding for\nword 35\nembedding for \nword 9925\nembedding for \nword 45180\nwt-1\nwt-2\nwt\nwt-3\ndh⨉3d\n1⨉dh\n|V|⨉dh\nP(wt=V42|wt-3,wt-2,wt-3)\n1⨉|V|\nFigure 7.12\nA simpliﬁed view of a feedforward neural language model moving through a text. At each\ntimestep t the network takes the 3 context words, converts each to a d-dimensional embeddings, and concate-\nnates the 3 embeddings together to get the 1×Nd unit input layer x for the network. These units are multiplied\nby a weight matrix W and bias vector b and then an activation function to produce a hidden layer h, which\nis then multiplied by another weight matrix U. (For graphic simplicity we don’t show b in this and future\npictures). Finally, a softmax output layer predicts at each node i the probability that the next word wt will be\nvocabulary word Vi. (This picture is simpliﬁed because it assumes we just look up in an embedding dictionary\nE the d-dimensional embedding vector for each word, precomputed by an algorithm like word2vec.)\nthat has one element equal to 1—in the dimension corresponding to that word’s\nindex in the vocabulary— while all the other elements are set to zero.\nThus in a one-hot representation for the word “toothpaste”, supposing it happens\nto have index 5 in the vocabulary, x5 is one and and xi = 0 ∀i ̸= 5, as shown here:\n[0 0 0 0 1 0 0 ... 0 0 0 0]\n1 2 3 4 5 6 7 ...\n... |V|\nFig. 7.13 shows the additional layers needed to learn the embeddings during LM\ntraining. Here the N=3 context words are represented as 3 one-hot vectors, fully\nconnected to the embedding layer via 3 instantiations of the E embedding matrix.\nNote that we don’t want to learn separate weight matrices for mapping each of the 3\nprevious words to the projection layer, we want one single embedding dictionary E\nthat’s shared among these three. That’s because over time, many different words will\nappear as wt−2 or wt−1, and we’d like to just represent each word with one vector,\nwhichever context position it appears in. The embedding weight matrix E thus has\na row for each word, each a vector of d dimensions, and hence has dimensionality\nV ×d.\nLet’s walk through the forward pass of Fig. 7.13.\n1. Select three embeddings from E: Given the three previous words, we look\nup their indices, create 3 one-hot vectors, and then multiply each by the em-\nbedding matrix E. Consider wt−3. The one-hot vector for ‘the’ is (index 35) is\nmultiplied by the embedding matrix E, to give the ﬁrst part of the ﬁrst hidden\nlayer, called the projection layer. Since each row of the input matrix E is just\nprojection layer\nan embedding for a word, and the input is a one-hot columnvector xi for word",
  "154": "148\nCHAPTER 7\n•\nNEURAL NETWORKS AND NEURAL LANGUAGE MODELS\nh1\nh2\ny1\nh3\nhdh\n…\n…\nU\nW\ny42\ny|V|\nProjection layer\n1⨉3d\nHidden layer\nOutput layer \nP(w|context)\n…\nin\nthe\nhole\n...\n...\nground\nthere\nlived\nword 42\nwt-1\nwt-2\nwt\nwt-3\ndh⨉3d\n1⨉dh\n|V|⨉dh\nP(wt=V42|wt-3,wt-2,wt-3)\n1⨉|V|\nInput layer\none-hot vectors\nindex\nword 35\n0 0\n1\n0\n0\n1\n|V|\n35\n0 0\n1\n0\n0\n1\n|V|\n45180\n0 0\n1\n0\n0\n1\n|V|\n9925\n0\n0\nindex \nword 9925\nindex \nword 45180\nE\n1⨉|V|\nd⨉|V|\nE is shared\nacross words\nFigure 7.13\nlearning all the way back to embeddings. notice that the embedding matrix E is shared among\nthe 3 context words.\nVi, the projection layer for input w will be Exi = ei, the embedding for word i.\nWe now concatenate the three embeddings for the context words.\n2. Multiply by W: We now multiply by W (and add b) and pass through the\nrectiﬁed linear (or other) activation function to get the hidden layer h.\n3. Multiply by U: h is now multiplied by U\n4. Apply softmax: After the softmax, each node i in the output layer estimates\nthe probability P(wt = i|wt−1,wt−2,wt−3)\nIn summary, if we use e to represent the projection layer, formed by concatenat-\ning the 3 embedding for the three context vectors, the equations for a neural language\nmodel become:\ne = (Ex1,Ex2,...,Ex)\n(7.28)\nh = σ(We+b)\n(7.29)\nz = Uh\n(7.30)\ny = softmax(z)\n(7.31)\n7.5.2\nTraining the neural language model\nTo train the model, i.e. to set all the parameters θ = E,W,U,b, we do gradient de-\nscent (Fig. 5.5), using error back propagation on the computation graph to compute\nthe gradient. Training thus not only sets the weights W and U of the network, but\nalso as we’re predicting upcoming words, we’re learning the embeddings E for each\nwords that best predict upcoming words.",
  "155": "7.6\n•\nSUMMARY\n149\nGenerally training proceedings by taking as input a very long text, concatenating\nall the sentences, start with random weights, and then iteratively moving through\nthe text predicting each word wt. At each word wt, the cross-entropy (negative log\nlikelihood) loss is:\nL = −log p(wt|wt−1,...,wt−n+1)\n(7.32)\nThe gradient is for this loss is then:\nθt+1 = θt −η ∂−log p(wt|wt−1,...,wt−n+1)\n∂θ\n(7.33)\nThis gradient can be computed in any standard neural network framework which\nwill then backpropagate through U, W, b, E.\nTraining the parameters to minimize loss will result both in an algorithm for\nlanguage modeling (a word predictor) but also a new set of embeddings E that can\nbe used as word representations for other tasks.\n7.6\nSummary\n• Neural networks are built out of neural units, originally inspired by human\nneurons but now simple an abstract computational device.\n• Each neural unit multiplies input values by a weight vector, adds a bias, and\nthen applies a non-linear activation function like sigmoid, tanh, or rectiﬁed\nlinear.\n• In a fully-connected, feedforward network, each unit in layer i is connected\nto each unit in layer i+1, and there are no cycles.\n• The power of neural networks comes from the ability of early layers to learn\nrepresentations that can be utilized by later layers in the network.\n• Neural networks are trained by optimization algorithms like gradient de-\nscent.\n• Error back propagation, backward differentiation on a computation graph,\nis used to compute the gradients of the loss function for a network.\n• Neural language models use a neural network as a probabilistic classiﬁer, to\ncompute the probability of the next word given the previous n words.\n• Neural language models can use pretrained embeddings, or can learn embed-\ndings from scratch in the process of language modeling.\nBibliographical and Historical Notes\nThe origins of neural networks lie in the 1940s McCulloch-Pitts neuron (McCul-\nloch and Pitts, 1943), a simpliﬁed model of the human neuron as a kind of com-\nputing element that could be described in terms of propositional logic. By the late\n1950s and early 1960s, a number of labs (including Frank Rosenblatt at Cornell and\nBernard Widrow at Stanford) developed research into neural networks; this phase\nsaw the development of the perceptron (Rosenblatt, 1958), and the transformation\nof the threshold into a bias, a notation we still use (Widrow and Hoff, 1960).",
  "156": "150\nCHAPTER 7\n•\nNEURAL NETWORKS AND NEURAL LANGUAGE MODELS\nThe ﬁeld of neural networks declined after it was shown that a single percep-\ntron unit was unable to model functions as simple as XOR (Minsky and Papert,\n1969). While some small amount of work continued during the next two decades,\na major revival for the ﬁeld didn’t come until the 1980s, when practical tools for\nbuilding deeper networks like error back propagation became widespread (Rumel-\nhart et al., 1986). During the 1980s a wide variety of neural network and related\narchitectures were developed, particularly for applications in psychology and cog-\nnitive science (Rumelhart and McClelland 1986b, McClelland and Elman 1986,\nRumelhart and McClelland 1986a,Elman 1990), for which the term connection-\nist or parallel distributed processing was often used (Feldman and Ballard 1982,\nconnectionist\nSmolensky 1988). Many of the principles and techniques developed in this period\nare foundational to modern work, including the ideas of distributed representations\n(Hinton, 1986), recurrent networks (Elman, 1990), and the use of tensors for com-\npositionality (Smolensky, 1990).\nBy the 1990s larger neural networks began to be applied to many practical lan-\nguage processing tasks as well, like handwriting recognition (LeCun et al. 1989,\nLeCun et al. 1990) and speech recognition (Morgan and Bourlard 1989, Morgan\nand Bourlard 1990). By the early 2000s, improvements in computer hardware and\nadvances in optimization and training techniques made it possible to train even larger\nand deeper networks, leading to the modern term deep learning (Hinton et al. 2006,\nBengio et al. 2007). We cover more related history in Chapter 9.\nThere are a number of excellent books on the subject. Goldberg (2017) has a\nsuperb and comprehensive coverage of neural networks for natural language pro-\ncessing. For neural networks in general see Goodfellow et al. (2016) and Nielsen\n(2015).",
  "157": "CHAPTER\n8\nPart-of-Speech Tagging\nDionysius Thrax of Alexandria (c. 100 B.C.), or perhaps someone else (it was a long\ntime ago), wrote a grammatical sketch of Greek (a “techn¯e”) that summarized the\nlinguistic knowledge of his day. This work is the source of an astonishing proportion\nof modern linguistic vocabulary, including words like syntax, diphthong, clitic, and\nanalogy. Also included are a description of eight parts-of-speech: noun, verb,\nparts-of-speech\npronoun, preposition, adverb, conjunction, participle, and article. Although earlier\nscholars (including Aristotle as well as the Stoics) had their own lists of parts-of-\nspeech, it was Thrax’s set of eight that became the basis for practically all subsequent\npart-of-speech descriptions of most European languages for the next 2000 years.\nSchoolhouse Rock was a series of popular animated educational television clips\nfrom the 1970s. Its Grammar Rock sequence included songs about exactly 8 parts-\nof-speech, including the late great Bob Dorough’s Conjunction Junction:\nConjunction Junction, what’s your function?\nHooking up words and phrases and clauses...\nAlthough the list of 8 was slightly modiﬁed from Thrax’s original, the astonishing\ndurability of the parts-of-speech through two millenia is an indicator of both the\nimportance and the transparency of their role in human language.1\nParts-of-speech (also known as POS, word classes, or syntactic categories) are\nPOS\nuseful because they reveal a lot about a word and its neighbors. Knowing whether a\nword is a noun or a verb tells us about likely neighboring words (nouns are preceded\nby determiners and adjectives, verbs by nouns) and syntactic structure word (nouns\nare generally part of noun phrases), making part-of-speech tagging a key aspect\nof parsing (Chapter 11). Parts of speech are useful features for labeling named\nentities like people or organizations in information extraction (Chapter 17), or for\ncoreference resolution (Chapter 20). A word’s part-of-speech can even play a role\nin speech recognition or synthesis, e.g., the word content is pronounced CONtent\nwhen it is a noun and conTENT when it is an adjective.\nThis chapter introduces parts-of-speech, and then introduces two algorithms for\npart-of-speech tagging, the task of assigning parts-of-speech to words. One is\ngenerative— Hidden Markov Model (HMM)—and one is discriminative—the Max-\nimum Entropy Markov Model (MEMM). Chapter 9 then introduces a third algorithm\nbased on the recurrent neural network (RNN). All three have roughly equal perfor-\nmance but, as we’ll see, have different tradeoffs.\n8.1\n(Mostly) English Word Classes\nUntil now we have been using part-of-speech terms like noun and verb rather\nfreely. In this section we give a more complete deﬁnition of these and other classes.\nWhile word classes do have semantic tendencies—adjectives, for example, often\n1\nNonetheless, eight isn’t very many and, as we’ll see, recent tagsets have more.",
  "158": "152\nCHAPTER 8\n•\nPART-OF-SPEECH TAGGING\ndescribe properties and nouns people— parts-of-speech are traditionally deﬁned in-\nstead based on syntactic and morphological function, grouping words that have sim-\nilar neighboring words (their distributional properties) or take similar afﬁxes (their\nmorphological properties).\nParts-of-speech can be divided into two broad supercategories: closed class\nclosed class\ntypes and open class types. Closed classes are those with relatively ﬁxed member-\nopen class\nship, such as prepositions—new prepositions are rarely coined. By contrast, nouns\nand verbs are open classes—new nouns and verbs like iPhone or to fax are contin-\nually being created or borrowed. Any given speaker or corpus may have different\nopen class words, but all speakers of a language, and sufﬁciently large corpora,\nlikely share the set of closed class words. Closed class words are generally function\nwords like of, it, and, or you, which tend to be very short, occur frequently, and\nfunction word\noften have structuring uses in grammar.\nFour major open classes occur in the languages of the world: nouns, verbs,\nadjectives, and adverbs. English has all four, although not every language does.\nThe syntactic class noun includes the words for most people, places, or things, but\nnoun\nothers as well. Nouns include concrete terms like ship and chair, abstractions like\nbandwidth and relationship, and verb-like terms like pacing as in His pacing to and\nfro became quite annoying. What deﬁnes a noun in English, then, are things like its\nability to occur with determiners (a goat, its bandwidth, Plato’s Republic), to take\npossessives (IBM’s annual revenue), and for most but not all nouns to occur in the\nplural form (goats, abaci).\nOpen class nouns fall into two classes. Proper nouns, like Regina, Colorado,\nproper noun\nand IBM, are names of speciﬁc persons or entities. In English, they generally aren’t\npreceded by articles (e.g., the book is upstairs, but Regina is upstairs). In written\nEnglish, proper nouns are usually capitalized. The other class, common nouns, are\ncommon noun\ndivided in many languages, including English, into count nouns and mass nouns.\ncount noun\nmass noun\nCount nouns allow grammatical enumeration, occurring in both the singular and plu-\nral (goat/goats, relationship/relationships) and they can be counted (one goat, two\ngoats). Mass nouns are used when something is conceptualized as a homogeneous\ngroup. So words like snow, salt, and communism are not counted (i.e., *two snows\nor *two communisms). Mass nouns can also appear without articles where singular\ncount nouns cannot (Snow is white but not *Goat is white).\nVerbs refer to actions and processes, including main verbs like draw, provide,\nverb\nand go. English verbs have inﬂections (non-third-person-sg (eat), third-person-sg\n(eats), progressive (eating), past participle (eaten)). While many researchers believe\nthat all human languages have the categories of noun and verb, others have argued\nthat some languages, such as Riau Indonesian and Tongan, don’t even make this\ndistinction (Broschart 1997; Evans 2000; Gil 2000) .\nThe third open class English form is adjectives, a class that includes many terms\nadjective\nfor properties or qualities. Most languages have adjectives for the concepts of color\n(white, black), age (old, young), and value (good, bad), but there are languages\nwithout adjectives. In Korean, for example, the words corresponding to English\nadjectives act as a subclass of verbs, so what is in English an adjective “beautiful”\nacts in Korean like a verb meaning “to be beautiful”.\nThe ﬁnal open class form, adverbs, is rather a hodge-podge in both form and\nadverb\nmeaning. In the following all the italicized words are adverbs:\nActually, I ran home extremely quickly yesterday\nWhat coherence the class has semantically may be solely that each of these\nwords can be viewed as modifying something (often verbs, hence the name “ad-",
  "159": "8.1\n•\n(MOSTLY) ENGLISH WORD CLASSES\n153\nverb”, but also other adverbs and entire verb phrases). Directional adverbs or loca-\ntive adverbs (home, here, downhill) specify the direction or location of some action;\nlocative\ndegree adverbs (extremely, very, somewhat) specify the extent of some action, pro-\ndegree\ncess, or property; manner adverbs (slowly, slinkily, delicately) describe the manner\nmanner\nof some action or process; and temporal adverbs describe the time that some ac-\ntemporal\ntion or event took place (yesterday, Monday). Because of the heterogeneous nature\nof this class, some adverbs (e.g., temporal adverbs like Monday) are tagged in some\ntagging schemes as nouns.\nThe closed classes differ more from language to language than do the open\nclasses. Some of the important closed classes in English include:\nprepositions: on, under, over, near, by, at, from, to, with\nparticles: up, down, on, off, in, out, at, by\ndeterminers: a, an, the\nconjunctions: and, but, or, as, if, when\npronouns: she, who, I, others\nauxiliary verbs: can, may, should, are\nnumerals: one, two, three, ﬁrst, second, third\nPrepositions occur before noun phrases. Semantically they often indicate spatial\npreposition\nor temporal relations, whether literal (on it, before then, by the house) or metaphor-\nical (on time, with gusto, beside herself), but often indicate other relations as well,\nlike marking the agent in (Hamlet was written by Shakespeare, A particle resembles\nparticle\na preposition or an adverb and is used in combination with a verb. Particles often\nhave extended meanings that aren’t quite the same as the prepositions they resemble,\nas in the particle over in she turned the paper over.\nA verb and a particle that act as a single syntactic and/or semantic unit are\ncalled a phrasal verb. The meaning of phrasal verbs is often problematically non-\nphrasal verb\ncompositional—not predictable from the distinct meanings of the verb and the par-\nticle. Thus, turn down means something like ‘reject’, rule out ‘eliminate’, ﬁnd out\n‘discover’, and go on ‘continue’.\nA closed class that occurs with nouns, often marking the beginning of a noun\nphrase, is the determiner. One small subtype of determiners is the article: English\ndeterminer\narticle\nhas three articles: a, an, and the. Other determiners include this and that (this chap-\nter, that page). A and an mark a noun phrase as indeﬁnite, while the can mark it\nas deﬁnite; deﬁniteness is a discourse property (Chapter 21). Articles are quite fre-\nquent in English; indeed, the is the most frequently occurring word in most corpora\nof written English, and a and an are generally right behind.\nConjunctions join two phrases, clauses, or sentences. Coordinating conjunc-\nconjunctions\ntions like and, or, and but join two elements of equal status. Subordinating conjunc-\ntions are used when one of the elements has some embedded status. For example,\nthat in “I thought that you might like some milk” is a subordinating conjunction\nthat links the main clause I thought with the subordinate clause you might like some\nmilk. This clause is called subordinate because this entire clause is the “content” of\nthe main verb thought. Subordinating conjunctions like that which link a verb to its\nargument in this way are also called complementizers.\ncomplementizer\nPronouns are forms that often act as a kind of shorthand for referring to some\npronoun\nnoun phrase or entity or event. Personal pronouns refer to persons or entities (you,\npersonal\nshe, I, it, me, etc.). Possessive pronouns are forms of personal pronouns that in-\npossessive\ndicate either actual possession or more often just an abstract relation between the\nperson and some object (my, your, his, her, its, one’s, our, their). Wh-pronouns\nwh\n(what, who, whom, whoever) are used in certain question forms, or may also act as",
  "160": "154\nCHAPTER 8\n•\nPART-OF-SPEECH TAGGING\ncomplementizers (Frida, who married Diego... ).\nA closed class subtype of English verbs are the auxiliary verbs. Cross-linguist-\nauxiliary\nically, auxiliaries mark semantic features of a main verb: whether an action takes\nplace in the present, past, or future (tense), whether it is completed (aspect), whether\nit is negated (polarity), and whether an action is necessary, possible, suggested, or\ndesired (mood). English auxiliaries include the copula verb be, the two verbs do and\ncopula\nhave, along with their inﬂected forms, as well as a class of modal verbs. Be is called\nmodal\na copula because it connects subjects with certain kinds of predicate nominals and\nadjectives (He is a duck). The verb have can mark the perfect tenses (I have gone, I\nhad gone), and be is used as part of the passive (We were robbed) or progressive (We\nare leaving) constructions. Modals are used to mark the mood associated with the\nevent depicted by the main verb: can indicates ability or possibility, may permission\nor possibility, must necessity. There is also a modal use of have (e.g., I have to go).\nEnglish also has many words of more or less unique function, including inter-\njections (oh, hey, alas, uh, um), negatives (no, not), politeness markers (please,\ninterjection\nnegative\nthank you), greetings (hello, goodbye), and the existential there (there are two on\nthe table) among others. These classes may be distinguished or lumped together as\ninterjections or adverbs depending on the purpose of the labeling.\n8.2\nThe Penn Treebank Part-of-Speech Tagset\nAn important tagset for English is the 45-tag Penn Treebank tagset (Marcus et al.,\n1993), shown in Fig. 8.1, which has been used to label many corpora. In such\nlabelings, parts-of-speech are generally represented by placing the tag after each\nword, delimited by a slash:\nTag\nDescription\nExample\nTag\nDescription\nExample\nTag\nDescription Example\nCC\ncoordinating\nconjunction\nand, but, or PDT\npredeterminer\nall, both\nVBP\nverb non-3sg\npresent\neat\nCD\ncardinal number\none, two\nPOS\npossessive ending ’s\nVBZ\nverb 3sg pres\neats\nDT\ndeterminer\na, the\nPRP\npersonal pronoun\nI, you, he\nWDT wh-determ.\nwhich, that\nEX\nexistential ‘there’\nthere\nPRP$ possess. pronoun\nyour, one’s WP\nwh-pronoun\nwhat, who\nFW\nforeign word\nmea culpa\nRB\nadverb\nquickly\nWP$\nwh-possess.\nwhose\nIN\npreposition/\nsubordin-conj\nof, in, by\nRBR\ncomparative\nadverb\nfaster\nWRB wh-adverb\nhow, where\nJJ\nadjective\nyellow\nRBS\nsuperlatv. adverb\nfastest\n$\ndollar sign\n$\nJJR\ncomparative adj\nbigger\nRP\nparticle\nup, off\n#\npound sign\n#\nJJS\nsuperlative adj\nwildest\nSYM\nsymbol\n+,%, &\n“\nleft quote\n‘ or “\nLS\nlist item marker\n1, 2, One\nTO\n“to”\nto\n”\nright quote\n’ or ”\nMD\nmodal\ncan, should UH\ninterjection\nah, oops\n(\nleft paren\n[, (, {, <\nNN\nsing or mass noun llama\nVB\nverb base form\neat\n)\nright paren\n], ), }, >\nNNS\nnoun, plural\nllamas\nVBD\nverb past tense\nate\n,\ncomma\n,\nNNP\nproper noun, sing. IBM\nVBG\nverb gerund\neating\n.\nsent-end punc . ! ?\nNNPS proper noun, plu.\nCarolinas\nVBN\nverb past part.\neaten\n:\nsent-mid punc : ; ... – -\nFigure 8.1\nPenn Treebank part-of-speech tags (including punctuation).\n(8.1) The/DT grand/JJ jury/NN commented/VBD on/IN a/DT number/NN of/IN\nother/JJ topics/NNS ./.",
  "161": "NNPS proper noun, plu.\nCarolinas\nVBN\nverb past part.\neaten\n:\nsent-mid punc : ; ... – -\nFigure 8.1\nPenn Treebank part-of-speech tags (including punctuation).\n(8.1) The/DT grand/JJ jury/NN commented/VBD on/IN a/DT number/NN of/IN\nother/JJ topics/NNS ./.\n(8.2) There/EX are/VBP 70/CD children/NNS there/RB",
  "162": "8.2\n•\nTHE PENN TREEBANK PART-OF-SPEECH TAGSET\n155\n(8.3) Preliminary/JJ ﬁndings/NNS were/VBD reported/VBN in/IN today/NN\n’s/POS New/NNP England/NNP Journal/NNP of/IN Medicine/NNP ./.\nExample (8.1) shows the determiners the and a, the adjectives grand and other,\nthe common nouns jury, number, and topics, and the past tense verb commented.\nExample (8.2) shows the use of the EX tag to mark the existential there construction\nin English, and, for comparison, another use of there which is tagged as an adverb\n(RB). Example (8.3) shows the segmentation of the possessive morpheme ’s a pas-\nsive construction, ‘were reported’, in which reported is marked as a past participle\n(VBN). Note that since New England Journal of Medicine is a proper noun, the Tree-\nbank tagging chooses to mark each noun in it separately as NNP, including journal\nand medicine, which might otherwise be labeled as common nouns (NN).\nCorpora labeled with parts-of-speech are crucial training (and testing) sets for\nstatistical tagging algorithms. Three main tagged corpora are consistently used for\ntraining and testing part-of-speech taggers for English. The Brown corpus is a mil-\nBrown\nlion words of samples from 500 written texts from different genres published in the\nUnited States in 1961. The WSJ corpus contains a million words published in the\nWSJ\nWall Street Journal in 1989. The Switchboard corpus consists of 2 million words\nSwitchboard\nof telephone conversations collected in 1990-1991. The corpora were created by\nrunning an automatic part-of-speech tagger on the texts and then human annotators\nhand-corrected each tag.\nThere are some minor differences in the tagsets used by the corpora. For example\nin the WSJ and Brown corpora, the single Penn tag TO is used for both the inﬁnitive\nto (I like to race) and the preposition to (go to the store), while in Switchboard the\ntag TO is reserved for the inﬁnitive use of to and the preposition is tagged IN:\nWell/UH ,/, I/PRP ,/, I/PRP want/VBP to/TO go/VB to/IN a/DT restauran-\nt/NN\nFinally, there are some idiosyncracies inherent in any tagset. For example, be-\ncause the Penn 45 tags were collapsed from a larger 87-tag tagset, the original\nBrown tagset, some potential useful distinctions were lost. The Penn tagset was\ndesigned for a treebank in which sentences were parsed, and so it leaves off syntac-\ntic information recoverable from the parse tree. Thus for example the Penn tag IN is\nused for both subordinating conjunctions like if, when, unless, after:\nafter/IN spending/VBG a/DT day/NN at/IN the/DT beach/NN\nand prepositions like in, on, after:\nafter/IN sunrise/NN\nWords are generally tokenized before tagging.\nThe Penn Treebank and the\nBritish National Corpus split contractions and the ’s-genitive from their stems:2\nwould/MD n’t/RB\nchildren/NNS ’s/POS\nThe Treebank tagset assumes that tokenization of multipart words like New\nYork is done at whitespace, thus tagging. a New York City ﬁrm as a/DT New/NNP\nYork/NNP City/NNP ﬁrm/NN.\nAnother commonly used tagset, the Universal POS tag set of the Universal De-\npendencies project (Nivre et al., 2016a), is used when building systems that can tag\nmany languages. See Section 8.7.\n2\nIndeed, the Treebank tag POS is used only for ’s, which must be segmented in tokenization.",
  "163": "156\nCHAPTER 8\n•\nPART-OF-SPEECH TAGGING\n8.3\nPart-of-Speech Tagging\nPart-of-speech tagging is the process of assigning a part-of-speech marker to each\npart-of-speech\ntagging\nword in an input text.3 The input to a tagging algorithm is a sequence of (tokenized)\nwords and a tagset, and the output is a sequence of tags, one per token.\nTagging is a disambiguation task; words are ambiguous —have more than one\nambiguous\npossible part-of-speech—and the goal is to ﬁnd the correct tag for the situation.\nFor example, book can be a verb (book that ﬂight) or a noun (hand me that book).\nThat can be a determiner (Does that ﬂight serve dinner) or a complementizer (I\nthought that your ﬂight was earlier). The goal of POS-tagging is to resolve these\nambiguity\nresolution\nambiguities, choosing the proper tag for the context. How common is tag ambiguity?\nFig. 8.2 shows that most word types (80-86%) are unambiguous (Janet is always\nNNP, funniest JJS, and hesitantly RB). But the ambiguous words, though accounting\nfor only 14-15% of the vocabulary, are very common words, and hence 55-67% of\nword tokens in running text are ambiguous.4\nTypes:\nWSJ\nBrown\nUnambiguous (1 tag)\n44,432 (86%)\n45,799 (85%)\nAmbiguous\n(2+ tags)\n7,025 (14%)\n8,050 (15%)\nTokens:\nUnambiguous (1 tag)\n577,421 (45%) 384,349 (33%)\nAmbiguous\n(2+ tags)\n711,780 (55%) 786,646 (67%)\nFigure 8.2\nTag ambiguity for word types in Brown and WSJ, using Treebank-3 (45-tag)\ntagging. Punctuation were treated as words, and words were kept in their original case.\nSome of the most ambiguous frequent words are that, back, down, put and set;\nhere are some examples of the 6 different parts-of-speech for the word back:\nearnings growth took a back/JJ seat\na small building in the back/NN\na clear majority of senators back/VBP the bill\nDave began to back/VB toward the door\nenable the country to buy back/RP about debt\nI was twenty-one back/RB then\nNonetheless, many words are easy to disambiguate, because their different tags\naren’t equally likely. For example, a can be a determiner or the letter a, but the\ndeterminer sense is much more likely. This idea suggests a simplistic baseline algo-\nrithm for part-of-speech tagging: given an ambiguous word, choose the tag which is\nmost frequent in the training corpus. This is a key concept:\nMost Frequent Class Baseline: Always compare a classiﬁer against a baseline at\nleast as good as the most frequent class baseline (assigning each token to the class\nit occurred in most often in the training set).\nHow good is this baseline? A standard way to measure the performance of part-\nof-speech taggers is accuracy: the percentage of tags correctly labeled (matching\naccuracy\n3\nTags are also applied to punctuation, so assumes tokenzing of commas, quotation marks, etc., and\ndisambiguating end-of-sentence periods from periods inside words (e.g., etc.).\n4\nNote the large differences across the two genres, especially in token frequency. Tags in the WSJ corpus\nare less ambiguous; its focus on ﬁnancial news leads to a more limited distribution of word usages than\nthe diverse genres of the Brown corpus.",
  "164": "8.4\n•\nHMM PART-OF-SPEECH TAGGING\n157\nhuman labels on a test set). If we train on the WSJ training corpus and test on sec-\ntions 22-24 of the same corpus the most-frequent-tag baseline achieves an accuracy\nof 92.34%. By contrast, the state of the art in part-of-speech tagging on this dataset\nis around 97% tag accuracy, a performance that is achievable by most algorithms\n(HMMs, MEMMs, neural networks, rule-based algorithms). See Section 8.7 on\nother languages and genres.\n8.4\nHMM Part-of-Speech Tagging\nIn this section we introduce the use of the Hidden Markov Model for part-of-speech\ntagging. The HMM is a sequence model. A sequence model or sequence classi-\nsequence model\nﬁer is a model whose job is to assign a label or class to each unit in a sequence,\nthus mapping a sequence of observations to a sequence of labels. An HMM is a\nprobabilistic sequence model: given a sequence of units (words, letters, morphemes,\nsentences, whatever), it computes a probability distribution over possible sequences\nof labels and chooses the best label sequence.\n8.4.1\nMarkov Chains\nThe HMM is based on augmenting the Markov chain. A Markov chain is a model\nMarkov chain\nthat tells us something about the probabilities of sequences of random variables,\nstates, each of which can take on values from some set. These sets can be words, or\ntags, or symbols representing anything, for example the weather. A Markov chain\nmakes a very strong assumption that if we want to predict the future in the sequence,\nall that matters is the current state. All the states before the current state have no im-\npact on the future except via the current state. It’s as if to predict tomorrow’s weather\nyou could examine today’s weather but you weren’t allowed to look at yesterday’s\nweather.\nWARM3\nHOT1\nCOLD2\n.8\n.6\n.1\n.1\n.3\n.6\n.1\n.1\n.3\ncharming\nuniformly\nare\n.1\n.4\n.5\n.5\n.5\n.2\n.6\n.2\n(a)\n(b)\nFigure 8.3\nA Markov chain for weather (a) and one for words (b), showing states and\ntransitions. A start distribution π is required; setting π = [0.1, 0.7, 0.2] for (a) would mean a\nprobability 0.7 of starting in state 2 (cold), probability 0.1 of starting in state 1 (hot), etc.\nMore formally, consider a sequence of state variables q1,q2,...,qi. A Markov\nmodel embodies the Markov assumption on the probabilities of this sequence: that\nMarkov\nassumption\nwhen predicting the future, the past doesn’t matter, only the present.\nMarkov Assumption:\nP(qi = a|q1...qi−1) = P(qi = a|qi−1)\n(8.4)\nFigure 8.3a shows a Markov chain for assigning a probability to a sequence of\nweather events, for which the vocabulary consists of HOT, COLD, and WARM. The",
  "165": "158\nCHAPTER 8\n•\nPART-OF-SPEECH TAGGING\nstates are represented as nodes in the graph, and the transitions, with their probabil-\nities, as edges. The transitions are probabilities: the values of arcs leaving a given\nstate must sum to 1. Figure 8.3b shows a Markov chain for assigning a probability to\na sequence of words w1...wn. This Markov chain should be familiar; in fact, it repre-\nsents a bigram language model, with each edge expressing the probability p(wi|w j)!\nGiven the two models in Fig. 8.3, we can assign a probability to any sequence from\nour vocabulary.\nFormally, a Markov chain is speciﬁed by the following components:\nQ = q1q2 ...qN\na set of N states\nA = a11a12 ...an1 ...ann\na transition probability matrix A, each aij represent-\ning the probability of moving from state i to state j, s.t.\nPn\nj=1 aij = 1 ∀i\nπ = π1,π2,...,πN\nan initial probability distribution over states. πi is the\nprobability that the Markov chain will start in state i.\nSome states j may have π j = 0, meaning that they cannot\nbe initial states. Also, Pn\ni=1 πi = 1\nBefore you go on, use the sample probabilities in Fig. 8.3a (with π = [.1,.7.,2])\nto compute the probability of each of the following sequences:\n(8.5) hot hot hot hot\n(8.6) cold hot cold hot\nWhat does the difference in these probabilities tell you about a real-world weather\nfact encoded in Fig. 8.3a?\n8.4.2\nThe Hidden Markov Model\nA Markov chain is useful when we need to compute a probability for a sequence\nof observable events. In many cases, however, the events we are interested in are\nhidden: we don’t observe them directly. For example we don’t normally observe\nhidden\npart-of-speech tags in a text. Rather, we see words, and must infer the tags from the\nword sequence. We call the tags hidden because they are not observed.\nA hidden Markov model (HMM) allows us to talk about both observed events\nHidden\nMarkov model\n(like words that we see in the input) and hidden events (like part-of-speech tags) that\nwe think of as causal factors in our probabilistic model. An HMM is speciﬁed by\nthe following components:\nQ = q1q2 ...qN\na set of N states\nA = a11 ...ai j ...aNN\na transition probability matrix A, each aij representing the probability\nof moving from state i to state j, s.t. PN\nj=1 aij = 1 ∀i\nO = o1o2 ...oT\na sequence of T observations, each one drawn from a vocabulary V =\nv1,v2,...,vV\nB = bi(ot)\na sequence of observation likelihoods, also called emission probabili-\nties, each expressing the probability of an observation ot being generated\nfrom a state i\nπ = π1,π2,...,πN\nan initial probability distribution over states. πi is the probability that\nthe Markov chain will start in state i. Some states j may have πj = 0,\nmeaning that they cannot be initial states. Also, Pn\ni=1 πi = 1",
  "166": "8.4\n•\nHMM PART-OF-SPEECH TAGGING\n159\nA ﬁrst-order hidden Markov model instantiates two simplifying assumptions.\nFirst, as with a ﬁrst-order Markov chain, the probability of a particular state depends\nonly on the previous state:\nMarkov Assumption:\nP(qi|q1...qi−1) = P(qi|qi−1)\n(8.7)\nSecond, the probability of an output observation oi depends only on the state that\nproduced the observation qi and not on any other states or any other observations:\nOutput Independence: P(oi|q1 ...qi,...,qT,o1,...,oi,...,oT) = P(oi|qi)\n(8.8)\n8.4.3\nThe components of an HMM tagger\nLet’s start by looking at the pieces of an HMM tagger, and then we’ll see how to use\nit to tag. An HMM has two components, the A and B probabilities.\nThe A matrix contains the tag transition probabilities P(ti|ti−1) which represent\nthe probability of a tag occurring given the previous tag. For example, modal verbs\nlike will are very likely to be followed by a verb in the base form, a VB, like race, so\nwe expect this probability to be high. We compute the maximum likelihood estimate\nof this transition probability by counting, out of the times we see the ﬁrst tag in a\nlabeled corpus, how often the ﬁrst tag is followed by the second:\nP(ti|ti−1) = C(ti−1,ti)\nC(ti−1)\n(8.9)\nIn the WSJ corpus, for example, MD occurs 13124 times of which it is followed\nby VB 10471, for an MLE estimate of\nP(VB|MD) = C(MD,VB)\nC(MD)\n= 10471\n13124 = .80\n(8.10)\nLet’s walk through an example, seeing how these probabilities are estimated and\nused in a sample tagging task, before we return to the algorithm for decoding.\nIn HMM tagging, the probabilities are estimated by counting on a tagged training\ncorpus. For this example we’ll use the tagged WSJ corpus.\nThe B emission probabilities, P(wi|ti), represent the probability, given a tag (say\nMD), that it will be associated with a given word (say will). The MLE of the emis-\nsion probability is\nP(wi|ti) = C(ti,wi)\nC(ti)\n(8.11)\nOf the 13124 occurrences of MD in the WSJ corpus, it is associated with will 4046\ntimes:\nP(will|MD) = C(MD,will)\nC(MD)\n= 4046\n13124 = .31\n(8.12)\nWe saw this kind of Bayesian modeling in Chapter 4; recall that this likelihood\nterm is not asking “which is the most likely tag for the word will?” That would be\nthe posterior P(MD|will). Instead, P(will|MD) answers the slightly counterintuitive\nquestion “If we were going to generate a MD, how likely is it that this modal would\nbe will?”\nThe A transition probabilities, and B observation likelihoods of the HMM are\nillustrated in Fig. 8.4 for three states in an HMM part-of-speech tagger; the full\ntagger would have one state for each tag.",
  "167": "160\nCHAPTER 8\n•\nPART-OF-SPEECH TAGGING\nNN3\nVB1\nMD2\na22\na11\na12\na21\na13\na33\na32\na23\na31\nP(\"aardvark\" | NN)\n...\nP(“will” | NN)\n...\nP(\"the\" | NN)\n...\nP(“back” | NN)\n...\nP(\"zebra\" | NN)\nB3\nP(\"aardvark\" | VB)\n...\nP(“will” | VB)\n...\nP(\"the\" | VB)\n...\nP(“back” | VB)\n...\nP(\"zebra\" | VB)\nB1\nP(\"aardvark\" | MD)\n...\nP(“will” | MD)\n...\nP(\"the\" | MD)\n...\nP(“back” | MD)\n...\nP(\"zebra\" | MD)\nB2\nFigure 8.4\nAn illustration of the two parts of an HMM representation: the A transition\nprobabilities used to compute the prior probability, and the B observation likelihoods that are\nassociated with each state, one likelihood for each possible observation word.\n8.4.4\nHMM tagging as decoding\nFor any model, such as an HMM, that contains hidden variables, the task of deter-\nmining the hidden variables sequence corresponding to the sequence of observations\nis called decoding. More formally,\ndecoding\nDecoding: Given as input an HMM λ = (A,B) and a sequence of ob-\nservations O = o1,o2,...,oT, ﬁnd the most probable sequence of states\nQ = q1q2q3 ...qT.\nFor part of speech tagging, the goal of HMM decoding is to choose the tag\nsequence tn\n1 that is most probable given the observation sequence of n words words\nwn\n1:\nˆtn\n1 = argmax\ntn\n1\nP(tn\n1|wn\n1)\n(8.13)\nThe way we’ll do this in the HMM is to use Bayes’ rule to instead compute:\nˆtn\n1 = argmax\ntn\n1\nP(wn\n1|tn\n1)P(tn\n1)\nP(wn\n1)\n(8.14)\nFurthermore, we simplify Eq. 8.14 by dropping the denominator P(wn\n1):\nˆtn\n1 = argmax\ntn\n1\nP(wn\n1|tn\n1)P(tn\n1)\n(8.15)\nHMM taggers make two further simplifying assumptions. The ﬁrst is that the\nprobability of a word appearing depends only on its own tag and is independent of\nneighboring words and tags:\nP(wn\n1|tn\n1) ≈\nn\nY\ni=1\nP(wi|ti)\n(8.16)\nThe second assumption, the bigram assumption, is that the probability of a tag\nis dependent only on the previous tag, rather than the entire tag sequence;\nP(tn\n1) ≈\nn\nY\ni=1\nP(ti|ti−1)\n(8.17)",
  "168": "8.4\n•\nHMM PART-OF-SPEECH TAGGING\n161\nPlugging the simplifying assumptions from Eq. 8.16 and Eq. 8.17 into Eq. 8.15\nresults in the following equation for the most probable tag sequence from a bigram\ntagger:\nˆtn\n1 = argmax\ntn\n1\nP(tn\n1|wn\n1) ≈argmax\ntn\n1\nn\nY\ni=1\nemission\nz }| {\nP(wi|ti)\ntransition\nz\n}|\n{\nP(ti|ti−1)\n(8.18)\nThe two parts of Eq. 8.18 correspond neatly to the B emission probability and\nA transition probability that we just deﬁned above!\n8.4.5\nThe Viterbi Algorithm\nThe decoding algorithm for HMMs is the Viterbi algorithm shown in Fig. 8.5. As\nViterbi\nalgorithm\nan instance of dynamic programming, Viterbi resembles the dynamic program-\nming minimum edit distance algorithm of Chapter 2.\nfunction VITERBI(observations of len T,state-graph of len N) returns best-path, path-prob\ncreate a path probability matrix viterbi[N,T]\nfor each state s from 1 to N do\n; initialization step\nviterbi[s,1]←πs ∗bs(o1)\nbackpointer[s,1]←0\nfor each time step t from 2 to T do\n; recursion step\nfor each state s from 1 to N do\nviterbi[s,t]←\nN\nmax\ns′=1 viterbi[s′,t −1] ∗as′,s ∗bs(ot)\nbackpointer[s,t]←\nN\nargmax\ns′=1\nviterbi[s′,t −1] ∗as′,s ∗bs(ot)\nbestpathprob←\nN\nmax\ns=1\nviterbi[s,T]\n; termination step\nbestpathpointer←\nN\nargmax\ns=1\nviterbi[s,T]\n; termination step\nbestpath←the path starting at state bestpathpointer, that follows backpointer[] to states back in time\nreturn bestpath, bestpathprob\nFigure 8.5\nViterbi algorithm for ﬁnding the optimal sequence of tags. Given an observation sequence and an\nHMM λ = (A,B), the algorithm returns the state path through the HMM that assigns maximum likelihood to\nthe observation sequence.\nThe Viterbi algorithm ﬁrst sets up a probability matrix or lattice, with one col-\numn for each observation ot and one row for each state in the state graph. Each col-\numn thus has a cell for each state qi in the single combined automaton. Figure 8.6\nshows an intuition of this lattice for the sentence Janet will back the bill.\nEach cell of the trellis, vt(j), represents the probability that the HMM is in state\nj after seeing the ﬁrst t observations and passing through the most probable state\nsequence q1,...,qt−1, given the HMM λ. The value of each cell vt(j) is computed\nby recursively taking the most probable path that could lead us to this cell. Formally,\neach cell expresses the probability\nvt(j) =\nmax\nq1,...,qt−1 P(q1...qt−1,o1,o2 ...ot,qt = j|λ)\n(8.19)\nWe represent the most probable path by taking the maximum over all possible\nprevious state sequences\nmax\nq1,...,qt−1. Like other dynamic programming algorithms,",
  "169": "162\nCHAPTER 8\n•\nPART-OF-SPEECH TAGGING\nJJ\nNNP\nNNP\nNNP\nMD\nMD\nMD\nMD\nVB\nVB\nJJ\nJJ\nJJ\nNN\nNN\nRB\nRB\nRB\nRB\nDT\nDT\nDT\nDT\nNNP\nJanet\nwill\nback\nthe\nbill\nNN\nVB\nMD\nNN\nVB\nJJ\nRB\nNNP\nDT\nNN\nVB\nFigure 8.6\nA sketch of the lattice for Janet will back the bill, showing the possible tags (qi)\nfor each word and highlighting the path corresponding to the correct tag sequence through the\nhidden states. States (parts-of-speech) which have a zero probability of generating a particular\nword according to the B matrix (such as the probability that a determiner DT will be realized\nas Janet) are greyed out.\nViterbi ﬁlls each cell recursively. Given that we had already computed the probabil-\nity of being in every state at time t −1, we compute the Viterbi probability by taking\nthe most probable of the extensions of the paths that lead to the current cell. For a\ngiven state qj at time t, the value vt(j) is computed as\nvt(j) =\nN\nmax\ni=1 vt−1(i) aij bj(ot)\n(8.20)\nThe three factors that are multiplied in Eq. 8.20 for extending the previous paths to\ncompute the Viterbi probability at time t are\nvt−1(i)\nthe previous Viterbi path probability from the previous time step\nai j\nthe transition probability from previous state qi to current state qj\nbj(ot)\nthe state observation likelihood of the observation symbol ot given\nthe current state j\n8.4.6\nWorking through an example\nLet’s tag the sentence Janet will back the bill; the goal is the correct series of tags\n(see also Fig. 8.6):\n(8.21) Janet/NNP will/MD back/VB the/DT bill/NN\nLet the HMM be deﬁned by the two tables in Fig. 8.7 and Fig. 8.8. Figure 8.7\nlists the ai j probabilities for transitioning between the hidden states (part-of-speech\ntags). Figure 8.8 expresses the bi(ot) probabilities, the observation likelihoods of\nwords given tags. This table is (slightly simpliﬁed) from counts in the WSJ corpus.\nSo the word Janet only appears as an NNP, back has 4 possible parts of speech, and\nthe word the can appear as a determiner or as an NNP (in titles like “Somewhere\nOver the Rainbow” all words are tagged as NNP).\nFigure 8.9 shows a ﬂeshed-out version of the sketch we saw in Fig. 8.6, the\nViterbi trellis for computing the best hidden state sequence for the observation se-\nquence Janet will back the bill.",
  "170": "8.4\n•\nHMM PART-OF-SPEECH TAGGING\n163\nNNP\nMD\nVB\nJJ\nNN\nRB\nDT\n<s >\n0.2767\n0.0006\n0.0031 0.0453 0.0449 0.0510 0.2026\nNNP\n0.3777\n0.0110\n0.0009 0.0084 0.0584 0.0090 0.0025\nMD\n0.0008\n0.0002\n0.7968 0.0005 0.0008 0.1698 0.0041\nVB\n0.0322\n0.0005\n0.0050 0.0837 0.0615 0.0514 0.2231\nJJ\n0.0366\n0.0004\n0.0001 0.0733 0.4509 0.0036 0.0036\nNN\n0.0096\n0.0176\n0.0014 0.0086 0.1216 0.0177 0.0068\nRB\n0.0068\n0.0102\n0.1011 0.1012 0.0120 0.0728 0.0479\nDT\n0.1147\n0.0021\n0.0002 0.2157 0.4744 0.0102 0.0017\nFigure 8.7\nThe A transition probabilities P(ti|ti−1) computed from the WSJ corpus without\nsmoothing. Rows are labeled with the conditioning event; thus P(VB|MD) is 0.7968.\nJanet\nwill\nback\nthe\nbill\nNNP\n0.000032 0\n0\n0.000048 0\nMD\n0\n0.308431 0\n0\n0\nVB\n0\n0.000028 0.000672 0\n0.000028\nJJ\n0\n0\n0.000340 0\n0\nNN\n0\n0.000200 0.000223 0\n0.002337\nRB\n0\n0\n0.010446 0\n0\nDT\n0\n0\n0\n0.506099 0\nFigure 8.8\nObservation likelihoods B computed from the WSJ corpus without smoothing,\nsimpliﬁed slightly.\nThere are N = 5 state columns. We begin in column 1 (for the word Janet) by\nsetting the Viterbi value in each cell to the product of the π transition probability\n(the start probability for that state i, which we get from the < s > entry of Fig. 8.7),\nand the observation likelihood of the word Janet given the tag for that cell. Most of\nthe cells in the column are zero since the word Janet cannot be any of those tags.\nThe reader should ﬁnd this in Fig. 8.9.\nNext, each cell in the will column gets updated. For each state, we compute the\nvalue viterbi[s,t] by taking the maximum over the extensions of all the paths from the\nprevious column that lead to the current cell according to Eq. 8.20. We have shown\nthe values for the MD, VB, and NN cells. Each cell gets the max of the 7 values\nfrom the previous column, multiplied by the appropriate transition probability; as it\nhappens in this case, most of them are zero from the previous column. The remaining\nvalue is multiplied by the relevant observation probability, and the (trivial) max is\ntaken. In this case the ﬁnal value, .0000002772, comes from the NNP state at the\nprevious column. The reader should ﬁll in the rest of the trellis in Fig. 8.9 and\nbacktrace to reconstruct the correct state sequence NNP MD VB DT NN.\n8.4.7\nExtending the HMM Algorithm to Trigrams\nPractical HMM taggers have a number of extensions of this simple model. One\nimportant missing feature is a wider tag context. In the tagger described above the\nprobability of a tag depends only on the previous tag:\nP(tn\n1) ≈\nn\nY\ni=1\nP(ti|ti−1)\n(8.22)\nIn practice we use more of the history, letting the probability of a tag depend on",
  "171": "164\nCHAPTER 8\n•\nPART-OF-SPEECH TAGGING\nπ\nP(NNP|start) \n= .28\n* P(MD|MD)\n= 0\n*  P(MD|NNP)\n.000009*.01  = .\n0000009 \nv1(2)=\n.0006 x 0 = \n0\nv1(1) =\n .28* .000032 \n= .000009\nt\nMD\nq2\nq1\no1\nJanet\nbill\nwill\no2\no3\nback\nVB\nJJ\nv1(3)=\n.0031 x 0 \n= 0\nv1(4)= .\n045*0=0\no4\n  *  P(MD|VB) \n= 0\n * P(MD|JJ)\n= 0\nP(VB|start) \n= .0031\nP(JJ |start) =\n.045\nbacktrace\nq3\nq4\nthe\nNN\nq5\nRB\nq6\nDT\nq7\nv2(2) =\nmax * .308 =\n.0000002772\nv2(5)=\nmax * .0002 \n= .0000000001\nv2(3)=\nmax * .000028 \n=     2.5e-11 \nv3(6)=\nmax * .0104\nv3(5)=\nmax * .\n000223\nv3(4)=\nmax * .00034\nv3(3)=\nmax * .00067\nv1(5)\nv1(6)\nv1(7)\nv2(1)\nv2(4)\nv2(6)\nv2(7)\nbacktrace\n* P(RB|NN)\n* P(NN|NN)\nstart\nstart\nstart\nstart\nstart\no5\nNNP\nP(MD|start) \n= .0006\nFigure 8.9\nThe ﬁrst few entries in the individual state columns for the Viterbi algorithm. Each cell keeps the\nprobability of the best path so far and a pointer to the previous cell along that path. We have only ﬁlled out\ncolumns 1 and 2; to avoid clutter most cells with value 0 are left empty. The rest is left as an exercise for the\nreader. After the cells are ﬁlled in, backtracing from the end state, we should be able to reconstruct the correct\nstate sequence NNP MD VB DT NN.\nthe two previous tags:\nP(tn\n1) ≈\nn\nY\ni=1\nP(ti|ti−1,ti−2)\n(8.23)\nExtending the algorithm from bigram to trigram taggers gives a small (perhaps a\nhalf point) increase in performance, but conditioning on two previous tags instead of\none requires a signiﬁcant change to the Viterbi algorithm. For each cell, instead of\ntaking a max over transitions from each cell in the previous column, we have to take\na max over paths through the cells in the previous two columns, thus considering N2\nrather than N hidden states at every observation.\nIn addition to increasing the context window, HMM taggers have a number of\nother advanced features. One is to let the tagger know the location of the end of the\nsentence by adding dependence on an end-of-sequence marker for tn+1. This gives\nthe following equation for part-of-speech tagging:\nˆtn\n1 = argmax\ntn\n1\nP(tn\n1|wn\n1) ≈argmax\ntn\n1\n\" n\nY\ni=1\nP(wi|ti)P(ti|ti−1,ti−2)\n#\nP(tn+1|tn)\n(8.24)\nIn tagging any sentence with Eq. 8.24, three of the tags used in the context will\nfall off the edge of the sentence, and hence will not match regular words. These tags,",
  "172": "8.4\n•\nHMM PART-OF-SPEECH TAGGING\n165\nt−1, t0, and tn+1, can all be set to be a single special ‘sentence boundary’ tag that is\nadded to the tagset, which assumes sentences boundaries have already been marked.\nOne problem with trigram taggers as instantiated in Eq. 8.24 is data sparsity.\nAny particular sequence of tags ti−2,ti−1,ti that occurs in the test set may simply\nnever have occurred in the training set. That means we cannot compute the tag\ntrigram probability just by the maximum likelihood estimate from counts, following\nEq. 8.25:\nP(ti|ti−1,ti−2) = C(ti−2,ti−1,ti)\nC(ti−2,ti−1)\n(8.25)\nJust as we saw with language modeling, many of these counts will be zero\nin any training set, and we will incorrectly predict that a given tag sequence will\nnever occur! What we need is a way to estimate P(ti|ti−1,ti−2) even if the sequence\nti−2,ti−1,ti never occurs in the training data.\nThe standard approach to solving this problem is the same interpolation idea\nwe saw in language modeling: estimate the probability by combining more robust,\nbut weaker estimators. For example, if we’ve never seen the tag sequence PRP VB\nTO, and so can’t compute P(TO|PRP,VB) from this frequency, we still could rely\non the bigram probability P(TO|VB), or even the unigram probability P(TO). The\nmaximum likelihood estimation of each of these probabilities can be computed from\na corpus with the following counts:\nTrigrams\nˆP(ti|ti−1,ti−2) = C(ti−2,ti−1,ti)\nC(ti−2,ti−1)\n(8.26)\nBigrams\nˆP(ti|ti−1) = C(ti−1,ti)\nC(ti−1)\n(8.27)\nUnigrams\nˆP(ti) = C(ti)\nN\n(8.28)\nThe standard way to combine these three estimators to estimate the trigram probabil-\nity P(ti|ti−1,ti−2) is via linear interpolation. We estimate the probability P(ti|ti−1ti−2)\nby a weighted sum of the unigram, bigram, and trigram probabilities:\nP(ti|ti−1ti−2) = λ3 ˆP(ti|ti−1ti−2)+λ2 ˆP(ti|ti−1)+λ1 ˆP(ti)\n(8.29)\nWe require λ1 + λ2 + λ3 = 1, ensuring that the resulting P is a probability distri-\nbution. The λs are set by deleted interpolation (Jelinek and Mercer, 1980): we\ndeleted\ninterpolation\nsuccessively delete each trigram from the training corpus and choose the λs so as to\nmaximize the likelihood of the rest of the corpus. The deletion helps to set the λs\nin such a way as to generalize to unseen data and not overﬁt. Figure 8.10 gives a\ndeleted interpolation algorithm for tag trigrams.\n8.4.8\nBeam Search\nWhen the number of states grows very large, the vanilla Viterbi algorithm be slow.\nThe complexity of the algorithm is O(N2T); N (the number of states) can be large\nfor trigram taggers, which have to consider every previous pair of the 45 tags, re-\nsulting in 453 = 91,125 computations per column. N can be even larger for other\napplications of Viterbi, for example to decoding in neural networks, as we will see\nin future chapters.\nOne common solution to the complexity problem is the use of beam search\nbeam search\ndecoding. In beam search, instead of keeping the entire column of states at each",
  "173": "166\nCHAPTER 8\n•\nPART-OF-SPEECH TAGGING\nfunction DELETED-INTERPOLATION(corpus) returns λ1,λ2,λ3\nλ1, λ2, λ3 ←0\nforeach trigram t1,t2,t3 with C(t1,t2,t3) > 0\ndepending on the maximum of the following three values\ncase C(t1,t2,t3)−1\nC(t1,t2)−1 : increment λ3 by C(t1,t2,t3)\ncase C(t2,t3)−1\nC(t2)−1 : increment λ2 by C(t1,t2,t3)\ncase C(t3)−1\nN−1 : increment λ1 by C(t1,t2,t3)\nend\nend\nnormalize λ1,λ2,λ3\nreturn λ1,λ2,λ3\nFigure 8.10\nThe deleted interpolation algorithm for setting the weights for combining un-\nigram, bigram, and trigram tag probabilities. If the denominator is 0 for any case, we deﬁne\nthe result of that case to be 0. N is the number of tokens in the corpus. After Brants (2000).\ntime point t, we just keep the best few hypothesis at that point. At time t this requires\ncomputing the Viterbi score for each of the N cells, sorting the scores, and keeping\nonly the best-scoring states. The rest are pruned out and not continued forward to\ntime t +1.\nOne way to implement beam search is to keep a ﬁxed number of states instead of\nall N current states. Here the beam width β is a ﬁxed number of states. Alternatively\nbeam width\nβ can be modeled as a ﬁxed percentage of the N states, or as a probability threshold.\nFigure 8.11 shows the search lattice using a beam width of 2 states.\nJJ\nNNP\nNNP\nNNP\nMD\nMD\nMD\nMD\nVB\nVB\nJJ\nJJ\nJJ\nNN\nNN\nRB\nRB\nRB\nRB\nDT\nDT\nDT\nDT\nNNP\nJanet\nwill\nback\nthe\nbill\nNN\nVB\nMD\nNN\nVB\nJJ\nRB\nNNP\nDT\nNN\nVB\nFigure 8.11\nA beam search version of Fig. 8.6, showing a beam width of 2. At each time\nt, all (non-zero) states are computed, but then they are sorted and only the best 2 states are\npropagated forward and the rest are pruned, shown in orange.",
  "174": "8.5\n•\nMAXIMUM ENTROPY MARKOV MODELS\n167\n8.4.9\nUnknown Words\nwords people\nnever use —\ncould be\nonly I\nknow them\nIshikawa Takuboku 1885–1912\nTo achieve high accuracy with part-of-speech taggers, it is also important to have\na good model for dealing with unknown words. Proper names and acronyms are\nunknown\nwords\ncreated very often, and even new common nouns and verbs enter the language at a\nsurprising rate. One useful feature for distinguishing parts of speech is word shape:\nwords starting with capital letters are likely to be proper nouns (NNP).\nBut the strongest source of information for guessing the part-of-speech of un-\nknown words is morphology. Words that end in -s are likely to be plural nouns\n(NNS), words ending with -ed tend to be past participles (VBN), words ending with\n-able adjectives (JJ), and so on. We store for each ﬁnal letter sequence (for sim-\nplicity referred to as word sufﬁxes) of up to 10 letters the statistics of the tag it was\nassociated with in training. We are thus computing for each sufﬁx of length i the\nprobability of the tag ti given the sufﬁx letters (Samuelsson 1993, Brants 2000):\nP(ti|ln−i+1 ...ln)\n(8.30)\nBack-off is used to smooth these probabilities with successively shorter sufﬁxes.\nBecause unknown words are unlikely to be closed-class words like prepositions,\nsufﬁx probabilities can be computed only for words whose training set frequency is\n≤10, or only for open-class words. Separate sufﬁx tries are kept for capitalized and\nuncapitalized words.\nFinally, because Eq. 8.30 gives a posterior estimate p(ti|wi), we can compute\nthe likelihood p(wi|ti) that HMMs require by using Bayesian inversion (i.e., using\nBayes rule and computation of the two priors P(ti) and P(ti|ln−i+1 ...ln)).\nIn addition to using capitalization information for unknown words, Brants (2000)\nalso uses capitalization for known words by adding a capitalization feature to each\ntag. Thus, instead of computing P(ti|ti−1,ti−2) as in Eq. 8.26, the algorithm com-\nputes the probability P(ti,ci|ti−1,ci−1,ti−2,ci−2). This is equivalent to having a cap-\nitalized and uncapitalized version of each tag, doubling the size of the tagset.\nCombining all these features, a trigram HMM like that of Brants (2000) has a\ntagging accuracy of 96.7% on the Penn Treebank, perhaps just slightly below the\nperformance of the best MEMM and neural taggers.\n8.5\nMaximum Entropy Markov Models\nWhile an HMM can achieve very high accuracy, we saw that it requires a number of\narchitectural innovations to deal with unknown words, backoff, sufﬁxes, and so on.\nIt would be so much easier if we could add arbitrary features directly into the model\nin a clean way, but that’s hard for generative models like HMMs. Luckily, we’ve\nalready seen a model for doing this: the logistic regression model of Chapter 5! But\nlogistic regression isn’t a sequence model; it assigns a class to a single observation.\nHowever, we could turn logistic regression into a discriminative sequence model\nsimply by running it on successive words, using the class assigned to the prior word",
  "175": "168\nCHAPTER 8\n•\nPART-OF-SPEECH TAGGING\nas a feature in the classiﬁcation of the next word. When we apply logistic regression\nin this way, it’s called the maximum entropy Markov model or MEMM5\nMEMM\nLet the sequence of words be W = wn\n1 and the sequence of tags T = tn\n1. In an\nHMM to compute the best tag sequence that maximizes P(T|W) we rely on Bayes’\nrule and the likelihood P(W|T):\nˆT = argmax\nT\nP(T|W)\n= argmax\nT\nP(W|T)P(T)\n= argmax\nT\nY\ni\nP(wordi|tagi)\nY\ni\nP(tagi|tagi−1)\n(8.31)\nIn an MEMM, by contrast, we compute the posterior P(T|W) directly, training it to\ndiscriminate among the possible tag sequences:\nˆT = argmax\nT\nP(T|W)\n= argmax\nT\nY\ni\nP(ti|wi,ti−1)\n(8.32)\nConsider tagging just one word. A multinomial logistic regression classiﬁer could\ncompute the single probability P(ti|wi,ti−1) in a different way that an HMM. Fig. 8.12\nshows the intuition of the difference via the direction of the arrows; HMMs compute\nlikelihood (observation word conditioned on tags) but MEMMs compute posterior\n(tags conditioned on observation words).\nwill\nMD\nVB\nDT\nNN\nJanet\nback\nthe\nbill\nNNP\nwill\nMD\nVB\nDT\nNN\nJanet\nback\nthe\nbill\nNNP\nFigure 8.12\nA schematic view of the HMM (top) and MEMM (bottom) representation of\nthe probability computation for the correct sequence of tags for the back sentence. The HMM\ncomputes the likelihood of the observation given the hidden state, while the MEMM computes\nthe posterior of each state, conditioned on the previous state and current observation.\n8.5.1\nFeatures in a MEMM\nOf course we don’t build MEMMs that condition just on wi and ti−1. The reason to\nuse a discriminative sequence model is that it’s easier to incorporate a lots of fea-\ntures.6 Figure 8.13 shows a graphical intuition of some of these additional features.\n5\n‘Maximum entropy model’ is an outdated name for logistic regression; see the history section.\n6\nBecause in HMMs all computation is based on the two probabilities P(tag|tag) and P(word|tag), if\nwe want to include some source of knowledge into the tagging process, we must ﬁnd a way to encode\nthe knowledge into one of these two probabilities. Each time we add a feature we have to do a lot of\ncomplicated conditioning which gets harder and harder as we have more and more such features.",
  "176": "8.5\n•\nMAXIMUM ENTROPY MARKOV MODELS\n169\nwill\nMD\nVB\nJanet\nback\nthe\nbill\nNNP\n<s>\nwi\nwi+1\nwi-1\nti-1\nti-2\nwi-1\nFigure 8.13\nAn MEMM for part-of-speech tagging showing the ability to condition on\nmore features.\nA basic MEMM part-of-speech tagger conditions on the observation word it-\nself, neighboring words, and previous tags, and various combinations, using feature\ntemplates like the following:\ntemplates\n⟨ti,wi−2⟩,⟨ti,wi−1⟩,⟨ti,wi⟩,⟨ti,wi+1⟩,⟨ti,wi+2⟩\n⟨ti,ti−1⟩,⟨ti,ti−2,ti−1⟩,\n⟨ti,ti−1,wi⟩,⟨ti,wi−1,wi⟩⟨ti,wi,wi+1⟩,\n(8.33)\nRecall from Chapter 5 that feature templates are used to automatically populate the\nset of features from every instance in the training and test set. Thus our example\nJanet/NNP will/MD back/VB the/DT bill/NN, when wi is the word back, would gen-\nerate the following features:\nti = VB and wi−2 = Janet\nti = VB and wi−1 = will\nti = VB and wi = back\nti = VB and wi+1 = the\nti = VB and wi+2 = bill\nti = VB and ti−1 = MD\nti = VB and ti−1 = MD and ti−2 = NNP\nti = VB and wi = back and wi+1 = the\nAlso necessary are features to deal with unknown words, expressing properties of\nthe word’s spelling or shape:\nwi contains a particular preﬁx (from all preﬁxes of length ≤4)\nwi contains a particular sufﬁx (from all sufﬁxes of length ≤4)\nwi contains a number\nwi contains an upper-case letter\nwi contains a hyphen\nwi is all upper case\nwi’s word shape\nwi’s short word shape\nwi is upper case and has a digit and a dash (like CFC-12)\nwi is upper case and followed within 3 words by Co., Inc., etc.\nWord shape features are used to represent the abstract letter pattern of the word\nword shape\nby mapping lower-case letters to ‘x’, upper-case to ‘X’, numbers to ’d’, and retaining\npunctuation. Thus for example I.M.F would map to X.X.X. and DC10-30 would\nmap to XXdd-dd. A second class of shorter word shape features is also used. In these\nfeatures consecutive character types are removed, so DC10-30 would be mapped to\nXd-d but I.M.F would still map to X.X.X. For example the word well-dressed would\ngenerate the following non-zero valued feature values:",
  "177": "170\nCHAPTER 8\n•\nPART-OF-SPEECH TAGGING\npreﬁx(wi) = w\npreﬁx(wi) = we\npreﬁx(wi) = wel\npreﬁx(wi) = well\nsufﬁx(wi) = ssed\nsufﬁx(wi) = sed\nsufﬁx(wi) = ed\nsufﬁx(wi) = d\nhas-hyphen(wi)\nword-shape(wi) = xxxx-xxxxxxx\nshort-word-shape(wi) = x-x\nFeatures for known words, like the templates in Eq. 8.33, are computed for every\nword seen in the training set. The unknown word features can also be computed for\nall words in training, or only on training words whose frequency is below some\nthreshold. The result of the known-word templates and word-signature features is a\nvery large set of features. Generally a feature cutoff is used in which features are\nthrown out if they have count < 5 in the training set.\n8.5.2\nDecoding and Training MEMMs\nThe most likely sequence of tags is then computed by combining these features of\nthe input word wi, its neighbors within l words wi+l\ni−l, and the previous k tags ti−1\ni−k as\nfollows (using θ to refer to feature weights instead of w to avoid the confusion with\nw meaning words):\nˆT = argmax\nT\nP(T|W)\n= argmax\nT\nY\ni\nP(ti|wi+l\ni−l,ti−1\ni−k )\n= argmax\nT\nY\ni\nexp\n\nX\nj\nθj f j(ti,wi+l\ni−l,ti−1\ni−k )\n\n\nX\nt′∈tagset\nexp\n\nX\nj\nθ j f j(t′,wi+l\ni−l,ti−1\ni−k )\n\n\n(8.34)\nHow should we decode to ﬁnd this optimal tag sequence ˆT? The simplest way\nto turn logistic regression into a sequence model is to build a local classiﬁer that\nclassiﬁes each word left to right, making a hard classiﬁcation of the ﬁrst word in\nthe sentence, then a hard decision on the second word, and so on. This is called a\ngreedy decoding algorithm, because we greedily choose the best tag for each word,\ngreedy\nas shown in Fig. 8.14.\nfunction GREEDY SEQUENCE DECODING(words W, model P) returns tag sequence T\nfor i = 1 to length(W)\nˆti = argmax\nt′∈T\nP(t′ | wi+l\ni−l,ti−1\ni−k )\nFigure 8.14\nIn greedy decoding we simply run the classiﬁer on each token, left to right,\neach time making a hard decision of which is the best tag.",
  "178": "8.6\n•\nBIDIRECTIONALITY\n171\nThe problem with the greedy algorithm is that by making a hard decision on\neach word before moving on to the next word, the classiﬁer can’t use evidence from\nfuture decisions. Although the greedy algorithm is very fast, and occasionally has\nsufﬁcient accuracy to be useful, in general the hard decision causes too much a drop\nin performance, and we don’t use it.\nInstead we decode an MEMM with the Viterbi algorithm just as with the HMM,\nViterbi\nﬁnding the sequence of part-of-speech tags that is optimal for the whole sentence.\nFor example, assume that our MEMM is only conditioning on the previous tag\nti−1 and observed word wi. Concretely, this involves ﬁlling an N × T array with\nthe appropriate values for P(ti|ti−1,wi), maintaining backpointers as we proceed. As\nwith HMM Viterbi, when the table is ﬁlled, we simply follow pointers back from the\nmaximum value in the ﬁnal column to retrieve the desired set of labels. The requisite\nchanges from the HMM-style application of Viterbi have to do only with how we\nﬁll each cell. Recall from Eq. 8.20 that the recursive step of the Viterbi equation\ncomputes the Viterbi value of time t for state j as\nvt(j) =\nN\nmax\ni=1\nvt−1(i)aij bj(ot); 1 ≤j ≤N,1 < t ≤T\n(8.35)\nwhich is the HMM implementation of\nvt(j) =\nN\nmax\ni=1\nvt−1(i) P(sj|si) P(ot|s j) 1 ≤j ≤N,1 < t ≤T\n(8.36)\nThe MEMM requires only a slight change to this latter formula, replacing the a and\nb prior and likelihood probabilities with the direct posterior:\nvt(j) =\nN\nmax\ni=1\nvt−1(i) P(sj|si,ot) 1 ≤j ≤N,1 < t ≤T\n(8.37)\nLearning in MEMMs relies on the same supervised learning algorithms we presented\nfor logistic regression. Given a sequence of observations, feature functions, and cor-\nresponding hidden states, we use gradient descent to train the weights to maximize\nthe log-likelihood of the training corpus.\n8.6\nBidirectionality\nThe one problem with the MEMM and HMM models as presented is that they are\nexclusively run left-to-right. While the Viterbi algorithm still allows present deci-\nsions to be inﬂuenced indirectly by future decisions, it would help even more if a\ndecision about word wi could directly use information about future tags ti+1 and ti+2.\nAdding bidirectionality has another useful advantage. MEMMs have a theoret-\nical weakness, referred to alternatively as the label bias or observation bias prob-\nlabel bias\nobservation\nbias\nlem (Lafferty et al. 2001, Toutanova et al. 2003). These are names for situations\nwhen one source of information is ignored because it is explained away by another\nsource. Consider an example from Toutanova et al. (2003), the sequence will/NN\nto/TO ﬁght/VB. The tag TO is often preceded by NN but rarely by modals (MD),\nand so that tendency should help predict the correct NN tag for will. But the previ-\nous transition P(twill|⟨s⟩) prefers the modal, and because P(TO|to,twill) is so close\nto 1 regardless of twill the model cannot make use of the transition probability and\nincorrectly chooses MD. The strong information that to must have the tag TO has ex-\nplained away the presence of TO and so the model doesn’t learn the importance of",
  "179": "172\nCHAPTER 8\n•\nPART-OF-SPEECH TAGGING\nthe previous NN tag for predicting TO. Bidirectionality helps the model by making\nthe link between TO available when tagging the NN.\nOne way to implement bidirectionality is to switch to a more powerful model\ncalled a conditional random ﬁeld or CRF. The CRF is an undirected graphical\nCRF\nmodel, which means that it’s not computing a probability for each tag at each time\nstep. Instead, at each time step the CRF computes log-linear functions over a clique,\na set of relevant features. Unlike for an MEMM, these might include output features\nof words in future time steps. The probability of the best sequence is similarly\ncomputed by the Viterbi algorithm. Because a CRF normalizes probabilities over all\ntag sequences, rather than over all the tags at an individual time t, training requires\ncomputing the sum over all possible labelings, which makes CRF training quite slow.\nSimpler methods can also be used; the Stanford tagger uses a bidirectional\nStanford tagger\nversion of the MEMM called a cyclic dependency network (Toutanova et al., 2003).\nAlternatively, any sequence model can be turned into a bidirectional model by\nusing multiple passes. For example, the ﬁrst pass would use only part-of-speech\nfeatures from already-disambiguated words on the left. In the second pass, tags for\nall words, including those on the right, can be used. Alternately, the tagger can be run\ntwice, once left-to-right and once right-to-left. In greedy decoding, for each word\nthe classiﬁer chooses the highest-scoring of the tag assigned by the left-to-right and\nright-to-left classiﬁer. In Viterbi decoding, the classiﬁer chooses the higher scoring\nof the two sequences (left-to-right or right-to-left). These bidirectional models lead\ndirectly into the bi-LSTM models that we will introduce in Chapter 9 as a standard\nneural sequence model.\n8.7\nPart-of-Speech Tagging for Other Languages\nAugmentations to tagging algorithms become necessary when dealing with lan-\nguages with rich morphology like Czech, Hungarian and Turkish.\nThese productive word-formation processes result in a large vocabulary for these\nlanguages: a 250,000 word token corpus of Hungarian has more than twice as many\nword types as a similarly sized corpus of English (Oravecz and Dienes, 2002), while\na 10 million word token corpus of Turkish contains four times as many word types\nas a similarly sized English corpus (Hakkani-T¨ur et al., 2002). Large vocabular-\nies mean many unknown words, and these unknown words cause signiﬁcant per-\nformance degradations in a wide variety of languages (including Czech, Slovene,\nEstonian, and Romanian) (Hajiˇc, 2000).\nHighly inﬂectional languages also have much more information than English\ncoded in word morphology, like case (nominative, accusative, genitive) or gender\n(masculine, feminine). Because this information is important for tasks like pars-\ning and coreference resolution, part-of-speech taggers for morphologically rich lan-\nguages need to label words with case and gender information. Tagsets for morpho-\nlogically rich languages are therefore sequences of morphological tags rather than a\nsingle primitive tag. Here’s a Turkish example, in which the word izin has three pos-\nsible morphological/part-of-speech tags and meanings (Hakkani-T¨ur et al., 2002):\n1. Yerdeki izin temizlenmesi gerek.\niz + Noun+A3sg+Pnon+Gen\nThe trace on the ﬂoor should be cleaned.\n2. ¨Uzerinde parmak izin kalmis¸\niz + Noun+A3sg+P2sg+Nom\nYour ﬁnger print is left on (it).",
  "180": "8.8\n•\nSUMMARY\n173\n3. Ic¸eri girmek ic¸in izin alman gerekiyor.\nizin + Noun+A3sg+Pnon+Nom\nYou need a permission to enter.\nUsing a morphological parse sequence like Noun+A3sg+Pnon+Gen as the part-\nof-speech tag greatly increases the number of parts-of-speech, and so tagsets can\nbe 4 to 10 times larger than the 50–100 tags we have seen for English. With such\nlarge tagsets, each word needs to be morphologically analyzed to generate the list\nof possible morphological tag sequences (part-of-speech tags) for the word. The\nrole of the tagger is then to disambiguate among these tags. This method also helps\nwith unknown words since morphological parsers can accept unknown stems and\nstill segment the afﬁxes properly.\nFor non-word-space languages like Chinese, word segmentation (Chapter 2) is\neither applied before tagging or done jointly. Although Chinese words are on aver-\nage very short (around 2.4 characters per unknown word compared with 7.7 for En-\nglish) the problem of unknown words is still large. While English unknown words\ntend to be proper nouns in Chinese the majority of unknown words are common\nnouns and verbs because of extensive compounding. Tagging models for Chinese\nuse similar unknown word features to English, including character preﬁx and suf-\nﬁx features, as well as novel features like the radicals of each character in a word.\n(Tseng et al., 2005b).\nA stanford for multilingual tagging is the Universal POS tag set of the Universal\nDependencies project, which contains 16 tags plus a wide variety of features that\ncan be added to them to create a large tagset for any language (Nivre et al., 2016a).\n8.8\nSummary\nThis chapter introduced parts-of-speech and part-of-speech tagging:\n• Languages generally have a small set of closed class words that are highly\nfrequent, ambiguous, and act as function words, and open-class words like\nnouns, verbs, adjectives. Various part-of-speech tagsets exist, of between 40\nand 200 tags.\n• Part-of-speech tagging is the process of assigning a part-of-speech label to\neach of a sequence of words.\n• Two common approaches to sequence modeling are a generative approach,\nHMM tagging, and a discriminative approach, MEMM tagging. We will see\na third, discriminative neural approach in Chapter 9.\n• The probabilities in HMM taggers are estimated by maximum likelihood es-\ntimation on tag-labeled training corpora. The Viterbi algorithm is used for\ndecoding, ﬁnding the most likely tag sequence\n• Beam search is a variant of Viterbi decoding that maintains only a fraction of\nhigh scoring states rather than all states during decoding.\n• Maximum entropy Markov model or MEMM taggers train logistic regres-\nsion models to pick the best tag given an observation word and its context and\nthe previous tags, and then use Viterbi to choose the best sequence of tags.\n• Modern taggers are generally run bidirectionally.",
  "181": "174\nCHAPTER 8\n•\nPART-OF-SPEECH TAGGING\nBibliographical and Historical Notes\nWhat is probably the earliest part-of-speech tagger was part of the parser in Zellig\nHarris’s Transformations and Discourse Analysis Project (TDAP), implemented be-\ntween June 1958 and July 1959 at the University of Pennsylvania (Harris, 1962),\nalthough earlier systems had used part-of-speech dictionaries. TDAP used 14 hand-\nwritten rules for part-of-speech disambiguation; the use of part-of-speech tag se-\nquences and the relative frequency of tags for a word preﬁgures all modern algo-\nrithms. The parser was implemented essentially as a cascade of ﬁnite-state trans-\nducers; see Joshi and Hopely (1999) and Karttunen (1999) for a reimplementation.\nThe Computational Grammar Coder (CGC) of Klein and Simmons (1963) had\nthree components: a lexicon, a morphological analyzer, and a context disambiguator.\nThe small 1500-word lexicon listed only function words and other irregular words.\nThe morphological analyzer used inﬂectional and derivational sufﬁxes to assign part-\nof-speech classes. These were run over words to produce candidate parts-of-speech\nwhich were then disambiguated by a set of 500 context rules by relying on sur-\nrounding islands of unambiguous words. For example, one rule said that between an\nARTICLE and a VERB, the only allowable sequences were ADJ-NOUN, NOUN-\nADVERB, or NOUN-NOUN. The TAGGIT tagger (Greene and Rubin, 1971) used\nthe same architecture as Klein and Simmons (1963), with a bigger dictionary and\nmore tags (87). TAGGIT was applied to the Brown corpus and, according to Francis\nand Kuˇcera (1982, p. 9), accurately tagged 77% of the corpus; the remainder of the\nBrown corpus was then tagged by hand. All these early algorithms were based on\na two-stage architecture in which a dictionary was ﬁrst used to assign each word a\nset of potential parts-of-speech, and then lists of hand-written disambiguation rules\nwinnowed the set down to a single part-of-speech per word.\nSoon afterwards probabilistic architectures began to be developed. Probabili-\nties were used in tagging by Stolz et al. (1965) and a complete probabilistic tagger\nwith Viterbi decoding was sketched by Bahl and Mercer (1976). The Lancaster-\nOslo/Bergen (LOB) corpus, a British English equivalent of the Brown corpus, was\ntagged in the early 1980’s with the CLAWS tagger (Marshall 1983; Marshall 1987;\nGarside 1987), a probabilistic algorithm that approximated a simpliﬁed HMM tag-\nger. The algorithm used tag bigram probabilities, but instead of storing the word\nlikelihood of each tag, the algorithm marked tags either as rare (P(tag|word) < .01)\ninfrequent (P(tag|word) < .10) or normally frequent (P(tag|word) > .10).\nDeRose (1988) developed a quasi-HMM algorithm, including the use of dy-\nnamic programming, although computing P(t|w)P(w) instead of P(w|t)P(w). The\nsame year, the probabilistic PARTS tagger of Church (1988), (1989) was probably\nthe ﬁrst implemented HMM tagger, described correctly in Church (1989), although\nChurch (1988) also described the computation incorrectly as P(t|w)P(w) instead\nof P(w|t)P(w). Church (p.c.) explained that he had simpliﬁed for pedagogical pur-\nposes because using the probability P(t|w) made the idea seem more understandable\nas “storing a lexicon in an almost standard form”.\nLater taggers explicitly introduced the use of the hidden Markov model (Ku-\npiec 1992; Weischedel et al. 1993; Sch¨utze and Singer 1994). Merialdo (1994)\nshowed that fully unsupervised EM didn’t work well for the tagging task and that\nreliance on hand-labeled data was important. Charniak et al. (1993) showed the im-\nportance of the most frequent tag baseline; the 92.3% number we give above was",
  "182": "showed that fully unsupervised EM didn’t work well for the tagging task and that\nreliance on hand-labeled data was important. Charniak et al. (1993) showed the im-\nportance of the most frequent tag baseline; the 92.3% number we give above was\nfrom Abney et al. (1999). See Brants (2000) for many implementation details of an\nHMM tagger whose performance is still roughly close to state of the art taggers.",
  "183": "EXERCISES\n175\nRatnaparkhi (1996) introduced the MEMM tagger, called MXPOST, and the\nmodern formulation is very much based on his work.\nThe idea of using letter sufﬁxes for unknown words is quite old; the early Klein\nand Simmons (1963) system checked all ﬁnal letter sufﬁxes of lengths 1-5. The\nprobabilistic formulation we described for HMMs comes from Samuelsson (1993).\nThe unknown word features described on page 169 come mainly from (Ratnaparkhi,\n1996), with augmentations from Toutanova et al. (2003) and Manning (2011).\nState of the art taggers use neural algorithms or (bidirectional) log-linear models\nToutanova et al. (2003). HMM (Brants 2000; Thede and Harper 1999) and MEMM\ntagger accuracies are likely just a tad lower.\nAn alternative modern formalism, the English Constraint Grammar systems (Karls-\nson et al. 1995; Voutilainen 1995; Voutilainen 1999), uses a two-stage formalism\nmuch like the early taggers from the 1950s and 1960s. A morphological analyzer\nwith tens of thousands of English word stem entries returns all parts-of-speech for a\nword, using a large feature-based tagset. So the word occurred is tagged with the op-\ntions ⟨V PCP2 SV⟩and ⟨V PAST VFIN SV⟩, meaning it can be a participle (PCP2)\nfor an intransitive (SV) verb, or a past (PAST) ﬁnite (VFIN) form of an intransitive\n(SV) verb. A set of 3,744 constraints are then applied to the input sentence to rule\nout parts-of-speech inconsistent with the context. For example here’s a rule for the\nambiguous word that that eliminates all tags except the ADV (adverbial intensiﬁer)\nsense (this is the sense in the sentence it isn’t that odd):\nADVERBIAL-THAT RULE Given input: “that”\nif (+1 A/ADV/QUANT); /* if next word is adj, adverb, or quantiﬁer */\n(+2 SENT-LIM);\n/* and following which is a sentence boundary, */\n(NOT -1 SVOC/A); /* and the previous word is not a verb like */\n/* ‘consider’ which allows adjs as object complements */\nthen eliminate non-ADV tags else eliminate ADV tag\nManning (2011) investigates the remaining 2.7% of errors in a state-of-the-art\ntagger, the bidirectional MEMM-style model described above (Toutanova et al.,\n2003). He suggests that a third or half of these remaining errors are due to errors or\ninconsistencies in the training data, a third might be solvable with richer linguistic\nmodels, and for the remainder the task is underspeciﬁed or unclear.\nSupervised tagging relies heavily on in-domain training data hand-labeled by\nexperts. Ways to relax this assumption include unsupervised algorithms for cluster-\ning words into part-of-speech-like classes, summarized in Christodoulopoulos et al.\n(2010), and ways to combine labeled and unlabeled data, for example by co-training\n(Clark et al. 2003; Søgaard 2010).\nSee Householder (1995) for historical notes on parts-of-speech, and Sampson\n(1987) and Garside et al. (1997) on the provenance of the Brown and other tagsets.\nExercises\n8.1\nFind one tagging error in each of the following sentences that are tagged with\nthe Penn Treebank tagset:\n1. I/PRP need/VBP a/DT ﬂight/NN from/IN Atlanta/NN\n2. Does/VBZ this/DT ﬂight/NN serve/VB dinner/NNS\n3. I/PRP have/VB a/DT friend/NN living/VBG in/IN Denver/NNP\n4. Can/VBP you/PRP list/VB the/DT nonstop/JJ afternoon/NN ﬂights/NNS",
  "184": "176\nCHAPTER 8\n•\nPART-OF-SPEECH TAGGING\n8.2\nUse the Penn Treebank tagset to tag each word in the following sentences\nfrom Damon Runyon’s short stories. You may ignore punctuation. Some of\nthese are quite difﬁcult; do your best.\n1. It is a nice night.\n2. This crap game is over a garage in Fifty-second Street...\n3. ...Nobody ever takes the newspapers she sells ...\n4. He is a tall, skinny guy with a long, sad, mean-looking kisser, and a\nmournful voice.\n5. ...I am sitting in Mindy’s restaurant putting on the geﬁllte ﬁsh, which is\na dish I am very fond of, ...\n6. When a guy and a doll get to taking peeks back and forth at each other,\nwhy there you are indeed.\n8.3\nNow compare your tags from the previous exercise with one or two friend’s\nanswers. On which words did you disagree the most? Why?\n8.4\nImplement the “most likely tag” baseline. Find a POS-tagged training set,\nand use it to compute for each word the tag that maximizes p(t|w). You will\nneed to implement a simple tokenizer to deal with sentence boundaries. Start\nby assuming that all unknown words are NN and compute your error rate on\nknown and unknown words. Now write at least ﬁve rules to do a better job of\ntagging unknown words, and show the difference in error rates.\n8.5\nBuild a bigram HMM tagger. You will need a part-of-speech-tagged corpus.\nFirst split the corpus into a training set and test set. From the labeled training\nset, train the transition and observation probabilities of the HMM tagger di-\nrectly on the hand-tagged data. Then implement the Viterbi algorithm so that\nyou can label an arbitrary test sentence. Now run your algorithm on the test\nset. Report its error rate and compare its performance to the most frequent tag\nbaseline.\n8.6\nDo an error analysis of your tagger. Build a confusion matrix and investigate\nthe most frequent errors. Propose some features for improving the perfor-\nmance of your tagger on these errors.",
  "185": "CHAPTER\n9\nSequence\nProcessing\nwith\nRecurrent Networks\nTime will explain.\nJane Austin, Persuasion\nIn Chapter 7, we explored feedforward neural networks along with their applications\nto neural language models and text classiﬁcation. In the case of language models,\nwe saw that such networks can be trained to make predictions about the next word in\na sequence given a limited context of preceding words — an approach that is remi-\nniscent of the Markov approach to language modeling discussed in Chapter 3. These\nmodels operated by accepting a small ﬁxed-sized window of tokens as input; longer\nsequences are processed by sliding this window over the input making incremental\npredictions, with the end result being a sequence of predictions spanning the input.\nFig. 9.1, reproduced here from Chapter 7, illustrates this approach with a window\nof size 3. Here, we’re predicting which word will come next given the window the\nground there. Subsequent words are predicted by sliding the window forward one\nword at a time.\nUnfortunately, the sliding window approach is problematic for a number of rea-\nsons. First, it shares the primary weakness of Markov approaches in that it limits\nthe context from which information can be extracted; anything outside the context\nwindow has no impact on the decision being made. This is problematic since there\nare many language tasks that require access to information that can be arbitrarily dis-\ntant from the point at which processing is happening. Second, the use of windows\nmakes it difﬁcult for networks to learn systematic patterns arising from phenomena\nlike constituency. For example, in Fig. 9.1 the phrase the ground appears twice in\ndifferent windows: once, as shown, in the ﬁrst and second positions in the window,\nand in in the preceding step in the second and third slots, thus forcing the network\nto learn two separate patterns for a single constituent.\nThe subject of this chapter is recurrent neural networks, a class of networks\ndesigned to address these problems by processing sequences explicitly as sequences,\nallowing us to handle variable length inputs without the use of arbitrary ﬁxed-sized\nwindows.\n9.1\nSimple Recurrent Networks\nA recurrent neural network is any network that contains is a cycle within its network\nconnections. That is, any network where the value of a unit is directly, or indirectly,\ndependent on its own output as an input. In general, such networks are difﬁcult to\nreason about, and to train. However, within the general class of recurrent networks\nthere are constrained architectures that have proven to be extremely useful when\n177",
  "186": "178\nCHAPTER 9\n•\nSEQUENCE PROCESSING WITH RECURRENT NETWORKS\nh1\nh2\ny1\nh3\nhdh\n…\n…\nU\nW\ny42\ny|V|\nProjection layer\n1⨉3d\nconcatenated embeddings\nfor context words\nHidden layer\nOutput layer P(w|u)\n…\nin\nthe\nhole\n...\n...\nground\nthere\nlived\nword 42\nembedding for\nword 35\nembedding for \nword 9925\nembedding for \nword 45180\nwt-1\nwt-2\nwt\nwt-3\ndh⨉3d\n1⨉dh\n|V|⨉dh\nP(wt=V42|wt-3,wt-2,wt-3)\n1⨉|V|\nFigure 9.1\nA simpliﬁed view of a feedforward neural language model moving through a text.\nAt each\ntimestep t the network takes the 3 context words, converts each to a d-dimensional embeddings, and con-\ncatenates the 3 embeddings together to get the 1×Nd unit input layer x for the network.\napplied to language problems. In this section, we’ll introduce a class of recurrent\nnetworks referred to as Simple Recurrent Networks (SRNs) or Elman Networks\nSimple\nRecurrent\nNetworks\nElman\nNetworks\n(Elman, 1990). These networks are useful in their own right, and will serve as the\nbasis for more complex approaches to be discussed later in this chapter and again in\nChapter 22.\nFig. 9.2 abstractly illustrates the recurrent structure of an SRN. As with ordinary\nfeed-forward networks, an input vector representing the current input element, xt,\nis multiplied by a weight matrix and then passed through an activation function to\ncompute an activation value for a layer of hidden of units. This hidden layer is,\nin turn, used to calculate a corresponding output, yt. Sequences are processed by\npresenting one element at a time to the network. The key difference from a feed-\nht\nyt\nxt\nFigure 9.2\nSimple recurrent neural network after Elman (Elman, 1990). The hidden layer\nincludes a recurrent connection as part of its input. That is, the activation value of the hidden\nlayer depends on the current input as well as the activation value of the hidden layer from the\nprevious timestep.",
  "187": "9.1\n•\nSIMPLE RECURRENT NETWORKS\n179\nU\nV\nW\nyt\nxt\nht\nht-1\nFigure 9.3\nSimple recurrent neural network illustrated as a feed-forward network.\nforward network lies in the recurrent link shown in the ﬁgure with the dashed line.\nThis link augments the input to the hidden layer with the activation value of the\nhidden layer from the preceding point in time.\nThe hidden layer from the previous timestep provides a form of memory, or\ncontext, that encodes earlier processing and informs the decisions to be made at later\npoints in time. Importantly, the architecture does not impose a ﬁxed-length limit\non this prior context; the context embodied in the previous hidden layer includes\ninformation extending back to the beginning of the sequence.\nAdding this temporal dimension makes recurrent networks appear to be more\nexotic than non-recurrent architectures. But in reality, they’re not all that different.\nGiven an input vector and the values for the hidden layer from the previous time\nstep, we’re still performing the standard feed-forward calculation. To see this, con-\nsider Fig. 9.3 which clariﬁes the nature of the recurrence and how it factors into the\ncomputation at the hidden layer. The most signiﬁcant addition lies in the new set of\nweights, U, that connect the hidden layer from the previous timestep to the current\nhidden layer. These weights determine how the network should make use of past\ncontext in calculating the output for the current input. As with the other weights in\nthe network, these connections will be trained via backpropagation.\n9.1.1\nInference in Simple RNNs\nForward inference (mapping a sequence of inputs to a sequence of outputs) in an\nSRN is nearly identical to what we’ve already seen with feedforward networks. To\ncompute an output yt for an input xt, we need the activation value for the hidden\nlayer ht. To calculate this, we compute the dot product of the input xt with the weight\nmatrix W, and the dot product of the hidden layer from the previous time step ht−1\nwith the weight matrix U. We add these values together and pass them through a\nsuitable activation function, g, to arrive at the activation value for the current hidden\nlayer, ht. Once we have the values for the hidden layer, we proceed with the usual\ncomputation to generate the output vector.\nht = g(Uht−1 +Wxt)\nyt = f(Vht)\nIn the commonly encountered case of soft classiﬁcation, ﬁnding yt consists of\na softmax computation that provides a normalized probability distribution over the",
  "188": "180\nCHAPTER 9\n•\nSEQUENCE PROCESSING WITH RECURRENT NETWORKS\nU\nV\nW\nU\nV\nW\nU\nV\nW\nx1\nx2\nx3\ny1\ny2\ny3\nh1\nh3\nh2\nh0\nFigure 9.4\nA simple recurrent neural network shown unrolled in time. Network layers are copied for each\ntimestep, while the weights U, V and W are shared in common across all timesteps.\npossible output classes.\nyt = softmax(Vht)\nThe sequential nature of simple recurrent networks can be illustrated by un-\nrolling the network in time as is shown in Fig. 9.4. In ﬁgures such as this, the\nvarious layers of units are copied for each time step to illustrate that they will have\ndiffering values over time. However the weights themselves are shared across the\nvarious timesteps. Finally, the fact that the computation at time t requires the value\nof the hidden layer from time t −1 mandates an incremental inference algorithm that\nproceeds from the start of the sequence to the end as shown in Fig. 9.5.\nfunction FORWARDRNN(x, network) returns output sequence y\nh0 ←0\nfor i←1 to LENGTH(x) do\nhi ←g(U hi−1 + W xi)\nyi ←f(V hi)\nreturn y\nFigure 9.5\nForward inference in a simple recurrent network.\n9.1.2\nTraining\nAs we did with feed-forward networks, we’ll use a training set, a loss function, and\nbackpropagation to adjust the sets of weights in these recurrent networks. As shown\nin Fig. 9.3, we now have 3 sets of weights to update: W, the weights from the input",
  "189": "9.1\n•\nSIMPLE RECURRENT NETWORKS\n181\nlayer to the hidden layer, U, the weights from the previous hidden layer to the current\nhidden layer, and ﬁnally V, the weights from the hidden layer to the output layer.\nBefore going on, let’s ﬁrst review some of the notation that we introduced in\nChapter 7. Assuming a network with an input layer x and a non-linear activation\nfunction g, we’ll use a[i] to refer to the activation value from a layer i, which is the\nresult of applying g to z[i], the weighted sum of the inputs to that layer. A simple\ntwo-layer feedforward network with W and V as the ﬁrst and second sets of weights\nrespectively, would be characterized as follows.\nz[1] = Wx\na[1] = g(z[1])\nz[2] = Ua[1]\na[2] = g(z[2])\ny = a[2]\nFig. 9.4 illustrates the two considerations that we didn’t have to worry about with\nbackpropagation in feed-forward networks. First, to compute the loss function for\nthe output at time t we need the hidden layer from time t −1. Second, the hidden\nlayer at time t inﬂuences both the output at time t and the hidden layer at time t +1\n(and hence the output and loss at t +1). It follows from this that to assess the error\naccruing to ht, we’ll need to know its inﬂuence on both the current output as well as\nthe next one.\nConsider the situation where we are examining an input/output pair at time 2 as\nshown in Fig. 9.4. What do we need to compute the gradients needed to update the\nweights U, V, and W here? Let’s start by reviewing how we compute the gradients\nrequired to update V (this computation is unchanged from feed-forward networks).\nTo review from Chapter 7, we need to compute the derivative of the loss function L\nwith respect to the weights V. However, since the loss is not expressed directly in\nterms of the weights, we apply the chain rule to get there indirectly.\n∂L\n∂V\n= ∂L\n∂a\n∂a\n∂z\n∂z\n∂V\nThe ﬁrst term is just the derivative of the loss function with respect to the network\noutput, which is just the activation of the output layer, a. The second term is the\nderivative of the network output with respect to the intermediate network activation\nz, which is a function of the activation function g. The ﬁnal term in our application of\nthe chain rule is the derivative of the network activation with respect to the weights\nV, which is just the activation value of the current hidden layer ht.\nIt’s useful here to use the ﬁrst two terms to deﬁne δ, an error term that represents\nhow much of the scalar loss is attributable to each of the units in the output layer.\nδout = ∂L\n∂a\n∂a\n∂z\n(9.1)\nδout = L′g′(z)\n(9.2)\nTherefore, the ﬁnal gradient we need to update the weight matrix V is just:\n∂L\n∂V\n= δoutht\n(9.3)\nMoving on, we need to compute the corresponding gradients for the weight ma-\ntrices W and U:\n∂L\n∂W and ∂L\n∂U . Here we encounter the ﬁrst substantive change from",
  "190": "182\nCHAPTER 9\n•\nSEQUENCE PROCESSING WITH RECURRENT NETWORKS\nU\nV\nW\nU\nV\nW\nU\nV\nW\nx1\nx2\nx3\ny1\ny2\ny3\nh1\nh3\nh2\nh0\nt1\nt2\nt3\nFigure 9.6\nThe backpropagation of errors in an SRN. The ti vectors represent the targets for each element\nof the sequence from the training data. The red arrows illustrate the ﬂow of backpropagated errors required to\ncalculate the updates for U, V and W at time 2. The two incoming arrows converging on h2 signal that these\nerrors need to be summed.\nfeed-forward networks. The hidden state at time t contributes to the output and asso-\nciated error at time t and to the output and error at the next timestep, t +1. Therefore,\nthe error term, δh, for the hidden layer must be the sum of the error term from the\ncurrent output and its error from the next time step.\nδh = g′(z)Vδout +δnext\nGiven this total error term for the hidden layer, we can compute the gradients for\nthe weights U and W in the usual way using the chain rule as we did in Chapter 7.\ndL\ndW\n= dL\ndz\ndz\nda\nda\ndW\ndL\ndU\n= dL\ndz\ndz\nda\nda\ndU\n∂L\n∂W\n= δhxt\n∂L\n∂U\n= δhht−1\nThese gradients provide us with the information needed to update the matrices U\nand W through ordinary backpropagation.\nWe’re not quite done yet, we still need to assign proportional blame (compute\nthe error term) back to the previous hidden layer ht−1 for use in further processing.",
  "191": "9.1\n•\nSIMPLE RECURRENT NETWORKS\n183\nfunction BACKPROPTHROUGHTIME(sequence, network) returns gradients for weight\nupdates\nforward pass to gather the loss\nbackward pass compute error terms and assess blame\nFigure 9.7\nBackpropagation training through time. The forward pass computes the re-\nquired loss values at each time step. The backward pass computes the gradients using the\nvalues from the forward pass.\nThis involves backpropagating the error from δh to ht−1 proportionally based on the\nweights in U.\nδnext = g′(z)Uδh\n(9.4)\nAt this point we have all the gradients needed to perform weight updates for each\nof our three sets of weights. Note that in this simple case there is no need to back-\npropagate the error through W to the input x, since the input training data is assumed\nto be ﬁxed. If we wished to update our input word or character embeddings we\nwould backpropagate the error through to them as well. We’ll discuss this more in\nSection 9.5.\nTaken together, all of these considerations lead to a two-pass algorithm for train-\ning the weights in SRNs. In the ﬁrst pass, we perform forward inference, computing\nht, yt, and an loss at each step in time, saving the value of the hidden layer at each\nstep for use at the next time step. In the second phase, we process the sequence\nin reverse, computing the required error terms gradients as we go, computing and\nsaving the error term for use in the hidden layer for each step backward.\nUnfortunately, computing the gradients and updating weights for each item of a\nsequence individually would be extremely time-consuming. Instead, much as we did\nwith mini-batch training in Chapter 7, we will accumulate gradients for the weights\nincrementally over the sequence, and then use those accumulated gradients in per-\nforming weight updates.\n9.1.3\nUnrolled Networks as Computational Graphs\nWe used the unrolled network shown in Fig. 9.4 as a way to understand the dynamic\nbehavior of these networks over time. However, with modern computational frame-\nworks and adequate computing resources, explicitly unrolling a recurrent network\ninto a deep feed-forward computational graph is quite practical for word-by-word\napproaches to sentence-level processing. In such an approach, we provide a tem-\nplate that speciﬁes the basic structure of the SRN, including all the necessary pa-\nrameters for the input, output, and hidden layers, the weight matrices, as well as the\nactivation and output functions to be used. Then, when provided with an input se-\nquence such as a training sentence, we can compile a feed-forward graph speciﬁc to\nthat input, and use that graph to perform forward inference or training via ordinary\nbackpropagation.\nFor applications that involve much longer input sequences, such as speech recog-\nnition, character-by-character sentence processing, or streaming of continuous in-\nputs, unrolling an entire input sequence may not be feasible. In these cases, we can\nunroll the input into manageable ﬁxed-length segments and treat each segment as a\ndistinct training item. This approach is called Truncated Backpropagation Through\nTime (TBTT).",
  "192": "184\nCHAPTER 9\n•\nSEQUENCE PROCESSING WITH RECURRENT NETWORKS\nJanet\nwill\nback\nRNN\nthe\nbill\nFigure 9.8\nPart-of-speech tagging as sequence labeling with a simple RNN. Pre-trained\nword embeddings serve as inputs and a softmax layer provides a probability distribution over\nthe part-of-speech tags as output at each time step.\n9.2\nApplications of RNNs\nSimple recurrent networks have proven to be an effective approach to language mod-\neling, sequence labeling tasks such as part-of-speech tagging, as well as sequence\nclassiﬁcation tasks such as sentiment analysis and topic classiﬁcation. And as we’ll\nsee in Chapter 22, they form the basic building blocks for sequence to sequence\napproaches to applications such as summarization and machine translation.\n9.2.1\nGeneration with Neural Language Models\n[Coming soon]\n9.2.2\nSequence Labeling\nIn sequence labeling, the network’s job is to assign a label to each element of a\nsequence chosen from a small ﬁxed set of labels. The canonical example of such a\ntask is part-of-speech tagging, discussed in Chapter 8. In a recurrent network-based\napproach to POS tagging, inputs are words and the outputs are tag probabilities\ngenerated by a softmax layer over the POS tagset, as illustrated in Fig. 9.8.\nIn this ﬁgure, the inputs at each time step are pre-trained word embeddings cor-\nresponding to the input tokens. The RNN block is an abstraction that represents\nan unrolled simple recurrent network consisting of an input layer, hidden layer, and\noutput layer at each time step, as well as the shared U, V and W weight matrices that\ncomprise the network. The outputs of the network at each time step represent the\ndistribution over the POS tagset generated by a softmax layer. To generate an actual\ntag sequence as output, we can run forward inference over the input sequence and\nselect the most likely tag from the softmax at each step. Since we’re using a softmax\nlayer to generate the probability distribution over the output tagset at each timestep,\nwe’ll rely on the cross entropy loss introduced in Chapter 7 to train the network.\nA closely related, and extremely useful, application of sequence labeling is to\nﬁnd and classify spans of text corresponding to items of interest in some task do-\nmain. An example of such a task is named entity recognition — the problem of\nnamed entity\nrecognition",
  "193": "9.2\n•\nAPPLICATIONS OF RNNS\n185\nﬁnding all the spans in a text that correspond to names of people, places or organi-\nzations (a problem we’ll study in gory detail in Chapter 17).\nTo turn a problem like this into a per-word sequence labeling task, we’ll use a\ntechnique called IOB encoding (Ramshaw and Marcus, 1995). In its simplest form,\nwe’ll label any token that begins a span of interest with the label B, tokens that occur\ninside a span are tagged with an I, and any tokens outside of any span of interest are\nlabeled O. Consider the following example:\n(9.5) United\nB\ncancelled\nO\nthe\nO\nﬂight\nO\nfrom\nO\nDenver\nB\nto\nO\nSan\nB\nFrancisco.\nI\nHere, the spans of interest are United, Denver and San Francisco.\nIn applications where we are interested in more than one class of entity (e.g.,\nﬁnding and distinguishing names of people, locations, or organizations), we can\nspecialize the B and I tags to represent each of the more speciﬁc classes, thus ex-\npanding the tagset from 3 tags to 2 ∗N + 1 where N is the number of classes we’re\ninterested in.\n(9.6) United\nB-ORG\ncancelled\nO\nthe\nO\nﬂight\nO\nfrom\nO\nDenver\nB-LOC\nto\nO\nSan\nB-LOC\nFrancisco.\nI-LOC\nWith such an encoding, the inputs are the usual word embeddings and the output\nconsistes of a sequence of softmax distributions over the tags at each point in the\nsequence.\n9.2.3\nViterbi and Conditional Random Fields (CRFs)\nAs we saw with applying logistic regression to part-of-speech tagging, choosing the\nmaximum probability label for each element in a sequence does not necessarily re-\nsult in an optimal (or even very good) tag sequence. In the case of IOB tagging, it\ndoesn’t even guarantee that the resulting sequence will be well-formed. For exam-\nple, nothing in approach described in the last section prevents an output sequence\nfrom containing an I following an O, even though such a transition is illegal. Simi-\nlarly, when dealing with multiple classes nothing would prevent an I-LOC tag from\nfollowing a B-PER tag.\nA simple solution to this problem is to use combine the sequence of probability\ndistributions provided by the softmax outputs with a tag-level language model as we\ndid with MEMMs in Chapter 8. Thereby allowing the use of the Viterbi algorithm\nto select the most likely tag sequence.\n[Or a CRF layer... Coming soon]\n9.2.4\nRNNs for Sequence Classiﬁcation\nAnother use of RNNs is to classify entire sequences rather than the tokens within\na sequence. We’ve already encountered this task in Chapter 4 with our discussion\nof sentiment analysis. Other examples include document-level topic classiﬁcation,\nspam detection, message routing for customer service applications, and deception\ndetection. In all of these applications, sequences of text are classiﬁed as belonging\nto one of a small number of categories.\nTo apply RNNs in this setting, the hidden layer from the ﬁnal state of the network\nis taken to constitute a compressed representation of the entire sequence. This com-\npressed sequence representation can then in turn serve as the input to a feed-forward\nnetwork trained to select the correct class. Fig. 9.10 illustrates this approach.",
  "194": "186\nCHAPTER 9\n•\nSEQUENCE PROCESSING WITH RECURRENT NETWORKS\nx1\nx2\nx3\nxn\nRNN\nhn\nSoftmax\nFigure 9.9\nSequence classiﬁcation using a simple RNN combined with a feedforward net-\nwork.\nNote that in this approach, there are no intermediate outputs for the items in the\nsequence preceding the last element, and therefore there are no loss terms associ-\nated with those individual items. Instead, the loss used to train the network weights\nis based on the loss from the ﬁnal classiﬁcation task. Speciﬁcally, we use the output\nfrom the softmax layer from the ﬁnal classiﬁer along with a cross-entropy loss func-\ntion to drive our network training. The loss is backpropagated all the way through\nthe weights in the feedforward classiﬁer through to its input, and then through to the\nthree sets of weights in the RNN as described earlier in Section 9.1.2. This combina-\ntion of a simple recurrent network with a feedforward classiﬁer is our ﬁrst example\nof a deep neural network.\n9.3\nDeep Networks: Stacked and Bidirectional RNNs\nAs suggested by the sequence classiﬁcation architecture shown in Fig. 9.9, recur-\nrent networks are in fact quite ﬂexible. Combining the feedforward nature of un-\nrolled computational graphs with vectors as common inputs and outputs, complex\nnetworks can be treated as modules that can be combined in creative ways. This\nsection introduces two of the more common network architectures used in language\nprocessing with RNNs.\n9.3.1\nStacked RNNs\nIn our examples thus far, the inputs to our RNNs have consisted of sequences of\nword or character embeddings (vectors) and the outputs have been vectors useful for\npredicting words, tags or sequence labels. However, nothing prevents us from using\nthe entire sequence of outputs from one RNN as an input sequence to another one.\nStacked RNNs consist of multiple networks where the output of one layer serves as\nStacked RNNs\nthe input to a subsequent layer, as shown in Fig. 9.10.\nIt has been demonstrated across numerous tasks that stacked RNNs can outper-",
  "195": "9.3\n•\nDEEP NETWORKS: STACKED AND BIDIRECTIONAL RNNS\n187\ny1\ny2\ny3\nyn\nx1\nx2\nx3\nxn\nRNN 1\nRNN 3\nRNN 2\nFigure 9.10\nStacked recurrent networks. The output of a lower level serves as the input to\nhigher levels with the output of the last network serving as the ﬁnal output.\nform single-layer networks. One reason for this success has to do with the networks\nability to induce representations at differing levels of abstraction across layers. Just\nas the early stages of the human visual system detects edges that are then used for\nﬁnding larger regions and shapes, the initial layers of stacked networks can induce\nrepresentations that serve as useful abstractions for further layers — representations\nthat might prove difﬁcult to induce in a single RNN.\n9.3.2\nBidirectional RNNs\nIn an simple recurrent network, the hidden state at a given time t represents every-\nthing the network knows about the sequence up to that point in the sequence. That\nis, the hidden state at time t is the result of a function of the inputs from the start up\nthrough time t. We can think of this as the context of the network to the left of the\ncurrent time.\nhforward\nt\n= SRNforward(x1 : xt)\nWhere h forward\nt\ncorresponds to the normal hidden state at time t, and represents\neverything the network has gleaned from the sequence to that point.\nOf course, in text-based applications we have access to the entire input sequence\nall at once. We might ask whether its helpful to take advantage of the context to\nthe right of the current input as well. One way to recover such information is to\ntrain a recurrent network on an input sequence in reverse, using the same kind of\nnetwork that we’ve been discussing. With this approach, the hidden state at time t\nnow represents information about the sequence to the right of the current input.\nhbackward\nt\n= SRNbackward(xn : xt)\nHere, the hidden state hbackward\nt\nrepresents all the information we have discerned\nabout the sequence from t to the end of the sequence.\nPutting these networks together results in a bidirectional RNN. A Bi-RNN con-\nbidirectional\nRNN\nsists of two independent recurrent networks, one where the input is processed from",
  "196": "188\nCHAPTER 9\n•\nSEQUENCE PROCESSING WITH RECURRENT NETWORKS\ny1\nx1\nx2\nx3\nxn\nRNN 1 (Left to Right)\nRNN 2 (Right to Left)\n+\ny2\n+\ny3\n+\nyn\n+\nFigure 9.11\nA bidirectional RNN. Separate models are trained in the forward and backward\ndirections with the output of each model at each time point concatenated to represent the state\nof affairs at that point in time. The box wrapped around the forward and backward network\nemphasizes the modular nature of this architecture.\nthe start to the end, and the other from the end to the start. We can then combine the\noutputs of the two networks into a single representation that captures the both the\nleft and right contexts of an input at each point in time.\nht = h forward\nt\n⊕hbackward\nt\n(9.7)\nFig. 9.11 illustrates a bidirectional network where the outputs of the forward and\nbackward pass are concatenated. Other simple ways to combine the forward and\nbackward contexts include element-wise addition or multiplication. The output at\neach step in time thus captures information to the left and to the right of the current\ninput. In sequence labeling applications, these concatenated outputs can serve as the\nbasis for a local labeling decision.\nBidirectional RNNs have also proven to be quite effective for sequence classi-\nﬁcation. Recall from Fig. 9.10, that for sequence classiﬁcation we used the ﬁnal\nhidden state of the RNN as the input to a subsequent feedforward classiﬁer. A dif-\nﬁculty with this approach is that the ﬁnal state naturally reﬂects more information\nabout the end of the sentence than its beginning. Bidirectional RNNs provide a\nsimple solution to this problem; as shown in Fig. 9.12, we simply combine the ﬁnal\nhidden states from the forward and backward passes and use that as input for follow-\non processing. Again, concatenation is a common approach to combining the two\noutputs but element-wise summation, multiplication or averaging are also used.\n9.4\nManaging Context in RNNs: LSTMs and GRUs\nIn practice, it is quite difﬁcult to train simple RNNs for tasks that require a network\nto make use of information distant from the current point of processing. Despite hav-\ning access to the entire preceding sequence, the information encoded in hidden states\ntends to be fairly local, more relevant to the most recent parts of the input sequence\nand recent decisions. However, it is often the case that long-distance information is\ncritical to many language applications.",
  "197": "9.4\n•\nMANAGING CONTEXT IN RNNS: LSTMS AND GRUS\n189\nx1\nx2\nx3\nxn\nRNN 1 (Left to Right)\nRNN 2 (Right to Left)\n+\nhn_forw\nh1_back\nSoftmax\nFigure 9.12\nA bidirectional RNN for sequence classiﬁcation. The ﬁnal hidden units from\nthe forward and backward passes are combined to represent the entire sequence. This com-\nbined representation serves as input to the subsequent classiﬁer.\nConsider the following example in the context of language models.\n(9.8) The ﬂights the airline was cancelling were full.\nAssigning a high probability to was following airline is straightforward since was\nprovides a strong local context for the singular agreement. However, assigning an\nappropriate probability to were is quite difﬁcult, not only because the plural ﬂights\nis quite distant, but also because the more recent context contains singular con-\nstituents. Ideally, a network should be able to retain the distant information about\nplural ﬂights until it is needed, all the while processing intermediate parts of the\nsequence correctly.\nOne reason for the inability of SRNs to carry forward critical information is that\nthe hidden layer in SRNs, and, by extension, the weights that determine the values\nin the hidden layer, are being asked to perform two tasks simultaneously: provide\ninformation useful to the decision being made in the current context, and updating\nand carrying forward information useful for future decisions.\nA second difﬁculty to successfully training simple recurrent networks arises\nfrom the need to backpropagate training error back in time through the hidden lay-\ners. Recall from Section 9.1.2 that the hidden layer at time t contributes to the loss\nat the next time step since it takes part in that calculation. As a result, during the\nbackward pass of training, the hidden layers are subject to repeated dot products, as\ndetermined by the length of the sequence. A frequent result of this process is that\nthe gradients are either driven to zero or saturate. Situations that are referred to as\nvanishing gradients or exploding gradients, respectively.\nTo address these issues more complex network architectures have been designed\nto explicitly manage the task of maintaining contextual information over time. These\napproaches treat context as a kind of memory unit that needs to be managed explic-\nitly. More speciﬁcally, the network needs to forget information that is no longer\nneeded and to remember information as needed for later decisions.",
  "198": "190\nCHAPTER 9\n•\nSEQUENCE PROCESSING WITH RECURRENT NETWORKS\n+\no\nst-1\ni\nf\ng\nx t\nht-1\nY\nY\nY\nst\nht\nFigure 9.13\nA single LSTM memory unit displayed as a computation graph.\n9.4.1\nLong Short-Term Memory\nLong short-term memory (LSTM) networks, divide the context management prob-\nlem into two sub-problems: removing information no longer needed from the con-\ntext, and adding information likely to be needed for later decision making. The key\nto the approach is to learn how to manage this context rather than hard-coding a\nstrategy into the architecture.\nLSTMs accomplish this through the use of specialized neural units that make use\nof gates that control the ﬂow of information into and out of the units that comprise\nthe network layers. These gates are implemented through the use of additional sets\nof weights that operate sequentially on the context layer.\ngt = tanh(Ught−1 +Wgxt)\nit = σ(Uiht−1 +Wixt)\nft = σ(Uf ht−1 +Wf xt)\not = σ(Uoht−1 +Woxt)\nct = ft ⊙ct−1 +it ⊙gt\nht = ot ⊙tanh(ct)\n[More on this]",
  "199": "9.4\n•\nMANAGING CONTEXT IN RNNS: LSTMS AND GRUS\n191\nh\nx\nxt\nxt\nht-1\nht\nht\nct-1\nct\nht-1\nxt\nht\nct-1\nct\nht-1\n(b)\n(a)\n(c)\n(d)\n⌃\ng\nz\na\n⌃\ng\nz\nLSTM\nUnit\nGRU\nUnit\na\nFigure 9.14\nBasic neural units used in feed-forward, simple recurrent networks (SRN),\nlong short-term memory (LSTM) and gate recurrent units.\n9.4.2\nGated Recurrent Units\nWhile relatively easy to deploy, LSTMs introduce a considerable number of param-\neters to our networks, and hence carry a much larger training burden. Gated Recur-\nrent Units (GRUs) try to ease this burden by collapsing the forget and add gates of\nLSTMs into a single update gate with a single set of weights.\n[coming soon]\n9.4.3\nGated Units, Layers and Networks\nThe neural units used in LSTMs and GRUs are obviously much more complex than\nbasic feed-forward networks. Fortunately, this complexity is largely encapsulated\nwithin the basic processing units, allowing us to maintain modularity and to eas-\nily experiment with different architectures. To see this, consider Fig. 9.14 which\nillustrates the inputs/outputs and weights associated with each kind of unit.\nAt the far left, (a) is the basic feed-forward unit h = g(Wx +b). A single set of\nweights and a single activation function determine its output, and when arranged in\na layer there is no connection between the units in the layer. Next, (b) represents the\nunit in an SRN. Now there are two inputs and additional set of weights to go with it.\nHowever, there is still a single activation function and output. When arranged as a\nlayer the hidden layer from each unit feeds in as an input to the next.\nFortunately, the increased complexity of the LSTM and GRU units is encapsu-\nlated within the units themselves. The only additional external complexity over the\nbasic recurrent unit (b) is the presence of the additional context vector input and out-\nput. This modularity is key to the power and widespread applicability of LSTM and\nGRU units. Speciﬁcally, LSTM and GRU units can be substituted into any of the\nnetwork architectures described in Section 9.3. And, as with SRNs, multi-layered\nnetworks making use of gated units can be unrolled into deep feed-forward networks\nand trained in the usual fashion with backpropagation.",
  "200": "192\nCHAPTER 9\n•\nSEQUENCE PROCESSING WITH RECURRENT NETWORKS\nJanet\nRNN\nBi-RNN\nJ\na\nn\nt\ne\n+\nwill\nBi-RNN\nw\ni\nl\nl\n+\n…\n…\nFigure 9.15\nSequence labeling RNN that accepts distributional word embeddings aug-\nmented with character-level word embeddings.\n9.5\nWords, Characters and Byte-Pairs\nTo this point, we’ve assumed that the inputs to our networks would be either pre-\ntrained or trained word embeddings. As we’ve seen, word-based embeddings are\ngreat at ﬁnding distributional (syntactic and semantic) similarity between words.\nHowever, there are signiﬁcant issues with any solely word-based approach:\n• For some languages and applications, the lexicon is simply too large to prac-\ntically represent every possible word as an embedding. Some means of com-\nposing words from smaller bits is needed.\n• No matter how large the lexicon, we will always encounter unknown words\ndue to new words entering the language, misspellings and borrowings from\nother languages.\n• Morphological information, below the word level, is clearly an important\nsource of information for many applications. Word-based methods are blind\nto such regularities.\nWe can overcome some of these issues by augmenting our input word repre-\nsentations with embeddings derived from the characters that make up the words.\nFig. 9.15 illustrates an approach in the context of part-of-speech tagging. The upper\npart of the diagram consists of an RNN that accepts an input sequence and outputs\na softmax distribution over the tags for each element of the input. Note that this\nRNN can be arbitrarily complex, consisting of stacked and/or bidirectional network\nlayers.\nThe inputs to this network consist of ordinary word embeddings enriched with\ncharacter information. Speciﬁcally, each input consists of the concatenation of the\nnormal word embedding with embeddings derived from a bidirectional RNN that\naccepts the character sequences for each word as input, as shown in the lower part\nof the ﬁgure.\nThe character sequence for each word in the input is run through a bidirectional\nRNN consisting of two independent RNNs — one that processes the sequence left-",
  "201": "9.6\n•\nSUMMARY\n193\nJ\na\nn\ne\nCharacter Projection Layer\nLSTM1\nLSTM1\nLSTM1\nLSTM1\nLSTM2\nLSTM2\nLSTM2\nLSTM2\nRight-to-left LSTM\nLeft-to-right LSTM\nt\nLSTM2\nLSTM1\nConcatenation\nCharacter-Level Word Embedding\nCharacter Embeddings\nFigure 9.16\nBi-RNN accepts word character sequences and emits embeddings derived\nfrom a forward and backward pass over the sequence. The network itself is trained in the\ncontext of a larger end-application where the loss is propagated all the way through to the\ncharacter vector embeddings.\nto-right and the other right-to-left. As discussed in Section ??, the ﬁnal hidden\nstates of the left-to-right and right-to-left networks are concatenated to represent the\ncomposite character-level representation of each word. Critically, these character\nembeddings are trained in the context of the overall task; the loss from the part-of-\nspeech softmax layer is propagated all the way back to the character embeddings.\n[more on byte-pair encoding approach]\n9.6\nSummary\n• Simple recurrent networks\n• Inference and training in SRNs.\n• Common use cases for RNNs\n– language modeling\n– sequence labeling\n– sequence classiﬁcation\n• LSTMs and GRUs\n• Characters as inputs",
  "202": "194\nCHAPTER 10\n•\nFORMAL GRAMMARS OF ENGLISH\nCHAPTER\n10\nFormal Grammars of English\nThe study of grammar has an ancient pedigree; Panini’s grammar of Sanskrit was\nwritten over two thousand years ago and is still referenced today in teaching San-\nskrit. Despite this history, knowledge of grammar remains spotty at best. In this\nchapter, we make a preliminary stab at addressing some of these gaps in our knowl-\nedge of grammar and syntax, as well as introducing some of the formal mechanisms\nthat are available for capturing this knowledge in a computationally useful manner.\nThe word syntax comes from the Greek s´yntaxis, meaning “setting out together\nsyntax\nor arrangement”, and refers to the way words are arranged together. We have seen\nvarious syntactic notions in previous chapters. The regular languages introduced\nin Chapter 2 offered a simple way to represent the ordering of strings of words, and\nChapter 3 showed how to compute probabilities for these word sequences. Chapter 8\nshowed that part-of-speech categories could act as a kind of equivalence class for\nwords. In this chapter and next few we introduce a variety of syntactic phenomena\nand models for syntax and grammar that go well beyond these simpler approaches.\nThe bulk of this chapter is devoted to the topic of context-free grammars. Context-\nfree grammars are the backbone of many formal models of the syntax of natural\nlanguage (and, for that matter, of computer languages). As such, they are integral to\nmany computational applications, including grammar checking, semantic interpreta-\ntion, dialogue understanding, and machine translation. They are powerful enough to\nexpress sophisticated relations among the words in a sentence, yet computationally\ntractable enough that efﬁcient algorithms exist for parsing sentences with them (as\nwe show in Chapter 11). In Chapter 12, we show that adding probability to context-\nfree grammars gives us a powerful model of disambiguation. And in Chapter 15 we\nshow how they provide a systematic framework for semantic interpretation.\nIn addition to an introduction to this grammar formalism, this chapter also pro-\nvides a brief overview of the grammar of English. To illustrate our grammars, we\nhave chosen a domain that has relatively simple sentences, the Air Trafﬁc Informa-\ntion System (ATIS) domain (Hemphill et al., 1990). ATIS systems were an early\nexample of spoken language systems for helping book airline reservations. Users\ntry to book ﬂights by conversing with the system, specifying constraints like I’d like\nto ﬂy from Atlanta to Denver.\n10.1\nConstituency\nThe fundamental notion underlying the idea of constituency is that of abstraction —\ngroups of words behaving as a single units, or constituents. A signiﬁcant part of\ndeveloping a grammar involves discovering the inventory of constituents present in\nthe language.\nHow do words group together in English? Consider the noun phrase, a sequence\nnoun phrase\nof words surrounding at least one noun. Here are some examples of noun phrases",
  "203": "10.2\n•\nCONTEXT-FREE GRAMMARS\n195\n(thanks to Damon Runyon):\nHarry the Horse\na high-class spot such as Mindy’s\nthe Broadway coppers\nthe reason he comes into the Hot Box\nthey\nthree parties from Brooklyn\nWhat evidence do we have that these words group together (or “form constituents”)?\nOne piece of evidence is that they can all appear in similar syntactic environments,\nfor example, before a verb.\nthree parties from Brooklyn arrive...\na high-class spot such as Mindy’s attracts...\nthe Broadway coppers love...\nthey sit\nBut while the whole noun phrase can occur before a verb, this is not true of each\nof the individual words that make up a noun phrase. The following are not grammat-\nical sentences of English (recall that we use an asterisk (*) to mark fragments that\nare not grammatical English sentences):\n*from arrive... *as attracts...\n*the is...\n*spot sat...\nThus, to correctly describe facts about the ordering of these words in English, we\nmust be able to say things like “Noun Phrases can occur before verbs”.\nOther kinds of evidence for constituency come from what are called preposed or\npreposed\npostposed constructions. For example, the prepositional phrase on September sev-\npostposed\nenteenth can be placed in a number of different locations in the following examples,\nincluding at the beginning (preposed) or at the end (postposed):\nOn September seventeenth, I’d like to ﬂy from Atlanta to Denver\nI’d like to ﬂy on September seventeenth from Atlanta to Denver\nI’d like to ﬂy from Atlanta to Denver on September seventeenth\nBut again, while the entire phrase can be placed differently, the individual words\nmaking up the phrase cannot be\n*On September, I’d like to ﬂy seventeenth from Atlanta to Denver\n*On I’d like to ﬂy September seventeenth from Atlanta to Denver\n*I’d like to ﬂy on September from Atlanta to Denver seventeenth\nSee Radford (1988) for further examples of groups of words behaving as a single\nconstituent.\n10.2\nContext-Free Grammars\nThe most widely used formal system for modeling constituent structure in English\nand other natural languages is the Context-Free Grammar, or CFG. Context-\nCFG\nfree grammars are also called Phrase-Structure Grammars, and the formalism\nis equivalent to Backus-Naur Form, or BNF. The idea of basing a grammar on\nconstituent structure dates back to the psychologist Wilhelm Wundt (1900) but was\nnot formalized until Chomsky (1956) and, independently, Backus (1959).\nA context-free grammar consists of a set of rules or productions, each of which\nrules",
  "204": "196\nCHAPTER 10\n•\nFORMAL GRAMMARS OF ENGLISH\nexpresses the ways that symbols of the language can be grouped and ordered to-\ngether, and a lexicon of words and symbols. For example, the following productions\nlexicon\nexpress that an NP (or noun phrase) can be composed of either a ProperNoun or\nNP\na determiner (Det) followed by a Nominal; a Nominal in turn can consist of one or\nmore Nouns.\nNP →Det Nominal\nNP →ProperNoun\nNominal →Noun | Nominal Noun\nContext-free rules can be hierarchically embedded, so we can combine the pre-\nvious rules with others, like the following, that express facts about the lexicon:\nDet →a\nDet →the\nNoun →ﬂight\nThe symbols that are used in a CFG are divided into two classes. The symbols\nthat correspond to words in the language (“the”, “nightclub”) are called terminal\nterminal\nsymbols; the lexicon is the set of rules that introduce these terminal symbols. The\nsymbols that express abstractions over these terminals are called non-terminals. In\nnon-terminal\neach context-free rule, the item to the right of the arrow (→) is an ordered list of one\nor more terminals and non-terminals; to the left of the arrow is a single non-terminal\nsymbol expressing some cluster or generalization. Notice that in the lexicon, the\nnon-terminal associated with each word is its lexical category, or part-of-speech,\nwhich we deﬁned in Chapter 8.\nA CFG can be thought of in two ways: as a device for generating sentences\nand as a device for assigning a structure to a given sentence. Viewing a CFG as a\ngenerator, we can read the →arrow as “rewrite the symbol on the left with the string\nof symbols on the right”.\nSo starting from the symbol:\nNP\nwe can use our ﬁrst rule to rewrite NP as:\nDet Nominal\nand then rewrite Nominal as:\nDet Noun\nand ﬁnally rewrite these parts-of-speech as:\na ﬂight\nWe say the string a ﬂight can be derived from the non-terminal NP. Thus, a CFG\ncan be used to generate a set of strings. This sequence of rule expansions is called a\nderivation of the string of words. It is common to represent a derivation by a parse\nderivation\ntree (commonly shown inverted with the root at the top). Figure 10.1 shows the tree\nparse tree\nrepresentation of this derivation.\nIn the parse tree shown in Fig. 10.1, we can say that the node NP dominates\ndominates\nall the nodes in the tree (Det, Nom, Noun, a, ﬂight). We can say further that it\nimmediately dominates the nodes Det and Nom.\nThe formal language deﬁned by a CFG is the set of strings that are derivable\nfrom the designated start symbol. Each grammar must have one designated start\nstart symbol\nsymbol, which is often called S. Since context-free grammars are often used to deﬁne\nsentences, S is usually interpreted as the “sentence” node, and the set of strings that\nare derivable from S is the set of sentences in some simpliﬁed version of English.\nLet’s add a few additional rules to our inventory. The following rule expresses\nthe fact that a sentence can consist of a noun phrase followed by a verb phrase:\nverb phrase\nS →NP VP\nI prefer a morning ﬂight",
  "205": "10.2\n•\nCONTEXT-FREE GRAMMARS\n197\nNP\nNom\nNoun\nﬂight\nDet\na\nFigure 10.1\nA parse tree for “a ﬂight”.\nA verb phrase in English consists of a verb followed by assorted other things;\nfor example, one kind of verb phrase consists of a verb followed by a noun phrase:\nVP →Verb NP\nprefer a morning ﬂight\nOr the verb may be followed by a noun phrase and a prepositional phrase:\nVP →Verb NP PP\nleave Boston in the morning\nOr the verb phrase may have a verb followed by a prepositional phrase alone:\nVP →Verb PP\nleaving on Thursday\nA prepositional phrase generally has a preposition followed by a noun phrase.\nFor example, a common type of prepositional phrase in the ATIS corpus is used to\nindicate location or direction:\nPP →Preposition NP\nfrom Los Angeles\nThe NP inside a PP need not be a location; PPs are often used with times and\ndates, and with other nouns as well; they can be arbitrarily complex. Here are ten\nexamples from the ATIS corpus:\nto Seattle\non these ﬂights\nin Minneapolis\nabout the ground transportation in Chicago\non Wednesday\nof the round trip ﬂight on United Airlines\nin the evening\nof the AP ﬁfty seven ﬂight\non the ninth of July\nwith a stopover in Nashville\nFigure 10.2 gives a sample lexicon, and Fig. 10.3 summarizes the grammar rules\nwe’ve seen so far, which we’ll call L0. Note that we can use the or-symbol | to\nindicate that a non-terminal has alternate possible expansions.\nWe can use this grammar to generate sentences of this “ATIS-language”. We\nstart with S, expand it to NP VP, then choose a random expansion of NP (let’s say, to\nI), and a random expansion of VP (let’s say, to Verb NP), and so on until we generate\nthe string I prefer a morning ﬂight. Figure 10.4 shows a parse tree that represents a\ncomplete derivation of I prefer a morning ﬂight.\nIt is sometimes convenient to represent a parse tree in a more compact format\ncalled bracketed notation; here is the bracketed representation of the parse tree of\nbracketed\nnotation\nFig. 10.4:\n(10.1)\n[S [NP [Pro I]] [VP [V prefer] [NP [Det a] [Nom [N morning] [Nom [N ﬂight]]]]]]",
  "206": "198\nCHAPTER 10\n•\nFORMAL GRAMMARS OF ENGLISH\nNoun →ﬂights | breeze | trip | morning\nVerb →is | prefer | like | need | want | ﬂy\nAdjective →cheapest | non-stop | ﬁrst | latest\n| other | direct\nPronoun →me | I | you | it\nProper-Noun →Alaska | Baltimore | Los Angeles\n| Chicago | United | American\nDeterminer →the | a | an | this | these | that\nPreposition →from | to | on | near\nConjunction →and | or | but\nFigure 10.2\nThe lexicon for L0.\nGrammar Rules\nExamples\nS →NP VP\nI + want a morning ﬂight\nNP →Pronoun\nI\n|\nProper-Noun\nLos Angeles\n|\nDet Nominal\na + ﬂight\nNominal →Nominal Noun\nmorning + ﬂight\n|\nNoun\nﬂights\nVP →Verb\ndo\n|\nVerb NP\nwant + a ﬂight\n|\nVerb NP PP\nleave + Boston + in the morning\n|\nVerb PP\nleaving + on Thursday\nPP →Preposition NP\nfrom + Los Angeles\nFigure 10.3\nThe grammar for L0, with example phrases for each rule.\nA CFG like that of L0 deﬁnes a formal language. We saw in Chapter 2 that a for-\nmal language is a set of strings. Sentences (strings of words) that can be derived by a\ngrammar are in the formal language deﬁned by that grammar, and are called gram-\nmatical sentences. Sentences that cannot be derived by a given formal grammar are\ngrammatical\nnot in the language deﬁned by that grammar and are referred to as ungrammatical.\nungrammatical\nThis hard line between “in” and “out” characterizes all formal languages but is only\na very simpliﬁed model of how natural languages really work. This is because de-\ntermining whether a given sentence is part of a given natural language (say, English)\noften depends on the context. In linguistics, the use of formal languages to model\nnatural languages is called generative grammar since the language is deﬁned by\ngenerative\ngrammar\nthe set of possible sentences “generated” by the grammar.\n10.2.1\nFormal Deﬁnition of Context-Free Grammar\nWe conclude this section with a quick, formal description of a context-free gram-\nmar and the language it generates. A context-free grammar G is deﬁned by four\nparameters: N, Σ, R, S (technically this is a “4-tuple”).",
  "207": "10.2\n•\nCONTEXT-FREE GRAMMARS\n199\nS\nVP\nNP\nNom\nNoun\nﬂight\nNom\nNoun\nmorning\nDet\na\nVerb\nprefer\nNP\nPro\nI\nFigure 10.4\nThe parse tree for “I prefer a morning ﬂight” according to grammar L0.\nN a set of non-terminal symbols (or variables)\nΣ a set of terminal symbols (disjoint from N)\nR a set of rules or productions, each of the form A →β ,\nwhere A is a non-terminal,\nβ is a string of symbols from the inﬁnite set of strings (Σ∪N)∗\nS\na designated start symbol and a member of N\nFor the remainder of the book we adhere to the following conventions when dis-\ncussing the formal properties of context-free grammars (as opposed to explaining\nparticular facts about English or other languages).\nCapital letters like A, B, and S\nNon-terminals\nS\nThe start symbol\nLower-case Greek letters like α, β, and γ\nStrings drawn from (Σ∪N)∗\nLower-case Roman letters like u, v, and w\nStrings of terminals\nA language is deﬁned through the concept of derivation. One string derives an-\nother one if it can be rewritten as the second one by some series of rule applications.\nMore formally, following Hopcroft and Ullman (1979),\nif A →β is a production of R and α and γ are any strings in the set\n(Σ∪N)∗, then we say that αAγ directly derives αβγ, or αAγ ⇒αβγ.\ndirectly derives\nDerivation is then a generalization of direct derivation:\nLet α1, α2, ..., αm be strings in (Σ∪N)∗,m ≥1, such that\nα1 ⇒α2,α2 ⇒α3,...,αm−1 ⇒αm\nWe say that α1 derives αm, or α1\n∗⇒αm.\nderives\nWe can then formally deﬁne the language LG generated by a grammar G as the\nset of strings composed of terminal symbols that can be derived from the designated",
  "208": "200\nCHAPTER 10\n•\nFORMAL GRAMMARS OF ENGLISH\nstart symbol S.\nLG = {w|w is in Σ∗and S ∗⇒w}\nThe problem of mapping from a string of words to its parse tree is called syn-\ntactic parsing; we deﬁne algorithms for parsing in Chapter 11.\nsyntactic\nparsing\n10.3\nSome Grammar Rules for English\nIn this section, we introduce a few more aspects of the phrase structure of English;\nfor consistency we will continue to focus on sentences from the ATIS domain. Be-\ncause of space limitations, our discussion is necessarily limited to highlights. Read-\ners are strongly advised to consult a good reference grammar of English, such as\nHuddleston and Pullum (2002).\n10.3.1\nSentence-Level Constructions\nIn the small grammar L0, we provided only one sentence-level construction for\ndeclarative sentences like I prefer a morning ﬂight. Among the large number of\nconstructions for English sentences, four are particularly common and important:\ndeclaratives, imperatives, yes-no questions, and wh-questions.\nSentences with declarative structure have a subject noun phrase followed by\ndeclarative\na verb phrase, like “I prefer a morning ﬂight”. Sentences with this structure have\na great number of different uses that we follow up on in Chapter 24. Here are a\nnumber of examples from the ATIS domain:\nI want a ﬂight from Ontario to Chicago\nThe ﬂight should be eleven a.m. tomorrow\nThe return ﬂight should leave at around seven p.m.\nSentences with imperative structure often begin with a verb phrase and have\nimperative\nno subject. They are called imperative because they are almost always used for\ncommands and suggestions; in the ATIS domain they are commands to the system.\nShow the lowest fare\nGive me Sunday’s ﬂights arriving in Las Vegas from New York City\nList all ﬂights between ﬁve and seven p.m.\nWe can model this sentence structure with another rule for the expansion of S:\nS →VP\nSentences with yes-no question structure are often (though not always) used to\nyes-no question\nask questions; they begin with an auxiliary verb, followed by a subject NP, followed\nby a VP. Here are some examples. Note that the third example is not a question at\nall but a request; Chapter 24 discusses the uses of these question forms to perform\ndifferent pragmatic functions such as asking, requesting, or suggesting.\nDo any of these ﬂights have stops?\nDoes American’s ﬂight eighteen twenty ﬁve serve dinner?\nCan you give me the same information for United?\nHere’s the rule:\nS →Aux NP VP",
  "209": "10.3\n•\nSOME GRAMMAR RULES FOR ENGLISH\n201\nThe most complex sentence-level structures we examine here are the various wh-\nstructures. These are so named because one of their constituents is a wh-phrase, that\nwh-phrase\nis, one that includes a wh-word (who, whose, when, where, what, which, how, why).\nwh-word\nThese may be broadly grouped into two classes of sentence-level structures. The\nwh-subject-question structure is identical to the declarative structure, except that\nthe ﬁrst noun phrase contains some wh-word.\nWhat airlines ﬂy from Burbank to Denver?\nWhich ﬂights depart Burbank after noon and arrive in Denver by six p.m?\nWhose ﬂights serve breakfast?\nHere is a rule. Exercise 10.7 discusses rules for the constituents that make up the\nWh-NP.\nS →Wh-NP VP\nIn the wh-non-subject-question structure, the wh-phrase is not the subject of the\nwh-non-subject-\nquestion\nsentence, and so the sentence includes another subject. In these types of sentences\nthe auxiliary appears before the subject NP, just as in the yes-no question structures.\nHere is an example followed by a sample rule:\nWhat ﬂights do you have from Burbank to Tacoma Washington?\nS →Wh-NP Aux NP VP\nConstructions like the wh-non-subject-question contain what are called long-\ndistance dependencies because the Wh-NP what ﬂights is far away from the predi-\nlong-distance\ndependencies\ncate that it is semantically related to, the main verb have in the VP. In some models\nof parsing and understanding compatible with the grammar rule above, long-distance\ndependencies like the relation between ﬂights and have are thought of as a semantic\nrelation. In such models, the job of ﬁguring out that ﬂights is the argument of have\nis done during semantic interpretation. In other models of parsing, the relationship\nbetween ﬂights and have is considered to be a syntactic relation, and the grammar is\nmodiﬁed to insert a small marker called a trace or empty category after the verb.\nWe return to such empty-category models when we introduce the Penn Treebank on\npage 208.\n10.3.2\nClauses and Sentences\nBefore we move on, we should clarify the status of the S rules in the grammars we\njust described. S rules are intended to account for entire sentences that stand alone\nas fundamental units of discourse. However, S can also occur on the right-hand side\nof grammar rules and hence can be embedded within larger sentences. Clearly then,\nthere’s more to being an S than just standing alone as a unit of discourse.\nWhat differentiates sentence constructions (i.e., the S rules) from the rest of the\ngrammar is the notion that they are in some sense complete. In this way they corre-\nspond to the notion of a clause, which traditional grammars often describe as form-\nclause\ning a complete thought. One way of making this notion of “complete thought” more\nprecise is to say an S is a node of the parse tree below which the main verb of the S\nhas all of its arguments. We deﬁne verbal arguments later, but for now let’s just see\nan illustration from the tree for I prefer a morning ﬂight in Fig. 10.4 on page 199.\nThe verb prefer has two arguments: the subject I and the object a morning ﬂight.\nOne of the arguments appears below the VP node, but the other one, the subject NP,\nappears only below the S node.",
  "210": "202\nCHAPTER 10\n•\nFORMAL GRAMMARS OF ENGLISH\n10.3.3\nThe Noun Phrase\nOur L0 grammar introduced three of the most frequent types of noun phrases that\noccur in English: pronouns, proper nouns and the NP →Det Nominal construction.\nThe central focus of this section is on the last type since that is where the bulk of\nthe syntactic complexity resides. These noun phrases consist of a head, the central\nnoun in the noun phrase, along with various modiﬁers that can occur before or after\nthe head noun. Let’s take a close look at the various parts.\nThe Determiner\nNoun phrases can begin with simple lexical determiners, as in the following exam-\nples:\na stop\nthe ﬂights\nthis ﬂight\nthose ﬂights\nany ﬂights\nsome ﬂights\nThe role of the determiner in English noun phrases can also be ﬁlled by more\ncomplex expressions, as follows:\nUnited’s ﬂight\nUnited’s pilot’s union\nDenver’s mayor’s mother’s canceled ﬂight\nIn these examples, the role of the determiner is ﬁlled by a possessive expression\nconsisting of a noun phrase followed by an ’s as a possessive marker, as in the\nfollowing rule.\nDet →NP ′s\nThe fact that this rule is recursive (since an NP can start with a Det) helps us\nmodel the last two examples above, in which a sequence of possessive expressions\nserves as a determiner.\nUnder some circumstances determiners are optional in English. For example,\ndeterminers may be omitted if the noun they modify is plural:\n(10.2) Show me ﬂights from San Francisco to Denver on weekdays\nAs we saw in Chapter 8, mass nouns also don’t require determination. Recall that\nmass nouns often (not always) involve something that is treated like a substance\n(including e.g., water and snow), don’t take the indeﬁnite article “a”, and don’t tend\nto pluralize. Many abstract nouns are mass nouns (music, homework). Mass nouns\nin the ATIS domain include breakfast, lunch, and dinner:\n(10.3) Does this ﬂight serve dinner?\nThe Nominal\nThe nominal construction follows the determiner and contains any pre- and post-\nhead noun modiﬁers. As indicated in grammar L0, in its simplest form a nominal\ncan consist of a single noun.\nNominal →Noun\nAs we’ll see, this rule also provides the basis for the bottom of various recursive\nrules used to capture more complex nominal constructions.",
  "211": "10.3\n•\nSOME GRAMMAR RULES FOR ENGLISH\n203\nBefore the Head Noun\nA number of different kinds of word classes can appear before the head noun (the\nCardinal\nnumbers\n“postdeterminers”) in a nominal. These include cardinal numbers, ordinal num-\nbers, quantiﬁers, and adjectives. Examples of cardinal numbers:\nordinal\nnumbers\nquantiﬁers\ntwo friends\none stop\nOrdinal numbers include ﬁrst, second, third, and so on, but also words like next,\nlast, past, other, and another:\nthe ﬁrst one\nthe next day\nthe second leg\nthe last ﬂight\nthe other American ﬂight\nSome quantiﬁers (many, (a) few, several) occur only with plural count nouns:\nmany fares\nAdjectives occur after quantiﬁers but before nouns.\na ﬁrst-class fare\na non-stop ﬂight\nthe longest layover\nthe earliest lunch ﬂight\nAdjectives can also be grouped into a phrase called an adjective phrase or AP.\nadjective\nphrase\nAPs can have an adverb before the adjective (see Chapter 8 for deﬁnitions of adjec-\ntives and adverbs):\nthe least expensive fare\nAfter the Head Noun\nA head noun can be followed by postmodiﬁers. Three kinds of nominal postmodi-\nﬁers are common in English:\nprepositional phrases\nall ﬂights from Cleveland\nnon-ﬁnite clauses\nany ﬂights arriving after eleven a.m.\nrelative clauses\na ﬂight that serves breakfast\ncommon in the ATIS corpus since they are used to mark the origin and destina-\ntion of ﬂights.\nHere are some examples of prepositional phrase postmodiﬁers, with brackets\ninserted to show the boundaries of each PP; note that two or more PPs can be strung\ntogether within a single NP:\nall ﬂights [from Cleveland] [to Newark]\narrival [in San Jose] [before seven p.m.]\na reservation [on ﬂight six oh six] [from Tampa] [to Montreal]\nHere’s a new nominal rule to account for postnominal PPs:\nNominal →Nominal PP\nThe three most common kinds of non-ﬁnite postmodiﬁers are the gerundive (-\nnon-ﬁnite\ning), -ed, and inﬁnitive forms.\nGerundive postmodiﬁers are so called because they consist of a verb phrase that\ngerundive\nbegins with the gerundive (-ing) form of the verb. Here are some examples:\nany of those [leaving on Thursday]\nany ﬂights [arriving after eleven a.m.]\nﬂights [arriving within thirty minutes of each other]",
  "212": "204\nCHAPTER 10\n•\nFORMAL GRAMMARS OF ENGLISH\nWe can deﬁne the Nominals with gerundive modiﬁers as follows, making use of\na new non-terminal GerundVP:\nNominal →Nominal GerundVP\nWe can make rules for GerundVP constituents by duplicating all of our VP pro-\nductions, substituting GerundV for V.\nGerundVP →GerundV NP\n|\nGerundV PP | GerundV | GerundV NP PP\nGerundV can then be deﬁned as\nGerundV →being | arriving | leaving | ...\nThe phrases in italics below are examples of the two other common kinds of\nnon-ﬁnite clauses, inﬁnitives and -ed forms:\nthe last ﬂight to arrive in Boston\nI need to have dinner served\nWhich is the aircraft used by this ﬂight?\nA postnominal relative clause (more correctly a restrictive relative clause), is\na clause that often begins with a relative pronoun (that and who are the most com-\nrelative\npronoun\nmon). The relative pronoun functions as the subject of the embedded verb in the\nfollowing examples:\na ﬂight that serves breakfast\nﬂights that leave in the morning\nthe one that leaves at ten thirty ﬁve\nWe might add rules like the following to deal with these:\nNominal →Nominal RelClause\nRelClause →(who | that) VP\nThe relative pronoun may also function as the object of the embedded verb, as\nin the following example; we leave for the reader the exercise of writing grammar\nrules for more complex relative clauses of this kind.\nthe earliest American Airlines ﬂight that I can get\nVarious postnominal modiﬁers can be combined, as the following examples\nshow:\na ﬂight [from Phoenix to Detroit] [leaving Monday evening]\nevening ﬂights [from Nashville to Houston] [that serve dinner]\na friend [living in Denver] [that would like to visit me here in Washington DC]\nBefore the Noun Phrase\nWord classes that modify and appear before NPs are called predeterminers. Many\npredeterminers\nof these have to do with number or amount; a common predeterminer is all:\nall the ﬂights\nall ﬂights\nall non-stop ﬂights\nThe example noun phrase given in Fig. 10.5 illustrates some of the complexity\nthat arises when these rules are combined.",
  "213": "10.3\n•\nSOME GRAMMAR RULES FOR ENGLISH\n205\nNP\nNP\nNom\nGerundiveVP\nleaving before 10\nNom\nPP\nto Tampa\nNom\nPP\nfrom Denver\nNom\nNoun\nﬂights\nNom\nNoun\nmorning\nDet\nthe\nPreDet\nall\nFigure 10.5\nA parse tree for “all the morning ﬂights from Denver to Tampa leaving before 10”.\n10.3.4\nThe Verb Phrase\nThe verb phrase consists of the verb and a number of other constituents. In the\nsimple rules we have built so far, these other constituents include NPs and PPs and\ncombinations of the two:\nVP →Verb\ndisappear\nVP →Verb NP\nprefer a morning ﬂight\nVP →Verb NP PP\nleave Boston in the morning\nVP →Verb PP\nleaving on Thursday\nVerb phrases can be signiﬁcantly more complicated than this. Many other kinds\nof constituents, such as an entire embedded sentence, can follow the verb. These are\ncalled sentential complements:\nsentential\ncomplements\nYou [VP [V said [S you had a two hundred sixty six dollar fare]]\n[VP [V Tell] [NP me] [S how to get from the airport in Philadelphia to down-\ntown]]\nI [VP [V think [S I would like to take the nine thirty ﬂight]]\nHere’s a rule for these:\nVP →Verb S\nSimilarly, another potential constituent of the VP is another VP. This is often the\ncase for verbs like want, would like, try, intend, need:\nI want [VP to ﬂy from Milwaukee to Orlando]\nHi, I want [VP to arrange three ﬂights]",
  "214": "206\nCHAPTER 10\n•\nFORMAL GRAMMARS OF ENGLISH\nFrame\nVerb\nExample\n/0\neat, sleep\nI ate\nNP\nprefer, ﬁnd, leave\nFind [NP the ﬂight from Pittsburgh to Boston]\nNP NP\nshow, give\nShow [NP me] [NP airlines with ﬂights from Pittsburgh]\nPPfrom PPto\nﬂy, travel\nI would like to ﬂy [PP from Boston] [PP to Philadelphia]\nNP PPwith\nhelp, load\nCan you help [NP me] [PP with a ﬂight]\nVPto\nprefer, want, need\nI would prefer [VPto to go by United airlines]\nVPbrst\ncan, would, might\nI can [VPbrst go from Boston]\nS\nmean\nDoes this mean [S AA has a hub in Boston]\nFigure 10.6\nSubcategorization frames for a set of example verbs.\nWhile a verb phrase can have many possible kinds of constituents, not every\nverb is compatible with every verb phrase. For example, the verb want can be used\neither with an NP complement (I want a ﬂight ... ) or with an inﬁnitive VP comple-\nment (I want to ﬂy to ...). By contrast, a verb like ﬁnd cannot take this sort of VP\ncomplement (* I found to ﬂy to Dallas).\nThis idea that verbs are compatible with different kinds of complements is a very\nold one; traditional grammar distinguishes between transitive verbs like ﬁnd, which\ntransitive\ntake a direct object NP (I found a ﬂight), and intransitive verbs like disappear,\nintransitive\nwhich do not (*I disappeared a ﬂight).\nWhere traditional grammars subcategorize verbs into these two categories (tran-\nsubcategorize\nsitive and intransitive), modern grammars distinguish as many as 100 subcategories.\nWe say that a verb like ﬁnd subcategorizes for an NP, and a verb like want sub-\nSubcategorizes\nfor\ncategorizes for either an NP or a non-ﬁnite VP. We also call these constituents the\ncomplements of the verb (hence our use of the term sentential complement above).\ncomplements\nSo we say that want can take a VP complement. These possible sets of complements\nare called the subcategorization frame for the verb. Another way of talking about\nSubcategorization\nframe\nthe relation between the verb and these other constituents is to think of the verb as\na logical predicate and the constituents as logical arguments of the predicate. So we\ncan think of such predicate-argument relations as FIND(I, A FLIGHT) or WANT(I, TO\nFLY). We talk more about this view of verbs and arguments in Chapter 14 when we\ntalk about predicate calculus representations of verb semantics. Subcategorization\nframes for a set of example verbs are given in Fig. 10.6.\nWe can capture the association between verbs and their complements by making\nseparate subtypes of the class Verb (e.g., Verb-with-NP-complement, Verb-with-Inf-\nVP-complement, Verb-with-S-complement, and so on):\nVerb-with-NP-complement →ﬁnd | leave | repeat | ...\nVerb-with-S-complement →think | believe | say | ...\nVerb-with-Inf-VP-complement →want | try | need | ...\nEach VP rule could then be modiﬁed to require the appropriate verb subtype:\nVP →Verb-with-no-complement\ndisappear\nVP →Verb-with-NP-comp NP\nprefer a morning ﬂight\nVP →Verb-with-S-comp S said there were two ﬂights\nA problem with this approach is the signiﬁcant increase in the number of rules\nand the associated loss of generality.",
  "215": "10.4\n•\nTREEBANKS\n207\n10.3.5\nCoordination\nThe major phrase types discussed here can be conjoined with conjunctions like and,\nconjunctions\nor, and but to form larger constructions of the same type. For example, a coordinate\ncoordinate\nnoun phrase can consist of two other noun phrases separated by a conjunction:\nPlease repeat [NP [NP the ﬂights] and [NP the costs]]\nI need to know [NP [NP the aircraft] and [NP the ﬂight number]]\nHere’s a rule that allows these structures:\nNP →NP and NP\nNote that the ability to form coordinate phrases through conjunctions is often\nused as a test for constituency. Consider the following examples, which differ from\nthe ones given above in that they lack the second determiner.\nPlease repeat the [Nom [Nom ﬂights] and [Nom costs]]\nI need to know the [Nom [Nom aircraft] and [Nom ﬂight number]]\nThe fact that these phrases can be conjoined is evidence for the presence of the\nunderlying Nominal constituent we have been making use of. Here’s a new rule for\nthis:\nNominal →Nominal and Nominal\nThe following examples illustrate conjunctions involving VPs and Ss.\nWhat ﬂights do you have [VP [VP leaving Denver] and [VP arriving in\nSan Francisco]]\n[S [S I’m interested in a ﬂight from Dallas to Washington] and [S I’m\nalso interested in going to Baltimore]]\nThe rules for VP and S conjunctions mirror the NP one given above.\nVP →VP and VP\nS →S and S\nSince all the major phrase types can be conjoined in this fashion, it is also pos-\nsible to represent this conjunction fact more generally; a number of grammar for-\nmalisms such as GPSG ((Gazdar et al., 1985)) do this using metarules such as the\nmetarules\nfollowing:\nX →X and X\nThis metarule simply states that any non-terminal can be conjoined with the same\nnon-terminal to yield a constituent of the same type. Of course, the variable X\nmust be designated as a variable that stands for any non-terminal rather than a non-\nterminal itself.\n10.4\nTreebanks\nSufﬁciently robust grammars consisting of context-free grammar rules can be used\nto assign a parse tree to any sentence. This means that it is possible to build a\ncorpus where every sentence in the collection is paired with a corresponding parse",
  "216": "208\nCHAPTER 10\n•\nFORMAL GRAMMARS OF ENGLISH\ntree. Such a syntactically annotated corpus is called a treebank. Treebanks play\ntreebank\nan important role in parsing, as we discuss in Chapter 11, as well as in linguistic\ninvestigations of syntactic phenomena.\nA wide variety of treebanks have been created, generally through the use of\nparsers (of the sort described in the next few chapters) to automatically parse each\nsentence, followed by the use of humans (linguists) to hand-correct the parses. The\nPenn Treebank project (whose POS tagset we introduced in Chapter 8) has pro-\nPenn Treebank\nduced treebanks from the Brown, Switchboard, ATIS, and Wall Street Journal cor-\npora of English, as well as treebanks in Arabic and Chinese. A number of treebanks\nuse the dependency representation we will introduce in Chapter 13, including many\nthat are part of the Universal Dependencies project (Nivre et al., 2016b).\n10.4.1\nExample: The Penn Treebank Project\nFigure 10.7 shows sentences from the Brown and ATIS portions of the Penn Tree-\nbank.1 Note the formatting differences for the part-of-speech tags; such small dif-\nferences are common and must be dealt with in processing treebanks. The Penn\nTreebank part-of-speech tagset was deﬁned in Chapter 8. The use of LISP-style\nparenthesized notation for trees is extremely common and resembles the bracketed\nnotation we saw earlier in (10.1). For those who are not familiar with it we show a\nstandard node-and-line tree representation in Fig. 10.8.\n((S\n(NP-SBJ (DT That)\n(JJ cold) (, ,)\n(JJ empty) (NN sky) )\n(VP (VBD was)\n(ADJP-PRD (JJ full)\n(PP (IN of)\n(NP (NN fire)\n(CC and)\n(NN light) ))))\n(. .) ))\n((S\n(NP-SBJ The/DT flight/NN )\n(VP should/MD\n(VP arrive/VB\n(PP-TMP at/IN\n(NP eleven/CD a.m/RB ))\n(NP-TMP tomorrow/NN )))))\n(a)\n(b)\nFigure 10.7\nParsed sentences from the LDC Treebank3 version of the Brown (a) and ATIS\n(b) corpora.\nFigure 10.9 shows a tree from the Wall Street Journal. This tree shows an-\nother feature of the Penn Treebanks: the use of traces (-NONE- nodes) to mark\ntraces\nlong-distance dependencies or syntactic movement. For example, quotations often\nsyntactic\nmovement\nfollow a quotative verb like say. But in this example, the quotation “We would have\nto wait until we have collected on those assets” precedes the words he said. An\nempty S containing only the node -NONE- marks the position after said where the\nquotation sentence often occurs. This empty node is marked (in Treebanks II and\nIII) with the index 2, as is the quotation S at the beginning of the sentence. Such\nco-indexing may make it easier for some parsers to recover the fact that this fronted\nor topicalized quotation is the complement of the verb said. A similar -NONE- node\n1\nThe Penn Treebank project released treebanks in multiple languages and in various stages; for ex-\nample, there were Treebank I (Marcus et al., 1993), Treebank II (Marcus et al., 1994), and Treebank III\nreleases of English treebanks. We use Treebank III for our examples.",
  "217": "10.4\n•\nTREEBANKS\n209\nS\n.\n.\nVP\nADJP-PRD\nPP\nNP\nNN\nlight\nCC\nand\nNN\nﬁre\nIN\nof\nJJ\nfull\nVBD\nwas\nNP-SBJ\nNN\nsky\nJJ\nempty\n,\n,\nJJ\ncold\nDT\nThat\nFigure 10.8\nThe tree corresponding to the Brown corpus sentence in the previous ﬁgure.\nmarks the fact that there is no syntactic subject right before the verb to wait; instead,\nthe subject is the earlier NP We. Again, they are both co-indexed with the index 1.\n( (S (‘‘ ‘‘)\n(S-TPC-2\n(NP-SBJ-1 (PRP We) )\n(VP (MD would)\n(VP (VB have)\n(S\n(NP-SBJ (-NONE- *-1) )\n(VP (TO to)\n(VP (VB wait)\n(SBAR-TMP (IN until)\n(S\n(NP-SBJ (PRP we) )\n(VP (VBP have)\n(VP (VBN collected)\n(PP-CLR (IN on)\n(NP (DT those)(NNS assets)))))))))))))\n(, ,) (’’ ’’)\n(NP-SBJ (PRP he) )\n(VP (VBD said)\n(S (-NONE- *T*-2) ))\n(. .) ))\nFigure 10.9\nA sentence from the Wall Street Journal portion of the LDC Penn Treebank.\nNote the use of the empty -NONE- nodes.\nThe Penn Treebank II and Treebank III releases added further information to\nmake it easier to recover the relationships between predicates and arguments. Cer-",
  "218": "210\nCHAPTER 10\n•\nFORMAL GRAMMARS OF ENGLISH\nGrammar\nLexicon\nS →NP VP .\nPRP →we | he\nS →NP VP\nDT →the | that | those\nS →“ S ” , NP VP .\nJJ →cold | empty | full\nS →-NONE-\nNN →sky | ﬁre | light | ﬂight | tomorrow\nNP →DT NN\nNNS →assets\nNP →DT NNS\nCC →and\nNP →NN CC NN\nIN →of | at | until | on\nNP →CD RB\nCD →eleven\nNP →DT JJ , JJ NN\nRB →a.m.\nNP →PRP\nVB →arrive | have | wait\nNP →-NONE-\nVBD →was | said\nVP →MD VP\nVBP →have\nVP →VBD ADJP\nVBN →collected\nVP →VBD S\nMD →should | would\nVP →VBN PP\nTO →to\nVP →VB S\nVP →VB SBAR\nVP →VBP VP\nVP →VBN PP\nVP →TO VP\nSBAR →IN S\nADJP →JJ PP\nPP →IN NP\nFigure 10.10\nA sample of the CFG grammar rules and lexical entries that would be ex-\ntracted from the three treebank sentences in Fig. 10.7 and Fig. 10.9.\ntain phrases were marked with tags indicating the grammatical function of the phrase\n(as surface subject, logical topic, cleft, non-VP predicates) its presence in particular\ntext categories (headlines, titles), and its semantic function (temporal phrases, lo-\ncations) (Marcus et al. 1994, Bies et al. 1995). Figure 10.9 shows examples of the\n-SBJ (surface subject) and -TMP (temporal phrase) tags. Figure 10.8 shows in addi-\ntion the -PRD tag, which is used for predicates that are not VPs (the one in Fig. 10.8\nis an ADJP). We’ll return to the topic of grammatical function when we consider\ndependency grammars and parsing in Chapter 13.\n10.4.2\nTreebanks as Grammars\nThe sentences in a treebank implicitly constitute a grammar of the language repre-\nsented by the corpus being annotated. For example, from the three parsed sentences\nin Fig. 10.7 and Fig. 10.9, we can extract each of the CFG rules in them. For sim-\nplicity, let’s strip off the rule sufﬁxes (-SBJ and so on). The resulting grammar is\nshown in Fig. 10.10.\nThe grammar used to parse the Penn Treebank is relatively ﬂat, resulting in very\nmany and very long rules. For example, among the approximately 4,500 different\nrules for expanding VPs are separate rules for PP sequences of any length and every\npossible arrangement of verb arguments:\nVP →VBD PP\nVP →VBD PP PP\nVP →VBD PP PP PP\nVP →VBD PP PP PP PP\nVP →VB ADVP PP\nVP →VB PP ADVP\nVP →ADVP VB PP",
  "219": "10.4\n•\nTREEBANKS\n211\nas well as even longer rules, such as\nVP →VBP PP PP PP PP PP ADVP PP\nwhich comes from the VP marked in italics:\nThis mostly happens because we go from football in the fall to lifting in the\nwinter to football again in the spring.\nSome of the many thousands of NP rules include\nNP →DT JJ NN\nNP →DT JJ NNS\nNP →DT JJ NN NN\nNP →DT JJ JJ NN\nNP →DT JJ CD NNS\nNP →RB DT JJ NN NN\nNP →RB DT JJ JJ NNS\nNP →DT JJ JJ NNP NNS\nNP →DT NNP NNP NNP NNP JJ NN\nNP →DT JJ NNP CC JJ JJ NN NNS\nNP →RB DT JJS NN NN SBAR\nNP →DT VBG JJ NNP NNP CC NNP\nNP →DT JJ NNS , NNS CC NN NNS NN\nNP →DT JJ JJ VBG NN NNP NNP FW NNP\nNP →NP JJ , JJ ‘‘ SBAR ’’ NNS\nThe last two of those rules, for example, come from the following two noun phrases:\n[DT The] [JJ state-owned] [JJ industrial] [VBG holding] [NN company] [NNP Instituto]\n[NNP Nacional] [FW de] [NNP Industria]\n[NP Shearson’s] [JJ easy-to-ﬁlm], [JJ black-and-white] “[SBAR Where We Stand]”\n[NNS commercials]\nViewed as a large grammar in this way, the Penn Treebank III Wall Street Journal\ncorpus, which contains about 1 million words, also has about 1 million non-lexical\nrule tokens, consisting of about 17,500 distinct rule types.\nVarious facts about the treebank grammars, such as their large numbers of ﬂat\nrules, pose problems for probabilistic parsing algorithms. For this reason, it is com-\nmon to make various modiﬁcations to a grammar extracted from a treebank. We\ndiscuss these further in Chapter 12.\n10.4.3\nHeads and Head Finding\nWe suggested informally earlier that syntactic constituents could be associated with\na lexical head; N is the head of an NP, V is the head of a VP. This idea of a head for\neach constituent dates back to Bloomﬁeld (1914). It is central to constituent-based\ngrammar formalisms such as Head-Driven Phrase Structure Grammar (Pollard and\nSag, 1994), as well as the dependency-based approaches to grammar we’ll discuss\nin Chapter 13. Heads and head-dependent relations have also come to play a central\nrole in computational linguistics with their use in probabilistic parsing (Chapter 12)\nand in dependency parsing (Chapter 13).\nIn one simple model of lexical heads, each context-free rule is associated with\na head (Charniak 1997, Collins 1999). The head is the word in the phrase that is\ngrammatically the most important. Heads are passed up the parse tree; thus, each\nnon-terminal in a parse tree is annotated with a single word, which is its lexical head.",
  "220": "212\nCHAPTER 10\n•\nFORMAL GRAMMARS OF ENGLISH\nS(dumped)\nVP(dumped)\nPP(into)\nNP(bin)\nNN(bin)\nbin\nDT(a)\na\nP\ninto\nNP(sacks)\nNNS(sacks)\nsacks\nVBD(dumped)\ndumped\nNP(workers)\nNNS(workers)\nworkers\nFigure 10.11\nA lexicalized tree from Collins (1999).\nFigure 10.11 shows an example of such a tree from Collins (1999), in which each\nnon-terminal is annotated with its head.\nFor the generation of such a tree, each CFG rule must be augmented to identify\none right-side constituent to be the head daughter. The headword for a node is\nthen set to the headword of its head daughter. Choosing these head daughters is\nsimple for textbook examples (NN is the head of NP) but is complicated and indeed\ncontroversial for most phrases. (Should the complementizer to or the verb be the\nhead of an inﬁnite verb-phrase?) Modern linguistic theories of syntax generally\ninclude a component that deﬁnes heads (see, e.g., (Pollard and Sag, 1994)).\nAn alternative approach to ﬁnding a head is used in most practical computational\nsystems. Instead of specifying head rules in the grammar itself, heads are identiﬁed\ndynamically in the context of trees for speciﬁc sentences. In other words, once\na sentence is parsed, the resulting tree is walked to decorate each node with the\nappropriate head. Most current systems rely on a simple set of hand-written rules,\nsuch as a practical one for Penn Treebank grammars given in Collins (1999) but\ndeveloped originally by Magerman (1995). For example, the rule for ﬁnding the\nhead of an NP is as follows (Collins, 1999, p. 238):\n• If the last word is tagged POS, return last-word.\n• Else search from right to left for the ﬁrst child which is an NN, NNP, NNPS, NX, POS,\nor JJR.\n• Else search from left to right for the ﬁrst child which is an NP.\n• Else search from right to left for the ﬁrst child which is a $, ADJP, or PRN.\n• Else search from right to left for the ﬁrst child which is a CD.\n• Else search from right to left for the ﬁrst child which is a JJ, JJS, RB or QP.\n• Else return the last word\nSelected other rules from this set are shown in Fig. 10.12. For example, for VP\nrules of the form VP →Y1 ··· Yn, the algorithm would start from the left of Y1 ···\nYn looking for the ﬁrst Yi of type TO; if no TOs are found, it would search for the\nﬁrst Yi of type VBD; if no VBDs are found, it would search for a VBN, and so on.\nSee Collins (1999) for more details.",
  "221": "10.5\n•\nGRAMMAR EQUIVALENCE AND NORMAL FORM\n213\nParent\nDirection\nPriority List\nADJP\nLeft\nNNS QP NN $ ADVP JJ VBN VBG ADJP JJR NP JJS DT FW RBR RBS\nSBAR RB\nADVP\nRight\nRB RBR RBS FW ADVP TO CD JJR JJ IN NP JJS NN\nPRN\nLeft\nPRT\nRight\nRP\nQP\nLeft\n$ IN NNS NN JJ RB DT CD NCD QP JJR JJS\nS\nLeft\nTO IN VP S SBAR ADJP UCP NP\nSBAR\nLeft\nWHNP WHPP WHADVP WHADJP IN DT S SQ SINV SBAR FRAG\nVP\nLeft\nTO VBD VBN MD VBZ VB VBG VBP VP ADJP NN NNS NP\nFigure 10.12\nSelected head rules from Collins (1999). The set of head rules is often called a head percola-\ntion table.\n10.5\nGrammar Equivalence and Normal Form\nA formal language is deﬁned as a (possibly inﬁnite) set of strings of words. This\nsuggests that we could ask if two grammars are equivalent by asking if they gener-\nate the same set of strings. In fact, it is possible to have two distinct context-free\ngrammars generate the same language.\nWe usually distinguish two kinds of grammar equivalence: weak equivalence\nand strong equivalence. Two grammars are strongly equivalent if they generate the\nsame set of strings and if they assign the same phrase structure to each sentence\n(allowing merely for renaming of the non-terminal symbols). Two grammars are\nweakly equivalent if they generate the same set of strings but do not assign the same\nphrase structure to each sentence.\nIt is sometimes useful to have a normal form for grammars, in which each of\nnormal form\nthe productions takes a particular form. For example, a context-free grammar is in\nChomsky normal form (CNF) (Chomsky, 1963) if it is ϵ-free and if in addition\nChomsky\nnormal form\neach production is either of the form A →B C or A →a. That is, the right-hand side\nof each rule either has two non-terminal symbols or one terminal symbol. Chomsky\nnormal form grammars are binary branching, that is they have binary trees (down\nbinary\nbranching\nto the prelexical nodes). We make use of this binary branching property in the CKY\nparsing algorithm in Chapter 11.\nAny context-free grammar can be converted into a weakly equivalent Chomsky\nnormal form grammar. For example, a rule of the form\nA →B C D\ncan be converted into the following two CNF rules (Exercise 10.8 asks the reader to\nformulate the complete algorithm):\nA →B X\nX →C D\nSometimes using binary branching can actually produce smaller grammars. For\nexample, the sentences that might be characterized as\nVP -> VBD NP PP*\nare represented in the Penn Treebank by this series of rules:\nVP →VBD NP PP\nVP →VBD NP PP PP",
  "222": "214\nCHAPTER 10\n•\nFORMAL GRAMMARS OF ENGLISH\nVP →VBD NP PP PP PP\nVP →VBD NP PP PP PP PP\n...\nbut could also be generated by the following two-rule grammar:\nVP →VBD NP PP\nVP →VP PP\nThe generation of a symbol A with a potentially inﬁnite sequence of symbols B with\na rule of the form A →A B is known as Chomsky-adjunction.\nChomsky-\nadjunction\n10.6\nLexicalized Grammars\nThe approach to grammar presented thus far emphasizes phrase-structure rules while\nminimizing the role of the lexicon.\nHowever, as we saw in the discussions of\nagreement, subcategorization, and long distance dependencies, this approach leads\nto solutions that are cumbersome at best, yielding grammars that are redundant,\nhard to manage, and brittle. To overcome these issues, numerous alternative ap-\nproaches have been developed that all share the common theme of making bet-\nter use of the lexicon. Among the more computationally relevant approaches are\nLexical-Functional Grammar (LFG) (Bresnan, 1982), Head-Driven Phrase Structure\nGrammar (HPSG) (Pollard and Sag, 1994), Tree-Adjoining Grammar (TAG) (Joshi,\n1985), and Combinatory Categorial Grammar (CCG). These approaches differ with\nrespect to how lexicalized they are — the degree to which they rely on the lexicon\nas opposed to phrase structure rules to capture facts about the language.\nThe following section provides an introduction to CCG, a heavily lexicalized\napproach motivated by both syntactic and semantic considerations, which we will\nreturn to in Chapter 14. Chapter 13 discusses dependency grammars, an approach\nthat eliminates phrase-structure rules entirely.\n10.6.1\nCombinatory Categorial Grammar\nIn this section, we provide an overview of categorial grammar (Ajdukiewicz 1935,\ncategorial\ngrammar\nBar-Hillel 1953), an early lexicalized grammar model, as well as an important mod-\nern extension, combinatory categorial grammar, or CCG (Steedman 1996,Steed-\ncombinatory\ncategorial\ngrammar\nman 1989,Steedman 2000).\nThe categorial approach consists of three major elements: a set of categories,\na lexicon that associates words with categories, and a set of rules that govern how\ncategories combine in context.\nCategories\nCategories are either atomic elements or single-argument functions that return a cat-\negory as a value when provided with a desired category as argument. More formally,\nwe can deﬁne C , a set of categories for a grammar as follows:\n• A ⊆C , where A is a given set of atomic elements\n• (X/Y), (X\\Y) ∈C , if X, Y ∈C\nThe slash notation shown here is used to deﬁne the functions in the grammar.\nIt speciﬁes the type of the expected argument, the direction it is expected be found,\nand the type of the result. Thus, (X/Y) is a function that seeks a constituent of type",
  "223": "10.6\n•\nLEXICALIZED GRAMMARS\n215\nY to its right and returns a value of X; (X\\Y) is the same except it seeks its argument\nto the left.\nThe set of atomic categories is typically very small and includes familiar el-\nements such as sentences and noun phrases. Functional categories include verb\nphrases and complex noun phrases among others.\nThe Lexicon\nThe lexicon in a categorial approach consists of assignments of categories to words.\nThese assignments can either be to atomic or functional categories, and due to lexical\nambiguity words can be assigned to multiple categories. Consider the following\nsample lexical entries.\nﬂight :\nN\nMiami :\nNP\ncancel : (S\\NP)/NP\nNouns and proper nouns like ﬂight and Miami are assigned to atomic categories,\nreﬂecting their typical role as arguments to functions. On the other hand, a transitive\nverb like cancel is assigned the category (S\\NP)/NP: a function that seeks an NP on\nits right and returns as its value a function with the type (S\\NP). This function can,\nin turn, combine with an NP on the left, yielding an S as the result. This captures the\nkind of subcategorization information discussed in Section 10.3.4, however here the\ninformation has a rich, computationally useful, internal structure.\nDitransitive verbs like give, which expect two arguments after the verb, would\nhave the category ((S\\NP)/NP)/NP: a function that combines with an NP on its\nright to yield yet another function corresponding to the transitive verb (S\\NP)/NP\ncategory such as the one given above for cancel.\nRules\nThe rules of a categorial grammar specify how functions and their arguments com-\nbine. The following two rule templates constitute the basis for all categorial gram-\nmars.\nX/Y Y ⇒X\n(10.4)\nY X\\Y ⇒X\n(10.5)\nThe ﬁrst rule applies a function to its argument on the right, while the second\nlooks to the left for its argument. We’ll refer to the ﬁrst as forward function appli-\ncation, and the second as backward function application. The result of applying\neither of these rules is the category speciﬁed as the value of the function being ap-\nplied.\nGiven these rules and a simple lexicon, let’s consider an analysis of the sentence\nUnited serves Miami. Assume that serves is a transitive verb with the category\n(S\\NP)/NP and that United and Miami are both simple NPs. Using both forward\nand backward function application, the derivation would proceed as follows:\nUnited\nserves\nMiami\nNP\n(S\\NP)/NP\nNP\n>\nS\\NP\n<\nS",
  "224": "216\nCHAPTER 10\n•\nFORMAL GRAMMARS OF ENGLISH\nCategorial grammar derivations are illustrated growing down from the words,\nrule applications are illustrated with a horizontal line that spans the elements in-\nvolved, with the type of the operation indicated at the right end of the line. In this\nexample, there are two function applications: one forward function application indi-\ncated by the > that applies the verb serves to the NP on its right, and one backward\nfunction application indicated by the < that applies the result of the ﬁrst to the NP\nUnited on its left.\nWith the addition of another rule, the categorial approach provides a straight-\nforward way to implement the coordination metarule described earlier on page 207.\nRecall that English permits the coordination of two constituents of the same type,\nresulting in a new constituent of the same type. The following rule provides the\nmechanism to handle such examples.\nX CONJ X ⇒X\n(10.6)\nThis rule states that when two constituents of the same category are separated by a\nconstituent of type CONJ they can be combined into a single larger constituent of\nthe same type. The following derivation illustrates the use of this rule.\nWe\nflew\nto\nGeneva\nand\ndrove\nto\nChamonix\nNP (S\\NP)/PP PP/NP\nNP\nCONJ (S\\NP)/PP PP/NP\nNP\n>\n>\nPP\nPP\n>\n>\nS\\NP\nS\\NP\n<Φ>\nS\\NP\n<\nS\nHere the two S\\NP constituents are combined via the conjunction operator <Φ>\nto form a larger constituent of the same type, which can then be combined with the\nsubject NP via backward function application.\nThese examples illustrate the lexical nature of the categorial grammar approach.\nThe grammatical facts about a language are largely encoded in the lexicon, while the\nrules of the grammar are boiled down to a set of three rules. Unfortunately, the basic\ncategorial approach does not give us any more expressive power than we had with\ntraditional CFG rules; it just moves information from the grammar to the lexicon. To\nmove beyond these limitations CCG includes operations that operate over functions.\nThe ﬁrst pair of operators permit us to compose adjacent functions.\nX/Y Y/Z ⇒X/Z\n(10.7)\nY\\Z X\\Y ⇒X\\Z\n(10.8)\nThe ﬁrst rule, called forward composition, can be applied to adjacent con-\nforward\ncomposition\nstituents where the ﬁrst is a function seeking an argument of type Y to its right, and\nthe second is a function that providesY as a result. This rule allows us to compose\nthese two functions into a single one with the type of the ﬁrst constituent and the\nargument of the second. Although the notation is a little awkward, the second rule,\nbackward composition is the same, except that we’re looking to the left instead of\nbackward\ncomposition\nto the right for the relevant arguments. Both kinds of composition are signalled by a\nB in CCG diagrams, accompanied by a < or > to indicate the direction.\nThe next operator is type raising. Type raising elevates simple categories to the\ntype raising\nstatus of functions. More speciﬁcally, type raising takes a category and converts\nit to function that seeks as an argument a function that takes the original category",
  "225": "10.6\n•\nLEXICALIZED GRAMMARS\n217\nas its argument. The following schema show two versions of type raising: one for\narguments to the right, and one for the left.\nX ⇒T/(T\\X)\n(10.9)\nX ⇒T\\(T/X)\n(10.10)\nThe category T in these rules can correspond to any of the atomic or functional\ncategories already present in the grammar.\nA particularly useful example of type raising transforms a simple NP argument\nin subject position to a function that can compose with a following VP. To see how\nthis works, let’s revisit our earlier example of United serves Miami. Instead of clas-\nsifying United as an NP which can serve as an argument to the function attached to\nserve, we can use type raising to reinvent it as a function in its own right as follows.\nNP ⇒S/(S\\NP)\nCombining this type-raised constituent with the forward composition rule (10.7)\npermits the following alternative to our previous derivation.\nUnited\nserves\nMiami\nNP\n(S\\NP)/NP\nNP\n>T\nS/(S\\NP)\n>B\nS/NP\n>\nS\nBy type raising United to S/(S\\NP), we can compose it with the transitive verb\nserves to yield the (S/NP) function needed to complete the derivation.\nThere are several interesting things to note about this derivation. First, is it\nprovides a left-to-right, word-by-word derivation that more closely mirrors the way\nhumans process language. This makes CCG a particularly apt framework for psy-\ncholinguistic studies. Second, this derivation involves the use of an intermediate\nunit of analysis, United serves, that does not correspond to a traditional constituent\nin English. This ability to make use of such non-constituent elements provides CCG\nwith the ability to handle the coordination of phrases that are not proper constituents,\nas in the following example.\n(10.11) We ﬂew IcelandAir to Geneva and SwissAir to London.\nHere, the segments that are being coordinated are IcelandAir to Geneva and\nSwissAir to London, phrases that would not normally be considered constituents, as\ncan be seen in the following standard derivation for the verb phrase ﬂew IcelandAir\nto Geneva.\nﬂew\nIcelandAir\nto\nGeneva\n(VP/PP)/NP\nNP\nPP/NP\nNP\n>\n>\nVP/PP\nPP\n>\nVP\nIn this derivation, there is no single constituent that corresponds to IcelandAir\nto Geneva, and hence no opportunity to make use of the <Φ> operator. Note that\ncomplex CCG categories can can get a little cumbersome, so we’ll use VP as a\nshorthand for (S\\NP) in this and the following derivations.\nThe following alternative derivation provides the required element through the\nuse of both backward type raising (10.10) and backward function composition (10.8).",
  "226": "218\nCHAPTER 10\n•\nFORMAL GRAMMARS OF ENGLISH\nﬂew\nIcelandAir\nto\nGeneva\n(VP/PP)/NP\nNP\nPP/NP\nNP\n<T\n>\n(VP/PP)\\((VP/PP)/NP)\nPP\n<T\nVP\\(VP/PP)\n<B\nVP\\((VP/PP)/NP)\n<\nVP\nApplying the same analysis to SwissAir to London satisﬁes the requirements\nfor the <Φ> operator, yielding the following derivation for our original example\n(10.11).\nﬂew\nIcelandAir\nto\nGeneva\nand\nSwissAir\nto\nLondon\n(VP/PP)/NP\nNP\nPP/NP\nNP\nCONJ\nNP\nPP/NP\nNP\n<T\n>\n<T\n>\n(VP/PP)\\((VP/PP)/NP)\nPP\n(VP/PP)\\((VP/PP)/NP)\nPP\n<T\n<T\nVP\\(VP/PP)\nVP\\(VP/PP)\n<\n<\nVP\\((VP/PP)/NP)\nVP\\((VP/PP)/NP)\n<Φ>\nVP\\((VP/PP)/NP)\n<\nVP\nFinally, let’s examine how these advanced operators can be used to handle long-\ndistance dependencies (also referred to as syntactic movement or extraction). As\nmentioned in Section 10.3.1, long-distance dependencies arise from many English\nconstructions including wh-questions, relative clauses, and topicalization. What\nthese constructions have in common is a constituent that appears somewhere dis-\ntant from its usual, or expected, location. Consider the following relative clause as\nan example.\nthe ﬂight that United diverted\nHere, divert is a transitive verb that expects two NP arguments, a subject NP to its\nleft and a direct object NP to its right; its category is therefore (S\\NP)/NP. However,\nin this example the direct object the ﬂight has been “moved” to the beginning of the\nclause, while the subject United remains in its normal position. What is needed is a\nway to incorporate the subject argument, while dealing with the fact that the ﬂight is\nnot in its expected location.\nThe following derivation accomplishes this, again through the combined use of\ntype raising and function composition.\nthe\nﬂight\nthat\nUnited\ndiverted\nNP/N\nN\n(NP\\NP)/(S/NP)\nNP\n(S\\NP)/NP\n>\n>T\nNP\nS/(S\\NP)\n>B\nS/NP\n>\nNP\\NP\n<\nNP\nAs we saw with our earlier examples, the ﬁrst step of this derivation is type raising\nUnited to the category S/(S\\NP) allowing it to combine with diverted via forward\ncomposition. The result of this composition is S/NP which preserves the fact that we\nare still looking for an NP to ﬁll the missing direct object. The second critical piece\nis the lexical category assigned to the word that: (NP\\NP)/(S/NP). This function\nseeks a verb phrase missing an argument to its right, and transforms it into an NP\nseeking a missing element to its left, precisely where we ﬁnd the ﬂight.",
  "227": "10.7\n•\nSUMMARY\n219\nCCGBank\nAs with phrase-structure approaches, treebanks play an important role in CCG-\nbased approaches to parsing. CCGBank (Hockenmaier and Steedman, 2007) is the\nlargest and most widely used CCG treebank. It was created by automatically trans-\nlating phrase-structure trees from the Penn Treebank via a rule-based approach. The\nmethod produced successful translations of over 99% of the trees in the Penn Tree-\nbank resulting in 48,934 sentences paired with CCG derivations. It also provides\na lexicon of 44,000 words with over 1200 categories. Chapter 12 will discuss how\nthese resources can be used to train CCG parsers.\n10.7\nSummary\nThis chapter has introduced a number of fundamental concepts in syntax through\nthe use of context-free grammars.\n• In many languages, groups of consecutive words act as a group or a con-\nstituent, which can be modeled by context-free grammars (which are also\nknown as phrase-structure grammars).\n• A context-free grammar consists of a set of rules or productions, expressed\nover a set of non-terminal symbols and a set of terminal symbols. Formally,\na particular context-free language is the set of strings that can be derived\nfrom a particular context-free grammar.\n• A generative grammar is a traditional name in linguistics for a formal lan-\nguage that is used to model the grammar of a natural language.\n• There are many sentence-level grammatical constructions in English; declar-\native, imperative, yes-no question, and wh-question are four common types;\nthese can be modeled with context-free rules.\n• An English noun phrase can have determiners, numbers, quantiﬁers, and\nadjective phrases preceding the head noun, which can be followed by a num-\nber of postmodiﬁers; gerundive VPs, inﬁnitives VPs, and past participial\nVPs are common possibilities.\n• Subjects in English agree with the main verb in person and number.\n• Verbs can be subcategorized by the types of complements they expect. Sim-\nple subcategories are transitive and intransitive; most grammars include\nmany more categories than these.\n• Treebanks of parsed sentences exist for many genres of English and for many\nlanguages. Treebanks can be searched with tree-search tools.\n• Any context-free grammar can be converted to Chomsky normal form, in\nwhich the right-hand side of each rule has either two non-terminals or a single\nterminal.\n• Lexicalized grammars place more emphasis on the structure of the lexicon,\nlessening the burden on pure phrase-structure rules.\n• Combinatorial categorial grammar (CCG) is an important computationally\nrelevant lexicalized approach.",
  "228": "220\nCHAPTER 10\n•\nFORMAL GRAMMARS OF ENGLISH\nBibliographical and Historical Notes\n[The origin of the idea of phrasal constituency, cited in Percival (1976)]:\nden sprachlichen Ausdruck f¨ur die willk¨urliche\nGliederung einer Gesammtvorstellung in ihre\nin logische Beziehung zueinander gesetzten Bestandteile’\n[the linguistic expression for the arbitrary division of a total idea\ninto its constituent parts placed in logical relations to one another]\nW. Wundt\nAccording to Percival (1976), the idea of breaking up a sentence into a hierar-\nchy of constituents appeared in the V¨olkerpsychologie of the groundbreaking psy-\nchologist Wilhelm Wundt (Wundt, 1900). Wundt’s idea of constituency was taken\nup into linguistics by Leonard Bloomﬁeld in his early book An Introduction to the\nStudy of Language (Bloomﬁeld, 1914). By the time of his later book, Language\n(Bloomﬁeld, 1933a), what was then called “immediate-constituent analysis” was a\nwell-established method of syntactic study in the United States. By contrast, tra-\nditional European grammar, dating from the Classical period, deﬁned relations be-\ntween words rather than constituents, and European syntacticians retained this em-\nphasis on such dependency grammars, the subject of Chapter 13.\nAmerican Structuralism saw a number of speciﬁc deﬁnitions of the immediate\nconstituent, couched in terms of their search for a “discovery procedure”: a method-\nological algorithm for describing the syntax of a language. In general, these attempt\nto capture the intuition that “The primary criterion of the immediate constituent is the\ndegree in which combinations behave as simple units” (Bazell, 1966, p. 284). The\nmost well known of the speciﬁc deﬁnitions is Harris’ idea of distributional similarity\nto individual units, with the substitutability test. Essentially, the method proceeded\nby breaking up a construction into constituents by attempting to substitute simple\nstructures for possible constituents—if a substitution of a simple form, say, man,\nwas substitutable in a construction for a more complex set (like intense young man),\nthen the form intense young man was probably a constituent. Harris’s test was the\nbeginning of the intuition that a constituent is a kind of equivalence class.\nThe ﬁrst formalization of this idea of hierarchical constituency was the phrase-\nstructure grammar deﬁned in Chomsky (1956) and further expanded upon (and\nargued against) in Chomsky (1957) and Chomsky (1975). From this time on, most\ngenerative linguistic theories were based at least in part on context-free grammars or\ngeneralizations of them (such as Head-Driven Phrase Structure Grammar (Pollard\nand Sag, 1994), Lexical-Functional Grammar (Bresnan, 1982), Government and\nBinding (Chomsky, 1981), and Construction Grammar (Kay and Fillmore, 1999),\ninter alia); many of these theories used schematic context-free templates known as\nX-bar schemata, which also relied on the notion of syntactic head.\nX-bar\nschemata\nShortly after Chomsky’s initial work, the context-free grammar was reinvented\nby Backus (1959) and independently by Naur et al. (1960) in their descriptions of\nthe ALGOL programming language; Backus (1996) noted that he was inﬂuenced by\nthe productions of Emil Post and that Naur’s work was independent of his (Backus’)\nown. (Recall the discussion on page ?? of multiple invention in science.) After this\nearly work, a great number of computational models of natural language processing\nwere based on context-free grammars because of the early development of efﬁcient\nalgorithms to parse these grammars (see Chapter 11).",
  "229": "EXERCISES\n221\nAs we have already noted, grammars based on context-free rules are not ubiqui-\ntous. Various classes of extensions to CFGs are designed speciﬁcally to handle long-\ndistance dependencies. We noted earlier that some grammars treat long-distance-\ndependent items as being related semantically but not syntactically; the surface syn-\ntax does not represent the long-distance link (Kay and Fillmore 1999, Culicover and\nJackendoff 2005). But there are alternatives.\nOne extended formalism is Tree Adjoining Grammar (TAG) (Joshi, 1985).\nThe primary TAG data structure is the tree, rather than the rule. Trees come in two\nkinds: initial trees and auxiliary trees. Initial trees might, for example, represent\nsimple sentential structures, and auxiliary trees add recursion into a tree. Trees are\ncombined by two operations called substitution and adjunction. The adjunction\noperation handles long-distance dependencies. See Joshi (1985) for more details.\nAn extension of Tree Adjoining Grammar, called Lexicalized Tree Adjoining Gram-\nmars is discussed in Chapter 12. Tree Adjoining Grammar is a member of the family\nof mildly context-sensitive languages.\nWe mentioned on page 208 another way of handling long-distance dependencies,\nbased on the use of empty categories and co-indexing. The Penn Treebank uses\nthis model, which draws (in various Treebank corpora) from the Extended Standard\nTheory and Minimalism (Radford, 1997).\nReaders interested in the grammar of English should get one of the three large\nreference grammars of English: Huddleston and Pullum (2002), Biber et al. (1999),\nand Quirk et al. (1985). Another useful reference is McCawley (1998).\nThere are many good introductory textbooks on syntax from different perspec-\ntives. Sag et al. (2003) is an introduction to syntax from a generative perspective,\ngenerative\nfocusing on the use of phrase-structure rules, uniﬁcation, and the type hierarchy in\nHead-Driven Phrase Structure Grammar. Van Valin, Jr. and La Polla (1997) is an\nintroduction from a functional perspective, focusing on cross-linguistic data and on\nfunctional\nthe functional motivation for syntactic structures.\nExercises\n10.1 Draw tree structures for the following ATIS phrases:\n1. Dallas\n2. from Denver\n3. after ﬁve p.m.\n4. arriving in Washington\n5. early ﬂights\n6. all redeye ﬂights\n7. on Thursday\n8. a one-way fare\n9. any delays in Denver\n10.2 Draw tree structures for the following ATIS sentences:\n1. Does American airlines have a ﬂight between ﬁve a.m. and six a.m.?\n2. I would like to ﬂy on American airlines.\n3. Please repeat that.\n4. Does American 487 have a ﬁrst-class section?\n5. I need to ﬂy between Philadelphia and Atlanta.\n6. What is the fare from Atlanta to Denver?",
  "230": "222\nCHAPTER 10\n•\nFORMAL GRAMMARS OF ENGLISH\n7. Is there an American airlines ﬂight from Philadelphia to Dallas?\n10.3 Assume a grammar that has many VP rules for different subcategorizations,\nas expressed in Section 10.3.4, and differently subcategorized verb rules like\nVerb-with-NP-complement. How would the rule for postnominal relative clauses\n(10.4) need to be modiﬁed if we wanted to deal properly with examples like\nthe earliest ﬂight that you have? Recall that in such examples the pronoun\nthat is the object of the verb get. Your rules should allow this noun phrase but\nshould correctly rule out the ungrammatical S *I get.\n10.4 Does your solution to the previous problem correctly model the NP the earliest\nﬂight that I can get? How about the earliest ﬂight that I think my mother\nwants me to book for her? Hint: this phenomenon is called long-distance\ndependency.\n10.5 Write rules expressing the verbal subcategory of English auxiliaries; for ex-\nample, you might have a rule verb-with-bare-stem-VP-complement →can.\n10.6 NPs like Fortune’s ofﬁce or my uncle’s marks are called possessive or genitive\npossessive\ngenitive\nnoun phrases. We can model possessive noun phrases by treating the sub-NP\nlike Fortune’s or my uncle’s as a determiner of the following head noun. Write\ngrammar rules for English possessives. You may treat ’s as if it were a separate\nword (i.e., as if there were always a space before ’s).\n10.7 Page 201 discussed the need for a Wh-NP constituent. The simplest Wh-NP\nis one of the Wh-pronouns (who, whom, whose, which). The Wh-words what\nand which can be determiners: which four will you have?, what credit do you\nhave with the Duke? Write rules for the different types of Wh-NPs.\n10.8 Write an algorithm for converting an arbitrary context-free grammar into Chom-\nsky normal form.",
  "231": "CHAPTER\n11\nSyntactic Parsing\nOne morning I shot an elephant in my pajamas.\nHow he got into my pajamas I don’t know.\nGroucho Marx, Animal Crackers, 1930\nSyntactic parsing is the task of recognizing a sentence and assigning a syntactic\nstructure to it. This chapter focuses on the structures assigned by context-free gram-\nmars of the kind described in Chapter 10. Since they are based on a purely declar-\native formalism, context-free grammars don’t specify how the parse tree for a given\nsentence should be computed. We therefore need to specify algorithms that employ\nthese grammars to efﬁciently produce correct trees.\nParse trees are directly useful in applications such as grammar checking in\nword-processing systems: a sentence that cannot be parsed may have grammatical\nerrors (or at least be hard to read). More typically, however, parse trees serve as an\nimportant intermediate stage of representation for semantic analysis (as we show in\nChapter 15) and thus play an important role in applications like question answering\nand information extraction. For example, to answer the question\nWhat books were written by British women authors before 1800?\nwe’ll need to know that the subject of the sentence was what books and that the by-\nadjunct was British women authors to help us ﬁgure out that the user wants a list of\nbooks (and not a list of authors).\nBefore presenting any algorithms, we begin by discussing how the ambiguity\narises again in this context and the problems it presents.\nThe section that fol-\nlows then presents the Cocke-Kasami-Younger (CKY) algorithm (Kasami 1965,\nYounger 1967), the standard dynamic programming approach to syntactic parsing.\nRecall that we’ve already seen applications of dynamic programming algorithms in\nthe Minimum-Edit-Distance and Viterbi algorithms of earlier chapters. Finally, we\ndiscuss partial parsing methods, for use in situations in which a superﬁcial syntac-\ntic analysis of an input may be sufﬁcient.\n11.1\nAmbiguity\nAmbiguity is perhaps the most serious problem faced by syntactic parsers. Chap-\nter 8 introduced the notions of part-of-speech ambiguity and part-of-speech dis-\nambiguation. Here, we introduce a new kind of ambiguity, called structural ambi-\nguity, which arises from many commonly used rules in phrase-structure grammars.\nStructural\nambiguity\nTo illustrate the issues associated with structural ambiguity, we’ll make use of a new\ntoy grammar L1, shown in Figure 11.1, which consists of the L0 grammar from the\nlast chapter augmented with a few additional rules.\nStructural ambiguity occurs when the grammar can assign more than one parse\nto a sentence. Groucho Marx’s well-known line as Captain Spaulding in Animal",
  "232": "224\nCHAPTER 11\n•\nSYNTACTIC PARSING\nGrammar\nLexicon\nS →NP VP\nDet →that | this | the | a\nS →Aux NP VP\nNoun →book | ﬂight | meal | money\nS →VP\nVerb →book | include | prefer\nNP →Pronoun\nPronoun →I | she | me\nNP →Proper-Noun\nProper-Noun →Houston | NWA\nNP →Det Nominal\nAux →does\nNominal →Noun\nPreposition →from | to | on | near | through\nNominal →Nominal Noun\nNominal →Nominal PP\nVP →Verb\nVP →Verb NP\nVP →Verb NP PP\nVP →Verb PP\nVP →VP PP\nPP →Preposition NP\nFigure 11.1\nThe L1 miniature English grammar and lexicon.\nS\nVP\nNP\nNominal\nPP\nin my pajamas\nNominal\nNoun\nelephant\nDet\nan\nVerb\nshot\nNP\nPronoun\nI\nS\nVP\nPP\nin my pajamas\nVP\nNP\nNominal\nNoun\nelephant\nDet\nan\nVerb\nshot\nNP\nPronoun\nI\nFigure 11.2\nTwo parse trees for an ambiguous sentence. The parse on the left corresponds to the humorous\nreading in which the elephant is in the pajamas, the parse on the right corresponds to the reading in which\nCaptain Spaulding did the shooting in his pajamas.\nCrackers is ambiguous because the phrase in my pajamas can be part of the NP\nheaded by elephant or a part of the verb phrase headed by shot. Figure 11.2 illus-\ntrates these two analyses of Marx’s line using rules from L1.\nStructural ambiguity, appropriately enough, comes in many forms. Two common\nkinds of ambiguity are attachment ambiguity and coordination ambiguity.\nA sentence has an attachment ambiguity if a particular constituent can be at-\nAttachment\nambiguity\ntached to the parse tree at more than one place. The Groucho Marx sentence is\nan example of PP-attachment ambiguity. Various kinds of adverbial phrases are\nalso subject to this kind of ambiguity. For instance, in the following example the\ngerundive-VP ﬂying to Paris can be part of a gerundive sentence whose subject is\nthe Eiffel Tower or it can be an adjunct modifying the VP headed by saw:",
  "233": "11.2\n•\nCKY PARSING: A DYNAMIC PROGRAMMING APPROACH\n225\n(11.1) We saw the Eiffel Tower ﬂying to Paris.\nIn coordination ambiguity different sets of phrases can be conjoined by a con-\nCoordination\nambiguity\njunction like and. For example, the phrase old men and women can be bracketed as\n[old [men and women]], referring to old men and old women, or as [old men] and\n[women], in which case it is only the men who are old.\nThese ambiguities combine in complex ways in real sentences. A program that\nsummarized the news, for example, would need to be able to parse sentences like\nthe following from the Brown corpus:\n(11.2) President Kennedy today pushed aside other White House business to\ndevote all his time and attention to working on the Berlin crisis address he\nwill deliver tomorrow night to the American people over nationwide\ntelevision and radio.\nThis sentence has a number of ambiguities, although since they are semantically\nunreasonable, it requires a careful reading to see them. The last noun phrase could be\nparsed [nationwide [television and radio]] or [[nationwide television] and radio].\nThe direct object of pushed aside should be other White House business but could\nalso be the bizarre phrase [other White House business to devote all his time and\nattention to working] (i.e., a structure like Kennedy afﬁrmed [his intention to propose\na new budget to address the deﬁcit]). Then the phrase on the Berlin crisis address he\nwill deliver tomorrow night to the American people could be an adjunct modifying\nthe verb pushed. A PP like over nationwide television and radio could be attached\nto any of the higher VPs or NPs (e.g., it could modify people or night).\nThe fact that there are many grammatically correct but semantically unreason-\nable parses for naturally occurring sentences is an irksome problem that affects all\nparsers. Ultimately, most natural language processing systems need to be able to\nchoose a single correct parse from the multitude of possible parses through a process\nof syntactic disambiguation. Effective disambiguation algorithms require statisti-\nSyntactic\ndisambiguation\ncal, semantic, and contextual knowledge sources that vary in how well they can be\nintegrated into parsing algorithms.\nFortunately, the CKY algorithm presented in the next section is designed to efﬁ-\nciently handle structural ambiguities of the kind we’ve been discussing. And as we’ll\nsee in Chapter 12, there are straightforward ways to integrate statistical techniques\ninto the basic CKY framework to produce highly accurate parsers.\n11.2\nCKY Parsing: A Dynamic Programming Approach\nThe previous section introduced some of the problems associated with ambiguous\ngrammars. Fortunately, dynamic programming provides a powerful framework for\naddressing these problems, just as it did with the Minimum Edit Distance, Viterbi,\nand Forward algorithms. Recall that dynamic programming approaches systemati-\ncally ﬁll in tables of solutions to sub-problems. When complete, the tables contain\nthe solution to all the sub-problems needed to solve the problem as a whole. In\nthe case of syntactic parsing, these sub-problems represent parse trees for all the\nconstituents detected in the input.\nThe dynamic programming advantage arises from the context-free nature of our\ngrammar rules — once a constituent has been discovered in a segment of the input\nwe can record its presence and make it available for use in any subsequent derivation\nthat might require it. This provides both time and storage efﬁciencies since subtrees",
  "234": "226\nCHAPTER 11\n•\nSYNTACTIC PARSING\ncan be looked up in a table, not reanalyzed. This section presents the Cocke-Kasami-\nYounger (CKY) algorithm, the most widely used dynamic-programming based ap-\nproach to parsing. Related approaches include the Earley algorithm (Earley, 1970)\nand chart parsing (Kaplan 1973, Kay 1982).\n11.2.1\nConversion to Chomsky Normal Form\nWe begin our investigation of the CKY algorithm by examining the requirement\nthat grammars used with it must be in Chomsky Normal Form (CNF). Recall from\nChapter 10 that grammars in CNF are restricted to rules of the form A →B C or\nA →w. That is, the right-hand side of each rule must expand either to two non-\nterminals or to a single terminal. Restricting a grammar to CNF does not lead to\nany loss in expressiveness, since any context-free grammar can be converted into\na corresponding CNF grammar that accepts exactly the same set of strings as the\noriginal grammar.\nLet’s start with the process of converting a generic CFG into one represented in\nCNF. Assuming we’re dealing with an ϵ-free grammar, there are three situations we\nneed to address in any generic grammar: rules that mix terminals with non-terminals\non the right-hand side, rules that have a single non-terminal on the right-hand side,\nand rules in which the length of the right-hand side is greater than 2.\nThe remedy for rules that mix terminals and non-terminals is to simply introduce\na new dummy non-terminal that covers only the original terminal. For example, a\nrule for an inﬁnitive verb phrase such as INF-VP →to VP would be replaced by the\ntwo rules INF-VP →TO VP and TO →to.\nRules with a single non-terminal on the right are called unit productions. We\nUnit\nproductions\ncan eliminate unit productions by rewriting the right-hand side of the original rules\nwith the right-hand side of all the non-unit production rules that they ultimately lead\nto. More formally, if A ∗⇒B by a chain of one or more unit productions and B →γ\nis a non-unit production in our grammar, then we add A →γ for each such rule in\nthe grammar and discard all the intervening unit productions. As we demonstrate\nwith our toy grammar, this can lead to a substantial ﬂattening of the grammar and a\nconsequent promotion of terminals to fairly high levels in the resulting trees.\nRules with right-hand sides longer than 2 are normalized through the introduc-\ntion of new non-terminals that spread the longer sequences over several new rules.\nFormally, if we have a rule like\nA →B C γ\nwe replace the leftmost pair of non-terminals with a new non-terminal and introduce\na new production result in the following new rules:\nA →X1 γ\nX1 →B C\nIn the case of longer right-hand sides, we simply iterate this process until the of-\nfending rule has been replaced by rules of length 2. The choice of replacing the\nleftmost pair of non-terminals is purely arbitrary; any systematic scheme that results\nin binary rules would sufﬁce.\nIn our current grammar, the rule S →Aux NP VP would be replaced by the two\nrules S →X1 VP and X1 →Aux NP.\nThe entire conversion process can be summarized as follows:\n1. Copy all conforming rules to the new grammar unchanged.",
  "235": "11.2\n•\nCKY PARSING: A DYNAMIC PROGRAMMING APPROACH\n227\nL1 Grammar\nL1 in CNF\nS →NP VP\nS →NP VP\nS →Aux NP VP\nS →X1 VP\nX1 →Aux NP\nS →VP\nS →book | include | prefer\nS →Verb NP\nS →X2 PP\nS →Verb PP\nS →VP PP\nNP →Pronoun\nNP →I | she | me\nNP →Proper-Noun\nNP →TWA | Houston\nNP →Det Nominal\nNP →Det Nominal\nNominal →Noun\nNominal →book | ﬂight | meal | money\nNominal →Nominal Noun\nNominal →Nominal Noun\nNominal →Nominal PP\nNominal →Nominal PP\nVP →Verb\nVP →book | include | prefer\nVP →Verb NP\nVP →Verb NP\nVP →Verb NP PP\nVP →X2 PP\nX2 →Verb NP\nVP →Verb PP\nVP →Verb PP\nVP →VP PP\nVP →VP PP\nPP →Preposition NP\nPP →Preposition NP\nFigure 11.3\nL1 Grammar and its conversion to CNF. Note that although they aren’t shown\nhere, all the original lexical entries from L1 carry over unchanged as well.\n2. Convert terminals within rules to dummy non-terminals.\n3. Convert unit-productions.\n4. Make all rules binary and add them to new grammar.\nFigure 11.3 shows the results of applying this entire conversion procedure to\nthe L1 grammar introduced earlier on page 224. Note that this ﬁgure doesn’t show\nthe original lexical rules; since these original lexical rules are already in CNF, they\nall carry over unchanged to the new grammar. Figure 11.3 does, however, show\nthe various places where the process of eliminating unit productions has, in effect,\ncreated new lexical rules. For example, all the original verbs have been promoted to\nboth VPs and to Ss in the converted grammar.\n11.2.2\nCKY Recognition\nWith our grammar now in CNF, each non-terminal node above the part-of-speech\nlevel in a parse tree will have exactly two daughters. A two-dimensional matrix can\nbe used to encode the structure of an entire tree. For a sentence of length n, we will\nwork with the upper-triangular portion of an (n+1)×(n+1) matrix. Each cell [i, j]\nin this matrix contains the set of non-terminals that represent all the constituents that\nspan positions i through j of the input. Since our indexing scheme begins with 0,\nit’s natural to think of the indexes as pointing at the gaps between the input words\n(as in 0 Book 1 that 2 ﬂight 3). It follows then that the cell that represents the entire\ninput resides in position [0,n] in the matrix.\nSince each non-terminal entry in our table has two daughters in the parse, it fol-\nlows that for each constituent represented by an entry [i, j], there must be a position\nin the input, k, where it can be split into two parts such that i < k < j. Given such",
  "236": "228\nCHAPTER 11\n•\nSYNTACTIC PARSING\na position k, the ﬁrst constituent [i,k] must lie to the left of entry [i, j] somewhere\nalong row i, and the second entry [k, j] must lie beneath it, along column j.\nTo make this more concrete, consider the following example with its completed\nparse matrix, shown in Fig. 11.4.\n(11.3) Book the ﬂight through Houston.\nThe superdiagonal row in the matrix contains the parts of speech for each input word\nin the input. The subsequent diagonals above that superdiagonal contain constituents\nthat cover all the spans of increasing length in the input.\nBook\nthe \nflight \nthrough\nHouston\nS, VP, Verb, \nNominal, \nNoun\nS,VP,X2\nS,VP,X2\nDet\nNP\nNP\nNominal,\nNoun\nNominal\nPrep\nPP\nNP,\nProper-\nNoun\n[0,1]\n[0,2]\n[0,3]\n[0,4]\n[0,5]\n[1,2]\n[1,3]\n[2,3]\n[1,4]\n[2,5]\n[2,4]\n[3,4]\n[4,5]\n[3,5]\n[1,5]\nFigure 11.4\nCompleted parse table for Book the ﬂight through Houston.\nGiven this setup, CKY recognition consists of ﬁlling the parse table in the right\nway. To do this, we’ll proceed in a bottom-up fashion so that at the point where\nwe are ﬁlling any cell [i, j], the cells containing the parts that could contribute to\nthis entry (i.e., the cells to the left and the cells below) have already been ﬁlled.\nThe algorithm given in Fig. 11.5 ﬁlls the upper-triangular matrix a column at a time\nworking from left to right, with each column ﬁlled from bottom to top, as the right\nside of Fig. 11.4 illustrates. This scheme guarantees that at each point in time we\nhave all the information we need (to the left, since all the columns to the left have\nalready been ﬁlled, and below since we’re ﬁlling bottom to top). It also mirrors on-\nline parsing since ﬁlling the columns from left to right corresponds to processing\neach word one at a time.\nfunction CKY-PARSE(words, grammar) returns table\nfor j←from 1 to LENGTH(words) do\nfor all {A | A →words[j] ∈grammar}\ntable[j −1, j]←table[ j −1, j] ∪A\nfor i←from j −2 downto 0 do\nfor k←i+1 to j −1 do\nfor all {A | A →BC ∈grammar and B ∈table[i,k] and C ∈table[k, j]}\ntable[i,j]←table[i,j] ∪A\nFigure 11.5\nThe CKY algorithm.",
  "237": "11.2\n•\nCKY PARSING: A DYNAMIC PROGRAMMING APPROACH\n229\n...\n...\n[0,n]\n[i,i+1]\n[i,i+2]\n[i,j-2]\n[i,j-1]\n[i+1,j]\n[i+2,j]\n[j-1,j]\n[j-2,j]\n[i,j]\n...\n[0,1]\n[n-1, n]\nFigure 11.6\nAll the ways to ﬁll the [i, j]th cell in the CKY table.\nThe outermost loop of the algorithm given in Fig. 11.5 iterates over the columns,\nand the second loop iterates over the rows, from the bottom up. The purpose of the\ninnermost loop is to range over all the places where a substring spanning i to j in\nthe input might be split in two. As k ranges over the places where the string can be\nsplit, the pairs of cells we consider move, in lockstep, to the right along row i and\ndown along column j. Figure 11.6 illustrates the general case of ﬁlling cell [i, j]. At\neach such split, the algorithm considers whether the contents of the two cells can be\ncombined in a way that is sanctioned by a rule in the grammar. If such a rule exists,\nthe non-terminal on its left-hand side is entered into the table.\nFigure 11.7 shows how the ﬁve cells of column 5 of the table are ﬁlled after the\nword Houston is read. The arrows point out the two spans that are being used to add\nan entry to the table. Note that the action in cell [0,5] indicates the presence of three\nalternative parses for this input, one where the PP modiﬁes the ﬂight, one where\nit modiﬁes the booking, and one that captures the second argument in the original\nVP →Verb NP PP rule, now captured indirectly with the VP →X2 PP rule.",
  "238": "230\nCHAPTER 11\n•\nSYNTACTIC PARSING\nBook\nthe \nflight \nthrough\nHouston\nS, VP, Verb, \nNominal, \nNoun\nS,VP,X2\nDet\nNP\nNominal,\nNoun\nNominal\nPrep\nNP,\nProper-\nNoun\n[0,1]\n[0,2]\n[0,3]\n[0,4]\n[0,5]\n[1,2]\n[1,3]\n[2,3]\n[1,4]\n[2,5]\n[2,4]\n[3,4]\n[4,5]\n[3,5]\n[1,5]\nBook\nthe \nflight \nthrough\nHouston\nS, VP, Verb, \nNominal, \nNoun\nS,VP,X2\nDet\nNP\nNP\nNominal,\nNoun\nPrep\nPP\nNP,\nProper-\nNoun\n[0,1]\n[0,2]\n[0,3]\n[0,4]\n[0,5]\n[1,2]\n[1,3]\n[2,3]\n[1,4]\n[2,5]\n[2,4]\n[3,4]\n[4,5]\n[3,5]\n[1,5]\nBook\nthe \nflight \nthrough\nHouston\nS, VP, Verb, \nNominal, \nNoun\nS,VP,X2\nDet\nNP\nNP\nNominal,\nNoun\nNominal\nPrep\nPP\nNP,\nProper-\nNoun\n[0,1]\n[0,2]\n[0,3]\n[0,4]\n[0,5]\n[1,2]\n[1,3]\n[2,3]\n[1,4]\n[2,5]\n[2,4]\n[3,4]\n[4,5]\n[3,5]\n[1,5]\nBook\nthe \nflight \nthrough\nHouston\nS, VP, Verb, \nNominal, \nNoun\nS,VP,X2\nDet\nNP\nNP\nNominal,\nNoun\nNominal\nPrep\nPP\nNP,\nProper-\nNoun\n[0,1]\n[0,2]\n[0,3]\n[0,4]\n[0,5]\n[1,2]\n[1,3]\n[2,3]\n[1,4]\n[2,5]\n[2,4]\n[3,4]\n[4,5]\n[3,5]\n[1,5]\nBook\nthe \nflight \nthrough\nHouston\nS, VP, Verb, \nNominal, \nNoun\nS,\nVP,\nX2\nDet\nNP\nNP\nNominal,\nNoun\nNominal\nPrep\nPP\nNP,\nProper-\nNoun\n[0,1]\n[0,2]\n[0,3]\n[0,4]\n[1,2]\n[1,3]\n[2,3]\n[1,4]\n[2,5]\n[2,4]\n[3,4]\n[4,5]\n[3,5]\n[1,5]\nS2, VP\nS3\nS1,VP, X2\nFigure 11.7\nFilling the cells of column 5 after reading the word Houston.",
  "239": "11.3\n•\nPARTIAL PARSING\n231\n11.2.3\nCKY Parsing\nThe algorithm given in Fig. 11.5 is a recognizer, not a parser; for it to succeed, it\nsimply has to ﬁnd an S in cell [0,n]. To turn it into a parser capable of returning all\npossible parses for a given input, we can make two simple changes to the algorithm:\nthe ﬁrst change is to augment the entries in the table so that each non-terminal is\npaired with pointers to the table entries from which it was derived (more or less as\nshown in Fig. 11.7), the second change is to permit multiple versions of the same\nnon-terminal to be entered into the table (again as shown in Fig. 11.7). With these\nchanges, the completed table contains all the possible parses for a given input. Re-\nturning an arbitrary single parse consists of choosing an S from cell [0,n] and then\nrecursively retrieving its component constituents from the table.\nOf course, returning all the parses for a given input may incur considerable cost\nsince an exponential number of parses may be associated with a given input. In such\ncases, returning all the parses will have an unavoidable exponential cost. Looking\nforward to Chapter 12, we can also think about retrieving the best parse for a given\ninput by further augmenting the table to contain the probabilities of each entry. Re-\ntrieving the most probable parse consists of running a suitably modiﬁed version of\nthe Viterbi algorithm from Chapter 8 over the completed parse table.\n11.2.4\nCKY in Practice\nFinally, we should note that while the restriction to CNF does not pose a prob-\nlem theoretically, it does pose some non-trivial problems in practice. Obviously, as\nthings stand now, our parser isn’t returning trees that are consistent with the grammar\ngiven to us by our friendly syntacticians. In addition to making our grammar devel-\nopers unhappy, the conversion to CNF will complicate any syntax-driven approach\nto semantic analysis.\nOne approach to getting around these problems is to keep enough information\naround to transform our trees back to the original grammar as a post-processing step\nof the parse. This is trivial in the case of the transformation used for rules with length\ngreater than 2. Simply deleting the new dummy non-terminals and promoting their\ndaughters restores the original tree.\nIn the case of unit productions, it turns out to be more convenient to alter the ba-\nsic CKY algorithm to handle them directly than it is to store the information needed\nto recover the correct trees. Exercise 11.3 asks you to make this change. Many of\nthe probabilistic parsers presented in Chapter 12 use the CKY algorithm altered in\njust this manner. Another solution is to adopt a more complex dynamic program-\nming solution that simply accepts arbitrary CFGs. The next section presents such an\napproach.\n11.3\nPartial Parsing\nMany language processing tasks do not require complex, complete parse trees for all\ninputs. For these tasks, a partial parse, or shallow parse, of input sentences may\npartial parse\nshallow parse\nbe sufﬁcient. For example, information extraction systems generally do not extract\nall the possible information from a text: they simply identify and classify the seg-\nments in a text that are likely to contain valuable information. Similarly, information\nretrieval systems may index texts according to a subset of the constituents found in",
  "240": "232\nCHAPTER 11\n•\nSYNTACTIC PARSING\nthem.\nThere are many different approaches to partial parsing.\nSome make use of\ncascades of ﬁnite state transducers to produce tree-like representations. These ap-\nproaches typically produce ﬂatter trees than the ones we’ve been discussing in this\nchapter and the previous one. This ﬂatness arises from the fact that ﬁnite state trans-\nducer approaches generally defer decisions that may require semantic or contex-\ntual factors, such as prepositional phrase attachments, coordination ambiguities, and\nnominal compound analyses. Nevertheless, the intent is to produce parse trees that\nlink all the major constituents in an input.\nAn alternative style of partial parsing is known as chunking. Chunking is the\nchunking\nprocess of identifying and classifying the ﬂat, non-overlapping segments of a sen-\ntence that constitute the basic non-recursive phrases corresponding to the major\ncontent-word parts-of-speech: noun phrases, verb phrases, adjective phrases, and\nprepositional phrases. THe task of ﬁnding all the base noun phrases in a text is\nparticularly common. Since chunked texts lack a hierarchical structure, a simple\nbracketing notation is sufﬁcient to denote the location and the type of the chunks in\na given example:\n(11.4) [NP The morning ﬂight] [PP from] [NP Denver] [VP has arrived.]\nThis bracketing notation makes clear the two fundamental tasks that are involved\nin chunking: segmenting (ﬁnding the non-overlapping extents of the chunks) and\nlabeling (assigning the correct tag to the discovered chunks).\nSome input words may not be part of any chunk, particularly in tasks like base\nNP:\n(11.5) [NP The morning ﬂight] from [NP Denver] has arrived.\nWhat constitutes a syntactic base phrase depends on the application (and whether\nthe phrases come from a treebank). Nevertheless, some standard guidelines are fol-\nlowed in most systems. First and foremost, base phrases of a given type do not\nrecursively contain any constituents of the same type. Eliminating this kind of recur-\nsion leaves us with the problem of determining the boundaries of the non-recursive\nphrases. In most approaches, base phrases include the headword of the phrase, along\nwith any pre-head material within the constituent, while crucially excluding any\npost-head material. Eliminating post-head modiﬁers obviates the need to resolve at-\ntachment ambiguities. This exclusion does lead to certain oddities, such as PPs and\nVPs often consisting solely of their heads. Thus, our earlier example a ﬂight from\nIndianapolis to Houston on NWA is reduced to the following:\n(11.6) [NP a ﬂight] [PP from] [NP Indianapolis][PP to][NP Houston][PP on][NP\nNWA]\n11.3.1\nMachine Learning-Based Approaches to Chunking\nState-of-the-art approaches to chunking use supervised machine learning to train a\nchunker by using annotated data as a training set and training any sequence labeler.\nIt’s common to model chunking as IOB tagging. In IOB tagging we introduce a tag\nIOB\nfor the beginning (B) and inside (I) of each chunk type, and one for tokens outside\n(O) any chunk. The number of tags is thus 2n + 1 tags, where n is the number\nof chunk types. IOB tagging can represent exactly the same information as the\nbracketed notation. The following example shows the bracketing notation of (11.4)\non page 232 reframed as a tagging task:\n(11.7) The\nB NP\nmorning\nI NP\nﬂight\nI NP\nfrom\nB PP\nDenver\nB NP\nhas\nB VP\narrived\nI VP",
  "241": "11.3\n•\nPARTIAL PARSING\n233\nB_NP\nI_NP\n?\n \n    \nThe\nflight\nfrom\nDenver\nhas\narrived\n \nClassifier\nDT\nNN\nNN\nIN\nNNP\nCorresponding feature representation\nThe, DT, B_NP, morning, NN, I_NP, flight, NN, from, IN, Denver, NNP\nLabel\nI_NP\nmorning\nFigure 11.8\nA sequence model for chunking. The chunker slides a context window over the sentence, clas-\nsifying words as it proceeds. At this point, the classiﬁer is attempting to label ﬂight, using features like words,\nembeddings, part-of-speech tags and previously assigned chunk tags.\nThe same sentence with only the base-NPs tagged illustrates the role of the O tags.\n(11.8) The\nB NP\nmorning\nI NP\nﬂight\nI NP\nfrom\nO\nDenver\nB NP\nhas\nO\narrived.\nO\nThere is no explicit encoding of the end of a chunk in IOB tagging; the end of any\nchunk is implicit in any transition from an I or B to a B or O tag. This encoding\nreﬂects the notion that when sequentially labeling words, it is generally easier (at\nleast in English) to detect the beginning of a new chunk than it is to know when a\nchunk has ended.\nSince annotation efforts are expensive and time consuming, chunkers usually\nrely on existing treebanks like the Penn Treebank (Chapter 10), extracting syntactic\nphrases from the full parse constituents of a sentence, ﬁnding the appropriate heads\nand then including the material to the left of the head, ignoring the text to the right.\nThis is somewhat error-prone since it relies on the accuracy of the head-ﬁnding rules\ndescribed in Chapter 10.\nGiven a training set, any sequence model can be used. Figure 11.8 shows an\nillustration of a simple feature-based model, using features like the words and parts-\nof-speech within a 2 word window, and the chunk tags of the preceding inputs in the\nwindow. In training, each training vector would consist of the values of 13 features;\nthe two words to the left of the decision point, their parts-of-speech and chunk tags,\nthe word to be tagged along with its part-of-speech, the two words that follow along\nwith their parts-of speech, and the correct chunk tag, in this case, I NP. During\nclassiﬁcation, the classiﬁer is given the same vector without the answer and assigns\nthe most appropriate tag from its tagset. Viterbi decoding is commonly used.",
  "242": "234\nCHAPTER 11\n•\nSYNTACTIC PARSING\n11.3.2\nChunking-System Evaluations\nAs with the evaluation of part-of-speech taggers, the evaluation of chunkers pro-\nceeds by comparing chunker output with gold-standard answers provided by human\nannotators. However, unlike part-of-speech tagging, word-by-word accuracy mea-\nsures are not appropriate. Instead, chunkers are evaluated according to the notions of\nprecision, recall, and the F-measure borrowed from the ﬁeld of information retrieval.\nPrecision measures the percentage of system-provided chunks that were correct.\nprecision\nCorrect here means that both the boundaries of the chunk and the chunk’s label are\ncorrect. Precision is therefore deﬁned as\nPrecision: = Number of correct chunks given by system\nTotal number of chunks given by system\nRecall measures the percentage of chunks actually present in the input that were\nrecall\ncorrectly identiﬁed by the system. Recall is deﬁned as\nRecall: = Number of correct chunks given by system\nTotal number of actual chunks in the text\nThe F-measure (van Rijsbergen, 1975) provides a way to combine these two\nF-measure\nmeasures into a single metric. The F-measure is deﬁned as\nFβ = (β 2 +1)PR\nβ 2P+R\nThe β parameter differentially weights the importance of recall and precision,\nbased perhaps on the needs of an application. Values of β > 1 favor recall, while\nvalues of β < 1 favor precision. When β = 1, precision and recall are equally bal-\nanced; this is sometimes called Fβ=1 or just F1:\nF1 = 2PR\nP+R\n(11.9)\nF-measure comes from a weighted harmonic mean of precision and recall. The\nharmonic mean of a set of numbers is the reciprocal of the arithmetic mean of recip-\nrocals:\nHarmonicMean(a1,a2,a3,a4,...,an) =\nn\n1\na1 + 1\na2 + 1\na3 +...+ 1\nan\n(11.10)\nand hence F-measure is\nF =\n1\nα 1\nP +(1−α) 1\nR\nor\n\u0012\nwith β 2 = 1−α\nα\n\u0013\nF = (β 2 +1)PR\nβ 2P+R\n(11.11)\n11.4\nSummary\nThe two major ideas introduced in this chapter are those of parsing and partial\nparsing. Here’s a summary of the main points we covered about these ideas:\n• Structural ambiguity is a signiﬁcant problem for parsers. Common sources\nof structural ambiguity include PP-attachment, coordination ambiguity,\nand noun-phrase bracketing ambiguity.\n• Dynamic programming parsing algorithms, such as CKY, use a table of\npartial parses to efﬁciently parse ambiguous sentences.",
  "243": "BIBLIOGRAPHICAL AND HISTORICAL NOTES\n235\n• CKY restricts the form of the grammar to Chomsky normal form (CNF).\n• Many practical problems, including information extraction problems, can be\nsolved without full parsing.\n• Partial parsing and chunking are methods for identifying shallow syntactic\nconstituents in a text.\n• State-of-the-art methods for partial parsing use supervised machine learning\ntechniques.\nBibliographical and Historical Notes\nWriting about the history of compilers, Knuth notes:\nIn this ﬁeld there has been an unusual amount of parallel discovery of\nthe same technique by people working independently.\nWell, perhaps not unusual, since multiple discovery is the norm in science (see\npage ??). But there has certainly been enough parallel publication that this his-\ntory errs on the side of succinctness in giving only a characteristic early mention of\neach algorithm; the interested reader should see Aho and Ullman (1972).\nBottom-up parsing seems to have been ﬁrst described by Yngve (1955), who\ngave a breadth-ﬁrst, bottom-up parsing algorithm as part of an illustration of a ma-\nchine translation procedure. Top-down approaches to parsing and translation were\ndescribed (presumably independently) by at least Glennie (1960), Irons (1961), and\nKuno and Oettinger (1963). Dynamic programming parsing, once again, has a his-\ntory of independent discovery. According to Martin Kay (personal communication),\na dynamic programming parser containing the roots of the CKY algorithm was ﬁrst\nimplemented by John Cocke in 1960. Later work extended and formalized the algo-\nrithm, as well as proving its time complexity (Kay 1967,Younger 1967,Kasami 1965).\nThe related well-formed substring table (WFST) seems to have been indepen-\nWFST\ndently proposed by Kuno (1965) as a data structure that stores the results of all pre-\nvious computations in the course of the parse. Based on a generalization of Cocke’s\nwork, a similar data structure had been independently described in Kay 1967, Kay 1973.\nThe top-down application of dynamic programming to parsing was described in\nEarley’s Ph.D. dissertation (Earley 1968, Earley 1970). Sheil (1976) showed the\nequivalence of the WFST and the Earley algorithm. Norvig (1991) shows that the\nefﬁciency offered by dynamic programming can be captured in any language with a\nmemoization function (such as in LISP) simply by wrapping the memoization oper-\nation around a simple top-down parser.\nWhile parsing via cascades of ﬁnite-state automata had been common in the\nearly history of parsing (Harris, 1962), the focus shifted to full CFG parsing quite\nsoon afterward. Church (1980) argued for a return to ﬁnite-state grammars as a\nprocessing model for natural language understanding; other early ﬁnite-state parsing\nmodels include Ejerhed (1988). Abney (1991) argued for the important practical role\nof shallow parsing.\nThe classic reference for parsing algorithms is Aho and Ullman (1972); although\nthe focus of that book is on computer languages, most of the algorithms have been\napplied to natural language. A good programming languages textbook such as Aho\net al. (1986) is also useful.",
  "244": "236\nCHAPTER 11\n•\nSYNTACTIC PARSING\nExercises\n11.1 Implement the algorithm to convert arbitrary context-free grammars to CNF.\nApply your program to the L1 grammar.\n11.2 Implement the CKY algorithm and test it with your converted L1 grammar.\n11.3 Rewrite the CKY algorithm given in Fig. 11.5 on page 228 so that it can accept\ngrammars that contain unit productions.\n11.4 Discuss the relative advantages and disadvantages of partial versus full pars-\ning.\n11.5 Discuss how to augment a parser to deal with input that may be incorrect, for\nexample, containing spelling errors or mistakes arising from automatic speech\nrecognition.",
  "245": "CHAPTER\n12\nStatistical Parsing\nThe characters in Damon Runyon’s short stories are willing to bet “on any propo-\nsition whatever”, as Runyon says about Sky Masterson in The Idyll of Miss Sarah\nBrown, from the probability of getting aces back-to-back to the odds against a man\nbeing able to throw a peanut from second base to home plate. There is a moral here\nfor language processing: with enough knowledge we can ﬁgure the probability of\njust about anything. The last two chapters have introduced sophisticated models of\nsyntactic structure and its parsing. Here, we show that it is possible to build proba-\nbilistic models of syntactic knowledge and use some of this probabilistic knowledge\nto build efﬁcient probabilistic parsers.\nOne crucial use of probabilistic parsing is to solve the problem of disambigua-\ntion. Recall from Chapter 11 that sentences on average tend to be syntactically\nambiguous because of phenomena like coordination ambiguity and attachment\nambiguity. The CKY parsing algorithm can represent these ambiguities in an efﬁ-\ncient way but is not equipped to resolve them. A probabilistic parser offers a solution\nto the problem: compute the probability of each interpretation and choose the most\nprobable interpretation. Thus, due to the prevalence of ambiguity, most modern\nparsers used for natural language understanding tasks (semantic analysis, summa-\nrization, question-answering, machine translation) are of necessity probabilistic.\nThe most commonly used probabilistic grammar formalism is the probabilistic\ncontext-free grammar (PCFG), a probabilistic augmentation of context-free gram-\nmars in which each rule is associated with a probability. We introduce PCFGs in the\nnext section, showing how they can be trained on Treebank grammars and how they\ncan be parsed. We present the most basic parsing algorithm for PCFGs, which is the\nprobabilistic version of the CKY algorithm that we saw in Chapter 11.\nWe then show a number of ways that we can improve on this basic probability\nmodel (PCFGs trained on Treebank grammars). One method of improving a trained\nTreebank grammar is to change the names of the non-terminals. By making the\nnon-terminals sometimes more speciﬁc and sometimes more general, we can come\nup with a grammar with a better probability model that leads to improved parsing\nscores. Another augmentation of the PCFG works by adding more sophisticated\nconditioning factors, extending PCFGs to handle probabilistic subcategorization\ninformation and probabilistic lexical dependencies.\nHeavily lexicalized grammar formalisms such as Lexical-Functional Grammar\n(LFG) (Bresnan, 1982), Head-Driven Phrase Structure Grammar (HPSG) (Pollard\nand Sag, 1994), Tree-Adjoining Grammar (TAG) (Joshi, 1985), and Combinatory\nCategorial Grammar (CCG) pose additional problems for probabilistic parsers. Sec-\ntion 12.7 introduces the task of supertagging and the use of heuristic search methods\nbased on the A* algorithm in the context of CCG parsing.\nFinally, we describe the standard techniques and metrics for evaluating parsers\nand discuss some relevant psychological results on human parsing.",
  "246": "238\nCHAPTER 12\n•\nSTATISTICAL PARSING\n12.1\nProbabilistic Context-Free Grammars\nThe simplest augmentation of the context-free grammar is the Probabilistic Context-\nFree Grammar (PCFG), also known as the Stochastic Context-Free Grammar\nPCFG\n(SCFG), ﬁrst proposed by Booth (1969). Recall that a context-free grammar G is\nSCFG\ndeﬁned by four parameters (N, Σ, R, S); a probabilistic context-free grammar is also\ndeﬁned by four parameters, with a slight augmentation to each of the rules in R:\nN a set of non-terminal symbols (or variables)\nΣ a set of terminal symbols (disjoint from N)\nR a set of rules or productions, each of the form A →β [p],\nwhere A is a non-terminal,\nβ is a string of symbols from the inﬁnite set of strings (Σ∪N)∗,\nand p is a number between 0 and 1 expressing P(β|A)\nS\na designated start symbol\nThat is, a PCFG differs from a standard CFG by augmenting each rule in R with\na conditional probability:\nA →β [p]\n(12.1)\nHere p expresses the probability that the given non-terminal A will be expanded\nto the sequence β. That is, p is the conditional probability of a given expansion β\ngiven the left-hand-side (LHS) non-terminal A. We can represent this probability as\nP(A →β)\nor as\nP(A →β|A)\nor as\nP(RHS|LHS)\nThus, if we consider all the possible expansions of a non-terminal, the sum of their\nprobabilities must be 1:\nX\nβ\nP(A →β) = 1\nFigure 12.1 shows a PCFG: a probabilistic augmentation of the L1 miniature En-\nglish CFG grammar and lexicon. Note that the probabilities of all of the expansions\nof each non-terminal sum to 1. Also note that these probabilities were made up\nfor pedagogical purposes. A real grammar has a great many more rules for each\nnon-terminal; hence, the probabilities of any particular rule would tend to be much\nsmaller.\nA PCFG is said to be consistent if the sum of the probabilities of all sentences\nconsistent\nin the language equals 1. Certain kinds of recursive rules cause a grammar to be\ninconsistent by causing inﬁnitely looping derivations for some sentences. For ex-\nample, a rule S →S with probability 1 would lead to lost probability mass due to\nderivations that never terminate. See Booth and Thompson (1973) for more details\non consistent and inconsistent grammars.",
  "247": "12.1\n•\nPROBABILISTIC CONTEXT-FREE GRAMMARS\n239\nGrammar\nLexicon\nS →NP VP\n[.80]\nDet →that [.10] | a [.30] | the [.60]\nS →Aux NP VP\n[.15]\nNoun →book [.10] | ﬂight [.30]\nS →VP\n[.05]\n| meal [.015] | money [.05]\nNP →Pronoun\n[.35]\n| ﬂight [.40] | dinner [.10]\nNP →Proper-Noun\n[.30]\nVerb →book [.30] | include [.30]\nNP →Det Nominal\n[.20]\n| prefer [.40]\nNP →Nominal\n[.15]\nPronoun →I [.40] | she [.05]\nNominal →Noun\n[.75]\n| me [.15] | you [.40]\nNominal →Nominal Noun [.20]\nProper-Noun →Houston [.60]\nNominal →Nominal PP\n[.05]\n| NWA [.40]\nVP →Verb\n[.35]\nAux →does [.60] | can [40]\nVP →Verb NP\n[.20]\nPreposition →from [.30] | to [.30]\nVP →Verb NP PP\n[.10]\n| on [.20] | near [.15]\nVP →Verb PP\n[.15]\n| through [.05]\nVP →Verb NP NP\n[.05]\nVP →VP PP\n[.15]\nPP →Preposition NP\n[1.0]\nFigure 12.1\nA PCFG that is a probabilistic augmentation of the L1 miniature English CFG\ngrammar and lexicon of Fig. 11.1. These probabilities were made up for pedagogical purposes\nand are not based on a corpus (since any real corpus would have many more rules, so the true\nprobabilities of each rule would be much smaller).\nHow are PCFGs used? A PCFG can be used to estimate a number of useful\nprobabilities concerning a sentence and its parse tree(s), including the probability of\na particular parse tree (useful in disambiguation) and the probability of a sentence\nor a piece of a sentence (useful in language modeling). Let’s see how this works.\n12.1.1\nPCFGs for Disambiguation\nA PCFG assigns a probability to each parse tree T (i.e., each derivation) of a sen-\ntence S. This attribute is useful in disambiguation. For example, consider the two\nparses of the sentence “Book the dinner ﬂight” shown in Fig. 12.2. The sensible\nparse on the left means “Book a ﬂight that serves dinner”. The nonsensical parse\non the right, however, would have to mean something like “Book a ﬂight on behalf\nof ‘the dinner”’ just as a structurally similar sentence like “Can you book John a\nﬂight?” means something like “Can you book a ﬂight on behalf of John?”\nThe probability of a particular parse T is deﬁned as the product of the probabil-\nities of all the n rules used to expand each of the n non-terminal nodes in the parse\ntree T, where each rule i can be expressed as LHSi →RHSi:\nP(T,S) =\nn\nY\ni=1\nP(RHSi|LHSi)\n(12.2)\nThe resulting probability P(T,S) is both the joint probability of the parse and the\nsentence and also the probability of the parse P(T). How can this be true? First, by\nthe deﬁnition of joint probability:\nP(T,S) = P(T)P(S|T)\n(12.3)",
  "248": "240\nCHAPTER 12\n•\nSTATISTICAL PARSING\nBut since a parse tree includes all the words of the sentence, P(S|T) is 1. Thus,\nP(T,S) = P(T)P(S|T) = P(T)\n(12.4)\nS\nVP\nNP\nNominal\nNoun\nﬂight\nNominal\nNoun\ndinner\nDet\nthe\nVerb\nBook\nS\nVP\nNP\nNominal\nNoun\nﬂight\nNP\nNominal\nNoun\ndinner\nDet\nthe\nVerb\nBook\nRules\nP\nRules\nP\nS\n→VP\n.05\nS\n→VP\n.05\nVP\n→Verb NP\n.20\nVP\n→Verb NP NP\n.10\nNP\n→Det Nominal\n.20\nNP\n→Det Nominal .20\nNominal →Nominal Noun .20\nNP\n→Nominal\n.15\nNominal →Noun\n.75\nNominal →Noun\n.75\nNominal →Noun\n.75\nVerb\n→book\n.30\nVerb\n→book\n.30\nDet\n→the\n.60\nDet\n→the\n.60\nNoun\n→dinner\n.10\nNoun\n→dinner\n.10\nNoun\n→ﬂight\n.40\nNoun\n→ﬂight\n.40\nFigure 12.2\nTwo parse trees for an ambiguous sentence. The parse on the left corresponds\nto the sensible meaning “Book a ﬂight that serves dinner”, while the parse on the right corre-\nsponds to the nonsensical meaning “Book a ﬂight on behalf of ‘the dinner’ ”.\nWe can compute the probability of each of the trees in Fig. 12.2 by multiplying\nthe probabilities of each of the rules used in the derivation. For example, the proba-\nbility of the left tree in Fig. 12.2a (call it Tleft) and the right tree (Fig. 12.2b or Tright)\ncan be computed as follows:\nP(Tle ft) = .05∗.20∗.20∗.20∗.75∗.30∗.60∗.10∗.40 = 2.2×10−6\nP(Tright) = .05∗.10∗.20∗.15∗.75∗.75∗.30∗.60∗.10∗.40 = 6.1×10−7\nWe can see that the left tree in Fig. 12.2 has a much higher probability than the\ntree on the right. Thus, this parse would correctly be chosen by a disambiguation\nalgorithm that selects the parse with the highest PCFG probability.\nLet’s formalize this intuition that picking the parse with the highest probability\nis the correct way to do disambiguation. Consider all the possible parse trees for a\ngiven sentence S. The string of words S is called the yield of any parse tree over S.\nyield",
  "249": "12.1\n•\nPROBABILISTIC CONTEXT-FREE GRAMMARS\n241\nThus, out of all parse trees with a yield of S, the disambiguation algorithm picks the\nparse tree that is most probable given S:\nˆT(S) =\nargmax\nTs.t.S=yield(T)\nP(T|S)\n(12.5)\nBy deﬁnition, the probability P(T|S) can be rewritten as P(T,S)/P(S), thus lead-\ning to\nˆT(S) =\nargmax\nTs.t.S=yield(T)\nP(T,S)\nP(S)\n(12.6)\nSince we are maximizing over all parse trees for the same sentence, P(S) will be\na constant for each tree, so we can eliminate it:\nˆT(S) =\nargmax\nTs.t.S=yield(T)\nP(T,S)\n(12.7)\nFurthermore, since we showed above that P(T,S) = P(T), the ﬁnal equation\nfor choosing the most likely parse neatly simpliﬁes to choosing the parse with the\nhighest probability:\nˆT(S) =\nargmax\nTs.t.S=yield(T)\nP(T)\n(12.8)\n12.1.2\nPCFGs for Language Modeling\nA second attribute of a PCFG is that it assigns a probability to the string of words\nconstituting a sentence. This is important in language modeling, whether for use\nin speech recognition, machine translation, spelling correction, augmentative com-\nmunication, or other applications. The probability of an unambiguous sentence is\nP(T,S) = P(T) or just the probability of the single parse tree for that sentence. The\nprobability of an ambiguous sentence is the sum of the probabilities of all the parse\ntrees for the sentence:\nP(S) =\nX\nTs.t.S=yield(T)\nP(T,S)\n(12.9)\n=\nX\nTs.t.S=yield(T)\nP(T)\n(12.10)\nAn additional feature of PCFGs that is useful for language modeling is their\nability to assign a probability to substrings of a sentence. For example, suppose we\nwant to know the probability of the next word wi in a sentence given all the words\nwe’ve seen so far w1,...,wi−1. The general formula for this is\nP(wi|w1,w2,...,wi−1) = P(w1,w2,...,wi−1,wi)\nP(w1,w2,...,wi−1)\n(12.11)\nWe saw in Chapter 3 a simple approximation of this probability using N-grams,\nconditioning on only the last word or two instead of the entire context; thus, the\nbigram approximation would give us\nP(wi|w1,w2,...,wi−1) ≈P(wi−1,wi)\nP(wi−1)\n(12.12)",
  "250": "242\nCHAPTER 12\n•\nSTATISTICAL PARSING\nBut the fact that the N-gram model can only make use of a couple words of\ncontext means it is ignoring potentially useful prediction cues. Consider predicting\nthe word after in the following sentence from Chelba and Jelinek (2000):\n(12.13) the contract ended with a loss of 7 cents after trading as low as 9 cents\nA trigram grammar must predict after from the words 7 cents, while it seems clear\nthat the verb ended and the subject contract would be useful predictors that a PCFG-\nbased parser could help us make use of. Indeed, it turns out that PCFGs allow us to\ncondition on the entire previous context w1,w2,...,wi−1 shown in Eq. 12.11.\nIn summary, this section and the previous one have shown that PCFGs can be\napplied both to disambiguation in syntactic parsing and to word prediction in lan-\nguage modeling. Both of these applications require that we be able to compute the\nprobability of parse tree T for a given sentence S. The next few sections introduce\nsome algorithms for computing this probability.\n12.2\nProbabilistic CKY Parsing of PCFGs\nThe parsing problem for PCFGs is to produce the most-likely parse ˆT for a given\nsentence S, that is,\nˆT(S) =\nargmax\nTs.t.S=yield(T)\nP(T)\n(12.14)\nThe algorithms for computing the most likely parse are simple extensions of the\nstandard algorithms for parsing; most modern probabilistic parsers are based on the\nprobabilistic CKY algorithm, ﬁrst described by Ney (1991).\nprobabilistic\nCKY\nAs with the CKY algorithm, we assume for the probabilistic CKY algorithm that\nthe PCFG is in Chomsky normal form. Recall from page 213 that grammars in CNF\nare restricted to rules of the form A →B C, or A →w. That is, the right-hand side\nof each rule must expand to either two non-terminals or to a single terminal.\nFor the CKY algorithm, we represented each sentence as having indices between\nthe words. Thus, an example sentence like\n(12.15) Book the ﬂight through Houston.\nwould assume the following indices between each word:\n(12.16)\n0⃝Book\n1⃝the\n2⃝ﬂight\n3⃝through\n4⃝Houston\n5⃝\nUsing these indices, each constituent in the CKY parse tree is encoded in a\ntwo-dimensional matrix. Speciﬁcally, for a sentence of length n and a grammar\nthat contains V non-terminals, we use the upper-triangular portion of an (n + 1) ×\n(n + 1) matrix. For CKY, each cell table[i, j] contained a list of constituents that\ncould span the sequence of words from i to j. For probabilistic CKY, it’s slightly\nsimpler to think of the constituents in each cell as constituting a third dimension of\nmaximum length V. This third dimension corresponds to each non-terminal that can\nbe placed in this cell, and the value of the cell is then a probability for that non-\nterminal/constituent rather than a list of constituents. In summary, each cell [i, j,A]\nin this (n+1)×(n+1)×V matrix is the probability of a constituent of type A that\nspans positions i through j of the input.\nFigure 12.3 gives pseudocode for this probabilistic CKY algorithm, extending\nthe basic CKY algorithm from Fig. 11.5.",
  "251": "12.3\n•\nWAYS TO LEARN PCFG RULE PROBABILITIES\n243\nfunction PROBABILISTIC-CKY(words,grammar) returns most probable parse\nand its probability\nfor j←from 1 to LENGTH(words) do\nfor all { A | A →words[j] ∈grammar}\ntable[j −1, j,A]←P(A →words[j])\nfor i←from j −2 downto 0 do\nfor k←i+1 to j −1 do\nfor all { A | A →BC ∈grammar,\nand table[i,k,B] > 0 and table[k, j,C] > 0 }\nif (table[i,j,A] < P(A →BC) × table[i,k,B] × table[k,j,C]) then\ntable[i,j,A]←P(A →BC) × table[i,k,B] × table[k,j,C]\nback[i,j,A]←{k,B,C}\nreturn BUILD TREE(back[1, LENGTH(words), S]), table[1, LENGTH(words), S]\nFigure 12.3\nThe probabilistic CKY algorithm for ﬁnding the maximum probability parse\nof a string of num words words given a PCFG grammar with num rules rules in Chomsky\nnormal form. back is an array of backpointers used to recover the best parse. The build tree\nfunction is left as an exercise to the reader.\nLike the basic CKY algorithm, the probabilistic CKY algorithm as shown in\nFig. 12.3 requires a grammar in Chomsky normal form. Converting a probabilistic\ngrammar to CNF requires that we also modify the probabilities so that the probability\nof each parse remains the same under the new CNF grammar. Exercise 12.2 asks\nyou to modify the algorithm for conversion to CNF in Chapter 11 so that it correctly\nhandles rule probabilities.\nIn practice, a generalized CKY algorithm that handles unit productions directly\nis typically used. Recall that Exercise 13.3 asked you to make this change in CKY;\nExercise 12.3 asks you to extend this change to probabilistic CKY.\nLet’s see an example of the probabilistic CKY chart, using the following mini-\ngrammar, which is already in CNF:\nS\n→NP VP\n.80\nDet\n→the\n.40\nNP\n→Det N\n.30\nDet\n→a\n.40\nVP\n→V NP\n.20\nN\n→meal\n.01\nV\n→includes .05\nN\n→flight .02\nGiven this grammar, Fig. 12.4 shows the ﬁrst steps in the probabilistic CKY\nparse of the following example:\n(12.17) The ﬂight includes a meal\n12.3\nWays to Learn PCFG Rule Probabilities\nWhere do PCFG rule probabilities come from? There are two ways to learn proba-\nbilities for the rules of a grammar. The simplest way is to use a treebank, a corpus\nof already parsed sentences. Recall that we introduced in Chapter 10 the idea of\ntreebanks and the commonly used Penn Treebank (Marcus et al., 1993), a collec-\ntion of parse trees in English, Chinese, and other languages that is distributed by the\nLinguistic Data Consortium. Given a treebank, we can compute the probability of\neach expansion of a non-terminal by counting the number of times that expansion",
  "252": "244\nCHAPTER 12\n•\nSTATISTICAL PARSING\nThe\nflight\n[0,1]\n[0,2]\n[0,3]\n[1,2]\n[1,3]\n[2,3]\nDet: .40\nincludes\na\nmeal\n[3,4]\n[4,5]\nN: .02\nV: .05\nNP: .30 *.40 *.02\n= .0024\n[0,4]\n[1,4]\n[2,4]\n[3,5]\n[2,5]\n[1,5]\n[0,5]\nDet: .40\nN: .01\nFigure 12.4\nThe beginning of the probabilistic CKY matrix. Filling out the rest of the chart\nis left as Exercise 12.4 for the reader.\noccurs and then normalizing.\nP(α →β|α) =\nCount(α →β)\nP\nγ Count(α →γ) = Count(α →β)\nCount(α)\n(12.18)\nIf we don’t have a treebank but we do have a (non-probabilistic) parser, we can\ngenerate the counts we need for computing PCFG rule probabilities by ﬁrst parsing\na corpus of sentences with the parser. If sentences were unambiguous, it would be\nas simple as this: parse the corpus, increment a counter for every rule in the parse,\nand then normalize to get probabilities.\nBut wait! Since most sentences are ambiguous, that is, have multiple parses, we\ndon’t know which parse to count the rules in. Instead, we need to keep a separate\ncount for each parse of a sentence and weight each of these partial counts by the\nprobability of the parse it appears in. But to get these parse probabilities to weight\nthe rules, we need to already have a probabilistic parser.\nThe intuition for solving this chicken-and-egg problem is to incrementally im-\nprove our estimates by beginning with a parser with equal rule probabilities, then\nparse the sentence, compute a probability for each parse, use these probabilities to",
  "253": "12.4\n•\nPROBLEMS WITH PCFGS\n245\nweight the counts, re-estimate the rule probabilities, and so on, until our proba-\nbilities converge. The standard algorithm for computing this solution is called the\ninside-outside algorithm; it was proposed by Baker (1979) as a generalization of the\ninside-outside\nforward-backward algorithm for HMMs. Like forward-backward, inside-outside is\na special case of the Expectation Maximization (EM) algorithm, and hence has two\nsteps: the expectation step, and the maximization step. See Lari and Young (1990)\nexpectation\nstep\nmaximization\nstep\nor Manning and Sch¨utze (1999) for a complete description of the algorithm.\nThis use of the inside-outside algorithm to estimate the rule probabilities for\na grammar is actually a kind of limited use of inside-outside. The inside-outside\nalgorithm can actually be used not only to set the rule probabilities but even to induce\nthe grammar rules themselves. It turns out, however, that grammar induction is so\ndifﬁcult that inside-outside by itself is not a very successful grammar inducer; see\nthe Historical Notes at the end of the chapter for pointers to other grammar induction\nalgorithms.\n12.4\nProblems with PCFGs\nWhile probabilistic context-free grammars are a natural extension to context-free\ngrammars, they have two main problems as probability estimators:\nPoor independence assumptions: CFG rules impose an independence assumption\non probabilities, resulting in poor modeling of structural dependencies across\nthe parse tree.\nLack of lexical conditioning: CFG rules don’t model syntactic facts about speciﬁc\nwords, leading to problems with subcategorization ambiguities, preposition\nattachment, and coordinate structure ambiguities.\nBecause of these problems, most current probabilistic parsing models use some\naugmented version of PCFGs, or modify the Treebank-based grammar in some way.\nIn the next few sections after discussing the problems in more detail we introduce\nsome of these augmentations.\n12.4.1\nIndependence Assumptions Miss Structural Dependencies\nBetween Rules\nLet’s look at these problems in more detail. Recall that in a CFG the expansion of a\nnon-terminal is independent of the context, that is, of the other nearby non-terminals\nin the parse tree. Similarly, in a PCFG, the probability of a particular rule like\nNP →Det N is also independent of the rest of the tree. By deﬁnition, the probability\nof a group of independent events is the product of their probabilities. These two facts\nexplain why in a PCFG we compute the probability of a tree by just multiplying the\nprobabilities of each non-terminal expansion.\nUnfortunately, this CFG independence assumption results in poor probability\nestimates. This is because in English the choice of how a node expands can after all\ndepend on the location of the node in the parse tree. For example, in English it turns\nout that NPs that are syntactic subjects are far more likely to be pronouns, and NPs\nthat are syntactic objects are far more likely to be non-pronominal (e.g., a proper\nnoun or a determiner noun sequence), as shown by these statistics for NPs in the",
  "254": "246\nCHAPTER 12\n•\nSTATISTICAL PARSING\nSwitchboard corpus (Francis et al., 1999):1\nPronoun Non-Pronoun\nSubject 91%\n9%\nObject\n34%\n66%\nUnfortunately, there is no way to represent this contextual difference in the prob-\nabilities in a PCFG. Consider two expansions of the non-terminal NP as a pronoun\nor as a determiner+noun. How shall we set the probabilities of these two rules? If\nwe set their probabilities to their overall probability in the Switchboard corpus, the\ntwo rules have about equal probability.\nNP →DT NN .28\nNP →PRP\n.25\nBecause PCFGs don’t allow a rule probability to be conditioned on surrounding\ncontext, this equal probability is all we get; there is no way to capture the fact that in\nsubject position, the probability for NP →PRP should go up to .91, while in object\nposition, the probability for NP →DT NN should go up to .66.\nThese dependencies could be captured if the probability of expanding an NP as\na pronoun (e.g., NP →PRP) versus a lexical NP (e.g., NP →DT NN) were condi-\ntioned on whether the NP was a subject or an object. Section 12.5 introduces the\ntechnique of parent annotation for adding this kind of conditioning.\n12.4.2\nLack of Sensitivity to Lexical Dependencies\nA second class of problems with PCFGs is their lack of sensitivity to the words in\nthe parse tree. Words do play a role in PCFGs since the parse probability includes\nthe probability of a word given a part-of-speech (i.e., from rules like V →sleep,\nNN →book, etc.).\nBut it turns out that lexical information is useful in other places in the grammar,\nsuch as in resolving prepositional phrase (PP) attachment ambiguities. Since prepo-\nsitional phrases in English can modify a noun phrase or a verb phrase, when a parser\nﬁnds a prepositional phrase, it must decide where to attach it into the tree. Consider\nthe following example:\n(12.19) Workers dumped sacks into a bin.\nFigure 12.5 shows two possible parse trees for this sentence; the one on the left is\nthe correct parse; Fig. 12.6 shows another perspective on the preposition attachment\nproblem, demonstrating that resolving the ambiguity in Fig. 12.5 is equivalent to\ndeciding whether to attach the prepositional phrase into the rest of the tree at the\nNP or VP nodes; we say that the correct parse requires VP attachment, and the\nVP attachment\nincorrect parse implies NP attachment.\nNP attachment\nWhy doesn’t a PCFG already deal with PP attachment ambiguities? Note that\nthe two parse trees in Fig. 12.5 have almost exactly the same rules; they differ only\nin that the left-hand parse has this rule:\nVP →VBD NP PP\n1\nDistribution of subjects from 31,021 declarative sentences; distribution of objects from 7,489 sen-\ntences. This tendency is caused by the use of subject position to realize the topic or old information\nin a sentence (Giv´on, 1990). Pronouns are a way to talk about old information, while non-pronominal\n(“lexical”) noun-phrases are often used to introduce new referents. We talk more about new and old\ninformation in Chapter 21.",
  "255": "12.4\n•\nPROBLEMS WITH PCFGS\n247\nS\nVP\nPP\nNP\nNN\nbin\nDT\na\nP\ninto\nNP\nNNS\nsacks\nVBD\ndumped\nNP\nNNS\nworkers\nS\nVP\nNP\nPP\nNP\nNN\nbin\nDT\na\nP\ninto\nNP\nNNS\nsacks\nVBD\ndumped\nNP\nNNS\nworkers\nFigure 12.5\nTwo possible parse trees for a prepositional phrase attachment ambiguity. The left parse is\nthe sensible one, in which “into a bin” describes the resulting location of the sacks. In the right incorrect parse,\nthe sacks to be dumped are the ones which are already “into a bin”, whatever that might mean.\nS\nVP\nNP\nNNS\nsacks\nVBD\ndumped\nNP\nNNS\nworkers\nPP\nNP\nNN\nbin\nDT\na\nP\ninto\nFigure 12.6\nAnother view of the preposition attachment problem. Should the PP on the right attach to the\nVP or NP nodes of the partial parse tree on the left?\nwhile the right-hand parse has these:\nVP →VBD NP\nNP →NP PP\nDepending on how these probabilities are set, a PCFG will always either prefer\nNP attachment or VP attachment. As it happens, NP attachment is slightly more\ncommon in English, so if we trained these rule probabilities on a corpus, we might\nalways prefer NP attachment, causing us to misparse this sentence.\nBut suppose we set the probabilities to prefer the VP attachment for this sen-\ntence. Now we would misparse the following sentence, which requires NP attach-\nment:\n(12.20) ﬁshermen caught tons of herring",
  "256": "248\nCHAPTER 12\n•\nSTATISTICAL PARSING\nWhat information in the input sentence lets us know that (12.20) requires NP\nattachment while (12.19) requires VP attachment?\nIt should be clear that these preferences come from the identities of the verbs,\nnouns, and prepositions. It seems that the afﬁnity between the verb dumped and the\npreposition into is greater than the afﬁnity between the noun sacks and the preposi-\ntion into, thus leading to VP attachment. On the other hand, in (12.20) the afﬁnity\nbetween tons and of is greater than that between caught and of, leading to NP attach-\nment.\nThus, to get the correct parse for these kinds of examples, we need a model that\nsomehow augments the PCFG probabilities to deal with these lexical dependency\nlexical\ndependency\nstatistics for different verbs and prepositions.\nCoordination ambiguities are another case in which lexical dependencies are\nthe key to choosing the proper parse. Figure 12.7 shows an example from Collins\n(1999) with two parses for the phrase dogs in houses and cats. Because dogs is\nsemantically a better conjunct for cats than houses (and because most dogs can’t ﬁt\ninside cats), the parse [dogs in [NP houses and cats]] is intuitively unnatural and\nshould be dispreferred. The two parses in Fig. 12.7, however, have exactly the same\nPCFG rules, and thus a PCFG will assign them the same probability.\nNP\nNP\nNoun\ncats\nConj\nand\nNP\nPP\nNP\nNoun\nhouses\nPrep\nin\nNP\nNoun\ndogs\nNP\nPP\nNP\nNP\nNoun\ncats\nConj\nand\nNP\nNoun\nhouses\nPrep\nin\nNP\nNoun\ndogs\nFigure 12.7\nAn instance of coordination ambiguity. Although the left structure is intu-\nitively the correct one, a PCFG will assign them identical probabilities since both structures\nuse exactly the same set of rules. After Collins (1999).\nIn summary, we have shown in this section and the previous one that probabilistic\ncontext-free grammars are incapable of modeling important structural and lexical\ndependencies. In the next two sections we sketch current methods for augmenting\nPCFGs to deal with both these issues.\n12.5\nImproving PCFGs by Splitting Non-Terminals\nLet’s start with the ﬁrst of the two problems with PCFGs mentioned above: their\ninability to model structural dependencies, like the fact that NPs in subject position\ntend to be pronouns, whereas NPs in object position tend to have full lexical (non-\npronominal) form. How could we augment a PCFG to correctly model this fact?\nOne idea would be to split the NP non-terminal into two versions: one for sub-\nsplit",
  "257": "12.5\n•\nIMPROVING PCFGS BY SPLITTING NON-TERMINALS\n249\njects, one for objects. Having two nodes (e.g., NPsubject and NPobject) would allow\nus to correctly model their different distributional properties, since we would have\ndifferent probabilities for the rule NPsubject →PRP and the rule NPobject →PRP.\nOne way to implement this intuition of splits is to do parent annotation (John-\nparent\nannotation\nson, 1998), in which we annotate each node with its parent in the parse tree. Thus,\nan NP node that is the subject of the sentence and hence has parent S would be anno-\ntated NPˆS, while a direct object NP whose parent is VP would be annotated NPˆVP.\nFigure 12.8 shows an example of a tree produced by a grammar that parent-annotates\nthe phrasal non-terminals (like NP and VP).\na)\nS\nVP\nNP\nNN\nﬂight\nDT\na\nVBD\nneed\nNP\nPRP\nI\nb)\nS\nVPˆS\nNPˆVP\nNN\nﬂight\nDT\na\nVBD\nneed\nNPˆS\nPRP\nI\nFigure 12.8\nA standard PCFG parse tree (a) and one which has parent annotation on the\nnodes which aren’t pre-terminal (b). All the non-terminal nodes (except the pre-terminal\npart-of-speech nodes) in parse (b) have been annotated with the identity of their parent.\nIn addition to splitting these phrasal nodes, we can also improve a PCFG by\nsplitting the pre-terminal part-of-speech nodes (Klein and Manning, 2003b). For ex-\nample, different kinds of adverbs (RB) tend to occur in different syntactic positions:\nthe most common adverbs with ADVP parents are also and now, with VP parents\nn’t and not, and with NP parents only and just. Thus, adding tags like RBˆADVP,\nRBˆVP, and RBˆNP can be useful in improving PCFG modeling.\nSimilarly, the Penn Treebank tag IN can mark a wide variety of parts-of-speech,\nincluding subordinating conjunctions (while, as, if), complementizers (that, for), and\nprepositions (of, in, from). Some of these differences can be captured by parent an-\nnotation (subordinating conjunctions occur under S, prepositions under PP), while\nothers require speciﬁcally splitting the pre-terminal nodes. Figure 12.9 shows an ex-\nample from Klein and Manning (2003b) in which even a parent-annotated grammar\nincorrectly parses works as a noun in to see if advertising works. Splitting pre-\nterminals to allow if to prefer a sentential complement results in the correct verbal\nparse.\nTo deal with cases in which parent annotation is insufﬁcient, we can also hand-\nwrite rules that specify a particular node split based on other features of the tree. For\nexample, to distinguish between complementizer IN and subordinating conjunction\nIN, both of which can have the same parent, we could write rules conditioned on\nother aspects of the tree such as the lexical identity (the lexeme that is likely to be a\ncomplementizer, as a subordinating conjunction).\nNode-splitting is not without problems; it increases the size of the grammar and\nhence reduces the amount of training data available for each grammar rule, leading\nto overﬁtting. Thus, it is important to split to just the correct level of granularity for a\nparticular training set. While early models employed hand-written rules to try to ﬁnd\nan optimal number of non-terminals (Klein and Manning, 2003b), modern models",
  "258": "250\nCHAPTER 12\n•\nSTATISTICAL PARSING\nautomatically search for the optimal splits. The split and merge algorithm of Petrov\nsplit and merge\net al. (2006), for example, starts with a simple X-bar grammar, alternately splits the\nnon-terminals, and merges non-terminals, ﬁnding the set of annotated nodes that\nmaximizes the likelihood of the training set treebank. As of the time of this writing,\nthe performance of the Petrov et al. (2006) algorithm was the best of any known\nparsing algorithm on the Penn Treebank.\n12.6\nProbabilistic Lexicalized CFGs\nThe previous section showed that a simple probabilistic CKY algorithm for pars-\ning raw PCFGs can achieve extremely high parsing accuracy if the grammar rule\nsymbols are redesigned by automatic splits and merges.\nIn this section, we discuss an alternative family of models in which instead of\nmodifying the grammar rules, we modify the probabilistic model of the parser to\nallow for lexicalized rules. The resulting family of lexicalized parsers includes the\nwell-known Collins parser (Collins, 1999) and Charniak parser (Charniak, 1997),\nCollins parser\nCharniak\nparser\nboth of which are publicly available and widely used throughout natural language\nprocessing.\nWe saw in Section 10.4.3 that syntactic constituents could be associated with a\nlexical head, and we deﬁned a lexicalized grammar in which each non-terminal\nlexicalized\ngrammar\nin the tree is annotated with its lexical head, where a rule like VP →VBD NP PP\nwould be extended as\nVP(dumped) →VBD(dumped) NP(sacks) PP(into)\n(12.21)\nIn the standard type of lexicalized grammar, we actually make a further exten-\nsion, which is to associate the head tag, the part-of-speech tags of the headwords,\nhead tag\nwith the non-terminal symbols as well. Each rule is thus lexicalized by both the\nVPˆS\nVPˆVP\nPPˆVP\nNPˆPP\nNNS\nworks\nNN\nadvertising\nIN\nif\nVB\nsee\nTO\nto\nVPˆS\nVPˆVP\nSBARˆVP\nSˆSBAR\nVPˆS\nVBZˆVP\nworks\nNPˆS\nNNˆNP\nadvertising\nINˆSBAR\nif\nVBˆVP\nsee\nTOˆVP\nto\nFigure 12.9\nAn incorrect parse even with a parent-annotated parse (left). The correct parse (right), was\nproduced by a grammar in which the pre-terminal nodes have been split, allowing the probabilistic grammar to\ncapture the fact that if prefers sentential complements. Adapted from Klein and Manning (2003b).",
  "259": "12.6\n•\nPROBABILISTIC LEXICALIZED CFGS\n251\nheadword and the head tag of each constituent resulting in a format for lexicalized\nrules like\nVP(dumped,VBD) →VBD(dumped,VBD) NP(sacks,NNS) PP(into,P)\n(12.22)\nWe show a lexicalized parse tree with head tags in Fig. 12.10, extended from Fig. 10.11.\nTOP\nS(dumped,VBD)\nVP(dumped,VBD)\nPP(into,P)\nNP(bin,NN)\nNN(bin,NN)\nbin\nDT(a,DT)\na\nP(into,P)\ninto\nNP(sacks,NNS)\nNNS(sacks,NNS)\nsacks\nVBD(dumped,VBD)\ndumped\nNP(workers,NNS)\nNNS(workers,NNS)\nworkers\nInternal Rules\nLexical Rules\nTOP\n→S(dumped,VBD)\nNNS(workers,NNS)\n→workers\nS(dumped,VBD)\n→NP(workers,NNS)\nVP(dumped,VBD)\nVBD(dumped,VBD) →dumped\nNP(workers,NNS)\n→NNS(workers,NNS)\nNNS(sacks,NNS)\n→sacks\nVP(dumped,VBD) →VBD(dumped, VBD) NP(sacks,NNS) PP(into,P) P(into,P)\n→into\nPP(into,P)\n→P(into,P)\nNP(bin,NN)\nDT(a,DT)\n→a\nNP(bin,NN)\n→DT(a,DT)\nNN(bin,NN)\nNN(bin,NN)\n→bin\nFigure 12.10\nA lexicalized tree, including head tags, for a WSJ sentence, adapted from Collins (1999). Below\nwe show the PCFG rules that would be needed for this parse tree, internal rules on the left, and lexical rules on\nthe right.\nTo generate such a lexicalized tree, each PCFG rule must be augmented to iden-\ntify one right-hand constituent to be the head daughter. The headword for a node is\nthen set to the headword of its head daughter, and the head tag to the part-of-speech\ntag of the headword. Recall that we gave in Fig. 10.12 a set of hand-written rules for\nidentifying the heads of particular constituents.\nA natural way to think of a lexicalized grammar is as a parent annotation, that\nis, as a simple context-free grammar with many copies of each rule, one copy for\neach possible headword/head tag for each constituent. Thinking of a probabilistic\nlexicalized CFG in this way would lead to the set of simple PCFG rules shown below\nthe tree in Fig. 12.10.\nNote that Fig. 12.10 shows two kinds of rules: lexical rules, which express\nlexical rules\nthe expansion of a pre-terminal to a word, and internal rules, which express the\ninternal rules\nother rule expansions. We need to distinguish these kinds of rules in a lexicalized\ngrammar because they are associated with very different kinds of probabilities. The\nlexical rules are deterministic, that is, they have probability 1.0 since a lexicalized\npre-terminal like NN(bin,NN) can only expand to the word bin. But for the internal\nrules, we need to estimate probabilities.",
  "260": "252\nCHAPTER 12\n•\nSTATISTICAL PARSING\nSuppose we were to treat a probabilistic lexicalized CFG like a really big CFG\nthat just happened to have lots of very complex non-terminals and estimate the\nprobabilities for each rule from maximum likelihood estimates. Thus, according\nto Eq. 12.18, the MLE estimate for the probability for the rule P(VP(dumped,VBD)\n→VBD(dumped, VBD) NP(sacks,NNS) PP(into,P)) would be\nCount(VP(dumped,VBD) →VBD(dumped, VBD) NP(sacks,NNS) PP(into,P))\nCount(VP(dumped,VBD))\n(12.23)\nBut there’s no way we can get good estimates of counts like those in (12.23)\nbecause they are so speciﬁc: we’re unlikely to see many (or even any) instances of a\nsentence with a verb phrase headed by dumped that has one NP argument headed by\nsacks and a PP argument headed by into. In other words, counts of fully lexicalized\nPCFG rules like this will be far too sparse, and most rule probabilities will come out\n0.\nThe idea of lexicalized parsing is to make some further independence assump-\ntions to break down each rule so that we would estimate the probability\nP(VP(dumped,VBD) →VBD(dumped, VBD) NP(sacks,NNS) PP(into,P))\nas the product of smaller independent probability estimates for which we could\nacquire reasonable counts.\nThe next section summarizes one such method, the\nCollins parsing method.\n12.6.1\nThe Collins Parser\nModern statistical parsers differ in exactly which independence assumptions they\nmake. In this section we describe a simpliﬁed version of Collins’s worth knowing\nabout; see the summary at the end of the chapter.\nThe ﬁrst intuition of the Collins parser is to think of the right-hand side of every\n(internal) CFG rule as consisting of a head non-terminal, together with the non-\nterminals to the left of the head and the non-terminals to the right of the head. In the\nabstract, we think about these rules as follows:\nLHS →Ln Ln−1 ...L1 H R1 ...Rn−1 Rn\n(12.24)\nSince this is a lexicalized grammar, each of the symbols like L1 or R3 or H or\nLHS is actually a complex symbol representing the category and its head and head\ntag, like VP(dumped,VP) or NP(sacks,NNS).\nNow, instead of computing a single MLE probability for this rule, we are going\nto break down this rule via a neat generative story, a slight simpliﬁcation of what is\ncalled Collins Model 1. This new generative story is that given the left-hand side,\nwe ﬁrst generate the head of the rule and then generate the dependents of the head,\none by one, from the inside out. Each of these generation steps will have its own\nprobability.\nWe also add a special STOP non-terminal at the left and right edges of the rule;\nthis non-terminal allows the model to know when to stop generating dependents on a\ngiven side. We generate dependents on the left side of the head until we’ve generated\nSTOP on the left side of the head, at which point we move to the right side of the\nhead and start generating dependents there until we generate STOP. So it’s as if we",
  "261": "12.6\n•\nPROBABILISTIC LEXICALIZED CFGS\n253\nare generating a rule augmented as follows:\nP(VP(dumped,VBD) →\n(12.25)\nSTOP VBD(dumped, VBD) NP(sacks,NNS) PP(into,P) STOP)\nLet’s see the generative story for this augmented rule. We make use of three\nkinds of probabilities: PH for generating heads, PL for generating dependents on the\nleft, and PR for generating dependents on the right.\n1. Generate the head VBD(dumped,VBD) with probability\nP(H|LHS) = P(VBD(dumped,VBD) | VP(dumped,VBD))\nVP(dumped,VBD)\nVBD(dumped,VBD)\n2. Generate the left dependent (which is STOP, since there isn’t\none) with probability\nP(STOP| VP(dumped,VBD) VBD(dumped,VBD))\nVP(dumped,VBD)\nVBD(dumped,VBD)\nSTOP\n3. Generate right dependent NP(sacks,NNS) with probability\nPr(NP(sacks,NNS| VP(dumped,VBD), VBD(dumped,VBD))\nVP(dumped,VBD)\nNP(sacks,NNS)\nVBD(dumped,VBD)\nSTOP\n4. Generate the right dependent PP(into,P) with probability\nPr(PP(into,P) | VP(dumped,VBD), VBD(dumped,VBD))\nVP(dumped,VBD)\nPP(into,P)\nNP(sacks,NNS)\nVBD(dumped,VBD)\nSTOP\n5) Generate the right dependent STOP with probability\nPr(STOP | VP(dumped,VBD), VBD(dumped,VBD))\nVP(dumped,VBD)\nSTOP\nPP(into,P)\nNP(sacks,NNS)\nVBD(dumped,VBD)\nSTOP\nIn summary, the probability of this rule\nP(VP(dumped,VBD) →\n(12.26)\nVBD(dumped, VBD) NP(sacks,NNS) PP(into,P))\nis estimated as\nPH(VBD|VP, dumped) × PL(STOP|VP,VBD,dumped)\n(12.27)\n× PR(NP(sacks,NNS)|VP,VBD,dumped)\n× PR(PP(into,P)|VP,VBD,dumped)\n× PR(STOP|VP,VBD,dumped)\nEach of these probabilities can be estimated from much smaller amounts of data\nthan the full probability in (12.26). For example, the maximum likelihood estimate",
  "262": "254\nCHAPTER 12\n•\nSTATISTICAL PARSING\nfor the component probability PR(NP(sacks,NNS)|VP,VBD,dumped) is\nCount( VP(dumped,VBD) with NNS(sacks)as a daughter somewhere on the right )\nCount( VP(dumped,VBD) )\n(12.28)\nThese counts are much less subject to sparsity problems than are complex counts\nlike those in (12.26).\nMore generally, if H is a head with head word hw and head tag ht, lw/lt and\nrw/rt are the word/tag on the left and right respectively, and P is the parent, then the\nprobability of an entire rule can be expressed as follows:\n1. Generate the head of the phrase H(hw,ht) with probability:\nPH(H(hw,ht)|P,hw,ht)\n2. Generate modiﬁers to the left of the head with total probability\nn+1\nY\ni=1\nPL(Li(lwi,lti)|P,H,hw,ht)\nsuch that Ln+1(lwn+1,ltn+1) =STOP, and we stop generating once we’ve gen-\nerated a STOP token.\n3. Generate modiﬁers to the right of the head with total probability:\nn+1\nY\ni=1\nPP(Ri(rwi,rti)|P,H,hw,ht)\nsuch that Rn+1(rwn+1,rtn+1) = STOP, and we stop generating once we’ve\ngenerated a STOP token.\n12.6.2\nAdvanced: Further Details of the Collins Parser\nThe actual Collins parser models are more complex (in a couple of ways) than the\nsimple model presented in the previous section. Collins Model 1 includes a distance\ndistance\nfeature. Thus, instead of computing PL and PR as follows,\nPL(Li(lwi,lti)|P,H,hw,ht)\n(12.29)\nPR(Ri(rwi,rti)|P,H,hw,ht)\n(12.30)\nCollins Model 1 conditions also on a distance feature:\nPL(Li(lwi,lti)|P,H,hw,ht,distanceL(i−1))\n(12.31)\nPR(Ri(rwi,rti)|P,H,hw,ht,distanceR(i−1))\n(12.32)\nThe distance measure is a function of the sequence of words below the previous\nmodiﬁers (i.e., the words that are the yield of each modiﬁer non-terminal we have\nalready generated on the left).\nThe simplest version of this distance measure is just a tuple of two binary fea-\ntures based on the surface string below these previous dependencies: (1) Is the string\nof length zero? (i.e., were no previous words generated?) (2) Does the string contain\na verb?",
  "263": "12.7\n•\nPROBABILISTIC CCG PARSING\n255\nCollins Model 2 adds more sophisticated features, conditioning on subcatego-\nrization frames for each verb and distinguishing arguments from adjuncts.\nFinally, smoothing is as important for statistical parsers as it was for N-gram\nmodels. This is particularly true for lexicalized parsers, since the lexicalized rules\nwill otherwise condition on many lexical items that may never occur in training\n(even using the Collins or other methods of independence assumptions).\nConsider the probability PR(Ri(rwi,rti)|P,hw,ht). What do we do if a particular\nright-hand constituent never occurs with this head? The Collins model addresses this\nproblem by interpolating three backed-off models: fully lexicalized (conditioning on\nthe headword), backing off to just the head tag, and altogether unlexicalized.\nBackoff\nPR(Ri(rwi,rti|...)\nExample\n1\nPR(Ri(rwi,rti)|P,hw,ht)\nPR(NP(sacks,NNS)|VP, VBD, dumped)\n2\nPR(Ri(rwi,rti)|P,ht)\nPR(NP(sacks,NNS)|VP,VBD)\n3\nPR(Ri(rwi,rti)|P)\nPR(NP(sacks,NNS)|VP)\nSimilar backoff models are built also for PL and PH. Although we’ve used the\nword “backoff”, in fact these are not backoff models but interpolated models. The\nthree models above are linearly interpolated, where e1, e2, and e3 are the maximum\nlikelihood estimates of the three backoff models above:\nPR(...) = λ1e1 +(1−λ1)(λ2e2 +(1−λ2)e3)\n(12.33)\nThe values of λ1andλ2 are set to implement Witten-Bell discounting (Witten and\nBell, 1991) following Bikel et al. (1997).\nThe Collins model deals with unknown words by replacing any unknown word\nin the test set, and any word occurring less than six times in the training set, with a\nspecial UNKNOWN word token. Unknown words in the test set are assigned a part-\nof-speech tag in a preprocessing step by the Ratnaparkhi (1996) tagger; all other\nwords are tagged as part of the parsing process.\nThe parsing algorithm for the Collins model is an extension of probabilistic\nCKY; see Collins (2003a). Extending the CKY algorithm to handle basic lexicalized\nprobabilities is left as Exercises 14.5 and 14.6 for the reader.\n12.7\nProbabilistic CCG Parsing\nLexicalized grammar frameworks such as CCG pose problems for which the phrase-\nbased methods we’ve been discussing are not particularly well-suited. To quickly\nreview, CCG consists of three major parts: a set of categories, a lexicon that asso-\nciates words with categories, and a set of rules that govern how categories combine\nin context. Categories can be either atomic elements, such as S and NP, or functions\nsuch as (S\\NP)/NP which speciﬁes the transitive verb category. Rules specify how\nfunctions, their arguments, and other functions combine. For example, the following\nrule templates, forward and backward function application, specify the way that\nfunctions apply to their arguments.\nX/Y Y ⇒X\nY X\\Y ⇒X\nThe ﬁrst rule applies a function to its argument on the right, while the second\nlooks to the left for its argument. The result of applying either of these rules is the",
  "264": "256\nCHAPTER 12\n•\nSTATISTICAL PARSING\ncategory speciﬁed as the value of the function being applied. For the purposes of\nthis discussion, we’ll rely on these two rules along with the forward and backward\ncomposition rules and type-raising, as described in Chapter 10.\n12.7.1\nAmbiguity in CCG\nAs is always the case in parsing, managing ambiguity is the key to successful CCG\nparsing. The difﬁculties with CCG parsing arise from the ambiguity caused by the\nlarge number of complex lexical categories combined with the very general nature of\nthe grammatical rules. To see some of the ways that ambiguity arises in a categorial\nframework, consider the following example.\n(12.34) United diverted the ﬂight to Reno.\nOur grasp of the role of the ﬂight in this example depends on whether the prepo-\nsitional phrase to Reno is taken as a modiﬁer of the ﬂight, as a modiﬁer of the entire\nverb phrase, or as a potential second argument to the verb divert. In a context-free\ngrammar approach, this ambiguity would manifest itself as a choice among the fol-\nlowing rules in the grammar.\nNominal →Nominal PP\nVP →VP PP\nVP →Verb NP PP\nIn a phrase-structure approach we would simply assign the word to to the cate-\ngory P allowing it to combine with Reno to form a prepositional phrase. The sub-\nsequent choice of grammar rules would then dictate the ultimate derivation. In the\ncategorial approach, we can associate to with distinct categories to reﬂect the ways\nin which it might interact with other elements in a sentence. The fairly abstract\ncombinatoric rules would then sort out which derivations are possible. Therefore,\nthe source of ambiguity arises not from the grammar but rather from the lexicon.\nLet’s see how this works by considering several possible derivations for this\nexample. To capture the case where the prepositional phrase to Reno modiﬁes the\nﬂight, we assign the preposition to the category (NP\\NP)/NP, which gives rise to\nthe following derivation.\nUnited\ndiverted\nthe\nﬂight\nto\nReno\nNP\n(S\\NP)/NP NP/N\nN\n(NP\\NP)/NP\nNP\n>\n>\nNP\nNP\\NP\n<\nNP\n>\nS\\NP\n<\nS\nHere, the category assigned to to expects to ﬁnd two arguments: one to the right as\nwith a traditional preposition, and one to the left that corresponds to the NP to be\nmodiﬁed.\nAlternatively, we could assign to to the category (S\\S)/NP, which permits the\nfollowing derivation where to Reno modiﬁes the preceding verb phrase.",
  "265": "12.7\n•\nPROBABILISTIC CCG PARSING\n257\nUnited\ndiverted\nthe\nﬂight\nto\nReno\nNP\n(S\\NP)/NP NP/N\nN\n(S\\S)/NP\nNP\n>\n>\nNP\nS\\S\n>\nS\\NP\n<B\nS\\NP\n<\nS\nA third possibility is to view divert as a ditransitive verb by assigning it to the\ncategory ((S\\NP)/PP)/NP, while treating to Reno as a simple prepositional phrase.\nUnited\ndiverted\nthe\nﬂight\nto\nReno\nNP\n((S\\NP)/PP)/NP NP/N\nN\nPP/NP\nNP\n>\n>\nNP\nPP\n>\n(S\\NP)/PP\n>\nS\\NP\n<\nS\nWhile CCG parsers are still subject to ambiguity arising from the choice of\ngrammar rules, including the kind of spurious ambiguity discussed in Chapter 10,\nit should be clear that the choice of lexical categories is the primary problem to be\naddressed in CCG parsing.\n12.7.2\nCCG Parsing Frameworks\nSince the rules in combinatory grammars are either binary or unary, a bottom-up,\ntabular approach based on the CKY algorithm should be directly applicable to CCG\nparsing. Recall from Fig. 12.3 that PCKY employs a table that records the location,\ncategory and probability of all valid constituents discovered in the input. Given an\nappropriate probability model for CCG derivations, the same kind of approach can\nwork for CCG parsing.\nUnfortunately, the large number of lexical categories available for each word,\ncombined with the promiscuity of CCG’s combinatoric rules, leads to an explosion\nin the number of (mostly useless) constituents added to the parsing table. The key\nto managing this explosion of zombie constituents is to accurately assess and ex-\nploit the most likely lexical categories possible for each word — a process called\nsupertagging.\nThe following sections describe two approaches to CCG parsing that make use of\nsupertags. Section 12.7.4, presents an approach that structures the parsing process\nas a heuristic search through the use of the A* algorithm. The following section\nthen brieﬂy describes a more traditional maximum entropy approach that manages\nthe search space complexity through the use of adaptive supertagging — a process\nthat iteratively considers more and more tags until a parse is found.\n12.7.3\nSupertagging\nChapter 8 introduced the task of part-of-speech tagging, the process of assigning the\ncorrect lexical category to each word in a sentence. Supertagging is the correspond-\nsupertagging\ning task for highly lexicalized grammar frameworks, where the assigned tags often\ndictate much of the derivation for a sentence.",
  "266": "258\nCHAPTER 12\n•\nSTATISTICAL PARSING\nCCG supertaggers rely on treebanks such as CCGbank to provide both the over-\nall set of lexical categories as well as the allowable category assignments for each\nword in the lexicon. CCGbank includes over 1000 lexical categories, however, in\npractice, most supertaggers limit their tagsets to those tags that occur at least 10\ntimes in the training corpus. This results in an overall total of around 425 lexical\ncategories available for use in the lexicon. Note that even this smaller number is\nlarge in contrast to the 45 POS types used by the Penn Treebank tagset.\nAs with traditional part-of-speech tagging, the standard approach to building a\nCCG supertagger is to use supervised machine learning to build a sequence classi-\nﬁer using labeled training data. A common approach is to use the maximum entropy\nMarkov model (MEMM), as described in Chapter 8, to ﬁnd the most likely sequence\nof tags given a sentence. The features in such a model consist of the current word\nwi, its surrounding words within l words wi+l\ni−l, as well as the k previously assigned\nsupertags ti−1\ni−k . This type of model is summarized in the following equation from\nChapter 8. Training by maximizing log-likelihood of the training corpus and decod-\ning via the Viterbi algorithm are the same as described in Chapter 8.\nˆT = argmax\nT\nP(T|W)\n= argmax\nT\nY\ni\nP(ti|wi+l\ni−l,ti−1\ni−k )\n= argmax\nT\nY\ni\nexp\n X\ni\nwi fi(ti,wi+l\ni−l,ti−1\ni−k )\n!\nX\nt′∈tagset\nexp\n X\ni\nwi fi(t′,wi+l\ni−l,ti−1\ni−k )\n!\n(12.35)\nWord and tag-based features with k and l both set to 2 provides reasonable results\ngiven sufﬁcient training data. Additional features such as POS tags and short char-\nacter sufﬁxes are also commonly used to improve performance.\nUnfortunately, even with additional features the large number of possible su-\npertags combined with high per-word ambiguity leads to error rates that are too\nhigh for practical use in a parser. More speciﬁcally, the single best tag sequence\nˆT will typically contain too many incorrect tags for effective parsing to take place.\nTo overcome this, we can instead return a probability distribution over the possible\nsupertags for each word in the input. The following table illustrates an example dis-\ntribution for a simple example sentence. In this table, each column represents the\nprobability of each supertag for a given word in the context of the input sentence.\nThe “...” represent all the remaining supertags possible for each word.\nUnited\nserves\nDenver\nN/N: 0.4 (S\\NP)/NP: 0.8\nNP: 0.9\nNP: 0.3\nN: 0.1\nN/N: 0.05\nS/S: 0.1\n...\n...\nS\\S: .05\n...\nIn a MEMM framework, the probability of the optimal tag sequence deﬁned in\nEq. 12.35 is efﬁciently computed with a suitably modiﬁed version of the Viterbi\nalgorithm. However, since Viterbi only ﬁnds the single best tag sequence it doesn’t",
  "267": "12.7\n•\nPROBABILISTIC CCG PARSING\n259\nprovide exactly what we need here; we need to know the probability of each pos-\nsible word/tag pair. The probability of any given tag for a word is the sum of the\nprobabilities of all the supertag sequences that contain that tag at that location. A\ntable representing these values can be computed efﬁciently by using a version of the\nforward-backward algorithm used for HMMs.\nThe same result can also be achieved through the use of deep learning approaches\nbased on recurrent neural networks (RNNs). Recent efforts have demonstrated con-\nsiderable success with RNNs as alternatives to HMM-based methods. These ap-\nproaches differ from traditional classiﬁer-based methods in the following ways:\n• The use of vector-based word representations (embeddings) rather than word-\nbased feature functions.\n• Input representations that span the entire sentence, as opposed to size-limited\nsliding windows.\n• Avoiding the use of high-level features, such as part of speech tags, since\nerrors in tag assignment can propagate to errors in supertags.\nAs with the forward-backward algorithm, RNN-based methods can provide a prob-\nability distribution over the lexical categories for each word in the input.\n12.7.4\nCCG Parsing using the A* Algorithm\nThe A* algorithm is a heuristic search method that employs an agenda to ﬁnd an\noptimal solution. Search states representing partial solutions are added to an agenda\nbased on a cost function, with the least-cost option being selected for further ex-\nploration at each iteration. When a state representing a complete solution is ﬁrst\nselected from the agenda, it is guaranteed to be optimal and the search terminates.\nThe A* cost function, f(n), is used to efﬁciently guide the search to a solution.\nThe f-cost has two components: g(n), the exact cost of the partial solution repre-\nsented by the state n, and h(n) a heuristic approximation of the cost of a solution\nthat makes use of n. When h(n) satisﬁes the criteria of not overestimating the actual\ncost, A* will ﬁnd an optimal solution. Not surprisingly, the closer the heuristic can\nget to the actual cost, the more effective A* is at ﬁnding a solution without having\nto explore a signiﬁcant portion of the solution space.\nWhen applied to parsing, search states correspond to edges representing com-\npleted constituents. As with the PCKY algorithm, edges specify a constituent’s start\nand end positions, its grammatical category, and its f-cost. Here, the g component\nrepresents the current cost of an edge and the h component represents an estimate\nof the cost to complete a derivation that makes use of that edge. The use of A*\nfor phrase structure parsing originated with (Klein and Manning, 2003a), while the\nCCG approach presented here is based on (Lewis and Steedman, 2014).\nUsing information from a supertagger, an agenda and a parse table are initial-\nized with states representing all the possible lexical categories for each word in the\ninput, along with their f-costs. The main loop removes the lowest cost edge from\nthe agenda and tests to see if it is a complete derivation. If it reﬂects a complete\nderivation it is selected as the best solution and the loop terminates. Otherwise, new\nstates based on the applicable CCG rules are generated, assigned costs, and entered\ninto the agenda to await further processing. The loop continues until a complete\nderivation is discovered, or the agenda is exhausted, indicating a failed parse. The\nalgorithm is given in Fig. 12.11.",
  "268": "260\nCHAPTER 12\n•\nSTATISTICAL PARSING\nfunction CCG-ASTAR-PARSE(words) returns table or failure\nsupertags←SUPERTAGGER(words)\nfor i←from 1 to LENGTH(words) do\nfor all {A | (words[i], A, score) ∈supertags}\nedge←MAKEEDGE(i−1,i,A,score)\ntable←INSERTEDGE(table,edge)\nagenda←INSERTEDGE(agenda, edge)\nloop do\nif EMPTY?(agenda) return failure\ncurrent←POP(agenda)\nif COMPLETEDPARSE?(current) return table\ntable←INSERTEDGE(chart, edge)\nfor each rule in APPLICABLERULES(edge) do\nsuccessor←APPLY(rule, edge)\nif successor not ∈in agenda or chart\nagenda←INSERTEDGE(agenda, successor)\nelse if successor ∈agenda with higher cost\nagenda←REPLACEEDGE(agenda, successor)\nFigure 12.11\nA*-based CCG parsing.\nHeuristic Functions\nBefore we can deﬁne a heuristic function for our A* search, we need to decide how\nto assess the quality of CCG derivations. For the generic PCFG model, we deﬁned\nthe probability of a tree as the product of the probability of the rules that made up\nthe tree. Given CCG’s lexical nature, we’ll make the simplifying assumption that the\nprobability of a CCG derivation is just the product of the probability of the supertags\nassigned to the words in the derivation, ignoring the rules used in the derivation.\nMore formally, given a sentence S and derivation D that contains suptertag sequence\nT, we have:\nP(D,S) = P(T,S)\n(12.36)\n=\nn\nY\ni=1\nP(ti|si)\n(12.37)\nTo better ﬁt with the traditional A* approach, we’d prefer to have states scored\nby a cost function where lower is better (i.e., we’re trying to minimize the cost of\na derivation). To achieve this, we’ll use negative log probabilities to score deriva-\ntions; this results in the following equation, which we’ll use to score completed CCG\nderivations.\nP(D,S) = P(T,S)\n(12.38)\n=\nn\nX\ni=1\n−logP(ti|si)\n(12.39)\nGiven this model, we can deﬁne our f-cost as follows. The f-cost of an edge is\nthe sum of two components: g(n), the cost of the span represented by the edge, and",
  "269": "12.7\n•\nPROBABILISTIC CCG PARSING\n261\nh(n), the estimate of the cost to complete a derivation containing that edge (these\nare often referred to as the inside and outside costs). We’ll deﬁne g(n) for an edge\nusing Equation 12.39. That is, it is just the sum of the costs of the supertags that\ncomprise the span.\nFor h(n), we need a score that approximates but never overestimates the actual\ncost of the ﬁnal derivation. A simple heuristic that meets this requirement assumes\nthat each of the words in the outside span will be assigned its most probable su-\npertag. If these are the tags used in the ﬁnal derivation, then its score will equal\nthe heuristic. If any other tags are used in the ﬁnal derivation the f-cost will be\nhigher since the new tags must have higher costs, thus guaranteeing that we will not\noverestimate.\nPutting this all together, we arrive at the following deﬁnition of a suitable f-cost\nfor an edge.\nf(wi,j,ti,j) = g(wi,j)+h(wi,j)\n(12.40)\n=\nj\nX\nk=i\n−logP(tk|wk)+\ni−1\nX\nk=1\nmax\nt∈tags(−logP(t|wk))+\nN\nX\nk=j+1\nmax\nt∈tags(−logP(t|wk))\nAs an example, consider an edge representing the word serves with the supertag\nN in the following example.\n(12.41) United serves Denver.\nThe g-cost for this edge is just the negative log probability of the tag, or X. The\noutside h-cost consists of the most optimistic supertag assignments for United and\nDenver. The resulting f-cost for this edge is therefore x+y+z = 1.494.\nAn Example\nFig. 12.12 shows the initial agenda and the progress of a complete parse for this\nexample. After initializing the agenda and the parse table with information from the\nsupertagger, it selects the best edge from the agenda — the entry for United with\nthe tag N/N and f-cost 0.591. This edge does not constitute a complete parse and is\ntherefore used to generate new states by applying all the relevant grammar rules. In\nthis case, applying forward application to United: N/N and serves: N results in the\ncreation of the edge United serves: N[0,2], 1.795 to the agenda.\nSkipping ahead, at the the third iteration an edge representing the complete\nderivation United serves Denver, S[0,3], .716 is added to the agenda. However,\nthe algorithm does not terminate at this point since the cost of this edge (.716) does\nnot place it at the top of the agenda. Instead, the edge representing Denver with the\ncategory NP is popped. This leads to the addition of another edge to the agenda\n(type-raising Denver). Only after this edge is popped and dealt with does the ear-\nlier state representing a complete derivation rise to the top of the agenda where it is\npopped, goal tested, and returned as a solution.\nThe effectiveness of the A* approach is reﬂected in the coloring of the states\nin Fig. 12.12 as well as the ﬁnal parsing table. The edges shown in blue (includ-\ning all the initial lexical category assignments not explicitly shown) reﬂect states in\nthe search space that never made it to the top of the agenda and, therefore, never",
  "270": "262\nCHAPTER 12\n•\nSTATISTICAL PARSING\nUnited serves: N[0,2]\n1.795\nUnited: N/N\n.591\nDenver: N/N\n2.494\nDenver: N\n1.795\nserves: N\n1.494\nUnited: S\\S\n1.494\nUnited: S/S\n1.1938\nUnited: NP\n.716\nDenver: NP\n.591\nserves: (S\\NP)/NP\n.591\nserves Denver: S\\NP[1,3]\n.591\nUnited serves Denver: S[0,3]\n.716\nDenver: S/(S\\NP)[0,1]\n.591\n1\n2\n3\n4\n5\n6\nInitial \nAgenda\nGoal State\n…\nS: 0.716\nS/NP: 0.591\nUnited\nserves\n[0,1]\n[0,2]\n[0,3]\n[1,2]\n[1,3]\n[2,3]\nN/N: 0.591\nNP: 0.716\nS/S: 1.1938\nS\\S: 1.494\n…\nDenver\n(S\\NP)/NP: 0.591\nN: 1.494\n…\nNP: 0.591\nN: 1.795\nN/N: 2.494\n…\nN: 1.795\nFigure 12.12\nExample of an A* search for the example “United serves Denver”. The circled numbers on the\nwhite boxes indicate the order in which the states are popped from the agenda. The costs in each state are based\non f-costs using negative log10 probabilities.\ncontributed any edges to the ﬁnal table. This is in contrast to the PCKY approach\nwhere the parser systematically ﬁlls the parse table with all possible constituents for\nall possible spans in the input, ﬁlling the table with myriad constituents that do not\ncontribute to the ﬁnal analysis.",
  "271": "12.8\n•\nEVALUATING PARSERS\n263\n12.8\nEvaluating Parsers\nThe standard techniques for evaluating parsers and grammars are called the PAR-\nSEVAL measures; they were proposed by Black et al. (1991) and were based on\nthe same ideas from signal-detection theory that we saw in earlier chapters. The\nintuition of the PARSEVAL metric is to measure how much the constituents in the\nhypothesis parse tree look like the constituents in a hand-labeled, gold-reference\nparse. PARSEVAL thus assumes we have a human-labeled “gold standard” parse\ntree for each sentence in the test set; we generally draw these gold-standard parses\nfrom a treebank like the Penn Treebank.\nGiven these gold-standard reference parses for a test set, a given constituent in\na hypothesis parse Ch of a sentence s is labeled “correct” if there is a constituent in\nthe reference parse Cr with the same starting point, ending point, and non-terminal\nsymbol.\nWe can then measure the precision and recall just as we did for chunking in the\nprevious chapter.\nlabeled recall: = # of correct constituents in hypothesis parse of s\n# of correct constituents in reference parse of s\nlabeled precision: = # of correct constituents in hypothesis parse of s\n# of total constituents in hypothesis parse of s\nAs with other uses of precision and recall, instead of reporting them separately,\nwe often report a single number, the F-measure (van Rijsbergen, 1975): The F-\nF-measure\nmeasure is deﬁned as\nFβ = (β 2 +1)PR\nβ 2P+R\nThe β parameter differentially weights the importance of recall and precision,\nbased perhaps on the needs of an application. Values of β > 1 favor recall and values\nof β < 1 favor precision. When β = 1, precision and recall are equally balanced;\nthis is sometimes called Fβ=1 or just F1:\nF1 = 2PR\nP+R\n(12.42)\nThe F-measure derives from a weighted harmonic mean of precision and recall.\nRemember that the harmonic mean of a set of numbers is the reciprocal of the arith-\nmetic mean of the reciprocals:\nHarmonicMean(a1,a2,a3,a4,...,an) =\nn\n1\na1 + 1\na2 + 1\na3 +...+ 1\nan\n(12.43)\nand hence the F-measure is\nF =\n1\nα 1\nP +(1−α) 1\nR\nor\n\u0012\nwith β 2 = 1−α\nα\n\u0013\nF = (β 2 +1)PR\nβ 2P+R\n(12.44)\nWe additionally use a new metric, crossing brackets, for each sentence s:\ncross-brackets: the number of constituents for which the reference parse has a\nbracketing such as ((A B) C) but the hypothesis parse has a bracketing such\nas (A (B C)).",
  "272": "264\nCHAPTER 12\n•\nSTATISTICAL PARSING\nAs of the time of this writing, the performance of modern parsers that are trained\nand tested on the Wall Street Journal treebank was somewhat higher than 90% recall,\n90% precision, and about 1% cross-bracketed constituents per sentence.\nFor comparing parsers that use different grammars, the PARSEVAL metric in-\ncludes a canonicalization algorithm for removing information likely to be grammar-\nspeciﬁc (auxiliaries, pre-inﬁnitival “to”, etc.) and for computing a simpliﬁed score\n(Black et al., 1991). The canonical implementation of the PARSEVAL metrics is\ncalled evalb (Sekine and Collins, 1997).\nevalb\nNonetheless, phrasal constituents are not always an appropriate unit for parser\nevaluation. In lexically-oriented grammars, such as CCG and LFG, the ultimate goal\nis to extract the appropriate predicate-argument relations or grammatical dependen-\ncies, rather than a speciﬁc derivation. Such relations are also more directly relevant\nto further semantic processing. For these purposes, we can use alternative evaluation\nmetrics based on measuring the precision and recall of labeled dependencies, where\nthe labels indicate the grammatical relations (Lin 1995, Carroll et al. 1998, Collins\net al. 1999).\nFinally, you might wonder why we don’t evaluate parsers by measuring how\nmany sentences are parsed correctly instead of measuring component accuracy in\nthe form of constituents or dependencies. The reason we use components is that it\ngives us a more ﬁne-grained metric. This is especially true for long sentences, where\nmost parsers don’t get a perfect parse. If we just measured sentence accuracy, we\nwouldn’t be able to distinguish between a parse that got most of the parts wrong and\none that just got one part wrong.\n12.9\nHuman Parsing\nAre the kinds of probabilistic parsing models we have been discussing also used by\nhumans when they are parsing? The answer to this question lies in a ﬁeld called\nhuman sentence processing. Recent studies suggest that there are at least two\nHuman\nsentence\nprocessing\nways in which humans apply probabilistic parsing algorithms, although there is still\ndisagreement on the details.\nOne family of studies has shown that when humans read, the predictability of a\nword seems to inﬂuence the reading time; more predictable words are read more\nReading time\nquickly. One way of deﬁning predictability is from simple bigram measures. For\nexample, Scott and Shillcock (2003) used an eye-tracker to monitor the gaze of\nparticipants reading sentences. They constructed the sentences so that some would\nhave a verb-noun pair with a high bigram probability (such as (12.45a)) and others\na verb-noun pair with a low bigram probability (such as (12.45b)).\n(12.45)\na) HIGH PROB: One way to avoid confusion is to make the changes\nduring vacation\nb) LOW PROB: One way to avoid discovery is to make the changes\nduring vacation\nThey found that the higher the bigram predictability of a word, the shorter the\ntime that participants looked at the word (the initial-ﬁxation duration).\nWhile this result provides evidence only for N-gram probabilities, more recent\nexperiments have suggested that the probability of an upcoming word given the\nsyntactic parse of the preceding sentence preﬁx also predicts word reading time\n(Hale 2001, Levy 2008).",
  "273": "12.9\n•\nHUMAN PARSING\n265\nThe second family of studies has examined how humans disambiguate sentences\nthat have multiple possible parses, suggesting that humans prefer whichever parse\nis more probable. These studies often rely on a speciﬁc class of temporarily am-\nbiguous sentences called garden-path sentences. These sentences, ﬁrst described\ngarden-path\nby Bever (1970), are sentences that are cleverly constructed to have three properties\nthat combine to make them very difﬁcult for people to parse:\n1. They are temporarily ambiguous: The sentence is unambiguous, but its ini-\ntial portion is ambiguous.\n2. One of the two or more parses in the initial portion is somehow preferable to\nthe human parsing mechanism.\n3. But the dispreferred parse is the correct one for the sentence.\nThe result of these three properties is that people are “led down the garden path”\ntoward the incorrect parse and then are confused when they realize it’s the wrong\none. Sometimes this confusion is quite conscious, as in Bever’s example (12.46);\nin fact, this sentence is so hard to parse that readers often need to be shown the\ncorrect structure. In the correct structure, raced is part of a reduced relative clause\nmodifying The horse, and means “The horse [which was raced past the barn] fell”;\nthis structure is also present in the sentence “Students taught by the Berlitz method\ndo worse when they get to France”.\n(12.46) The horse raced past the barn fell.\n(a)\nS\nVP\nPP\nNP\nN\nbarn\nDet\nthe\nP\npast\nV\nraced\nNP\nN\nhorse\nDet\nThe\n?\nV\nfell\n(b)\nS\nVP\nV\nfell\nNP\nVP\nPP\nNP\nN\nbarn\nDet\nthe\nP\npast\nV\nraced\nNP\nN\nhorse\nDet\nThe\nOther times, the confusion caused by a garden-path sentence is so subtle that it\ncan only be measured by a slight increase in reading time. Thus, in (12.47) readers\noften misparse the solution as the direct object of forgot rather than as the subject\nof an embedded sentence. This misparse is subtle, and is only noticeable because\nexperimental participants take longer to read the word was than in control sentences.\nThis “mini garden path” effect at the word was suggests that subjects had chosen the\ndirect object parse and had to reanalyze or rearrange their parse now that they realize\nthey are in a sentential complement.\n(12.47) The student forgot the solution was in the back of the book.",
  "274": "266\nCHAPTER 12\n•\nSTATISTICAL PARSING\nS\nVP\nNP\nN\nsolution\nDet\nthe\nV\nforgot\nNP\nN\nstudents\nDet\nThe\nS\nVP\nS\nVP\nV\nwas\nNP\nN\nsolution\nDet\nthe\nV\nforgot\nNP\nN\nstudents\nDet\nThe\nWhile many factors seem to play a role in these preferences for a particular (in-\ncorrect) parse, at least one factor seems to be syntactic probabilities, especially lex-\nicalized (subcategorization) probabilities. For example, the probability of the verb\nforgot taking a direct object (VP →V NP) is higher than the probability of it taking a\nsentential complement (VP →V S); this difference causes readers to expect a direct\nobject after forget and be surprised (longer reading times) when they encounter a\nsentential complement. By contrast, a verb which prefers a sentential complement\n(like hope) didn’t cause extra reading time at was. The garden path in (12.46) is at\nleast partially caused by the low probability of the reduced relative clause construc-\ntion.\n12.10\nSummary\nThis chapter has sketched the basics of probabilistic parsing, concentrating on\nprobabilistic context-free grammars and probabilistic lexicalized context-free\ngrammars.\n• Probabilistic grammars assign a probability to a sentence or string of words\nwhile attempting to capture more sophisticated syntactic information than the\nN-gram grammars of Chapter 3.\n• A\nprobabilistic\ncontext-free\ngrammar\n(PCFG)\nis\na\ncontext-free\ngrammar in which every rule is annotated with the probability of that rule\nbeing chosen. Each PCFG rule is treated as if it were conditionally inde-\npendent; thus, the probability of a sentence is computed by multiplying the\nprobabilities of each rule in the parse of the sentence.\n• The probabilistic CKY (Cocke-Kasami-Younger) algorithm is a probabilistic\nversion of the CKY parsing algorithm. There are also probabilistic versions\nof other parsers like the Earley algorithm.\n• PCFG probabilities can be learned by counting in a parsed corpus or by pars-\ning a corpus. The inside-outside algorithm is a way of dealing with the fact\nthat the sentences being parsed are ambiguous.\n• Raw PCFGs suffer from poor independence assumptions among rules and lack\nof sensitivity to lexical dependencies.\n• One way to deal with this problem is to split and merge non-terminals (auto-\nmatically or by hand).",
  "275": "BIBLIOGRAPHICAL AND HISTORICAL NOTES\n267\n• Probabilistic lexicalized CFGs are another solution to this problem in which\nthe basic PCFG model is augmented with a lexical head for each rule. The\nprobability of a rule can then be conditioned on the lexical head or nearby\nheads.\n• Parsers for lexicalized PCFGs (like the Charniak and Collins parsers) are\nbased on extensions to probabilistic CKY parsing.\n• Parsers are evaluated with three metrics: labeled recall, labeled precision,\nand cross-brackets.\n• Evidence from garden-path sentences and other on-line sentence-processing\nexperiments suggest that the human parser uses some kinds of probabilistic\ninformation about grammar.\nBibliographical and Historical Notes\nMany of the formal properties of probabilistic context-free grammars were ﬁrst\nworked out by Booth (1969) and Salomaa (1969). Baker (1979) proposed the inside-\noutside algorithm for unsupervised training of PCFG probabilities, and used a CKY-\nstyle parsing algorithm to compute inside probabilities. Jelinek and Lafferty (1991)\nextended the CKY algorithm to compute probabilities for preﬁxes. Stolcke (1995)\ndrew on both of these algorithms in adapting the Earley algorithm to use with\nPCFGs.\nA number of researchers starting in the early 1990s worked on adding lexical de-\npendencies to PCFGs and on making PCFG rule probabilities more sensitive to sur-\nrounding syntactic structure. For example, Schabes et al. (1988) and Schabes (1990)\npresented early work on the use of heads. Many papers on the use of lexical depen-\ndencies were ﬁrst presented at the DARPA Speech and Natural Language Workshop\nin June 1990. A paper by Hindle and Rooth (1990) applied lexical dependencies\nto the problem of attaching prepositional phrases; in the question session to a later\npaper, Ken Church suggested applying this method to full parsing (Marcus, 1990).\nEarly work on such probabilistic CFG parsing augmented with probabilistic depen-\ndency information includes Magerman and Marcus (1991), Black et al. (1992), Bod\n(1993), and Jelinek et al. (1994), in addition to Collins (1996), Charniak (1997), and\nCollins (1999) discussed above. Other recent PCFG parsing models include Klein\nand Manning (2003a) and Petrov et al. (2006).\nThis early lexical probabilistic work led initially to work focused on solving\nspeciﬁc parsing problems like preposition-phrase attachment by using methods in-\ncluding transformation-based learning (TBL) (Brill and Resnik, 1994), maximum\nentropy (Ratnaparkhi et al., 1994), memory-based Learning (Zavrel and Daelemans,\n1997), log-linear models (Franz, 1997), decision trees that used semantic distance\nbetween heads (computed from WordNet) (Stetina and Nagao, 1997), and boosting\n(Abney et al., 1999).\nAnother direction extended the lexical probabilistic parsing work to build prob-\nabilistic formulations of grammars other than PCFGs, such as probabilistic TAG\ngrammar (Resnik 1992, Schabes 1992), based on the TAG grammars discussed in\nChapter 10, probabilistic LR parsing (Briscoe and Carroll, 1993), and probabilistic\nlink grammar (Lafferty et al., 1992). An approach to probabilistic parsing called\nsupertagging extends the part-of-speech tagging metaphor to parsing by using very\nsupertagging\ncomplex tags that are, in fact, fragments of lexicalized parse trees (Bangalore and",
  "276": "268\nCHAPTER 12\n•\nSTATISTICAL PARSING\nJoshi 1999, Joshi and Srinivas 1994), based on the lexicalized TAG grammars of\nSchabes et al. (1988). For example, the noun purchase would have a different tag\nas the ﬁrst noun in a noun compound (where it might be on the left of a small tree\ndominated by Nominal) than as the second noun (where it might be on the right).\nGoodman (1997), Abney (1997), and Johnson et al. (1999) gave early discus-\nsions of probabilistic treatments of feature-based grammars.\nOther recent work\non building statistical models of feature-based grammar formalisms like HPSG and\nLFG includes (Riezler et al. 2002, Kaplan et al. 2004), and Toutanova et al. (2005).\nWe mentioned earlier that discriminative approaches to parsing fall into the two\nbroad categories of dynamic programming methods and discriminative reranking\nmethods. Recall that discriminative reranking approaches require N-best parses.\nParsers based on A* search can easily be modiﬁed to generate N-best lists just by\ncontinuing the search past the ﬁrst-best parse (Roark, 2001). Dynamic programming\nalgorithms like the ones described in this chapter can be modiﬁed by the elimina-\ntion of the dynamic programming with heavy pruning (Collins 2000, Collins and\nKoo 2005, Bikel 2004), or through new algorithms (Jim´enez and Marzal 2000,Char-\nniak and Johnson 2005,Huang and Chiang 2005), some adapted from speech recog-\nnition algorithms such as those of Schwartz and Chow (1990) (see Section ??).\nIn dynamic programming methods, instead of outputting and then reranking an\nN-best list, the parses are represented compactly in a chart, and log-linear and other\nmethods are applied for decoding directly from the chart. Such modern methods\ninclude (Johnson 2001, Clark and Curran 2004), and Taskar et al. (2004). Other\nreranking developments include changing the optimization criterion (Titov and Hen-\nderson, 2006).\nCollins’ (1999) dissertation includes a very readable survey of the ﬁeld and an\nintroduction to his parser. Manning and Sch¨utze (1999) extensively cover proba-\nbilistic parsing.\nThe ﬁeld of grammar induction is closely related to statistical parsing, and a\nparser is often used as part of a grammar induction algorithm. One of the earliest\nstatistical works in grammar induction was Horning (1969), who showed that PCFGs\ncould be induced without negative evidence. Early modern probabilistic grammar\nwork showed that simply using EM was insufﬁcient (Lari and Young 1990, Carroll\nand Charniak 1992). Recent probabilistic work, such as Yuret (1998), Clark (2001),\nKlein and Manning (2002), and Klein and Manning (2004), are summarized in Klein\n(2005) and Adriaans and van Zaanen (2004). Work since that summary includes\nSmith and Eisner (2005), Haghighi and Klein (2006), and Smith and Eisner (2007).\nExercises\n12.1 Implement the CKY algorithm.\n12.2 Modify the algorithm for conversion to CNF from Chapter 11 to correctly\nhandle rule probabilities. Make sure that the resulting CNF assigns the same\ntotal probability to each parse tree.\n12.3 Recall that Exercise 13.3 asked you to update the CKY algorithm to han-\ndle unit productions directly rather than converting them to CNF. Extend this\nchange to probabilistic CKY.\n12.4 Fill out the rest of the probabilistic CKY chart in Fig. 12.4.",
  "277": "EXERCISES\n269\n12.5 Sketch how the CKY algorithm would have to be augmented to handle lexi-\ncalized probabilities.\n12.6 Implement your lexicalized extension of the CKY algorithm.\n12.7 Implement the PARSEVAL metrics described in Section 12.8. Next, either\nuse a treebank or create your own hand-checked parsed test set. Now use your\nCFG (or other) parser and grammar, parse the test set and compute labeled\nrecall, labeled precision, and cross-brackets.",
  "278": "270\nCHAPTER 13\n•\nDEPENDENCY PARSING\nCHAPTER\n13\nDependency Parsing\nThe focus of the three previous chapters has been on context-free grammars and\ntheir use in automatically generating constituent-based representations. Here we\npresent another family of grammar formalisms called dependency grammars that\ndependency\ngrammars\nare quite important in contemporary speech and language processing systems. In\nthese formalisms, phrasal constituents and phrase-structure rules do not play a direct\nrole. Instead, the syntactic structure of a sentence is described solely in terms of the\nwords (or lemmas) in a sentence and an associated set of directed binary grammatical\nrelations that hold among the words.\nThe following diagram illustrates a dependency-style analysis using the standard\ngraphical method favored in the dependency-parsing community.\n(13.1)\nI prefer the morning ﬂight through Denver\nnsubj\ndobj\ndet\nnmod\nnmod\ncase\nroot\nRelations among the words are illustrated above the sentence with directed, la-\nbeled arcs from heads to dependents. We call this a typed dependency structure\ntyped\ndependency\nbecause the labels are drawn from a ﬁxed inventory of grammatical relations. It also\nincludes a root node that explicitly marks the root of the tree, the head of the entire\nstructure.\nFigure 13.1 shows the same dependency analysis as a tree alongside its corre-\nsponding phrase-structure analysis of the kind given in Chapter 10. Note the ab-\nsence of nodes corresponding to phrasal constituents or lexical categories in the\ndependency parse; the internal structure of the dependency parse consists solely\nof directed relations between lexical items in the sentence. These relationships di-\nrectly encode important information that is often buried in the more complex phrase-\nstructure parses. For example, the arguments to the verb prefer are directly linked to\nit in the dependency structure, while their connection to the main verb is more dis-\ntant in the phrase-structure tree. Similarly, morning and Denver, modiﬁers of ﬂight,\nare linked to it directly in the dependency structure.\nA major advantage of dependency grammars is their ability to deal with lan-\nguages that are morphologically rich and have a relatively free word order. For\nfree word order\nexample, word order in Czech can be much more ﬂexible than in English; a gram-\nmatical object might occur before or after a location adverbial. A phrase-structure\ngrammar would need a separate rule for each possible place in the parse tree where\nsuch an adverbial phrase could occur. A dependency-based approach would just\nhave one link type representing this particular adverbial relation. Thus, a depen-\ndency grammar approach abstracts away from word-order information, representing\nonly the information that is necessary for the parse.\nAn additional practical motivation for a dependency-based approach is that the\nhead-dependent relations provide an approximation to the semantic relationship be-",
  "279": "13.1\n•\nDEPENDENCY RELATIONS\n271\nprefer\nﬂight\nDenver\nthrough\nmorning\nthe\nI\nS\nVP\nNP\nNom\nPP\nNP\nPro\nDenver\nP\nthrough\nNom\nNoun\nﬂight\nNom\nNoun\nmorning\nDet\nthe\nVerb\nprefer\nNP\nPro\nI\nFigure 13.1\nA dependency-style parse alongside the corresponding constituent-based analysis for I prefer the\nmorning ﬂight through Denver.\ntween predicates and their arguments that makes them directly useful for many ap-\nplications such as coreference resolution, question answering and information ex-\ntraction. Constituent-based approaches to parsing provide similar information, but it\noften has to be distilled from the trees via techniques such as the head ﬁnding rules\ndiscussed in Chapter 10.\nIn the following sections, we’ll discuss in more detail the inventory of relations\nused in dependency parsing, as well as the formal basis for these dependency struc-\ntures. We’ll then move on to discuss the dominant families of algorithms that are\nused to automatically produce these structures. Finally, we’ll discuss how to eval-\nuate dependency parsers and point to some of the ways they are used in language\nprocessing applications.\n13.1\nDependency Relations\nThe traditional linguistic notion of grammatical relation provides the basis for the\ngrammatical\nrelation\nbinary relations that comprise these dependency structures. The arguments to these\nrelations consist of a head and a dependent. We’ve already discussed the notion of\nhead\ndependent\nheads in Chapter 10 and Chapter 12 in the context of constituent structures. There,\nthe head word of a constituent was the central organizing word of a larger constituent\n(e.g, the primary noun in a noun phrase, or verb in a verb phrase). The remaining\nwords in the constituent are either direct, or indirect, dependents of their head. In\ndependency-based approaches, the head-dependent relationship is made explicit by\ndirectly linking heads to the words that are immediately dependent on them, bypass-\ning the need for constituent structures.\nIn addition to specifying the head-dependent pairs, dependency grammars allow\nus to further classify the kinds of grammatical relations, or grammatical function,\ngrammatical\nfunction",
  "280": "272\nCHAPTER 13\n•\nDEPENDENCY PARSING\nClausal Argument Relations Description\nNSUBJ\nNominal subject\nDOBJ\nDirect object\nIOBJ\nIndirect object\nCCOMP\nClausal complement\nXCOMP\nOpen clausal complement\nNominal Modiﬁer Relations\nDescription\nNMOD\nNominal modiﬁer\nAMOD\nAdjectival modiﬁer\nNUMMOD\nNumeric modiﬁer\nAPPOS\nAppositional modiﬁer\nDET\nDeterminer\nCASE\nPrepositions, postpositions and other case markers\nOther Notable Relations\nDescription\nCONJ\nConjunct\nCC\nCoordinating conjunction\nFigure 13.2\nSelected dependency relations from the Universal Dependency set. (de Marn-\neffe et al., 2014)\nin terms of the role that the dependent plays with respect to its head. Familiar notions\nsuch as subject, direct object and indirect object are among the kind of relations we\nhave in mind. In English these notions strongly correlate with, but by no means de-\ntermine, both position in a sentence and constituent type and are therefore somewhat\nredundant with the kind of information found in phrase-structure trees. However, in\nmore ﬂexible languages the information encoded directly in these grammatical rela-\ntions is critical since phrase-based constituent syntax provides little help.\nNot surprisingly, linguists have developed taxonomies of relations that go well\nbeyond the familiar notions of subject and object. While there is considerable vari-\nation from theory to theory, there is enough commonality that efforts to develop a\ncomputationally useful standard are now possible. The Universal Dependencies\nUniversal\nDependencies\nproject (Nivre et al., 2016b) provides an inventory of dependency relations that are\nlinguistically motivated, computationally useful, and cross-linguistically applicable.\nFig. 13.2 shows a subset of the relations from this effort. Fig. 13.3 provides some\nexample sentences illustrating selected relations.\nThe motivation for all of the relations in the Universal Dependency scheme is\nbeyond the scope of this chapter, but the core set of frequently used relations can be\nbroken into two sets: clausal relations that describe syntactic roles with respect to a\npredicate (often a verb), and modiﬁer relations that categorize the ways that words\nthat can modify their heads.\nConsider the following example sentence:\n(13.2)\nUnited canceled the morning ﬂights to Houston\nnsubj\ndobj\ndet\nnmod\nnmod\ncase\nroot\nThe clausal relations NSUBJ and DOBJ identify the subject and direct object of\nthe predicate cancel, while the NMOD, DET, and CASE relations denote modiﬁers of\nthe nouns ﬂights and Houston.",
  "281": "13.2\n•\nDEPENDENCY FORMALISMS\n273\nRelation\nExamples with head and dependent\nNSUBJ\nUnited canceled the ﬂight.\nDOBJ\nUnited diverted the ﬂight to Reno.\nWe booked her the ﬁrst ﬂight to Miami.\nIOBJ\nWe booked her the ﬂight to Miami.\nNMOD\nWe took the morning ﬂight.\nAMOD\nBook the cheapest ﬂight.\nNUMMOD\nBefore the storm JetBlue canceled 1000 ﬂights.\nAPPOS\nUnited, a unit of UAL, matched the fares.\nDET\nThe ﬂight was canceled.\nWhich ﬂight was delayed?\nCONJ\nWe ﬂew to Denver and drove to Steamboat.\nCC\nWe ﬂew to Denver and drove to Steamboat.\nCASE\nBook the ﬂight through Houston.\nFigure 13.3\nExamples of core Universal Dependency relations.\n13.2\nDependency Formalisms\nIn their most general form, the dependency structures we’re discussing are simply\ndirected graphs. That is, structures G = (V,A) consisting of a set of vertices V, and\na set of ordered pairs of vertices A, which we’ll refer to as arcs.\nFor the most part we will assume that the set of vertices, V, corresponds exactly\nto the set of words in a given sentence. However, they might also correspond to\npunctuation, or when dealing with morphologically complex languages the set of\nvertices might consist of stems and afﬁxes. The set of arcs, A, captures the head-\ndependent and grammatical function relationships between the elements in V.\nFurther constraints on these dependency structures are speciﬁc to the underlying\ngrammatical theory or formalism. Among the more frequent restrictions are that the\nstructures must be connected, have a designated root node, and be acyclic or planar.\nOf most relevance to the parsing approaches discussed in this chapter is the common,\ncomputationally-motivated, restriction to rooted trees. That is, a dependency tree\ndependency\ntree\nis a directed graph that satisﬁes the following constraints:\n1. There is a single designated root node that has no incoming arcs.\n2. With the exception of the root node, each vertex has exactly one incoming arc.\n3. There is a unique path from the root node to each vertex in V.\nTaken together, these constraints ensure that each word has a single head, that the\ndependency structure is connected, and that there is a single root node from which\none can follow a unique directed path to each of the words in the sentence.\n13.2.1\nProjectivity\nThe notion of projectivity imposes an additional constraint that is derived from the\norder of the words in the input, and is closely related to the context-free nature of\nhuman languages discussed in Chapter 10. An arc from a head to a dependent is\nsaid to be projective if there is a path from the head to every word that lies between\nthe head and the dependent in the sentence. A dependency tree is then said to be\nprojective if all the arcs that make it up are projective. All the dependency trees\nwe’ve seen thus far have been projective. There are, however, many perfectly valid",
  "282": "274\nCHAPTER 13\n•\nDEPENDENCY PARSING\nconstructions which lead to non-projective trees, particularly in languages with a\nrelatively ﬂexible word order.\nConsider the following example.\n(13.3)\nJetBlue canceled our ﬂight this morning which was already late\nnsubj\ndobj\nmod\ndet\nnmod\ndet\ncase\nmod\nadv\nroot\nIn this example, the arc from ﬂight to its modiﬁer was is non-projective since\nthere is no path from ﬂight to the intervening words this and morning. As we can\nsee from this diagram, projectivity (and non-projectivity) can be detected in the way\nwe’ve been drawing our trees. A dependency tree is projective if it can be drawn\nwith no crossing edges. Here there is no way to link ﬂight to its dependent was\nwithout crossing the arc that links morning to its head.\nOur concern with projectivity arises from two related issues. First, the most\nwidely used English dependency treebanks were automatically derived from phrase-\nstructure treebanks through the use of head-ﬁnding rules (Chapter 10). The trees\ngenerated in such a fashion are guaranteed to be projective since they’re generated\nfrom context-free grammars.\nSecond, there are computational limitations to the most widely used families of\nparsing algorithms. The transition-based approaches discussed in Section 13.4 can\nonly produce projective trees, hence any sentences with non-projective structures\nwill necessarily contain some errors. This limitation is one of the motivations for\nthe more ﬂexible graph-based parsing approach described in Section 13.5.\n13.3\nDependency Treebanks\nAs with constituent-based methods, treebanks play a critical role in the development\nand evaluation of dependency parsers. Dependency treebanks have been created\nusing similar approaches to those discussed in Chapter 10 — having human annota-\ntors directly generate dependency structures for a given corpus, or using automatic\nparsers to provide an initial parse and then having annotators hand correct those\nparsers. We can also use a deterministic process to translate existing constituent-\nbased treebanks into dependency trees through the use of head rules.\nFor the most part, directly annotated dependency treebanks have been created for\nmorphologically rich languages such as Czech, Hindi and Finnish that lend them-\nselves to dependency grammar approaches, with the Prague Dependency Treebank\n(Bejˇcek et al., 2013) for Czech being the most well-known effort. The major English\ndependency treebanks have largely been extracted from existing resources such as\nthe Wall Street Journal sections of the Penn Treebank(Marcus et al., 1993). The\nmore recent OntoNotes project (Hovy et al. 2006,Weischedel et al. 2011) extends\nthis approach going beyond traditional news text to include conversational telephone\nspeech, weblogs, usenet newsgroups, broadcast, and talk shows in English, Chinese\nand Arabic.\nThe translation process from constituent to dependency structures has two sub-\ntasks: identifying all the head-dependent relations in the structure and identifying\nthe correct dependency relations for these relations. The ﬁrst task relies heavily on",
  "283": "13.4\n•\nTRANSITION-BASED DEPENDENCY PARSING\n275\nthe use of head rules discussed in Chapter 10 ﬁrst developed for use in lexicalized\nprobabilistic parsers (Magerman 1994,Collins 1999,Collins 2003b). Here’s a simple\nand effective algorithm from Xia and Palmer (2001).\n1. Mark the head child of each node in a phrase structure, using the appropriate\nhead rules.\n2. In the dependency structure, make the head of each non-head child depend on\nthe head of the head-child.\nWhen a phrase-structure parse contains additional information in the form of\ngrammatical relations and function tags, as in the case of the Penn Treebank, these\ntags can be used to label the edges in the resulting tree. When applied to the parse\ntree in Fig. 13.4, this algorithm would produce the dependency structure in Fig. 13.4.\n(13.4)\nVinken will join the board as a nonexecutive director Nov 29\nsbj\naux\ndobj\nclr\ntmp\nnmod\ncase\nnmod\namod\nnum\nroot\nThe primary shortcoming of these extraction methods is that they are limited by\nthe information present in the original constituent trees. Among the most impor-\ntant issues are the failure to integrate morphological information with the phrase-\nstructure trees, the inability to easily represent non-projective structures, and the\nlack of internal structure to most noun-phrases, as reﬂected in the generally ﬂat\nrules used in most treebank grammars. For these reasons, outside of English, most\ndependency treebanks are developed directly using human annotators.\n13.4\nTransition-Based Dependency Parsing\nOur ﬁrst approach to dependency parsing is motivated by a stack-based approach\ncalled shift-reduce parsing originally developed for analyzing programming lan-\nshift-reduce\nparsing\nguages (Aho and Ullman, 1972). This classic approach is simple and elegant, em-\nploying a context-free grammar, a stack, and a list of tokens to be parsed. Input\ntokens are successively shifted onto the stack and the top two elements of the stack\nare matched against the right-hand side of the rules in the grammar; when a match is\nfound the matched elements are replaced on the stack (reduced) by the non-terminal\nfrom the left-hand side of the rule being matched. In adapting this approach for\ndependency parsing, we forgo the explicit use of a grammar and alter the reduce\noperation so that instead of adding a non-terminal to a parse tree, it introduces a\ndependency relation between a word and its head. More speciﬁcally, the reduce ac-\ntion is replaced with two possible actions: assert a head-dependent relation between\nthe word at the top of the stack and the word below it, or vice versa. Figure 13.5\nillustrates the basic operation of such a parser.\nA key element in transition-based parsing is the notion of a conﬁguration which\nconﬁguration\nconsists of a stack, an input buffer of words, or tokens, and a set of relations rep-\nresenting a dependency tree. Given this framework, the parsing process consists of\na sequence of transitions through the space of possible conﬁgurations. The goal of",
  "284": "276\nCHAPTER 13\n•\nDEPENDENCY PARSING\nS\nVP\nVP\nNP-TMP\nCD\n29\nNNP\nNov\nPP-CLR\nNP\nNN\ndirector\nJJ\nnonexecutive\nDT\na\nIN\nas\nNP\nNN\nboard\nDT\nthe\nVB\njoin\nMD\nwill\nNP-SBJ\nNNP\nVinken\nS(join)\nVP(join)\nVP(join)\nNP-TMP(29)\nCD\n29\nNNP\nNov\nPP-CLR(director)\nNP(director)\nNN\ndirector\nJJ\nnonexecutive\nDT\na\nIN\nas\nNP(board)\nNN\nboard\nDT\nthe\nVB\njoin\nMD\nwill\nNP-SBJ(Vinken)\nNNP\nVinken\njoin\n29\nNov\ndirector\nnonexecutive\na\nas\nboard\nthe\nwill\nVinken\nFigure 13.4\nA phrase-structure tree from the Wall Street Journal component of the Penn Treebank 3.\nthis process is to ﬁnd a ﬁnal conﬁguration where all the words have been accounted\nfor and an appropriate dependency tree has been synthesized.\nTo implement such a search, we’ll deﬁne a set of transition operators, which\nwhen applied to a conﬁguration produce new conﬁgurations. Given this setup, we\ncan view the operation of a parser as a search through a space of conﬁgurations for\na sequence of transitions that leads from a start state to a desired goal state. At the\nstart of this process we create an initial conﬁguration in which the stack contains the",
  "285": "13.4\n•\nTRANSITION-BASED DEPENDENCY PARSING\n277\nDependency\nRelations\nwn\nw1\nw2\ns2\n...\ns1\nsn\nParser\nInput buffer\nStack\nOracle\nFigure 13.5\nBasic transition-based parser. The parser examines the top two elements of the\nstack and selects an action based on consulting an oracle that examines the current conﬁgura-\ntion.\nROOT node, the word list is initialized with the set of the words or lemmatized tokens\nin the sentence, and an empty set of relations is created to represent the parse. In the\nﬁnal goal state, the stack and the word list should be empty, and the set of relations\nwill represent the ﬁnal parse.\nIn the standard approach to transition-based parsing, the operators used to pro-\nduce new conﬁgurations are surprisingly simple and correspond to the intuitive ac-\ntions one might take in creating a dependency tree by examining the words in a\nsingle pass over the input from left to right (Covington, 2001):\n• Assign the current word as the head of some previously seen word,\n• Assign some previously seen word as the head of the current word,\n• Or postpone doing anything with the current word, adding it to a store for later\nprocessing.\nTo make these actions more precise, we’ll create three transition operators that\nwill operate on the top two elements of the stack:\n• LEFTARC: Assert a head-dependent relation between the word at the top of\nstack and the word directly beneath it; remove the lower word from the stack.\n• RIGHTARC: Assert a head-dependent relation between the second word on\nthe stack and the word at the top; remove the word at the top of the stack;\n• SHIFT: Remove the word from the front of the input buffer and push it onto\nthe stack.\nThis particular set of operators implements what is known as the arc standard\narc standard\napproach to transition-based parsing (Covington 2001,Nivre 2003). There are two\nnotable characteristics to this approach: the transition operators only assert relations\nbetween elements at the top of the stack, and once an element has been assigned\nits head it is removed from the stack and is not available for further processing.\nAs we’ll see, there are alternative transition systems which demonstrate different\nparsing behaviors, but the arc standard approach is quite effective and is simple to\nimplement.",
  "286": "278\nCHAPTER 13\n•\nDEPENDENCY PARSING\nTo assure that these operators are used properly we’ll need to add some pre-\nconditions to their use. First, since, by deﬁnition, the ROOT node cannot have any\nincoming arcs, we’ll add the restriction that the LEFTARC operator cannot be ap-\nplied when ROOT is the second element of the stack. Second, both reduce operators\nrequire two elements to be on the stack to be applied. Given these transition opera-\ntors and preconditions, the speciﬁcation of a transition-based parser is quite simple.\nFig. 13.6 gives the basic algorithm.\nfunction DEPENDENCYPARSE(words) returns dependency tree\nstate←{[root], [words], [] } ; initial conﬁguration\nwhile state not ﬁnal\nt←ORACLE(state)\n; choose a transition operator to apply\nstate←APPLY(t, state) ; apply it, creating a new state\nreturn state\nFigure 13.6\nA generic transition-based dependency parser\nAt each step, the parser consults an oracle (we’ll come back to this shortly) that\nprovides the correct transition operator to use given the current conﬁguration. It then\napplies that operator to the current conﬁguration, producing a new conﬁguration.\nThe process ends when all the words in the sentence have been consumed and the\nROOT node is the only element remaining on the stack.\nThe efﬁciency of transition-based parsers should be apparent from the algorithm.\nThe complexity is linear in the length of the sentence since it is based on a single left\nto right pass through the words in the sentence. More speciﬁcally, each word must\nﬁrst be shifted onto the stack and then later reduced.\nNote that unlike the dynamic programming and search-based approaches dis-\ncussed in Chapters 12 and 13, this approach is a straightforward greedy algorithm\n— the oracle provides a single choice at each step and the parser proceeds with that\nchoice, no other options are explored, no backtracking is employed, and a single\nparse is returned in the end.\nFigure 13.7 illustrates the operation of the parser with the sequence of transitions\nleading to a parse for the following example.\n(13.5)\nBook me the morning ﬂight\ndobj\niobj\ndet\nnmod\nroot\nLet’s consider the state of the conﬁguration at Step 2, after the word me has been\npushed onto the stack.\nStack\nWord List\nRelations\n[root, book, me] [the, morning, ﬂight]\nThe correct operator to apply here is RIGHTARC which assigns book as the head of\nme and pops me from the stack resulting in the following conﬁguration.\nStack\nWord List\nRelations\n[root, book] [the, morning, ﬂight] (book →me)",
  "287": "13.4\n•\nTRANSITION-BASED DEPENDENCY PARSING\n279\nStep\nStack\nWord List\nAction\nRelation Added\n0\n[root]\n[book, me, the, morning, ﬂight]\nSHIFT\n1\n[root, book]\n[me, the, morning, ﬂight]\nSHIFT\n2\n[root, book, me]\n[the, morning, ﬂight]\nRIGHTARC\n(book →me)\n3\n[root, book]\n[the, morning, ﬂight]\nSHIFT\n4\n[root, book, the]\n[morning, ﬂight]\nSHIFT\n5\n[root, book, the, morning]\n[ﬂight]\nSHIFT\n6\n[root, book, the, morning, ﬂight]\n[]\nLEFTARC\n(morning ←ﬂight)\n7\n[root, book, the, ﬂight]\n[]\nLEFTARC\n(the ←ﬂight)\n8\n[root, book, ﬂight]\n[]\nRIGHTARC\n(book →ﬂight)\n9\n[root, book]\n[]\nRIGHTARC\n(root →book)\n10\n[root]\n[]\nDone\nFigure 13.7\nTrace of a transition-based parse.\nAfter several subsequent applications of the SHIFT and LEFTARC operators, the con-\nﬁguration in Step 6 looks like the following:\nStack\nWord List\nRelations\n[root, book, the, morning, ﬂight]\n[]\n(book →me)\nHere, all the remaining words have been passed onto the stack and all that is left\nto do is to apply the appropriate reduce operators. In the current conﬁguration, we\nemploy the LEFTARC operator resulting in the following state.\nStack\nWord List\nRelations\n[root, book, the, ﬂight]\n[]\n(book →me)\n(morning ←ﬂight)\nAt this point, the parse for this sentence consists of the following structure.\n(13.6)\nBook me the morning ﬂight\ndobj\nnmod\nThere are several important things to note when examining sequences such as\nthe one in Figure 13.7. First, the sequence given is not the only one that might lead\nto a reasonable parse. In general, there may be more than one path that leads to the\nsame result, and due to ambiguity, there may be other transition sequences that lead\nto different equally valid parses.\nSecond, we are assuming that the oracle always provides the correct operator\nat each point in the parse — an assumption that is unlikely to be true in practice.\nAs a result, given the greedy nature of this algorithm, incorrect choices will lead to\nincorrect parses since the parser has no opportunity to go back and pursue alternative\nchoices. Section 13.4.2 will introduce several techniques that allow transition-based\napproaches to explore the search space more fully.\nFinally, for simplicity, we have illustrated this example without the labels on\nthe dependency relations. To produce labeled trees, we can parameterize the LEFT-\nARC and RIGHTARC operators with dependency labels, as in LEFTARC(NSUBJ) or\nRIGHTARC(DOBJ). This is equivalent to expanding the set of transition operators\nfrom our original set of three to a set that includes LEFTARC and RIGHTARC opera-\ntors for each relation in the set of dependency relations being used, plus an additional\none for the SHIFT operator. This, of course, makes the job of the oracle more difﬁcult\nsince it now has a much larger set of operators from which to choose.",
  "288": "280\nCHAPTER 13\n•\nDEPENDENCY PARSING\n13.4.1\nCreating an Oracle\nState-of-the-art transition-based systems use supervised machine learning methods\nto train classiﬁers that play the role of the oracle. Given appropriate training data,\nthese methods learn a function that maps from conﬁgurations to transition operators.\nAs with all supervised machine learning methods, we will need access to appro-\npriate training data and we will need to extract features useful for characterizing the\ndecisions to be made. The source for this training data will be representative tree-\nbanks containing dependency trees. The features will consist of many of the same\nfeatures we encountered in Chapter 8 for part-of-speech tagging, as well as those\nused in Chapter 12 for statistical parsing models.\nGenerating Training Data\nLet’s revisit the oracle from the algorithm in Fig. 13.6 to fully understand the learn-\ning problem. The oracle takes as input a conﬁguration and returns as output a tran-\nsition operator. Therefore, to train a classiﬁer, we will need conﬁgurations paired\nwith transition operators (i.e., LEFTARC, RIGHTARC, or SHIFT). Unfortunately,\ntreebanks pair entire sentences with their corresponding trees, and therefore they\ndon’t directly provide what we need.\nTo generate the required training data, we will employ the oracle-based parsing\nalgorithm in a clever way. We will supply our oracle with the training sentences\nto be parsed along with their corresponding reference parses from the treebank. To\nproduce training instances, we will then simulate the operation of the parser by run-\nning the algorithm and relying on a new training oracle to give us correct transition\ntraining oracle\noperators for each successive conﬁguration.\nTo see how this works, let’s ﬁrst review the operation of our parser. It begins with\na default initial conﬁguration where the stack contains the ROOT, the input list is just\nthe list of words, and the set of relations is empty. The LEFTARC and RIGHTARC\noperators each add relations between the words at the top of the stack to the set of\nrelations being accumulated for a given sentence. Since we have a gold-standard\nreference parse for each training sentence, we know which dependency relations are\nvalid for a given sentence. Therefore, we can use the reference parse to guide the\nselection of operators as the parser steps through a sequence of conﬁgurations.\nTo be more precise, given a reference parse and a conﬁguration, the training\noracle proceeds as follows:\n• Choose LEFTARC if it produces a correct head-dependent relation given the\nreference parse and the current conﬁguration,\n• Otherwise, choose RIGHTARC if (1) it produces a correct head-dependent re-\nlation given the reference parse and (2) all of the dependents of the word at\nthe top of the stack have already been assigned,\n• Otherwise, choose SHIFT.\nThe restriction on selecting the RIGHTARC operator is needed to ensure that a\nword is not popped from the stack, and thus lost to further processing, before all its\ndependents have been assigned to it.\nMore formally, during training the oracle has access to the following informa-\ntion:\n• A current conﬁguration with a stack S and a set of dependency relations Rc\n• A reference parse consisting of a set of vertices V and a set of dependency\nrelations Rp",
  "289": "13.4\n•\nTRANSITION-BASED DEPENDENCY PARSING\n281\nStep\nStack\nWord List\nPredicted Action\n0\n[root]\n[book, the, ﬂight, through, houston]\nSHIFT\n1\n[root, book]\n[the, ﬂight, through, houston]\nSHIFT\n2\n[root, book, the]\n[ﬂight, through, houston]\nSHIFT\n3\n[root, book, the, ﬂight]\n[through, houston]\nLEFTARC\n4\n[root, book, ﬂight]\n[through, houston]\nSHIFT\n5\n[root, book, ﬂight, through]\n[houston]\nSHIFT\n6\n[root, book, ﬂight, through, houston]\n[]\nLEFTARC\n7\n[root, book, ﬂight, houston ]\n[]\nRIGHTARC\n8\n[root, book, ﬂight]\n[]\nRIGHTARC\n9\n[root, book]\n[]\nRIGHTARC\n10\n[root]\n[]\nDone\nFigure 13.8\nGenerating training items consisting of conﬁguration/predicted action pairs by\nsimulating a parse with a given reference parse.\nGiven this information, the oracle chooses transitions as follows:\nLEFTARC(r): if (S1 r S2) ∈Rp\nRIGHTARC(r): if (S2 r S1) ∈Rp and ∀r′,w s.t.(S1 r′ w) ∈Rp then (S1 r′ w) ∈\nRc\nSHIFT: otherwise\nLet’s walk through some the steps of this process with the following example as\nshown in Fig. 13.8.\n(13.7)\nBook the ﬂight through Houston\ndobj\ndet\nnmod\ncase\nroot\nAt Step 1, LEFTARC is not applicable in the initial conﬁguration since it asserts\na relation, (root ←book), not in the reference answer; RIGHTARC does assert a\nrelation contained in the ﬁnal answer (root →book), however book has not been\nattached to any of its dependents yet, so we have to defer, leaving SHIFT as the only\npossible action. The same conditions hold in the next two steps. In step 3, LEFTARC\nis selected to link the to its head.\nNow consider the situation in Step 4.\nStack\nWord buffer\nRelations\n[root, book, ﬂight] [through, Houston] (the ←ﬂight)\nHere, we might be tempted to add a dependency relation between book and ﬂight,\nwhich is present in the reference parse. But doing so now would prevent the later\nattachment of Houston since ﬂight would have been removed from the stack. For-\ntunately, the precondition on choosing RIGHTARC prevents this choice and we’re\nagain left with SHIFT as the only viable option. The remaining choices complete the\nset of operators needed for this example.\nTo recap, we derive appropriate training instances consisting of conﬁguration-\ntransition pairs from a treebank by simulating the operation of a parser in the con-\ntext of a reference dependency tree. We can deterministically record correct parser\nactions at each step as we progress through each training example, thereby creating\nthe training set we require.",
  "290": "282\nCHAPTER 13\n•\nDEPENDENCY PARSING\nFeatures\nHaving generated appropriate training instances (conﬁguration-transition pairs), we\nneed to extract useful features from the conﬁgurations so what we can train classi-\nﬁers. The features that are used to train transition-based systems vary by language,\ngenre, and the kind of classiﬁer being employed. For example, morphosyntactic\nfeatures such as case marking on subjects or direct objects may be more or less im-\nportant depending on the language being processed. That said, the basic features that\nwe have already seen with part-of-speech tagging and partial parsing have proven to\nbe useful in training dependency parsers across a wide range of languages. Word\nforms, lemmas and parts of speech are all powerful features, as are the head, and\ndependency relation to the head.\nIn the transition-based parsing framework, such features need to be extracted\nfrom the conﬁgurations that make up the training data. Recall that conﬁgurations\nconsist of three elements: the stack, the buffer and the current set of relations. In\nprinciple, any property of any or all of these elements can be represented as features\nin the usual way for training. However, to avoid sparsity and encourage generaliza-\ntion, it is best to focus the learning algorithm on the most useful aspects of decision\nmaking at each point in the parsing process. The focus of feature extraction for\ntransition-based parsing is, therefore, on the top levels of the stack, the words near\nthe front of the buffer, and the dependency relations already associated with any of\nthose elements.\nBy combining simple features, such as word forms or parts of speech, with spe-\nciﬁc locations in a conﬁguration, we can employ the notion of a feature template\nfeature\ntemplate\nthat we’ve already encountered with sentiment analysis and part-of-speech tagging.\nFeature templates allow us to automatically generate large numbers of speciﬁc fea-\ntures from a training set. As an example, consider the following feature templates\nthat are based on single positions in a conﬁguration.\n⟨s1.w,op⟩,⟨s2.w,op⟩⟨s1.t,op⟩,⟨s2.t,op⟩\n⟨b1.w,op⟩,⟨b1.t,op⟩⟨s1.wt,op⟩\n(13.8)\nIn these examples, individual features are denoted as location.property, where s\ndenotes the stack, b the word buffer, and r the set of relations. Individual properties\nof locations include w for word forms, l for lemmas, and t for part-of-speech. For\nexample, the feature corresponding to the word form at the top of the stack would be\ndenoted as s1.w, and the part of speech tag at the front of the buffer b1.t. We can also\ncombine individual features via concatenation into more speciﬁc features that may\nprove useful. For example, the feature designated by s1.wt represents the word form\nconcatenated with the part of speech of the word at the top of the stack. Finally, op\nstands for the transition operator for the training example in question (i.e., the label\nfor the training instance).\nLet’s consider the simple set of single-element feature templates given above\nin the context of the following intermediate conﬁguration derived from a training\noracle for Example 13.2.\nStack\nWord buffer\nRelations\n[root, canceled, ﬂights] [to Houston] (canceled →United)\n(ﬂights →morning)\n(ﬂights →the)\nThe correct transition here is SHIFT (you should convince yourself of this before",
  "291": "13.4\n•\nTRANSITION-BASED DEPENDENCY PARSING\n283\nproceeding). The application of our set of feature templates to this conﬁguration\nwould result in the following set of instantiated features.\n⟨s1.w = ﬂights,op = shift⟩\n(13.9)\n⟨s2.w = canceled,op = shift⟩\n⟨s1.t = NNS,op = shift⟩\n⟨s2.t = VBD,op = shift⟩\n⟨b1.w = to,op = shift⟩\n⟨b1.t = TO,op = shift⟩\n⟨s1.wt = ﬂightsNNS,op = shift⟩\nGiven that the left and right arc transitions operate on the top two elements of\nthe stack, features that combine properties from these positions are even more useful.\nFor example, a feature like s1.t ◦s2.t concatenates the part of speech tag of the word\nat the top of the stack with the tag of the word beneath it.\n⟨s1.t ◦s2.t = NNSVBD,op = shift⟩\n(13.10)\nNot surprisingly, if two properties are useful then three or more should be even\nbetter. Figure 13.9 gives a baseline set of feature templates that have been employed\nin various state-of-the-art systems (Zhang and Clark 2008,Huang and Sagae 2010,Zhang\nand Nivre 2011).\nNote that some of these features make use of dynamic features — features such\nas head words and dependency relations that have been predicted at earlier steps in\nthe parsing process, as opposed to features that are derived from static properties of\nthe input.\nSource\nFeature templates\nOne word s1.w\ns1.t\ns1.wt\ns2.w\ns2.t\ns2.wt\nb1.w\nb1.w\nb0.wt\nTwo word s1.w◦s2.w\ns1.t ◦s2.t\ns1.t ◦b1.w\ns1.t ◦s2.wt\ns1.w◦s2.w◦s2.t s1.w◦s1.t ◦s2.t\ns1.w◦s1.t ◦s2.t\ns1.w◦s1.t\nFigure 13.9\nStandard feature templates for training transition-based dependency parsers.\nIn the template speciﬁcations sn refers to a location on the stack, bn refers to a location in the\nword buffer, w refers to the wordform of the input, and t refers to the part of speech of the\ninput.\nLearning\nOver the years, the dominant approaches to training transition-based dependency\nparsers have been multinomial logistic regression and support vector machines, both\nof which can make effective use of large numbers of sparse features of the kind\ndescribed in the last section.\nMore recently, neural network, or deep learning,\napproaches of the kind described in Chapter 8 have been applied successfully to\ntransition-based parsing (Chen and Manning, 2014). These approaches eliminate the\nneed for complex, hand-crafted features and have been particularly effective at over-\ncoming the data sparsity issues normally associated with training transition-based\nparsers.",
  "292": "284\nCHAPTER 13\n•\nDEPENDENCY PARSING\n13.4.2\nAdvanced Methods in Transition-Based Parsing\nThe basic transition-based approach can be elaborated in a number of ways to im-\nprove performance by addressing some of the most obvious ﬂaws in the approach.\nAlternative Transition Systems\nThe arc-standard transition system described above is only one of many possible sys-\ntems. A frequently used alternative is the arc eager transition system. The arc eager\narc eager\napproach gets its name from its ability to assert rightward relations much sooner\nthan in the arc standard approach. To see this, let’s revisit the arc standard trace of\nExample 13.7, repeated here.\nBook the ﬂight through Houston\ndobj\ndet\nnmod\ncase\nroot\nConsider the dependency relation between book and ﬂight in this analysis. As\nis shown in Fig. 13.8, an arc-standard approach would assert this relation at Step 8,\ndespite the fact that book and ﬂight ﬁrst come together on the stack much earlier at\nStep 4. The reason this relation can’t be captured at this point is due to the presence\nof the post-nominal modiﬁer through Houston. In an arc-standard approach, depen-\ndents are removed from the stack as soon as they are assigned their heads. If ﬂight\nhad been assigned book as its head in Step 4, it would no longer be available to serve\nas the head of Houston.\nWhile this delay doesn’t cause any issues in this example, in general the longer\na word has to wait to get assigned its head the more opportunities there are for\nsomething to go awry. The arc-eager system addresses this issue by allowing words\nto be attached to their heads as early as possible, before all the subsequent words\ndependent on them have been seen. This is accomplished through minor changes to\nthe LEFTARC and RIGHTARC operators and the addition of a new REDUCE operator.\n• LEFTARC: Assert a head-dependent relation between the word at the front of\nthe input buffer and the word at the top of the stack; pop the stack.\n• RIGHTARC: Assert a head-dependent relation between the word on the top of\nthe stack and the word at front of the input buffer; shift the word at the front\nof the input buffer to the stack.\n• SHIFT: Remove the word from the front of the input buffer and push it onto\nthe stack.\n• REDUCE: Pop the stack.\nThe LEFTARC and RIGHTARC operators are applied to the top of the stack and\nthe front of the input buffer, instead of the top two elements of the stack as in the\narc-standard approach. The RIGHTARC operator now moves the dependent to the\nstack from the buffer rather than removing it, thus making it available to serve as the\nhead of following words. The new REDUCE operator removes the top element from\nthe stack. Together these changes permit a word to be eagerly assigned its head and\nstill allow it to serve as the head for later dependents. The trace shown in Fig. 13.10\nillustrates the new decision sequence for this example.\nIn addition to demonstrating the arc-eager transition system, this example demon-\nstrates the power and ﬂexibility of the overall transition-based approach. We were\nable to swap in a new transition system without having to make any changes to the",
  "293": "13.4\n•\nTRANSITION-BASED DEPENDENCY PARSING\n285\nStep\nStack\nWord List\nAction\nRelation Added\n0\n[root]\n[book, the, ﬂight, through, houston]\nRIGHTARC\n(root →book)\n1\n[root, book]\n[the, ﬂight, through, houston]\nSHIFT\n2\n[root, book, the]\n[ﬂight, through, houston]\nLEFTARC\n(the ←ﬂight)\n3\n[root, book]\n[ﬂight, through, houston]\nRIGHTARC\n(book →ﬂight)\n4\n[root, book, ﬂight]\n[through, houston]\nSHIFT\n5\n[root, book, ﬂight, through]\n[houston]\nLEFTARC\n(through ←houston)\n6\n[root, book, ﬂight]\n[houston]\nRIGHTARC\n(ﬂight →houston)\n7\n[root, book, ﬂight, houston]\n[]\nREDUCE\n8\n[root, book, ﬂight]\n[]\nREDUCE\n9\n[root, book]\n[]\nREDUCE\n10\n[root]\n[]\nDone\nFigure 13.10\nA processing trace of Book the ﬂight through Houston using the arc-eager\ntransition operators.\nunderlying parsing algorithm. This ﬂexibility has led to the development of a di-\nverse set of transition systems that address different aspects of syntax and semantics\nincluding: assigning part of speech tags (Choi and Palmer, 2011a), allowing the\ngeneration of non-projective dependency structures (Nivre, 2009), assigning seman-\ntic roles (Choi and Palmer, 2011b), and parsing texts containing multiple languages\n(Bhat et al., 2017).\nBeam Search\nThe computational efﬁciency of the transition-based approach discussed earlier de-\nrives from the fact that it makes a single pass through the sentence, greedily making\ndecisions without considering alternatives. Of course, this is also the source of its\ngreatest weakness – once a decision has been made it can not be undone, even in\nthe face of overwhelming evidence arriving later in a sentence. Another approach\nis to systematically explore alternative decision sequences, selecting the best among\nthose alternatives. The key problem for such a search is to manage the large number\nof potential sequences. Beam search accomplishes this by combining a breadth-ﬁrst\nBeam search\nsearch strategy with a heuristic ﬁlter that prunes the search frontier to stay within a\nﬁxed-size beam width.\nbeam width\nIn applying beam search to transition-based parsing, we’ll elaborate on the al-\ngorithm given in Fig. 13.6. Instead of choosing the single best transition operator\nat each iteration, we’ll apply all applicable operators to each state on an agenda and\nthen score the resulting conﬁgurations. We then add each of these new conﬁgura-\ntions to the frontier, subject to the constraint that there has to be room within the\nbeam. As long as the size of the agenda is within the speciﬁed beam width, we can\nadd new conﬁgurations to the agenda. Once the agenda reaches the limit, we only\nadd new conﬁgurations that are better than the worst conﬁguration on the agenda\n(removing the worst element so that we stay within the limit). Finally, to insure that\nwe retrieve the best possible state on the agenda, the while loop continues as long as\nthere are non-ﬁnal states on the agenda.\nThe beam search approach requires a more elaborate notion of scoring than we\nused with the greedy algorithm. There, we assumed that a classiﬁer trained using\nsupervised machine learning would serve as an oracle, selecting the best transition\noperator based on features extracted from the current conﬁguration. Regardless of\nthe speciﬁc learning approach, this choice can be viewed as assigning a score to all\nthe possible transitions and picking the best one.\nˆT(c) = argmaxScore(t,c)",
  "294": "286\nCHAPTER 13\n•\nDEPENDENCY PARSING\nWith a beam search we are now searching through the space of decision se-\nquences, so it makes sense to base the score for a conﬁguration on its entire history.\nMore speciﬁcally, we can deﬁne the score for a new conﬁguration as the score of its\npredecessor plus the score of the operator used to produce it.\nConﬁgScore(c0) = 0.0\nConﬁgScore(ci) = ConﬁgScore(ci−1)+Score(ti,ci−1)\nThis score is used both in ﬁltering the agenda and in selecting the ﬁnal answer.\nThe new beam search version of transition-based parsing is given in Fig. 13.11.\nfunction DEPENDENCYBEAMPARSE(words, width) returns dependency tree\nstate←{[root], [words], [], 0.0}\n;initial conﬁguration\nagenda←⟨state⟩;\ninitial agenda\nwhile agenda contains non-ﬁnal states\nnewagenda←⟨⟩\nfor each state ∈agenda do\nfor all {t | t ∈VALIDOPERATORS(state)} do\nchild←APPLY(t, state)\nnewagenda←ADDTOBEAM(child, newagenda, width)\nagenda←newagenda\nreturn BESTOF(agenda)\nfunction ADDTOBEAM(state, agenda, width) returns updated agenda\nif LENGTH(agenda) < width then\nagenda←INSERT(state, agenda)\nelse if SCORE(state) > SCORE(WORSTOF(agenda))\nagenda←REMOVE(WORSTOF(agenda))\nagenda←INSERT(state, agenda)\nreturn agenda\nFigure 13.11\nBeam search applied to transition-based dependency parsing.\n13.5\nGraph-Based Dependency Parsing\nGraph-based approaches to dependency parsing search through the space of possible\ntrees for a given sentence for a tree (or trees) that maximize some score. These\nmethods encode the search space as directed graphs and employ methods drawn\nfrom graph theory to search the space for optimal solutions. More formally, given a\nsentence S we’re looking for the best dependency tree in Gs, the space of all possible\ntrees for that sentence, that maximizes some score.\nˆT(S) = argmax\nt∈GS\nscore(t,S)\nAs with the probabilistic approaches to context-free parsing discussed in Chap-\nter 12, the overall score for a tree can be viewed as a function of the scores of the\nparts of the tree. The focus of this section is on edge-factored approaches where the\nedge-factored",
  "295": "13.5\n•\nGRAPH-BASED DEPENDENCY PARSING\n287\nscore for a tree is based on the scores of the edges that comprise the tree.\nscore(t,S) =\nX\ne∈t\nscore(e)\nThere are several motivations for the use of graph-based methods. First, unlike\ntransition-based approaches, these methods are capable of producing non-projective\ntrees. Although projectivity is not a signiﬁcant issue for English, it is deﬁnitely a\nproblem for many of the world’s languages. A second motivation concerns parsing\naccuracy, particularly with respect to longer dependencies. Empirically, transition-\nbased methods have high accuracy on shorter dependency relations but accuracy de-\nclines signiﬁcantly as the distance between the head and dependent increases (Mc-\nDonald and Nivre, 2011). Graph-based methods avoid this difﬁculty by scoring\nentire trees, rather than relying on greedy local decisions.\nThe following section examines a widely-studied approach based on the use of a\nmaximum spanning tree (MST) algorithm for weighted, directed graphs. We then\nmaximum\nspanning tree\ndiscuss features that are typically used to score trees, as well as the methods used to\ntrain the scoring models.\n13.5.1\nParsing\nThe approach described here uses an efﬁcient greedy algorithm to search for optimal\nspanning trees in directed graphs. Given an input sentence, it begins by constructing\na fully-connected, weighted, directed graph where the vertices are the input words\nand the directed edges represent all possible head-dependent assignments. An addi-\ntional ROOT node is included with outgoing edges directed at all of the other vertices.\nThe weights in the graph reﬂect the score for each possible head-dependent relation\nas provided by a model generated from training data. Given these weights, a maxi-\nmum spanning tree of this graph emanating from the ROOT represents the preferred\ndependency parse for the sentence. A directed graph for the example Book that\nﬂight is shown in Fig. 13.12, with the maximum spanning tree corresponding to the\ndesired parse shown in blue. For ease of exposition, we’ll focus here on unlabeled\ndependency parsing. Graph-based approaches to labeled parsing are discussed in\nSection 13.5.3.\nBefore describing the algorithm it’s useful to consider two intuitions about di-\nrected graphs and their spanning trees. The ﬁrst intuition begins with the fact that\nevery vertex in a spanning tree has exactly one incoming edge. It follows from this\nthat every connected component of a spanning tree will also have one incoming edge.\nThe second intuition is that the absolute values of the edge scores are not critical to\ndetermining its maximum spanning tree. Instead, it is the relative weights of the\nedges entering each vertex that matters. If we were to subtract a constant amount\nfrom each edge entering a given vertex it would have no impact on the choice of\nthe maximum spanning tree since every possible spanning tree would decrease by\nexactly the same amount.\nThe ﬁrst step of the algorithm itself is quite straightforward. For each vertex\nin the graph, an incoming edge (representing a possible head assignment) with the\nhighest score is chosen. If the resulting set of edges produces a spanning tree then\nwe’re done. More formally, given the original fully-connected graph G = (V,E), a\nsubgraph T = (V,F) is a spanning tree if it has no cycles and each vertex (other than\nthe root) has exactly one edge entering it. If the greedy selection process produces\nsuch a tree then it is the best possible one.",
  "296": "288\nCHAPTER 13\n•\nDEPENDENCY PARSING\nroot\nBook\nthat\nﬂight\n12\n4\n4\n5\n6\n8\n7\n5\n7\nFigure 13.12\nInitial rooted, directed graph for Book that ﬂight.\nUnfortunately, this approach doesn’t always lead to a tree since the set of edges\nselected may contain cycles. Fortunately, in yet another case of multiple discovery,\nthere is a straightforward way to eliminate cycles generated during the greedy se-\nlection phase. Chu and Liu (1965) and Edmonds (1967) independently developed\nan approach that begins with greedy selection and follows with an elegant recursive\ncleanup phase that eliminates cycles.\nThe cleanup phase begins by adjusting all the weights in the graph by subtracting\nthe score of the maximum edge entering each vertex from the score of all the edges\nentering that vertex. This is where the intuitions mentioned earlier come into play.\nWe have scaled the values of the edges so that the weight of the edges in the cycle\nhave no bearing on the weight of any of the possible spanning trees. Subtracting the\nvalue of the edge with maximum weight from each edge entering a vertex results\nin a weight of zero for all of the edges selected during the greedy selection phase,\nincluding all of the edges involved in the cycle.\nHaving adjusted the weights, the algorithm creates a new graph by selecting a\ncycle and collapsing it into a single new node. Edges that enter or leave the cycle\nare altered so that they now enter or leave the newly collapsed node. Edges that do\nnot touch the cycle are included and edges within the cycle are dropped.\nNow, if we knew the maximum spanning tree of this new graph, we would have\nwhat we need to eliminate the cycle. The edge of the maximum spanning tree di-\nrected towards the vertex representing the collapsed cycle tells us which edge to\ndelete to eliminate the cycle. How do we ﬁnd the maximum spanning tree of this\nnew graph? We recursively apply the algorithm to the new graph. This will either\nresult in a spanning tree or a graph with a cycle. The recursions can continue as long\nas cycles are encountered. When each recursion completes we expand the collapsed\nvertex, restoring all the vertices and edges from the cycle with the exception of the\nsingle edge to be deleted.\nPutting all this together, the maximum spanning tree algorithm consists of greedy\nedge selection, re-scoring of edge costs and a recursive cleanup phase when needed.\nThe full algorithm is shown in Fig. 13.13.\nFig. 13.14 steps through the algorithm with our Book that ﬂight example. The\nﬁrst row of the ﬁgure illustrates greedy edge selection with the edges chosen shown",
  "297": "13.5\n•\nGRAPH-BASED DEPENDENCY PARSING\n289\nfunction MAXSPANNINGTREE(G=(V,E), root,score) returns spanning tree\nF←[]\nT’←[]\nscore’←[]\nfor each v ∈V do\nbestInEdge←argmaxe=(u,v)∈E score[e]\nF←F ∪bestInEdge\nfor each e=(u,v) ∈E do\nscore’[e]←score[e] −score[bestInEdge]\nif T=(V,F) is a spanning tree then return it\nelse\nC←a cycle in F\nG’←CONTRACT(G,C)\nT’←MAXSPANNINGTREE(G’,root,score’)\nT←EXPAND(T’, C)\nreturn T\nfunction CONTRACT(G,C) returns contracted graph\nfunction EXPAND(T, C) returns expanded graph\nFigure 13.13\nThe Chu-Liu Edmonds algorithm for ﬁnding a maximum spanning tree in a\nweighted directed graph.\nin blue (corresponding to the set F in the algorithm). This results in a cycle between\nthat and ﬂight. The scaled weights using the maximum value entering each node are\nshown in the graph to the right.\nCollapsing the cycle between that and ﬂight to a single node (labelled tf) and\nrecursing with the newly scaled costs is shown in the second row. The greedy selec-\ntion step in this recursion yields a spanning tree that links root to book, as well as an\nedge that links book to the contracted node. Expanding the contracted node, we can\nsee that this edge corresponds to the edge from book to ﬂight in the original graph.\nThis in turn tells us which edge to drop to eliminate the cycle\nOn arbitrary directed graphs, this version of the CLE algorithm runs in O(mn)\ntime, where m is the number of edges and n is the number of nodes. Since this par-\nticular application of the algorithm begins by constructing a fully connected graph\nm = n2 yielding a running time of O(n3). Gabow et al. (1986) present a more efﬁ-\ncient implementation with a running time of O(m+nlogn).\n13.5.2\nFeatures and Training\nGiven a sentence, S, and a candidate tree, T, edge-factored parsing models reduce\nthe score for the tree to a sum of the scores of the edges that comprise the tree.\nscore(S,T) =\nX\ne∈T\nscore(S,e)\nEach edge score can, in turn, be reduced to a weighted sum of features extracted\nfrom it.\nscore(S,e) =\nN\nX\ni=1\nwi fi(S,e)",
  "298": "290\nCHAPTER 13\n•\nDEPENDENCY PARSING\nroot\nBook\ntf\nroot\nBook\nthat\nﬂight\n0\n-3\n-4\n-7\n-1\n-6\n-2\nroot\nBook\n12\nthat\n7\nﬂight\n8\n-4\n-3\n0\n-2\n-6\n-1\n-7\n0\n0\nroot\nBook\n0\ntf\n-1\n0\n-3\n-4\n-7\n-1\n-6\n-2\nroot\nBook\n12\nthat\n7\nﬂight\n8\n12\n4\n4\n5\n6\n8\n7\n5\n7\nDeleted from cycle\nFigure 13.14\nChu-Liu-Edmonds graph-based example for Book that ﬂight\nOr more succinctly.\nscore(S,e) = w· f\nGiven this formulation, we are faced with two problems in training our parser:\nidentifying relevant features and ﬁnding the weights used to score those features.\nThe features used to train edge-factored models mirror those used in training\ntransition-based parsers (as shown in Fig. 13.9). This is hardly surprising since in\nboth cases we’re trying to capture information about the relationship between heads\nand their dependents in the context of a single relation. To summarize this earlier\ndiscussion, commonly used features include:\n• Wordforms, lemmas, and parts of speech of the headword and its dependent.\n• Corresponding features derived from the contexts before, after and between\nthe words.\n• Word embeddings.\n• The dependency relation itself.\n• The direction of the relation (to the right or left).\n• The distance from the head to the dependent.\nAs with transition-based approaches, pre-selected combinations of these features are\noften used as well.\nGiven a set of features, our next problem is to learn a set of weights correspond-\ning to each. Unlike many of the learning problems discussed in earlier chapters,",
  "299": "13.6\n•\nEVALUATION\n291\nhere we are not training a model to associate training items with class labels, or\nparser actions. Instead, we seek to train a model that assigns higher scores to cor-\nrect trees than to incorrect ones. An effective framework for problems like this is to\nuse inference-based learning combined with the perceptron learning rule. In this\ninference-based\nlearning\nframework, we parse a sentence (i.e, perform inference) from the training set using\nsome initially random set of initial weights. If the resulting parse matches the cor-\nresponding tree in the training data, we do nothing to the weights. Otherwise, we\nﬁnd those features in the incorrect parse that are not present in the reference parse\nand we lower their weights by a small amount based on the learning rate. We do this\nincrementally for each sentence in our training data until the weights converge.\nMore recently, recurrent neural network (RNN) models have demonstrated state-\nof-the-art performance in shared tasks on multilingual parsing (Zeman et al. 2017,Dozat\net al. 2017). These neural approaches rely solely on lexical information in the form\nof word embeddings, eschewing the use of hand-crafted features such as those de-\nscribed earlier.\n13.5.3\nAdvanced Issues in Graph-Based Parsing\n13.6\nEvaluation\nAs with phrase structure-based parsing, the evaluation of dependency parsers pro-\nceeds by measuring how well they work on a test-set. An obvious metric would be\nexact match (EM) — how many sentences are parsed correctly. This metric is quite\npessimistic, with most sentences being marked wrong. Such measures are not ﬁne-\ngrained enough to guide the development process. Our metrics need to be sensitive\nenough to tell if actual improvements are being made.\nFor these reasons, the most common method for evaluating dependency parsers\nare labeled and unlabeled attachment accuracy. Labeled attachment refers to the\nproper assignment of a word to its head along with the correct dependency relation.\nUnlabeled attachment simply looks at the correctness of the assigned head, ignor-\ning the dependency relation. Given a system output and a corresponding reference\nparse, accuracy is simply the percentage of words in an input that are assigned the\ncorrect head with the correct relation. This metrics are usually referred to as the\nlabeled attachment score (LAS) and unlabeled attachment score (UAS). Finally, we\ncan make use of a label accuracy score (LS), the percentage of tokens with correct\nlabels, ignoring where the relations are coming from.\nAs an example, consider the reference parse and system parse for the following\nexample shown in Fig. 13.15.\n(13.11) Book me the ﬂight through Houston.\nThe system correctly ﬁnds 4 of the 6 dependency relations present in the refer-\nence parse and therefore receives an LAS of 2/3. However, one of the 2 incorrect\nrelations found by the system holds between book and ﬂight, which are in a head-\ndependent relation in the reference parse; therefore the system therefore achieves an\nUAS of 5/6.\nBeyond attachment scores, we may also be interested in how well a system is\nperforming on a particular kind of dependency relation, for example NSUBJ, across\na development corpus. Here we can make use of the notions of precision and recall\nintroduced in Chapter 8, measuring the percentage of relations labeled NSUBJ by\nthe system that were correct (precision), and the percentage of the NSUBJ relations",
  "300": "292\nCHAPTER 13\n•\nDEPENDENCY PARSING\nBook me the\nﬂight\nthrough Houston\nReference\nobj\niobj\ndet\nnmod\ncase\nroot\nBook me the ﬂight through Houston\nSystem\nx-comp\nnsubj\ndet\nnmod\ncase\nroot\nFigure 13.15\nReference and system parses for Book me the ﬂight through Houston, resulting in an LAS of\n4/6 and an UAS of 5/6.\npresent in the development set that were in fact discovered by the system (recall).\nWe can employ a confusion matrix to keep track of how often each dependency type\nwas confused for another.\n13.7\nSummary\nThis chapter has introduced the concept of dependency grammars and dependency\nparsing. Here’s a summary of the main points that we covered:\n• In dependency-based approaches to syntax, the structure of a sentence is de-\nscribed in terms of a set of binary relations that hold between the words in a\nsentence. Larger notions of constituency are not directly encoded in depen-\ndency analyses.\n• The relations in a dependency structure capture the head-dependent relation-\nship among the words in a sentence.\n• Dependency-based analyses provides information directly useful in further\nlanguage processing tasks including information extraction, semantic parsing\nand question answering\n• Transition-based parsing systems employ a greedy stack-based algorithm to\ncreate dependency structures.\n• Graph-based methods for creating dependency structures are based on the use\nof maximum spanning tree methods from graph theory.\n• Both transition-based and graph-based approaches are developed using super-\nvised machine learning techniques.\n• Treebanks provide the data needed to train these systems. Dependency tree-\nbanks can be created directly by human annotators or via automatic transfor-\nmation from phrase-structure treebanks.\n• Evaluation of dependency parsers is based on labeled and unlabeled accuracy\nscores as measured against withheld development and test corpora.\nBibliographical and Historical Notes\nThe dependency-based approach to grammar is much older than the relatively re-\ncent phrase-structure or constituency grammars that have been the primary focus of\nboth theoretical and computational linguistics for years. It has its roots in the an-\ncient Greek and Indian linguistic traditions. Contemporary theories of dependency",
  "301": "BIBLIOGRAPHICAL AND HISTORICAL NOTES\n293\ngrammar all draw heavily on the work of Tesni`ere (1959). The most inﬂuential\ndependency grammar frameworks include Meaning-Text Theory (MTT) (Mel’˘cuk,\n1988), Word Grammar (Hudson, 1984), Functional Generative Description (FDG)\n(Sgall et al., 1986). These frameworks differ along a number of dimensions in-\ncluding the degree and manner in which they deal with morphological, syntactic,\nsemantic and pragmatic factors, their use of multiple layers of representation, and\nthe set of relations used to categorize dependency relations.\nAutomatic parsing using dependency grammars was ﬁrst introduced into compu-\ntational linguistics by early work on machine translation at the RAND Corporation\nled by David Hays. This work on dependency parsing closely paralleled work on\nconstituent parsing and made explicit use of grammars to guide the parsing process.\nAfter this early period, computational work on dependency parsing remained inter-\nmittent over the following decades. Notable implementations of dependency parsers\nfor English during this period include Link Grammar (Sleator and Temperley, 1993),\nConstraint Grammar (Karlsson et al., 1995), and MINIPAR (Lin, 2003).\nDependency parsing saw a major resurgence in the late 1990’s with the appear-\nance of large dependency-based treebanks and the associated advent of data driven\napproaches described in this chapter. Eisner (1996) developed an efﬁcient dynamic\nprogramming approach to dependency parsing based on bilexical grammars derived\nfrom the Penn Treebank. Covington (2001) introduced the deterministic word by\nword approach underlying current transition-based approaches. Yamada and Mat-\nsumoto (2003) and Kudo and Matsumoto (2002) introduced both the shift-reduce\nparadigm and the use of supervised machine learning in the form of support vector\nmachines to dependency parsing.\nNivre (2003) deﬁned the modern, deterministic, transition-based approach to de-\npendency parsing. Subsequent work by Nivre and his colleagues formalized and an-\nalyzed the performance of numerous transition systems, training methods, and meth-\nods for dealing with non-projective language Nivre and Scholz 2004,Nivre 2006,Nivre\nand Nilsson 2005,Nivre et al. 2007,Nivre 2007.\nThe graph-based maximum spanning tree approach to dependency parsing was\nintroduced by McDonald et al. 2005,McDonald et al. 2005.\nThe earliest source of data for training and evaluating dependency English parsers\ncame from the WSJ Penn Treebank (Marcus et al., 1993) described in Chapter 10.\nThe use of head-ﬁnding rules developed for use with probabilistic parsing facili-\ntated the automatic extraction of dependency parses from phrase-based ones (Xia\nand Palmer, 2001).\nThe long-running Prague Dependency Treebank project (Hajiˇc, 1998) is the most\nsigniﬁcant effort to directly annotate a corpus with multiple layers of morphological,\nsyntactic and semantic information. The current PDT 3.0 now contains over 1.5 M\ntokens (Bejˇcek et al., 2013).\nUniversal Dependencies (UD) (Nivre et al., 2016b) is a project directed at cre-\nating a consistent framework for dependency treebank annotation across languages\nwith the goal of advancing parser development across the worlds languages. Under\nthe auspices of this effort, treebanks for over 30 languages have been annotated and\nmade available in a single consistent format. The UD annotation scheme evolved out\nof several distinct efforts including Stanford dependencies (de Marneffe et al. 2006,\nde Marneffe and Manning 2008, de Marneffe et al. 2014), Google’s universal part-\nof-speech tags (Petrov et al., 2012), and the Interset interlingua for morphosyntactic\ntagsets (Zeman, 2008). Driven in part by the UD framework, dependency treebanks\nof a signiﬁcant size and quality are now available in over 30 languages (Nivre et al.,",
  "302": "294\nCHAPTER 13\n•\nDEPENDENCY PARSING\n2016b).\nThe Conference on Natural Language Learning (CoNLL) has conducted an in-\nﬂuential series of shared tasks related to dependency parsing over the years (Buch-\nholz and Marsi 2006, Nilsson et al. 2007, Surdeanu et al. 2008a, Hajiˇc et al. 2009).\nMore recent evaluations have focused on parser robustness with respect to morpho-\nlogically rich languages (Seddah et al., 2013), and non-canonical language forms\nsuch as social media, texts, and spoken language (Petrov and McDonald, 2012).\nChoi et al. (2015) presents a detailed performance analysis of 10 state-of-the-art de-\npendency parsers across an impressive range of metrics, as well as DEPENDABLE, a\nrobust parser evaluation tool.\nExercises",
  "303": "CHAPTER\n14\nThe Representation of Sen-\ntence Meaning\nISHMAEL: Surely all this is not without meaning.\nHerman Melville, Moby Dick\nThe approach to semantics introduced here, and elaborated on in the next two chap-\nters, is based on the idea that the meaning of linguistic expressions can be cap-\ntured in formal structures called meaning representations. Correspondingly, the\nmeaning\nrepresentations\nframeworks that specify the syntax and semantics of these representations are called\nmeaning representation languages. These meaning representations play a role\nmeaning\nrepresentation\nlanguages\nanalogous to that of the syntactic representations introduced in earlier chapters—\nthey abstract away from surface forms and facilitate downstream processing.\nThe need for meaning representations arises when neither the raw linguistic in-\nputs nor any of the syntactic structures derivable from them facilitate the kind of se-\nmantic processing that is required. We need representations that bridge the gap from\nlinguistic inputs to the knowledge of the world needed to perform tasks. Consider\nthe following ordinary language tasks that require some form of semantic processing\nof natural language:\n• Deciding what to order at a restaurant by reading a menu\n• Learning to use a new piece of software by reading the manual\n• Answering essay questions on an exam\n• Realizing that you’ve been insulted\n• Following recipes\nGrammatical representations aren’t sufﬁcient to accomplish these tasks. What\nis required are representations that link the linguistic elements to the non-linguistic\nknowledge of the world needed to successfully accomplish the tasks:\n• Reading a menu and deciding what to order, giving advice about where to go\nto dinner, following a recipe, and generating new recipes all require knowl-\nedge about food and its preparation, what people like to eat, and what restau-\nrants are like.\n• Answering and grading essay questions requires background knowledge about\nthe topic of the question, the desired knowledge level of the students, and how\nsuch questions are normally answered.\n• Learning to use a piece of software by reading a manual, or giving advice\nabout how to use the software, requires knowledge about current computers,\nthe speciﬁc software in question, similar software applications, and knowl-\nedge about users in general.\nIn this chapter, we assume that linguistic expressions have meaning representa-\ntions that are made up of the same kind of stuff that is used to represent this kind\nof everyday common-sense knowledge of the world. The process whereby such\n295",
  "304": "296\nCHAPTER 14\n•\nTHE REPRESENTATION OF SENTENCE MEANING\n∃e,y Having(e)∧Haver(e,Speaker)∧HadThing(e,y)∧Car(y)\nh / have-01\nc / car\ni / i \narg0\narg1\n(h / have-01\n        arg0: (i / i)\n        arg1: (c / car))\nHaving:\nHaver:\nSpeaker\nHadThing:\nCar\nFigure 14.1\nA list of symbols, two directed graphs, and a record structure: a sampler of\nmeaning representations for I have a car.\nrepresentations are created and assigned to linguistic inputs is called semantic anal-\nysis, and the entire enterprise of designing meaning representations and associated\nsemantic analyzers is referred to as computational semantics.\ncomputational\nsemantics\nTo make these notions a bit more concrete, consider Fig. 14.1, which shows\nexample meaning representations for the sentence I have a car using four com-\nmonly used meaning representation languages. The top row illustrates a sentence\nin First-Order Logic, covered in detail in Section 14.3; the directed graph and its\ncorresponding textual form is an example of an Abstract Meaning Representa-\ntion (AMR) form, to be discussed in Chapter 18, and ﬁnally a Frame-Based or\nSlot-Filler representation, discussed in Section 14.5 and again in Chapter 17.\nWhile there are non-trivial differences among these approaches, they all share\nthe notion that a meaning representation consists of structures composed from a\nset of symbols, or representational vocabulary. When appropriately arranged, these\nsymbol structures are taken to correspond to objects, properties of objects, and rela-\ntions among objects in some state of affairs being represented or reasoned about. In\nthis case, all four representations make use of symbols corresponding to the speaker,\na car, and a relation denoting the possession of one by the other.\nImportantly, these representations can be viewed from at least two distinct per-\nspectives in all of these approaches: as representations of the meaning of the par-\nticular linguistic input I have a car, and as representations of the state of affairs in\nsome world. It is this dual perspective that allows these representations to be used\nto link linguistic inputs to the world and to our knowledge of it.\nThis chapter introduces the basics of what is needed in a meaning representa-\ntion. A number of extremely important issues are therefore deferred to later chap-\nters. The focus of this chapter is on representing the literal meaning of individual\nliteral meaning\nsentences. By this, we mean representations that are derived from the core conven-\ntional meanings of words and that do not reﬂect much of the context in which they\noccur. Chapter 15 and Chapter 16 introduce techniques for generating these formal\nmeaning representations for linguistic inputs. Chapter 17 focuses on the extraction\nof entities, relations events and times, Chapter 18 and Chapter 19 on semantic struc-\nture of verbs and their arguments, while the task of producing representations for",
  "305": "14.1\n•\nCOMPUTATIONAL DESIDERATA FOR REPRESENTATIONS\n297\nlarger stretches of discourse is deferred to Chapter 20 and Chapter 21.\nThere are four major parts to this chapter. Section 14.1 explores some of the key\ncomputational requirements for what we need in a meaning representation language.\nSection 14.2 discusses how we can provide some guarantees that these representa-\ntions will actually do what we need them to do—provide a correspondence to the\nstate of affairs being represented. Section 14.3 then introduces First-Order Logic,\nwhich has historically been the primary technique for investigating issues in natural\nlanguage semantics. Section 14.4 then describes how FOL can be used to capture the\nsemantics of events and states in English.\n14.1\nComputational Desiderata for Representations\nWe begin by considering the issue of why meaning representations are needed and\nwhat they should do for us. To focus this discussion, we use the task of giving advice\nabout restaurants to tourists. Assume that we have a computer system that accepts\nspoken language inputs from tourists and constructs appropriate responses using a\nknowledge base of relevant domain knowledge. A series of examples will serve to\nintroduce some of the basic requirements that a meaning representation must fulﬁll\nand some of the complications that inevitably arise in the process of designing such\nmeaning representations.\n14.1.1\nVeriﬁability\nConsider the following simple question:\n(14.1) Does Maharani serve vegetarian food?\nThis example illustrates the most basic requirement for a meaning representation:\nit must be possible to use the representation to determine the relationship between\nthe meaning of a sentence and the state of the world as we know it. In other words,\nwe need to be able to determine the truth of our representations. Section 14.2 ex-\nplores the standard approach to this topic in some detail. For now, let’s assume that\ncomputational semantic systems require the ability to compare, or match, meaning\nrepresentations associated with linguistic expressions with formal representations in\na knowledge base, its store of information about its world.\nknowledge base\nIn this example, assume that the meaning of this question involves the proposi-\ntion Maharani serves vegetarian food. For now, we will simply gloss this represen-\ntation as\nServes(Maharani,VegetarianFood)\n(14.2)\nThis representation of the input can be matched against our knowledge base of facts\nabout a set of restaurants. If the system ﬁnds a representation matching this propo-\nsition in its knowledge base, it can return an afﬁrmative answer. Otherwise, it must\neither say No if its knowledge of local restaurants is complete, or say that it doesn’t\nknow if there is reason to believe that its knowledge is incomplete.\nThis notion, known as veriﬁability, describes a system’s ability to compare the\nveriﬁability\nstate of affairs described by a representation to the state of affairs in some world as\nmodeled in a knowledge base.",
  "306": "298\nCHAPTER 14\n•\nTHE REPRESENTATION OF SENTENCE MEANING\n14.1.2\nUnambiguous Representations\nSemantics, like all the other domains we have studied, is subject to ambiguity.\nSpeciﬁcally, individual linguistic expressions can have different meaning represen-\ntations assigned to them based on the circumstances in which they occur. Consider\nthe following example from the BERP corpus:\n(14.3) I wanna eat someplace that’s close to ICSI.\nGiven the allowable argument structures for the verb eat, this sentence can either\nmean that the speaker wants to eat at some nearby location, or under a Godzilla-as-\nspeaker interpretation, the speaker may want to devour some nearby location. The\nanswer generated by the system for this request will depend on which interpretation\nis chosen as the correct one.\nSince ambiguities such as this abound in all genres of all languages, some means\nof determining that certain interpretations are preferable (or alternatively, not as\npreferable) to others is needed. The various linguistic phenomena that give rise\nto such ambiguities and the techniques that can be employed to deal with them are\ndiscussed in detail in the next four chapters.\nOur concern in this chapter, however, is with the status of our meaning repre-\nsentations with respect to ambiguity, and not with the means by which we might\narrive at correct interpretations. Since we reason about, and act upon, the semantic\ncontent of linguistic inputs, the ﬁnal representation of an input’s meaning should be\nfree from any ambiguity.1\nA concept closely related to ambiguity is vagueness. Like ambiguity, vagueness\nvagueness\ncan make it difﬁcult to determine what to do with a particular input on the basis\nof its meaning representation. Vagueness, however, does not give rise to multiple\nrepresentations. Consider the following request as an example:\n(14.4) I want to eat Italian food.\nWhile the use of the phrase Italian food may provide enough information for a\nrestaurant advisor to provide reasonable recommendations, it is nevertheless quite\nvague as to what the user really wants to eat. Therefore, a vague representation\nof the meaning of this phrase may be appropriate for some purposes, while a more\nspeciﬁc representation may be needed for other purposes. It will, therefore, be ad-\nvantageous for a meaning representation language to support representations that\nmaintain a certain level of vagueness. Note that it is not always easy to distinguish\nambiguity from vagueness. Zwicky and Sadock (1975) provide a useful set of tests\nthat can be used as diagnostics.\n14.1.3\nCanonical Form\nThe notion that single sentences can be assigned multiple meanings leads to the\nrelated phenomenon of distinct inputs that should be assigned the same meaning\nrepresentation. Consider the following alternative ways of expressing (14.1):\n(14.5) Does Maharani have vegetarian dishes?\n(14.6) Do they have vegetarian food at Maharani?\n(14.7) Are vegetarian dishes served at Maharani?\n(14.8) Does Maharani serve vegetarian fare?\n1\nThis does not preclude the use of intermediate semantic representations that maintain some level of\nambiguity on the way to a single unambiguous form. Examples of such representations are discussed in\nChapter 15.",
  "307": "14.1\n•\nCOMPUTATIONAL DESIDERATA FOR REPRESENTATIONS\n299\nGiven that these alternatives use different words and have widely varying syn-\ntactic analyses, it would not be unreasonable to expect them to have quite different\nmeaning representations. Such a situation would, however, have undesirable con-\nsequences for how we determine the truth of our representations. If the system’s\nknowledge base contains only a single representation of the fact in question, then\nthe representations underlying all but one of our alternatives will fail to produce\na match. We could, of course, store all possible alternative representations of the\nsame fact in the knowledge base, but doing so would lead to an enormous number\nof problems related to keeping such a knowledge base consistent.\nThe way out of this dilemma is motivated by the fact that since the answers given\nfor each of these alternatives should be the same in all situations, we might say that\nthey all mean the same thing, at least for the purposes of giving restaurant recom-\nmendations. In other words, at least in this domain, we can legitimately consider\nassigning the same meaning representation to the propositions underlying each of\nthese requests. Taking such an approach would guarantee that our simple scheme\nfor answering yes-no questions will still work.\nThe notion that distinct inputs that mean the same thing should have the same\nmeaning representation is known as the doctrine of canonical form. This approach\ncanonical form\ngreatly simpliﬁes various reasoning tasks since systems need only deal with a single\nmeaning representation for a potentially wide range of expressions.\nCanonical form does complicate the task of semantic analysis. To see this, note\nthat the alternatives given above use completely different words and syntax to refer\nto vegetarian fare and to what restaurants do with it. To assign the same representa-\ntion to all of these requests, our system would have to conclude that vegetarian fare,\nvegetarian dishes, and vegetarian food refer to the same thing in this context, that\nthe use here of having and serving are similarly equivalent, and that the different\nsyntactic parses underlying these requests are all compatible with the same meaning\nrepresentation.\nBeing able to assign the same representation to such diverse inputs is a tall or-\nder. Fortunately, systematic meaning relationships among word senses and among\ngrammatical constructions can be exploited to make this task tractable. Consider the\nissue of the meanings of the words food, dish, and fare in these examples. A little\nintrospection or a glance at a dictionary reveals that these words have a fair number\nof distinct uses. However, it also reveals that at least one sense is shared among them\nall. If a system has the ability to choose that shared sense, then an identical meaning\nrepresentation can be assigned to the phrases containing these words.\nJust as there are systematic relationships among the meanings of different words,\nthere are similar relationships related to the role that syntactic analyses play in as-\nsigning meanings to sentences. Speciﬁcally, alternative syntactic analyses often have\nmeanings that are, if not identical, at least systematically related to one another.\nConsider the following pair of examples:\n(14.9) Maharani serves vegetarian dishes.\n(14.10) Vegetarian dishes are served by Maharani.\nDespite the different placement of the arguments to serve in these examples, we\ncan still assign Maharani and vegetarian dishes to the same roles in both of these\nexamples because of our knowledge of the relationship between active and passive\nsentence constructions. In particular, we can use knowledge of where grammatical\nsubjects and direct objects appear in these constructions to assign Maharani to the\nrole of the server, and vegetarian dishes to the role of thing being served in both\nof these examples, despite the fact that they appear in different surface locations.",
  "308": "300\nCHAPTER 14\n•\nTHE REPRESENTATION OF SENTENCE MEANING\nThe precise role of the grammar in the construction of meaning representations is\ncovered in Chapter 15.\n14.1.4\nInference and Variables\nContinuing with the topic of the computational purposes that meaning representa-\ntions should serve, we should consider more complex requests such as the following:\n(14.11) Can vegetarians eat at Maharani?\nHere, it would be a mistake to invoke canonical form to force our system to as-\nsign the same representation to this request as for the previous examples. That this\nrequest results in the same answer as the others arises, not because they mean the\nsame thing, but because there is a common-sense connection between what vegetar-\nians eat and what vegetarian restaurants serve. This is a fact about the world and\nnot a fact about any particular kind of linguistic regularity. This implies that no\napproach based on canonical form and simple matching will give us an appropriate\nanswer to this request. What is needed is a systematic way to connect the meaning\nrepresentation of this request with the facts about the world as they are represented\nin a knowledge base.\nWe use the term inference to refer generically to a system’s ability to draw valid\ninference\nconclusions based on the meaning representation of inputs and its store of back-\nground knowledge. It must be possible for the system to draw conclusions about the\ntruth of propositions that are not explicitly represented in the knowledge base but\nthat are nevertheless logically derivable from the propositions that are present.\nNow consider the following somewhat more complex request:\n(14.12) I’d like to ﬁnd a restaurant where I can get vegetarian food.\nUnlike our previous examples, this request does not make reference to any particular\nrestaurant. The user is expressing a desire for information about an unknown and\nunnamed entity that is a restaurant that serves vegetarian food. Since this request\ndoes not mention any particular restaurant, the kind of simple matching-based ap-\nproach we have been advocating is not going to work. Rather, answering this request\nrequires a more complex kind of matching that involves the use of variables. We can\ngloss a representation containing such variables as follows:\nServes(x,VegetarianFood)\n(14.13)\nMatching such a proposition succeeds only if the variable x can be replaced by some\nknown object in the knowledge base in such a way that the entire proposition will\nthen match. The concept that is substituted for the variable can then be used to fulﬁll\nthe user’s request. Of course, this simple example only hints at the issues involved\nin the use of such variables. Sufﬁce it to say that linguistic inputs contain many\ninstances of all kinds of indeﬁnite references, and it is, therefore, critical for any\nmeaning representation language to be able to handle this kind of expression.\n14.1.5\nExpressiveness\nFinally, to be useful, a meaning representation scheme must be expressive enough\nto handle a wide range of subject matter. The ideal situation would be to have a sin-\ngle meaning representation language that could adequately represent the meaning\nof any sensible natural language utterance. Although this is probably too much to\nexpect from any single representational system, First-Order Logic, as described in",
  "309": "14.2\n•\nMODEL-THEORETIC SEMANTICS\n301\nSection 14.3, is expressive enough to handle quite a lot of what needs to be repre-\nsented.\n14.2\nModel-Theoretic Semantics\nThe last two sections focused on various desiderata for meaning representations and\non some of the ways in which natural languages convey meaning. We haven’t said\nmuch formally about what it is about meaning representation languages that allows\nthem to do all the things we want them to. In particular, we might like to have\nsome kind of guarantee that these representations can do the work that we require of\nthem: bridge the gap from merely formal representations to representations that tell\nus something about some state of affairs in the world.\nTo see how we might provide such a guarantee, let’s start with the basic notions\nshared by most meaning representation schemes. What they all have in common\nis the ability to represent objects, properties of objects, and relations among ob-\njects. This ability can be formalized by the notion of a model. A model is a formal\nmodel\nconstruct that stands for the particular state of affairs in the world. Expressions in\na meaning representation language can be mapped in a systematic way to the ele-\nments of the model. If the model accurately captures the facts we’re interested in\nconcerning some state of affairs, then a consistent mapping between the meaning\nrepresentation and the model provides the bridge between the meaning representa-\ntion and world being considered. As we show, models provide a surprisingly simple\nand powerful way to ground the expressions in meaning representation languages.\nFirst, some terminology. The vocabulary of a meaning representation consists of\ntwo parts: the non-logical vocabulary and the logical vocabulary. The non-logical\nvocabulary consists of the open-ended set of names for the objects, properties, and\nnon-logical\nvocabulary\nrelations that make up the world we’re trying to represent. These appear in various\nschemes as predicates, nodes, labels on links, or labels in slots in frames, The log-\nical vocabulary consists of the closed set of symbols, operators, quantiﬁers, links,\nlogical\nvocabulary\netc., that provide the formal means for composing expressions in a given meaning\nrepresentation language.\nWe’ll start by requiring that each element of the non-logical vocabulary have a\ndenotation in the model. By denotation, we simply mean that every element of the\ndenotation\nnon-logical vocabulary corresponds to a ﬁxed, well-deﬁned part of the model. Let’s\nstart with objects, the most basic notion in most representational schemes. The do-\nmain of a model is simply the set of objects that are part of the application, or state\ndomain\nof affairs, being represented. Each distinct concept, category, or individual in an ap-\nplication denotes a unique element in the domain. A domain is therefore formally a\nset. Note that it isn’t mandatory that every element of the domain have a correspond-\ning concept in our meaning representation; it’s perfectly acceptable to have domain\nelements that aren’t mentioned or conceived of in the meaning representation. Nor\ndo we require that elements of the domain have a single denoting concept in the\nmeaning representation; a given element in the domain might have several distinct\nrepresentations denoting it, such as Mary, WifeOf(Abe), or MotherOf(Robert).\nWe can capture properties of objects in a model by denoting those domain ele-\nments that have the property in question; that is, properties denote sets. Similarly,\nrelations among objects denote sets of ordered lists, or tuples, of domain elements\nthat take part in the corresponding relations. This approach to properties and rela-\ntions is thus an extensional one: the denotation of properties like red is the set of\nextensional",
  "310": "302\nCHAPTER 14\n•\nTHE REPRESENTATION OF SENTENCE MEANING\nthings we think are red, the denotation of a relation like Married is simply set of\npairs of domain elements that are married. To summarize:\n• Objects denote elements of the domain\n• Properties denote sets of elements of the domain\n• Relations denote sets of tuples of elements of the domain\nThere is one additional element that we need to make this scheme work. We\nneed a mapping that systematically gets us from our meaning representation to the\ncorresponding denotations. More formally, we need a function that maps from the\nnon-logical vocabulary of our meaning representation to the proper denotations in\nthe model. We’ll call such a mapping an interpretation.\ninterpretation\nTo make these notions more concrete, let’s return to our restaurant advice appli-\ncation. Assume that our application domain consists of sets of restaurants, patrons,\nand various facts about the likes and dislikes of the patrons, and facts about the\nrestaurants such as their cuisine, typical cost, and noise level.\nTo begin populating our domain, D, let’s assume that we’re dealing with four pa-\ntrons designated by the non-logical symbols Matthew, Franco, Katie, and Caroline.\nThese four symbols will denote four unique domain elements. We’ll use the con-\nstants a,b,c and, d to stand for these domain elements. Note that we’re deliberately\nusing meaningless, non-mnemonic names for our domain elements to emphasize the\nfact that whatever it is that we know about these entities has to come from the formal\nproperties of the model and not from the names of the symbols. Continuing, let’s\nassume that our application includes three restaurants, designated as Frasca, Med,\nand Rio in our meaning representation, that denote the domain elements e, f, and g.\nFinally, let’s assume that we’re dealing with the three cuisines Italian, Mexican, and\nEclectic, denoted by h,i, and j in our model.\nHaving populated the domain, let’s move on to the properties and relations we\nbelieve to be true in this particular state of affairs. For our application, we need to\nrepresent various properties of restaurants such as the fact that some are noisy or\nexpensive. Properties like Noisy denote the subset of restaurants from our domain\nthat are known to be noisy. Two-place relational notions, such as which restaurants\nindividual patrons Like, denote ordered pairs, or tuples, of the objects from the do-\nmain. And, since we decided to represent cuisines as objects in our model, we can\ncapture which restaurants Serve which cuisines as a set of tuples. One possible state\nof affairs using this scheme is given in Fig. 14.2.\nGiven this simple scheme, we can ground our meaning representations by con-\nsulting the appropriate denotations in the corresponding model. For example, we can\nevaluate a representation claiming that Matthew likes the Rio, or that The Med serves\nItalian by mapping the objects in the meaning representations to their corresponding\ndomain elements and mapping any links, predicates, or slots in the meaning repre-\nsentation to the appropriate relations in the model. More concretely, we can verify\na representation asserting that Matthew likes Frasca by ﬁrst using our interpretation\nfunction to map the symbol Matthew to its denotation a, Frasca to e, and the Likes\nrelation to the appropriate set of tuples. We then check that set of tuples for the\npresence of the tuple ⟨a,e⟩. If, as it is in this case, the tuple is present in the model,\nthen we can conclude that Matthew likes Frasca is true; if it isn’t then we can’t.\nThis is all pretty straightforward—we’re using sets and operations on sets to\nground the expressions in our meaning representations. Of course, the more inter-\nesting part comes when we consider more complex examples such as the following:\n(14.14) Katie likes the Rio and Matthew likes the Med.",
  "311": "14.2\n•\nMODEL-THEORETIC SEMANTICS\n303\nDomain\nD = {a,b,c,d,e, f,g,h,i, j}\nMatthew, Franco, Katie and Caroline\na,b,c,d\nFrasca, Med, Rio\ne, f,g\nItalian, Mexican, Eclectic\nh,i, j\nProperties\nNoisy\nNoisy = {e, f,g}\nFrasca, Med, and Rio are noisy\nRelations\nLikes\nLikes = {⟨a, f⟩,⟨c, f⟩,⟨c,g⟩,⟨b,e⟩,⟨d, f⟩,⟨d,g⟩}\nMatthew likes the Med\nKatie likes the Med and Rio\nFranco likes Frasca\nCaroline likes the Med and Rio\nServes\nServes = {⟨f, j⟩,⟨g,i⟩,⟨e,h⟩}\nMed serves eclectic\nRio serves Mexican\nFrasca serves Italian\nFigure 14.2\nA model of the restaurant world.\n(14.15) Katie and Caroline like the same restaurants.\n(14.16) Franco likes noisy, expensive restaurants.\n(14.17) Not everybody likes Frasca.\nOur simple scheme for grounding the meaning of representations is not adequate\nfor examples such as these. Plausible meaning representations for these examples\nwill not map directly to individual entities, properties, or relations. Instead, they\ninvolve complications such as conjunctions, equality, quantiﬁed variables, and nega-\ntions. To assess whether these statements are consistent with our model, we’ll have\nto tear them apart, assess the parts, and then determine the meaning of the whole\nfrom the meaning of the parts according to the details of how the whole is assem-\nbled.\nConsider the ﬁrst example given above. A meaning representation for an exam-\nple like this will include two distinct propositions expressing the individual patron’s\npreferences, conjoined with some kind of implicit or explicit conjunction operator.\nOur model doesn’t have a relation that encodes pairwise preferences for all of the\npatrons and restaurants in our model, nor does it need to. We know from our model\nthat Matthew likes the Med and separately that Katie likes the Rio (that is, the tuples\n⟨a, f⟩and ⟨c,g⟩are members of the set denoted by the Likes relation). All we really\nneed to know is how to deal with the semantics of the conjunction operator. If we\nassume the simplest possible semantics for the English word and, the whole state-\nment is true if it is the case that each of the components is true in our model. In this\ncase, both components are true since the appropriate tuples are present and therefore\nthe sentence as a whole is true.\nWhat we’ve done with this example is provide a truth-conditional semantics\ntruth-\nconditional\nsemantics\nfor the assumed conjunction operator in some meaning representation.\nThat is,\nwe’ve provided a method for determining the truth of a complex expression from\nthe meanings of the parts (by consulting a model) and the meaning of an operator by\nconsulting a truth table. Meaning representation languages are truth-conditional to\nthe extent that they give a formal speciﬁcation as to how we can determine the mean-",
  "312": "304\nCHAPTER 14\n•\nTHE REPRESENTATION OF SENTENCE MEANING\nFormula →AtomicFormula\n|\nFormula Connective Formula\n|\nQuantiﬁer Variable,... Formula\n|\n¬ Formula\n|\n(Formula)\nAtomicFormula →Predicate(Term,...)\nTerm →Function(Term,...)\n|\nConstant\n|\nVariable\nConnective →∧| ∨| =⇒\nQuantiﬁer →∀| ∃\nConstant →A | VegetarianFood | Maharani···\nVariable →x | y | ···\nPredicate →Serves | Near | ···\nFunction →LocationOf | CuisineOf | ···\nFigure 14.3\nA context-free grammar speciﬁcation of the syntax of First-Order Logic rep-\nresentations. Adapted from Russell and Norvig (2002)\n.\ning of complex sentences from the meaning of their parts. In particular, we need to\nknow the semantics of the entire logical vocabulary of the meaning representation\nscheme being used.\nNote that although the details of how this happens depends on details of the\nparticular meaning representation being used, it should be clear that assessing the\ntruth conditions of examples like these involves nothing beyond the simple set op-\nerations we’ve been discussing. We return to these issues in the next section, where\nwe discuss them in the context of the semantics of First-Order Logic.\n14.3\nFirst-Order Logic\nFirst-Order Logic (FOL) is a ﬂexible, well-understood, and computationally tractable\nmeaning representation language that satisﬁes many of the desiderata given in Sec-\ntion 14.1. It provides a sound computational basis for the veriﬁability, inference,\nand expressiveness requirements, as well as a sound model-theoretic semantics.\nAn additional attractive feature of FOL is that it makes very few speciﬁc com-\nmitments as to how things ought to be represented. And, the speciﬁc commitments\nit does make are ones that are fairly easy to live with and that are shared by many of\nthe schemes mentioned earlier; the represented world consists of objects, properties\nof objects, and relations among objects.\nThe remainder of this section introduces the basic syntax and semantics of FOL\nand then describes the application of FOL to the representation of events.\n14.3.1\nBasic Elements of First-Order Logic\nLet’s explore FOL by ﬁrst examining its various atomic elements and then showing\nhow they can be composed to create larger meaning representations. Figure 14.3,\nwhich provides a complete context-free grammar for the particular syntax of FOL\nthat we will use, is our roadmap for this section.\nLet’s begin by examining the notion of a term, the FOL device for representing\nterm",
  "313": "14.3\n•\nFIRST-ORDER LOGIC\n305\nobjects. As can be seen from Fig. 14.3, FOL provides three ways to represent these\nbasic building blocks: constants, functions, and variables. Each of these devices can\nbe thought of as designating an object in the world under consideration.\nConstants in FOL refer to speciﬁc objects in the world being described. Such\nConstants\nconstants are conventionally depicted as either single capitalized letters such as A\nand B or single capitalized words that are often reminiscent of proper nouns such as\nMaharani and Harry. Like programming language constants, FOL constants refer\nto exactly one object. Objects can, however, have multiple constants that refer to\nthem.\nFunctions in FOL correspond to concepts that are often expressed in English as\nFunctions\ngenitives such as Frasca’s location. A FOL translation of such an expression might\nlook like the following.\nLocationOf(Frasca)\n(14.18)\nFOL functions are syntactically the same as single argument predicates. It is im-\nportant to remember, however, that while they have the appearance of predicates,\nthey are in fact terms in that they refer to unique objects. Functions provide a con-\nvenient way to refer to speciﬁc objects without having to associate a named constant\nwith them. This is particularly convenient in cases in which many named objects,\nlike restaurants, have a unique concept such as a location associated with them.\nVariables are variable our ﬁnal FOL mechanism for referring to objects. Vari-\nvariable\nables, depicted as single lower-case letters, let us make assertions and draw infer-\nences about objects without having to make reference to any particular named ob-\nject. This ability to make statements about anonymous objects comes in two ﬂavors:\nmaking statements about a particular unknown object and making statements about\nall the objects in some arbitrary world of objects. We return to the topic of variables\nafter we have presented quantiﬁers, the elements of FOL that make variables useful.\nNow that we have the means to refer to objects, we can move on to the FOL\nmechanisms that are used to state relations that hold among objects. Predicates are\nsymbols that refer to, or name, the relations that hold among some ﬁxed number\nof objects in a given domain. Returning to the example introduced informally in\nSection 14.1, a reasonable FOL representation for Maharani serves vegetarian food\nmight look like the following formula:\nServes(Maharani,VegetarianFood)\n(14.19)\nThis FOL sentence asserts that Serves, a two-place predicate, holds between the\nobjects denoted by the constants Maharani and VegetarianFood.\nA somewhat different use of predicates is illustrated by the following fairly typ-\nical representation for a sentence like Maharani is a restaurant:\nRestaurant(Maharani)\n(14.20)\nThis is an example of a one-place predicate that is used, not to relate multiple objects,\nbut rather to assert a property of a single object. In this case, it encodes the category\nmembership of Maharani.\nWith the ability to refer to objects, to assert facts about objects, and to relate\nobjects to one another, we can create rudimentary composite representations. These\nrepresentations correspond to the atomic formula level in Fig. 14.3. This ability\nto compose complex representations is, however, not limited to the use of single\npredicates. Larger composite representations can also be put together through the\nuse of logical connectives. As can be seen from Fig. 14.3, logical connectives let\nlogical\nconnectives",
  "314": "306\nCHAPTER 14\n•\nTHE REPRESENTATION OF SENTENCE MEANING\nus create larger representations by conjoining logical formulas using one of three\noperators. Consider, for example, the following BERP sentence and one possible\nrepresentation for it:\n(14.21) I only have ﬁve dollars and I don’t have a lot of time.\nHave(Speaker,FiveDollars)∧¬Have(Speaker,LotOfTime)\n(14.22)\nThe semantic representation for this example is built up in a straightforward way\nfrom semantics of the individual clauses through the use of the ∧and ¬ operators.\nNote that the recursive nature of the grammar in Fig. 14.3 allows an inﬁnite number\nof logical formulas to be created through the use of these connectives. Thus, as with\nsyntax, we can use a ﬁnite device to create an inﬁnite number of representations.\n14.3.2\nVariables and Quantiﬁers\nWe now have all the machinery necessary to return to our earlier discussion of vari-\nables. As noted above, variables are used in two ways in FOL: to refer to particular\nanonymous objects and to refer generically to all objects in a collection. These two\nuses are made possible through the use of operators known as quantiﬁers. The two\nquantiﬁers\noperators that are basic to FOL are the existential quantiﬁer, which is denoted ∃and\nis pronounced as “there exists”, and the universal quantiﬁer, which is denoted ∀and\nis pronounced as “for all”.\nThe need for an existentially quantiﬁed variable is often signaled by the presence\nof an indeﬁnite noun phrase in English. Consider the following example:\n(14.23) a restaurant that serves Mexican food near ICSI.\nHere, reference is being made to an anonymous object of a speciﬁed category with\nparticular properties. The following would be a reasonable representation of the\nmeaning of such a phrase:\n∃xRestaurant(x) ∧Serves(x,MexicanFood)\n(14.24)\n∧Near((LocationOf(x),LocationOf(ICSI))\nThe existential quantiﬁer at the head of this sentence instructs us on how to\ninterpret the variable x in the context of this sentence. Informally, it says that for\nthis sentence to be true there must be at least one object such that if we were to\nsubstitute it for the variable x, the resulting sentence would be true. For example,\nif AyCaramba is a Mexican restaurant near ICSI, then substituting AyCaramba for x\nresults in the following logical formula:\nRestaurant(AyCaramba)∧Serves(AyCaramba,MexicanFood)\n(14.25)\n∧Near((LocationOf(AyCaramba),LocationOf(ICSI))\nBased on the semantics of the ∧operator, this sentence will be true if all of its\nthree component atomic formulas are true. These in turn will be true if they are\neither present in the system’s knowledge base or can be inferred from other facts in\nthe knowledge base.\nThe use of the universal quantiﬁer also has an interpretation based on substi-\ntution of known objects for variables. The substitution semantics for the universal\nquantiﬁer takes the expression for all quite literally; the ∀operator states that for the\nlogical formula in question to be true, the substitution of any object in the knowledge\nbase for the universally quantiﬁed variable should result in a true formula. This is in",
  "315": "14.3\n•\nFIRST-ORDER LOGIC\n307\nmarked contrast to the ∃operator, which only insists on a single valid substitution\nfor the sentence to be true.\nConsider the following example:\n(14.26) All vegetarian restaurants serve vegetarian food.\nA reasonable representation for this sentence would be something like the following:\n∀xVegetarianRestaurant(x) =⇒Serves(x,VegetarianFood)\n(14.27)\nFor this sentence to be true, it must be the case that every substitution of a known\nobject for x must result in a sentence that is true. We can divide the set of all possible\nsubstitutions into the set of objects consisting of vegetarian restaurants and the set\nconsisting of everything else. Let us ﬁrst consider the case in which the substituted\nobject actually is a vegetarian restaurant; one such substitution would result in the\nfollowing sentence:\nVegetarianRestaurant(Maharani) =⇒Serves(Maharani,VegetarianFood)\n(14.28)\nIf we assume that we know that the consequent clause\nServes(Maharani,VegetarianFood)\n(14.29)\nis true, then this sentence as a whole must be true. Both the antecedent and the\nconsequent have the value True and, therefore, according to the ﬁrst two rows of\nFig. 14.4 on page 309 the sentence itself can have the value True. This result will be\nthe same for all possible substitutions of Terms representing vegetarian restaurants\nfor x.\nRemember, however, that for this sentence to be true, it must be true for all\npossible substitutions. What happens when we consider a substitution from the set\nof objects that are not vegetarian restaurants? Consider the substitution of a non-\nvegetarian restaurant such as Ay Caramba’s for the variable x:\nVegetarianRestaurant(AyCaramba)\n=⇒\nServes(AyCaramba,VegetarianFood)\nSince the antecedent of the implication is False, we can determine from Fig. 14.4\nthat the sentence is always True, again satisfying the ∀constraint.\nNote that it may still be the case that Ay Caramba serves vegetarian food with-\nout actually being a vegetarian restaurant. Note also, that despite our choice of\nexamples, there are no implied categorical restrictions on the objects that can be\nsubstituted for x by this kind of reasoning. In other words, there is no restriction of\nx to restaurants or concepts related to them. Consider the following substitution:\nVegetarianRestaurant(Carburetor) =⇒Serves(Carburetor,VegetarianFood)\nHere the antecedent is still false, and hence, the rule remains true under this kind of\nirrelevant substitution.\nTo review, variables in logical formulas must be either existentially (∃) or uni-\nversally (∀) quantiﬁed. To satisfy an existentially quantiﬁed variable, at least one\nsubstitution must result in a true sentence. Sentences with universally quantiﬁed\nvariables must be true under all possible substitutions.",
  "316": "308\nCHAPTER 14\n•\nTHE REPRESENTATION OF SENTENCE MEANING\n14.3.3\nLambda Notation\nThe ﬁnal element we need to complete our discussion of FOL is called the lambda\nnotation (Church, 1940). This notation provides a way to abstract from fully speci-\nlambda\nnotation\nﬁed FOL formula in a way that will be particularly useful for semantic analysis. The\nlambda notation extends the syntax of FOL to include expressions of the following\nform:\nλx.P(x)\n(14.30)\nSuch expressions consist of the Greek symbol λ, followed by one or more variables,\nfollowed by a FOL formula that makes use of those variables.\nThe usefulness of these λ-expressions is based on the ability to apply them to\nlogical terms to yield new FOL expressions where the formal parameter variables are\nbound to the speciﬁed terms. This process is known as λ-reduction and consists of\nλ -reduction\na simple textual replacement of the λ variables with the speciﬁed FOL terms, accom-\npanied by the subsequent removal of the λ. The following expressions illustrate the\napplication of a λ-expression to the constant A, followed by the result of performing\na λ-reduction on this expression:\nλx.P(x)(A)\n(14.31)\nP(A)\nAn important and useful variation of this technique is the use of one λ-expression\nas the body of another as in the following expression:\nλx.λy.Near(x,y)\n(14.32)\nThis fairly abstract expression can be glossed as the state of something being\nnear something else. The following expressions illustrate a single λ-application and\nsubsequent reduction with this kind of embedded λ-expression:\nλx.λy.Near(x,y)(Bacaro)\n(14.33)\nλy.Near(Bacaro,y)\nThe important point here is that the resulting expression is still a λ-expression;\nthe ﬁrst reduction bound the variable x and removed the outer λ, thus revealing the\ninner expression. As might be expected, this resulting λ-expression can, in turn,\nbe applied to another term to arrive at a fully speciﬁed logical formula, as in the\nfollowing:\nλy.Near(Bacaro,y)(Centro)\n(14.34)\nNear(Bacaro,Centro)\nThis general technique, called currying2 (Sch¨onkﬁnkel, 1924) is a way of con-\ncurrying\nverting a predicate with multiple arguments into a sequence of single-argument pred-\nicates.\nAs we show in Chapter 15, the λ-notation provides a way to incrementally gather\narguments to a predicate when they do not all appear together as daughters of the\npredicate in a parse tree.\n2\nCurrying is the standard term, although Heim and Kratzer (1998) present an interesting argument for\nthe term Sch¨onkﬁnkelization over currying, since Curry later built on Sch¨onﬁnkel’s work.",
  "317": "14.3\n•\nFIRST-ORDER LOGIC\n309\n14.3.4\nThe Semantics of First-Order Logic\nThe various objects, properties, and relations represented in a FOL knowledge base\nacquire their meanings by virtue of their correspondence to objects, properties, and\nrelations out in the external world being modeled. We can accomplish this by em-\nploying the model-theoretic approach introduced in Section 14.2. Recall that this\napproach employs simple set-theoretic notions to provide a truth-conditional map-\nping from the expressions in a meaning representation to the state of affairs being\nmodeled. We can apply this approach to FOL by going through all the elements in\nFig. 14.3 on page 304 and specifying how each should be accounted for.\nWe can start by asserting that the objects in our world, FOL terms, denote ele-\nments in a domain, and asserting that atomic formulas are captured either as sets of\ndomain elements for properties, or as sets of tuples of elements for relations. As an\nexample, consider the following:\n(14.35) Centro is near Bacaro.\nCapturing the meaning of this example in FOL involves identifying the Terms\nand Predicates that correspond to the various grammatical elements in the sentence\nand creating logical formulas that capture the relations implied by the words and\nsyntax of the sentence. For this example, such an effort might yield something like\nthe following:\nNear(Centro,Bacaro)\n(14.36)\nThe meaning of this logical formula is based on whether the domain elements de-\nnoted by the terms Centro and Bacaro are contained among the tuples denoted by\nthe relation denoted by the predicate Near in the current model.\nThe interpretations of formulas involving logical connectives is based on the\nmeaning of the components in the formulas combined with the meanings of the\nconnectives they contain. Figure 14.4 gives interpretations for each of the logical\noperators shown in Fig. 14.3.\nP\nQ\n¬ P\nP ∧Q\nP ∨Q\nP\n=⇒\nQ\nFalse\nFalse\nTrue\nFalse\nFalse\nTrue\nFalse\nTrue\nTrue\nFalse\nTrue\nTrue\nTrue\nFalse\nFalse\nFalse\nTrue\nFalse\nTrue\nTrue\nFalse\nTrue\nTrue\nTrue\nFigure 14.4\nTruth table giving the semantics of the various logical connectives.\nThe semantics of the ∧(and) and ¬ (not) operators are fairly straightforward,\nand are correlated with at least some of the senses of the corresponding English\nterms. However, it is worth pointing out that the ∨(or) operator is not disjunctive\nin the same way that the corresponding English word is, and that the =⇒(im-\nplies) operator is only loosely based on any common-sense notions of implication\nor causation.\nThe ﬁnal bit we need to address involves variables and quantiﬁers. Recall that\nthere are no variables in our set-based models, only elements of the domain and\nrelations that hold among them. We can provide a model-based account for formulas\nwith variables by employing the notion of a substitution introduced earlier on page\n306. Formulas involving ∃are true if a substitution of terms for variables results\nin a formula that is true in the model. Formulas involving ∀must be true under all\npossible substitutions.",
  "318": "310\nCHAPTER 14\n•\nTHE REPRESENTATION OF SENTENCE MEANING\n14.3.5\nInference\nOne of the most important desiderata given in Section 14.1 for a meaning rep-\nresentation language is that it should support inference, or deduction. That is, the\nability to add valid new propositions to a knowledge base or to determine the truth of\npropositions not explicitly contained within a knowledge base. This section brieﬂy\ndiscusses modus ponens, the most widely implemented inference method provided\nby FOL. Applications of modus ponens to inference in discourse is discussed in\nChapter 21.\nModus ponens is a familiar form of inference that corresponds to what is in-\nModus ponens\nformally known as if-then reasoning. We can abstractly deﬁne modus ponens as\nfollows, where α and β should be taken as FOL formulas:\nα\nα =⇒β\nβ\n(14.37)\nA schema like this indicates that the formula below the line can be inferred from\nthe formulas above the line by some form of inference. Modus ponens simply states\nthat if the left-hand side of an implication rule is true, then the right-hand side of the\nrule can be inferred. In the following discussions, we will refer to the left-hand side\nof an implication as the antecedent and the right-hand side as the consequent.\nFor a typical use of modus ponens, consider the following example, which uses\na rule from the last section:\nVegetarianRestaurant(Leaf)\n∀xVegetarianRestaurant(x) =⇒Serves(x,VegetarianFood)\nServes(Leaf,VegetarianFood)\n(14.38)\nHere, the formula VegetarianRestaurant(Leaf) matches the antecedent of the rule,\nthus allowing us to use modus ponens to conclude Serves(Leaf,VegetarianFood).\nModus ponens can be put to practical use in one of two ways: forward chaining\nand backward chaining. In forward chaining systems, modus ponens is used in\nforward\nchaining\nprecisely the manner just described. As individual facts are added to the knowledge\nbase, modus ponens is used to ﬁre all applicable implication rules. In this kind of\narrangement, as soon as a new fact is added to the knowledge base, all applicable\nimplication rules are found and applied, each resulting in the addition of new facts to\nthe knowledge base. These new propositions in turn can be used to ﬁre implication\nrules applicable to them. The process continues until no further facts can be deduced.\nThe forward chaining approach has the advantage that facts will be present in\nthe knowledge base when needed, because, in a sense all inference is performed in\nadvance. This can substantially reduce the time needed to answer subsequent queries\nsince they should all amount to simple lookups. The disadvantage of this approach\nis that facts that will never be needed may be inferred and stored.\nIn backward chaining, modus ponens is run in reverse to prove speciﬁc propo-\nbackward\nchaining\nsitions called queries. The ﬁrst step is to see if the query formula is true by determin-\ning if it is present in the knowledge base. If it is not, then the next step is to search\nfor applicable implication rules present in the knowledge base. An applicable rule is\none whereby the consequent of the rule matches the query formula. If there are any\nsuch rules, then the query can be proved if the antecedent of any one them can be\nshown to be true. Not surprisingly, this can be performed recursively by backward",
  "319": "14.4\n•\nEVENT AND STATE REPRESENTATIONS\n311\nchaining on the antecedent as a new query. The Prolog programming language is a\nbackward chaining system that implements this strategy.\nTo see how this works, let’s assume that we have been asked to verify the truth of\nthe proposition Serves(Leaf,VegetarianFood), assuming the facts given above the\nline in (14.38). Since this proposition is not present in the knowledge base, a search\nfor an applicable rule is initiated resulting in the rule given above. After substituting\nthe constant Leaf for the variable x, our next task is to prove the antecedent of the\nrule, VegetarianRestaurant(Leaf), which, of course, is one of the facts we are given.\nNote that it is critical to distinguish between reasoning by backward chaining\nfrom queries to known facts and reasoning backwards from known consequents to\nunknown antecedents. To be speciﬁc, by reasoning backwards we mean that if the\nconsequent of a rule is known to be true, we assume that the antecedent will be as\nwell. For example, let’s assume that we know that Serves(Leaf,VegetarianFood) is\ntrue. Since this fact matches the consequent of our rule, we might reason backwards\nto the conclusion that VegetarianRestaurant(Leaf).\nWhile backward chaining is a sound method of reasoning, reasoning backwards\nis an invalid, though frequently useful, form of plausible reasoning. Plausible rea-\nsoning from consequents to antecedents is known as abduction, and as we show in\nabduction\nChapter 21, is often useful in accounting for many of the inferences people make\nwhile analyzing extended discourses.\nWhile forward and backward reasoning are sound, neither is complete. This\ncomplete\nmeans that there are valid inferences that cannot be found by systems using these\nmethods alone. Fortunately, there is an alternative inference technique called reso-\nlution that is sound and complete. Unfortunately, inference systems based on res-\nresolution\nolution are far more computationally expensive than forward or backward chaining\nsystems. In practice, therefore, most systems use some form of chaining and place\na burden on knowledge-base developers to encode the knowledge in a fashion that\npermits the necessary inferences to be drawn.\n14.4\nEvent and State Representations\nMuch of the semantics that we wish to capture consists of representations of states\nand events. States are conditions, or properties, that remain unchanged over an\nextended period of time, and events denote changes in some state of affairs. The\nrepresentation of both states and events may involve a host of participants, props,\ntimes and locations.\nThe representations for events and states that we have used thus far have con-\nsisted of single predicates with as many arguments as are needed to incorporate all\nthe roles associated with a given example. For example, the representation for Leaf\nserves vegetarian fare consists of a single predicate with arguments for the entity\ndoing the serving and the thing served.\nServes(Leaf,VegetarianFare)\n(14.39)\nThis approach assumes that the predicate used to represent an event verb has the\nsame number of arguments as are present in the verb’s syntactic subcategorization\nframe. Unfortunately, this is clearly not always the case. Consider the following\nexamples of the verb eat:\n(14.40) I ate.",
  "320": "312\nCHAPTER 14\n•\nTHE REPRESENTATION OF SENTENCE MEANING\n(14.41) I ate a turkey sandwich.\n(14.42) I ate a turkey sandwich at my desk.\n(14.43) I ate at my desk.\n(14.44) I ate lunch.\n(14.45) I ate a turkey sandwich for lunch.\n(14.46) I ate a turkey sandwich for lunch at my desk.\nClearly, choosing the correct number of arguments for the predicate represent-\ning the meaning of eat is a tricky problem. These examples introduce ﬁve distinct\narguments, or roles, in an array of different syntactic forms, locations, and combina-\ntions. Unfortunately, predicates in FOL have ﬁxed arity – they take a ﬁxed number\narity\nof arguments.\nTo address this problem, we introduce the notion of an event variable to allow\nevent variable\nus to make assertions about particular events. To do this, we can refactor our event\npredicates to have an existentially quantiﬁed variable as their ﬁrst, and only, argu-\nment. Using this event variable, we can introduce additional predicates to represent\nthe other information we have about the event. These predicates take an event vari-\nable as their ﬁrst argument and related FOL terms as their second argument. The\nfollowing formula illustrates this scheme with the meaning representation of 14.41\nfrom our earlier discussion.\n∃e Eating(e) ∧Eater(e,Speaker)∧Eaten(e,TurkeySandwich)\nHere, the quantiﬁed variable e stands for the eating event and is used to bind the\nevent predicate with the core information provided via the named roles Eater and\nEaten. To handle the more complex examples, we simply add additional relations\nto capture the provided information, as in the following for 14.46.\n∃e Eating(e) ∧Eater(e,Speaker)∧Eaten(e,TurkeySandwich)\n(14.47)\n∧Meal(e,Lunch)∧Location(e,Desk)\nEvent representations of this sort are referred to as neo-Davidsonian event repre-\nneo-\nDavidsonian\nsentations (Davidson, 1967; Parsons, 1990) after the philosopher Donald Davidson\nwho introduced the notion of an event variable (Davidson, 1967). To summarize, in\nthe neo-Davidsonian approach to event representations:\n• Events are captured with predicates that take a single event variable as an\nargument.\n• There is no need to specify a ﬁxed number of arguments for a given FOL\npredicate; rather, as many roles and ﬁllers can be glued on as are provided in\nthe input.\n• No more roles are postulated than are mentioned in the input.\n• The logical connections among closely related inputs that share the same pred-\nicate are satisﬁed without the need for additional inference.\nThis approach still leaves us with the problem of determining the set of predi-\ncates needed to represent roles associated with speciﬁc events like Eater and Eaten,\nas well as more general concepts like Location and Time. We’ll return to this prob-\nlem in more detail in Chapter 18.\n14.4.1\nRepresenting Time\nIn our discussion of events, we did not seriously address the issue of capturing the\ntime when the represented events are supposed to have occurred. The representation",
  "321": "14.4\n•\nEVENT AND STATE REPRESENTATIONS\n313\nof such information in a useful form is the domain of temporal logic. This dis-\ntemporal logic\ncussion introduces the most basic concerns of temporal logic and brieﬂy discusses\nthe means by which human languages convey temporal information, which, among\nother things, includes tense logic, the ways that verb tenses convey temporal infor-\ntense logic\nmation. A more detailed discussion of robust approaches to the representation and\nanalysis of temporal expressions is presented in Chapter 17.\nThe most straightforward theory of time holds that it ﬂows inexorably forward\nand that events are associated with either points or intervals in time, as on a timeline.\nGiven these notions, we can order distinct events by situating them on the timeline.\nMore speciﬁcally, we can say that one event precedes another if the ﬂow of time\nleads from the ﬁrst event to the second. Accompanying these notions in most theo-\nries is the idea of the current moment in time. Combining this notion with the idea\nof a temporal ordering relationship yields the familiar notions of past, present, and\nfuture.\nNot surprisingly, a large number of schemes can represent this kind of temporal\ninformation. The one presented here is a fairly simple one that stays within the FOL\nframework of reiﬁed events that we have been pursuing. Consider the following\nexamples:\n(14.48) I arrived in New York.\n(14.49) I am arriving in New York.\n(14.50) I will arrive in New York.\nThese sentences all refer to the same kind of event and differ solely in the tense of\nthe verb. In our current scheme for representing events, all three would share the\nfollowing kind of representation, which lacks any temporal information:\n∃eArriving(e)∧Arriver(e,Speaker)∧Destination(e,NewYork)\n(14.51)\nThe temporal information provided by the tense of the verbs can be exploited\nby predicating additional information about the event variable e. Speciﬁcally, we\ncan add temporal variables representing the interval corresponding to the event, the\nend point of the event, and temporal predicates relating this end point to the current\ntime as indicated by the tense of the verb. Such an approach yields the following\nrepresentations for our arriving examples:\n∃e,i,n Arriving(e) ∧Arriver(e,Speaker)∧Destination(e,NewYork)\n∧IntervalOf(e,i)∧EndPoint(i,n)∧Precedes(n,Now)\n∃e,i,n Arriving(e) ∧Arriver(e,Speaker)∧Destination(e,NewYork)\n∧IntervalOf(e,i)∧MemberOf(i,Now)\n∃e,i,n Arriving(e) ∧Arriver(e,Speaker)∧Destination(e,NewYork)\n∧IntervalOf(e,i)∧EndPoint(i,n)∧Precedes(Now,n)\nThis representation introduces a variable to stand for the interval of time as-\nsociated with the event and a variable that stands for the end of that interval. The\ntwo-place predicate Precedes represents the notion that the ﬁrst time-point argument\nprecedes the second in time; the constant Now refers to the current time. For past\nevents, the end point of the interval must precede the current time. Similarly, for fu-\nture events the current time must precede the end of the event. For events happening\nin the present, the current time is contained within the event interval.\nUnfortunately, the relation between simple verb tenses and points in time is by\nno means straightforward. Consider the following examples:",
  "322": "314\nCHAPTER 14\n•\nTHE REPRESENTATION OF SENTENCE MEANING\n(14.52) Ok, we ﬂy from San Francisco to Boston at 10.\n(14.53) Flight 1390 will be at the gate an hour now.\nIn the ﬁrst example, the present tense of the verb ﬂy is used to refer to a future event,\nwhile in the second the future tense is used to refer to a past event.\nMore complications occur when we consider some of the other verb tenses. Con-\nsider the following examples:\n(14.54) Flight 1902 arrived late.\n(14.55) Flight 1902 had arrived late.\nAlthough both refer to events in the past, representing them in the same way seems\nwrong. The second example seems to have another unnamed event lurking in the\nbackground (e.g., Flight 1902 had already arrived late when something else hap-\npened). To account for this phenomena, Reichenbach (1947) introduced the notion\nof a reference point. In our simple temporal scheme, the current moment in time\nreference point\nis equated with the time of the utterance and is used as a reference point for when\nthe event occurred (before, at, or after). In Reichenbach’s approach, the notion of\nthe reference point is separated from the utterance time and the event time. The\nfollowing examples illustrate the basics of this approach:\n(14.56) When Mary’s ﬂight departed, I ate lunch.\n(14.57) When Mary’s ﬂight departed, I had eaten lunch.\nIn both of these examples, the eating event has happened in the past, that is, prior\nto the utterance. However, the verb tense in the ﬁrst example indicates that the eating\nevent began when the ﬂight departed, while the second example indicates that the\neating was accomplished prior to the ﬂight’s departure. Therefore, in Reichenbach’s\nterms the departure event speciﬁes the reference point. These facts can be accom-\nmodated by additional constraints relating the eating and departure events. In the\nﬁrst example, the reference point precedes the eating event, and in the second exam-\nple, the eating precedes the reference point. Figure 14.5 illustrates Reichenbach’s\napproach with the primary English tenses. Exercise 14.6 asks you to represent these\nexamples in FOL.\nPast Perfect\nSimple Past\nPresent Perfect\nSimple Future\nFuture Perfect\nPresent\nE\nE\nE\nE\nR\nR\nU\nR,E\nU\nR,U\nU,R,E\nU,R\nU\nFigure 14.5\nReichenbach’s approach applied to various English tenses. In these diagrams,\ntime ﬂows from left to right, an E denotes the time of the event, an R denotes the reference\ntime, and an U denotes the time of the utterance.",
  "323": "14.4\n•\nEVENT AND STATE REPRESENTATIONS\n315\nThis discussion has focused narrowly on the broad notions of past, present, and\nfuture and how they are signaled by various English verb tenses. Of course, lan-\nguages also have many other more direct and more speciﬁc ways to convey temporal\ninformation, including the use of a wide variety of temporal expressions, as in the\nfollowing ATIS examples:\n(14.58) I’d like to go at 6:45, in the morning.\n(14.59) Somewhere around noon, please.\nAs we show in Chapter 17, grammars for such temporal expressions are of consid-\nerable practical importance to information extraction and question-answering appli-\ncations.\nFinally, we should note that a systematic conceptual organization is reﬂected in\nexamples like these. In particular, temporal expressions in English are frequently ex-\npressed in spatial terms, as is illustrated by the various uses of at, in, somewhere, and\nnear in these examples (Lakoff and Johnson, 1980; Jackendoff, 1983). Metaphori-\ncal organizations such as these, in which one domain is systematically expressed in\nterms of another, are very common in languages of the world.\n14.4.2\nAspect\nIn the last section, we discussed ways to represent the time of an event with respect\nto the time of an utterance describing it. In this section, we address the notion of\naspect, which concerns a cluster of related topics, including whether an event has\naspect\nended or is ongoing, whether it is conceptualized as happening at a point in time or\nover some interval, and whether any particular state in the world comes about be-\ncause of it. Based on these and related notions, event expressions have traditionally\nbeen divided into four general classes illustrated in the following examples:\nStative: I know my departure gate.\nActivity: John is ﬂying.\nAccomplishment: Sally booked her ﬂight.\nAchievement: She found her gate.\nAlthough the earliest versions of this classiﬁcation were discussed by Aristotle,\nthe one presented here is due to Vendler (1967).\nStative expressions represent the notion of an event participant having a partic-\nStative\nexpressions\nular property, or being in a state, at a given point in time. As such, these expressions\ncan be thought of as capturing an aspect of a world at a single point in time. Consider\nthe following ATIS examples.\n(14.60) I like Flight 840 arriving at 10:06.\n(14.61) I need the cheapest fare.\n(14.62) I want to go ﬁrst class.\nIn examples like these, the event participant denoted by the subject can be seen as\nexperiencing something at a speciﬁc point in time. Whether or not the experiencer\nwas in the same state earlier or will be in the future is left unspeciﬁed.\nActivity expressions describe events undertaken by a participant and have no\nActivity\nexpressions\nparticular end point. Unlike statives, activities are seen as occurring over some span\nof time and are therefore not associated with single points in time. Consider the\nfollowing examples:\n(14.63) She drove a Mazda.",
  "324": "316\nCHAPTER 14\n•\nTHE REPRESENTATION OF SENTENCE MEANING\n(14.64) I live in Brooklyn.\nThese examples both specify that the subject is engaged in, or has engaged in, the\nactivity speciﬁed by the verb for some period of time.\nThe ﬁnal aspectual class, achievement expressions, is similar to accomplish-\nachievement\nexpressions\nments in that these expressions result in a state. Consider the following:\n(14.65) She found her gate.\n(14.66) I reached New York.\nUnlike accomplishments, achievement events are thought of as happening in an in-\nstant and are not equated with any particular activity leading up to the state. To be\nmore speciﬁc, the events in these examples may have been preceded by extended\nsearching or traveling events, but the events corresponding directly to found and\nreach are conceived of as points, not intervals.\nNote that since both accomplishments and achievements are events that result\nin a state, they are sometimes characterized as subtypes of a single aspectual class.\nMembers of this combined class are known as telic eventualities.\ntelic\neventualities\n14.5\nDescription Logics\nAs noted at the beginning of this chapter, a fair number of representational schemes\nhave been invented to capture the meaning of linguistic utterances. It is now widely\naccepted that meanings represented in these various approaches can, in principle, be\ntranslated into equivalent statements in FOL with relative ease. The difﬁculty is that\nin many of these approaches the semantics of a statement are deﬁned procedurally.\nThat is, the meaning arises from whatever the system that interprets it does with it.\nDescription logics are an effort to better specify the semantics of these earlier\nstructured network representations and to provide a conceptual framework that is\nespecially well suited to certain kinds of domain modeling. Formally, the term De-\nscription Logics refers to a family of logical approaches that correspond to varying\nsubsets of FOL. The restrictions placed on the expressiveness of Description Logics\nserve to guarantee the tractability of various critical kinds of inference. Our focus\nhere, however, will be on the modeling aspects of DLs rather than on computational\ncomplexity issues.\nWhen using Description Logics to model an application domain, the emphasis\nis on the representation of knowledge about categories, individuals that belong to\nthose categories, and the relationships that can hold among these individuals. The\nset of categories, or concepts, that make up a particular application domain is called\nits terminology. The portion of a knowledge base that contains the terminology is\nterminology\ntraditionally called the TBox; this is in contrast to the ABox that contains facts about\nTBox\nABox\nindividuals. The terminology is typically arranged into a hierarchical organization\ncalled an ontology that captures the subset/superset relations among the categories.\nontology\nReturning to our earlier culinary domain, we represented domain concepts like\nusing unary predicates such as Restaurant(x); the DL equivalent simply omits the\nvariable, so the restaurant category is simply written as Restaurant.3 To capture\nthe fact that a particular domain element, such as Frasca, is a restaurant, we assert\nRestaurant(Frasca) in much the same way we would in FOL. The semantics of\n3\nDL statements are conventionally typeset with a sans serif font. We’ll follow that convention here,\nreverting to our standard mathematical notation when giving FOL equivalents of DL statements.",
  "325": "14.5\n•\nDESCRIPTION LOGICS\n317\nthese categories are speciﬁed in precisely the same way that was introduced earlier in\nSection 14.2: a category like Restaurant simply denotes the set of domain elements\nthat are restaurants.\nOnce we’ve speciﬁed the categories of interest in a particular domain, the next\nstep is to arrange them into a hierarchical structure. There are two ways to cap-\nture the hierarchical relationships present in a terminology: we can directly assert\nrelations between categories that are related hierarchically, or we can provide com-\nplete deﬁnitions for our concepts and then rely on inference to provide hierarchical\nrelationships. The choice between these methods hinges on the use to which the re-\nsulting categories will be put and the feasibility of formulating precise deﬁnitions for\nmany naturally occurring categories. We’ll discuss the ﬁrst option here and return to\nthe notion of deﬁnitions later in this section.\nTo directly specify a hierarchical structure, we can assert subsumption relations\nsubsumption\nbetween the appropriate concepts in a terminology. The subsumption relation is\nconventionally written as C ⊑D and is read as C is subsumed by D; that is, all\nmembers of the category C are also members of the category D. Not surprisingly, the\nformal semantics of this relation are provided by a simple set relation; any domain\nelement that is in the set denoted by C is also in the set denoted by D.\nAdding the following statements to the TBox asserts that all restaurants are com-\nmercial establishments and, moreover, that there are various subtypes of restaurants.\nRestaurant ⊑CommercialEstablishment\n(14.67)\nItalianRestaurant ⊑Restaurant\n(14.68)\nChineseRestaurant ⊑Restaurant\n(14.69)\nMexicanRestaurant ⊑Restaurant\n(14.70)\nOntologies such as this are conventionally illustrated with diagrams such as the one\nshown in Fig. 14.6, where subsumption relations are denoted by links between the\nnodes representing the categories.\nRestaurant\nChinese\nRestaurant \nMexican\nRestaurant\nItalian\nRestaurant\nCommercial\nEstablishment\nFigure 14.6\nA graphical network representation of a set of subsumption relations in the\nrestaurant domain.\nNote, that it was precisely the vague nature of semantic network diagrams like\nthis that motivated the development of Description Logics. For example, from this",
  "326": "318\nCHAPTER 14\n•\nTHE REPRESENTATION OF SENTENCE MEANING\ndiagram we can’t tell whether the given set of categories is exhaustive or disjoint.\nThat is, we can’t tell if these are all the kinds of restaurants that we’ll be dealing with\nin our domain or whether there might be others. We also can’t tell if an individual\nrestaurant must fall into only one of these categories, or if it is possible, for example,\nfor a restaurant to be both Italian and Chinese. The DL statements given above are\nmore transparent in their meaning; they simply assert a set of subsumption relations\nbetween categories and make no claims about coverage or mutual exclusion.\nIf an application requires coverage and disjointness information, then such in-\nformation must be made explicitly. The simplest ways to capture this kind of in-\nformation is through the use of negation and disjunction operators. For example,\nthe following assertion would tell us that Chinese restaurants can’t also be Italian\nrestaurants.\nChineseRestaurant ⊑not ItalianRestaurant\n(14.71)\nSpecifying that a set of subconcepts covers a category can be achieved with disjunc-\ntion, as in the following:\nRestaurant ⊑\n(14.72)\n(or ItalianRestaurant ChineseRestaurant MexicanRestaurant)\nHaving a hierarchy such as the one given in Fig. 14.6 tells us next to nothing\nabout the concepts in it. We certainly don’t know anything about what makes a\nrestaurant a restaurant, much less Italian, Chinese, or expensive. What is needed are\nadditional assertions about what it means to be a member of any of these categories.\nIn Description Logics such statements come in the form of relations between the\nconcepts being described and other concepts in the domain. In keeping with its\norigins in structured network representations, relations in Description Logics are\ntypically binary and are often referred to as roles, or role-relations.\nTo see how such relations work, let’s consider some of the facts about restaurants\ndiscussed earlier in the chapter. We’ll use the hasCuisine relation to capture infor-\nmation as to what kinds of food restaurants serve and the hasPriceRange relation\nto capture how pricey particular restaurants tend to be. We can use these relations\nto say something more concrete about our various classes of restaurants. Let’s start\nwith our ItalianRestaurant concept. As a ﬁrst approximation, we might say some-\nthing uncontroversial like Italian restaurants serve Italian cuisine. To capture these\nnotions, let’s ﬁrst add some new concepts to our terminology to represent various\nkinds of cuisine.\nMexicanCuisine ⊑Cuisine\nItalianCuisine ⊑Cuisine\nChineseCuisine ⊑Cuisine\nVegetarianCuisine ⊑Cuisine\nExpensiveRestaurant ⊑Restaurant\nModerateRestaurant ⊑Restaurant\nCheapRestaurant ⊑Restaurant\nNext, let’s revise our earlier version of ItalianRestaurant to capture cuisine\ninformation.\nItalianRestaurant ⊑Restaurant⊓∃hasCuisine.ItalianCuisine\n(14.73)\nThe correct way to read this expression is that individuals in the category Italian-\nRestaurant are subsumed both by the category Restaurant and by an unnamed",
  "327": "14.5\n•\nDESCRIPTION LOGICS\n319\nclass deﬁned by the existential clause—the set of entities that serve Italian cuisine.\nAn equivalent statement in FOL would be\n∀xItalianRestaurant(x) →Restaurant(x)\n(14.74)\n∧(∃yServes(x,y)∧ItalianCuisine(y))\nThis FOL translation should make it clear what the DL assertions given above do\nand do not entail. In particular, they don’t say that domain entities classiﬁed as Ital-\nian restaurants can’t engage in other relations like being expensive or even serving\nChinese cuisine. And critically, they don’t say much about domain entities that we\nknow do serve Italian cuisine. In fact, inspection of the FOL translation makes it\nclear that we cannot infer that any new entities belong to this category based on their\ncharacteristics. The best we can do is infer new facts about restaurants that we’re\nexplicitly told are members of this category.\nOf course, inferring the category membership of individuals given certain char-\nacteristics is a common and critical reasoning task that we need to support. This\nbrings us back to the alternative approach to creating hierarchical structures in a\nterminology: actually providing a deﬁnition of the categories we’re creating in the\nform of necessary and sufﬁcient conditions for category membership. In this case,\nwe might explicitly provide a deﬁnition for ItalianRestaurant as being those restau-\nrants that serve Italian cuisine, and ModerateRestaurant as being those whose\nprice range is moderate.\nItalianRestaurant ≡Restaurant⊓∃hasCuisine.ItalianCuisine\n(14.75)\nModerateRestaurant ≡Restaurant⊓hasPriceRange.ModeratePrices\n(14.76)\nWhile our earlier statements provided necessary conditions for membership in these\ncategories, these statements provide both necessary and sufﬁcient conditions.\nFinally, let’s now consider the superﬁcially similar case of vegetarian restaurants.\nClearly, vegetarian restaurants are those that serve vegetarian cuisine. But they don’t\nmerely serve vegetarian fare, that’s all they serve. We can accommodate this kind of\nconstraint by adding an additional restriction in the form of a universal quantiﬁer to\nour earlier description of VegetarianRestaurants, as follows:\nVegetarianRestaurant ≡Restaurant\n(14.77)\n⊓∃hasCuisine.VegetarianCuisine\n⊓∀hasCuisine.VegetarianCuisine\nInference\nParalleling the focus of Description Logics on categories, relations, and individuals\nis a processing focus on a restricted subset of logical inference. Rather than employ-\ning the full range of reasoning permitted by FOL, DL reasoning systems emphasize\nthe closely coupled problems of subsumption and instance checking.\nSubsumption, as a form of inference, is the task of determining, based on the\nSubsumption\nfacts asserted in a terminology, whether a superset/subset relationship exists between\ntwo concepts. Correspondingly, instance checking asks if an individual can be a\ninstance\nchecking\nmember of a particular category given the facts we know about both the individual\nand the terminology. The inference mechanisms underlying subsumption and in-\nstance checking go beyond simply checking for explicitly stated subsumption rela-\ntions in a terminology. They must explicitly reason using the relational information",
  "328": "320\nCHAPTER 14\n•\nTHE REPRESENTATION OF SENTENCE MEANING\nRestaurant\nChinese\nRestaurant \nMexican\nRestaurant\nItalian\nRestaurant\nExpensive\nRestaurant\nCheap\nRestaurant\nModerate\nRestaurant\nIlFornaio\nVegetarian\nRestaurant\nFigure 14.7\nA graphical network representation of the complete set of subsumption rela-\ntions in the restaurant domain given the current set of assertions in the TBox.\nasserted about the terminology to infer appropriate subsumption and membership\nrelations.\nReturning to our restaurant domain, let’s add a new kind of restaurant using the\nfollowing statement:\nIlFornaio ⊑ModerateRestaurant⊓∃hasCuisine.ItalianCuisine\n(14.78)\nGiven this assertion, we might ask whether the IlFornaio chain of restaurants might\nbe classiﬁed as an Italian restaurant or a vegetarian restaurant. More precisely, we\ncan pose the following questions to our reasoning system:\nIlFornaio ⊑ItalianRestaurant\n(14.79)\nIlFornaio ⊑VegetarianRestaurant\n(14.80)\nThe answer to the ﬁrst question is positive since IlFornaio meets the criteria we\nspeciﬁed for the category ItalianRestaurant: it’s a Restaurant since we explicitly\nclassiﬁed it as a ModerateRestaurant, which is a subtype of Restaurant, and it\nmeets the has.Cuisine class restriction since we’ve asserted that directly.\nThe answer to the second question is negative. Recall, that our criteria for veg-\netarian restaurants contains two requirements: it has to serve vegetarian fare, and\nthat’s all it can serve. Our current deﬁnition for IlFornaio fails on both counts since\nwe have not asserted any relations that state that IlFornaio serves vegetarian fare,\nand the relation we have asserted, hasCuisine.ItalianCuisine, contradicts the sec-\nond criteria.\nA related reasoning task, based on the basic subsumption inference, is to derive\nthe implied hierarchy for a terminology given facts about the categories in the ter-\nimplied\nhierarchy\nminology. This task roughly corresponds to a repeated application of the subsump-\ntion operator to pairs of concepts in the terminology. Given our current collection of\nstatements, the expanded hierarchy shown in Fig. 14.7 can be inferred. You should\nconvince yourself that this diagram contains all and only the subsumption links that\nshould be present given our current knowledge.\nInstance checking is the task of determining whether a particular individual can\nbe classiﬁed as a member of a particular category. This process takes what is known",
  "329": "14.6\n•\nSUMMARY\n321\nabout a given individual, in the form of relations and explicit categorical statements,\nand then compares that information with what is known about the current terminol-\nogy. It then returns a list of the most speciﬁc categories to which the individual can\nbelong.\nAs an example of a categorization problem, consider an establishment that we’re\ntold is a restaurant and serves Italian cuisine.\nRestaurant(Gondolier)\nhasCuisine(Gondolier,ItalianCuisine)\nHere, we’re being told that the entity denoted by the term Gondolier is a restau-\nrant and serves Italian food. Given this new information and the contents of our\ncurrent TBox, we might reasonably like to ask if this is an Italian restaurant, if it is\na vegetarian restaurant, or if it has moderate prices.\nAssuming the deﬁnitional statements given earlier, we can indeed categorize\nthe Gondolier as an Italian restaurant. That is, the information we’ve been given\nabout it meets the necessary and sufﬁcient conditions required for membership in\nthis category. And as with the IlFornaio category, this individual fails to match the\nstated criteria for the VegetarianRestaurant. Finally, the Gondolier might also\nturn out to be a moderately priced restaurant, but we can’t tell at this point since\nwe don’t know anything about its prices. What this means is that given our current\nknowledge the answer to the query ModerateRestaurant(Gondolier) would be false\nsince it lacks the required hasPriceRange relation.\nThe implementation of subsumption, instance checking, as well as other kinds of\ninferences needed for practical applications, varies according to the expressivity of\nthe Description Logic being used. However, for a Description Logic of even modest\npower, the primary implementation techniques are based on satisﬁability methods\nthat in turn rely on the underlying model-based semantics introduced earlier in this\nchapter.\nOWL and the Semantic Web\nThe highest-proﬁle role for Description Logics, to date, has been as a part of the\ndevelopment of the Semantic Web. The Semantic Web is an ongoing effort to pro-\nvide a way to formally specify the semantics of the contents of the Web (Fensel\net al., 2003). A key component of this effort involves the creation and deployment\nof ontologies for various application areas of interest. The meaning representation\nlanguage used to represent this knowledge is the Web Ontology Language (OWL)\nWeb Ontology\nLanguage\n(McGuiness and van Harmelen, 2004). OWL embodies a Description Logic that\ncorresponds roughly to the one we’ve been describing here.\n14.6\nSummary\nThis chapter has introduced the representational approach to meaning. The follow-\ning are some of the highlights of this chapter:\n• A major approach to meaning in computational linguistics involves the cre-\nation of formal meaning representations that capture the meaning-related\ncontent of linguistic inputs. These representations are intended to bridge the\ngap from language to common-sense knowledge of the world.",
  "330": "322\nCHAPTER 14\n•\nTHE REPRESENTATION OF SENTENCE MEANING\n• The frameworks that specify the syntax and semantics of these representa-\ntions are called meaning representation languages. A wide variety of such\nlanguages are used in natural language processing and artiﬁcial intelligence.\n• Such representations need to be able to support the practical computational\nrequirements of semantic processing. Among these are the need to determine\nthe truth of propositions, to support unambiguous representations, to rep-\nresent variables, to support inference, and to be sufﬁciently expressive.\n• Human languages have a wide variety of features that are used to convey\nmeaning. Among the most important of these is the ability to convey a predicate-\nargument structure.\n• First-Order Logic is a well-understood, computationally tractable meaning\nrepresentation language that offers much of what is needed in a meaning rep-\nresentation language.\n• Important elements of semantic representation including states and events\ncan be captured in FOL.\n• Semantic networks and frames can be captured within the FOL framework.\n• Modern Description Logics consist of useful and computationally tractable\nsubsets of full First-Order Logic. The most prominent use of a description\nlogic is the Web Ontology Language (OWL), used in the speciﬁcation of the\nSemantic Web.\nBibliographical and Historical Notes\nThe earliest computational use of declarative meaning representations in natural lan-\nguage processing was in the context of question-answering systems (Green et al.,\n1961; Raphael, 1968; Lindsey, 1963). These systems employed ad hoc representa-\ntions for the facts needed to answer questions. Questions were then translated into\na form that could be matched against facts in the knowledge base. Simmons (1965)\nprovides an overview of these early efforts.\nWoods (1967) investigated the use of FOL-like representations in question an-\nswering as a replacement for the ad hoc representations in use at the time. Woods\n(1973) further developed and extended these ideas in the landmark Lunar system.\nInterestingly, the representations used in Lunar had both truth-conditional and pro-\ncedural semantics. Winograd (1972) employed a similar representation based on the\nMicro-Planner language in his SHRDLU system.\nDuring this same period, researchers interested in the cognitive modeling of lan-\nguage and memory had been working with various forms of associative network\nrepresentations. Masterman (1957) was the ﬁrst to make computational use of a\nsemantic network-like knowledge representation, although semantic networks are\ngenerally credited to Quillian (1968). A considerable amount of work in the se-\nmantic network framework was carried out during this era (Norman and Rumelhart,\n1975; Schank, 1972; Wilks, 1975c, 1975b; Kintsch, 1974). It was during this pe-\nriod that a number of researchers began to incorporate Fillmore’s notion of case roles\n(Fillmore, 1968) into their representations. Simmons (1973) was the earliest adopter\nof case roles as part of representations for natural language processing.\nDetailed analyses by Woods (1975) and Brachman (1979) aimed at ﬁguring out\nwhat semantic networks actually mean led to the development of a number of more",
  "331": "EXERCISES\n323\nsophisticated network-like languages including KRL (Bobrow and Winograd, 1977)\nand KL-ONE (Brachman and Schmolze, 1985). As these frameworks became more\nsophisticated and well deﬁned, it became clear that they were restricted variants of\nFOL coupled with specialized indexing inference procedures. A useful collection of\npapers covering much of this work can be found in Brachman and Levesque (1985).\nRussell and Norvig (2002) describe a modern perspective on these representational\nefforts.\nLinguistic efforts to assign semantic structures to natural language sentences in\nthe generative era began with the work of Katz and Fodor (1963). The limitations\nof their simple feature-based representations and the natural ﬁt of logic to many\nof the linguistic problems of the day quickly led to the adoption of a variety of\npredicate-argument structures as preferred semantic representations (Lakoff, 1972;\nMcCawley, 1968). The subsequent introduction by Montague (1973) of the truth-\nconditional model-theoretic framework into linguistic theory led to a much tighter\nintegration between theories of formal syntax and a wide range of formal semantic\nframeworks. Good introductions to Montague semantics and its role in linguistic\ntheory can be found in Dowty et al. (1981) and Partee (1976).\nThe representation of events as reiﬁed objects is due to Davidson (1967). The\napproach presented here, which explicitly reiﬁes event participants, is due to Parsons\n(1990).\nMost current computational approaches to temporal reasoning are based on Allen’s\nnotion of temporal intervals (Allen, 1984); see Chapter 17. ter Meulen (1995) pro-\nvides a modern treatment of tense and aspect. Davis (1990) describes the use of FOL\nto represent knowledge across a wide range of common-sense domains including\nquantities, space, time, and beliefs.\nA recent comprehensive treatment of logic and language can be found in van\nBenthem and ter Meulen (1997). A classic semantics text is Lyons (1977). McCaw-\nley (1993) is an indispensable textbook covering a wide range of topics concerning\nlogic and language. Chierchia and McConnell-Ginet (1991) also broadly covers\nsemantic issues from a linguistic perspective. Heim and Kratzer (1998) is a more\nrecent text written from the perspective of current generative theory.\nExercises\n14.1 Peruse your daily newspaper for three examples of ambiguous sentences or\nheadlines. Describe the various sources of the ambiguities.\n14.2 Consider a domain in which the word coffee can refer to the following con-\ncepts in a knowledge-based system: a caffeinated or decaffeinated beverage,\nground coffee used to make either kind of beverage, and the beans themselves.\nGive arguments as to which of the following uses of coffee are ambiguous and\nwhich are vague.\n1. I’ve had my coffee for today.\n2. Buy some coffee on your way home.\n3. Please grind some more coffee.\n14.3 The following rule, which we gave as a translation for Example 14.26, is not\na reasonable deﬁnition of what it means to be a vegetarian restaurant.\n∀xVegetarianRestaurant(x) =⇒Serves(x,VegetarianFood)",
  "332": "324\nCHAPTER 14\n•\nTHE REPRESENTATION OF SENTENCE MEANING\nGive a FOL rule that better deﬁnes vegetarian restaurants in terms of what they\nserve.\n14.4 Give FOL translations for the following sentences:\n1. Vegetarians do not eat meat.\n2. Not all vegetarians eat eggs.\n14.5 Give a set of facts and inferences necessary to prove the following assertions:\n1. McDonald’s is not a vegetarian restaurant.\n2. Some vegetarians can eat at McDonald’s.\nDon’t just place these facts in your knowledge base. Show that they can be\ninferred from some more general facts about vegetarians and McDonald’s.\n14.6 For the following sentences, give FOL translations that capture the temporal\nrelationships between the events.\n1. When Mary’s ﬂight departed, I ate lunch.\n2. When Mary’s ﬂight departed, I had eaten lunch.\n14.7 On page 309, we gave the representation Near(Centro,Bacaro) as a transla-\ntion for the sentence Centro is near Bacaro. In a truth-conditional semantics,\nthis formula is either true or false given some model. Critique this truth-\nconditional approach with respect to the meaning of words like near.",
  "333": "CHAPTER\n15\nComputational Semantics\nPlaceholder\n325",
  "334": "CHAPTER\n16\nSemantic Parsing\nPlaceholder\n326",
  "335": "CHAPTER\n17\nInformation Extraction\nI am the very model of a modern Major-General,\nI’ve information vegetable, animal, and mineral,\nI know the kings of England, and I quote the ﬁghts historical\nFrom Marathon to Waterloo, in order categorical...\nGilbert and Sullivan, Pirates of Penzance\nImagine that you are an analyst with an investment ﬁrm that tracks airline stocks.\nYou’re given the task of determining the relationship (if any) between airline an-\nnouncements of fare increases and the behavior of their stocks the next day. His-\ntorical data about stock prices is easy to come by, but what about the airline an-\nnouncements? You will need to know at least the name of the airline, the nature of\nthe proposed fare hike, the dates of the announcement, and possibly the response of\nother airlines. Fortunately, these can be all found in news articles like this one:\nCiting high fuel prices, United Airlines said Friday it has increased fares\nby $6 per round trip on ﬂights to some cities also served by lower-\ncost carriers. American Airlines, a unit of AMR Corp., immediately\nmatched the move, spokesman Tim Wagner said. United, a unit of UAL\nCorp., said the increase took effect Thursday and applies to most routes\nwhere it competes against discount carriers, such as Chicago to Dallas\nand Denver to San Francisco.\nThis chapter presents techniques for extracting limited kinds of semantic con-\ntent from text. This process of information extraction (IE), turns the unstructured\ninformation\nextraction\ninformation embedded in texts into structured data, for example for populating a\nrelational database to enable further processing.\nWe begin with the ﬁrst step in most IE tasks, ﬁnding the proper names or named\nentities in a text. The task of named entity recognition (NER) is to ﬁnd each\nnamed entity\nrecognition\nmention of a named entity in the text and label its type. What constitutes a named\nentity type is task speciﬁc; people, places, and organizations are common, but gene\nor protein names (Cohen and Demner-Fushman, 2014) or ﬁnancial asset classes\nmight be relevant for some tasks. Once all the named entities in a text have been\nextracted, they can be linked together in sets corresponding to real-world entities,\ninferring, for example, that mentions of United Airlines and United refer to the same\ncompany. This is the joint task of coreference resolution and entity linking which\nwe defer til Chapter 20.\nNext, we turn to the task of relation extraction: ﬁnding and classifying semantic\nrelation\nextraction\nrelations among the text entities. These are often binary relations like child-of, em-\nployment, part-whole, and geospatial relations. Relation extraction has close links\nto populating a relational database.\nFinally, we discuss three tasks related to events. Event extraction is ﬁnding\nevent\nextraction\nevents in which these entities participate, like, in our sample text, the fare increases",
  "336": "328\nCHAPTER 17\n•\nINFORMATION EXTRACTION\nby United and American and the reporting events said and cite. Event coreference\n(Chapter 20) is needed to ﬁgure out which event mentions in a text refer to the same\nevent; in our running example the two instances of increase and the phrase the move\nall refer to the same event.\nTo ﬁgure out when the events in a text happened we extract temporal expres-\nsions like days of the week (Friday and Thursday), relative expressions like two\ntemporal\nexpression\ndays from now or next year and times such as 3:30 P.M.. These expressions must be\nnormalized onto speciﬁc calendar dates or times of day to situate events in time. In\ntemporal\nnormalization\nour sample task, this will allow us to link Friday to the time of United’s announce-\nment, and Thursday to the previous day’s fare increase, and produce a timeline in\nwhich United’s announcement follows the fare increase and American’s announce-\nment follows both of those events.\nFinally, many texts describe recurring stereotypical events or situations. The task\nof template ﬁlling is to ﬁnd such situations in documents and ﬁll in the template\ntemplate ﬁlling\nslots. These slot-ﬁllers may consist of text segments extracted directly from the text,\nor concepts like times, amounts, or ontology entities that have been inferred from\ntext elements through additional processing.\nOur airline text is an example of this kind of stereotypical situation since airlines\noften raise fares and then wait to see if competitors follow along. In this situa-\ntion, we can identify United as a lead airline that initially raised its fares, $6 as the\namount, Thursday as the increase date, and American as an airline that followed\nalong, leading to a ﬁlled template like the following.\nFARE-RAISE ATTEMPT:\n\n\nLEAD AIRLINE:\nUNITED AIRLINES\nAMOUNT:\n$6\nEFFECTIVE DATE:\n2006-10-26\nFOLLOWER:\nAMERICAN AIRLINES\n\n\n17.1\nNamed Entity Recognition\nThe ﬁrst step in information extraction is to detect the entities in the text. A named\nentity is, roughly speaking, anything that can be referred to with a proper name:\nnamed entity\na person, a location, an organization. The term is commonly extended to include\nthings that aren’t entities per se, including dates, times, and other kinds of temporal\nexpressions, and even numerical expressions like prices. Here’s the sample text\ntemporal\nexpressions\nintroduced earlier with the named entities marked:\nCiting high fuel prices, [ORG United Airlines] said [TIME Friday] it\nhas increased fares by [MONEY $6] per round trip on ﬂights to some\ncities also served by lower-cost carriers. [ORG American Airlines], a\nunit of [ORG AMR Corp.], immediately matched the move, spokesman\n[PER Tim Wagner] said. [ORG United], a unit of [ORG UAL Corp.],\nsaid the increase took effect [TIME Thursday] and applies to most\nroutes where it competes against discount carriers, such as [LOC Chicago]\nto [LOC Dallas] and [LOC Denver] to [LOC San Francisco].\nThe text contains 13 mentions of named entities including 5 organizations, 4 loca-\ntions, 2 times, 1 person, and 1 mention of money.\nIn addition to their use in extracting events and the relationship between par-\nticipants, named entities are useful for many other language processing tasks. In",
  "337": "17.1\n•\nNAMED ENTITY RECOGNITION\n329\nsentiment analysis we might want to know a consumer’s sentiment toward a partic-\nular entity. Entities are a useful ﬁrst stage in question answering, or for linking text\nto information in structured knowledge sources like Wikipedia.\nFigure 17.1 shows typical generic named entity types. Many applications will\nalso need to use speciﬁc entity types like proteins, genes, commercial products, or\nworks of art.\nType\nTag Sample Categories\nExample sentences\nPeople\nPER people, characters\nTuring is a giant of computer science.\nOrganization ORG companies, sports teams\nThe IPCC warned about the cyclone.\nLocation\nLOC regions, mountains, seas\nThe Mt. Sanitas loop is in Sunshine Canyon.\nGeo-Political\nEntity\nGPE countries, states, provinces\nPalo Alto is raising the fees for parking.\nFacility\nFAC bridges, buildings, airports\nConsider the Golden Gate Bridge.\nVehicles\nVEH planes, trains, automobiles\nIt was a classic Ford Falcon.\nFigure 17.1\nA list of generic named entity types with the kinds of entities they refer to.\nNamed entity recognition means ﬁnding spans of text that constitute proper\nnames and then classifying the type of the entity. Recognition is difﬁcult partly be-\ncause of the ambiguity of segmentation; we need to decide what’s an entity and what\nisn’t, and where the boundaries are. Another difﬁculty is caused by type ambiguity.\nThe mention JFK can refer to a person, the airport in New York, or any number of\nschools, bridges, and streets around the United States. Some examples of this kind\nof cross-type confusion are given in Figures 17.2 and 17.3.\nName\nPossible Categories\nWashington\nPerson, Location, Political Entity, Organization, Vehicle\nDowning St.\nLocation, Organization\nIRA\nPerson, Organization, Monetary Instrument\nLouis Vuitton\nPerson, Organization, Commercial Product\nFigure 17.2\nCommon categorical ambiguities associated with various proper names.\n[PER Washington] was born into slavery on the farm of James Burroughs.\n[ORG Washington] went up 2 games to 1 in the four-game series.\nBlair arrived in [LOC Washington] for what may well be his last state visit.\nIn June, [GPE Washington] passed a primary seatbelt law.\nThe [VEH Washington] had proved to be a leaky ship, every passage I made...\nFigure 17.3\nExamples of type ambiguities in the use of the name Washington.\n17.1.1\nNER as Sequence Labeling\nThe standard algorithm for named entity recognition is as a word-by-word sequence\nlabeling task, in which the assigned tags capture both the boundary and the type. A\nsequence classiﬁer like an MEMM/CRF or a bi-LSTM is trained to label the tokens\nin a text with tags that indicate the presence of particular kinds of named entities.\nConsider the following simpliﬁed excerpt from our running example.\n[ORG American Airlines], a unit of [ORG AMR Corp.], immediately matched\nthe move, spokesman [PER Tim Wagner] said.",
  "338": "330\nCHAPTER 17\n•\nINFORMATION EXTRACTION\nFigure 17.4 shows the same excerpt represented with IOB tagging. In IOB tag-\nIOB\nging we introduce a tag for the beginning (B) and inside (I) of each entity type,\nand one for tokens outside (O) any entity. The number of tags is thus 2n + 1 tags,\nwhere n is the number of entity types. IOB tagging can represent exactly the same\ninformation as the bracketed notation.\nWords\nIOB Label\nIO Label\nAmerican\nB-ORG\nI-ORG\nAirlines\nI-ORG\nI-ORG\n,\nO\nO\na\nO\nO\nunit\nO\nO\nof\nO\nO\nAMR\nB-ORG\nI-ORG\nCorp.\nI-ORG\nI-ORG\n,\nO\nO\nimmediately\nO\nO\nmatched\nO\nO\nthe\nO\nO\nmove\nO\nO\n,\nO\nO\nspokesman\nO\nO\nTim\nB-PER\nI-PER\nWagner\nI-PER\nI-PER\nsaid\nO\nO\n.\nO\nO\nFigure 17.4\nNamed entity tagging as a sequence model, showing IOB and IO encodings.\nWe’ve also shown IO tagging, which loses some information by eliminating the\nB tag. Without the B tag IO tagging is unable to distinguish between two entities of\nthe same type that are right next to each other. Since this situation doesn’t arise very\noften (usually there is at least some punctuation or other deliminator), IO tagging\nmay be sufﬁcient, and has the advantage of using only n+1 tags.\nIn the following three sections we introduce the three standard families of al-\ngorithms for NER tagging: feature based (MEMM/CRF), neural (bi-LSTM), and\nrule-based.\n17.1.2\nA feature-based algorithm for NER\nidentity of wi, identity of neighboring words\nembeddings for wi, embeddings for neighboring words\npart of speech of wi, part of speech of neighboring words\nbase-phrase syntactic chunk label of wi and neighboring words\npresence of wi in a gazetteer\nwi contains a particular preﬁx (from all preﬁxes of length ≤4)\nwi contains a particular sufﬁx (from all sufﬁxes of length ≤4)\nwi is all upper case\nword shape of wi, word shape of neighboring words\nshort word shape of wi, short word shape of neighboring words\npresence of hyphen\nFigure 17.5\nTypical features for a feature-based NER system.",
  "339": "17.1\n•\nNAMED ENTITY RECOGNITION\n331\nThe ﬁrst approach is to extract features and train an MEMM or CRF sequence\nmodel of the type we saw for part-of-speech tagging in Chapter 8. Figure 17.5 lists\nstandard features used in such feature-based systems. We’ve seen many of these\nfeatures before in the context of part-of-speech tagging, particularly for tagging un-\nknown words. This is not surprising, as many unknown words are in fact named\nentities. Word shape features are thus particularly important in the context of NER.\nRecall that word shape features are used to represent the abstract letter pattern of\nword shape\nthe word by mapping lower-case letters to ‘x’, upper-case to ‘X’, numbers to ’d’, and\nretaining punctuation. Thus for example I.M.F would map to X.X.X. and DC10-30\nwould map to XXdd-dd. A second class of shorter word shape features is also used.\nIn these features consecutive character types are removed, so DC10-30 would be\nmapped to Xd-d but I.M.F would still map to X.X.X. This feature by itself accounts\nfor a considerable part of the success of feature-based NER systems for English\nnews text. Shape features are also particularly important in recognizing names of\nproteins and genes in biological texts.\nFor example the named entity token L’Occitane would generate the following\nnon-zero valued feature values:\npreﬁx(wi) = L\nsufﬁx(wi) = tane\npreﬁx(wi) = L’\nsufﬁx(wi) = ane\npreﬁx(wi) = L’O\nsufﬁx(wi) = ne\npreﬁx(wi) = L’Oc\nsufﬁx(wi) = e\nword-shape(wi) = X’Xxxxxxxx\nshort-word-shape(wi) = X’Xx\nA gazetteer is a list of place names, often providing millions of entries for lo-\ngazetteer\ncations with detailed geographical and political information.1 A related resource\nis name-lists; the United States Census Bureau also provides extensive lists of ﬁrst\nnames and surnames derived from its decadal census in the U.S.2 Similar lists of cor-\nporations, commercial products, and all manner of things biological and mineral are\nalso available from a variety of sources. Gazetteer and name features are typically\nimplemented as a binary feature for each name list. Unfortunately, such lists can\nbe difﬁcult to create and maintain, and their usefulness varies considerably. While\ngazetteers can be quite effective, lists of persons and organizations are not always\nhelpful (Mikheev et al., 1999).\nFeature effectiveness depends on the application, genre, media, and language.\nFor example, shape features, critical for English newswire texts, are of little use\nwith automatic speech recognition transcripts, or other non-edited or informally-\nedited sources, or for languages like Chinese that don’t use orthographic case. The\nfeatures in Fig. 17.5 should therefore be thought of as only a starting point.\nFigure 17.6 illustrates the result of adding part-of-speech tags, syntactic base-\nphrase chunk tags, and some shape information to our earlier example.\nGiven such a training set, a sequence classiﬁer like an MEMM can be trained to\nlabel new sentences. Figure 17.7 illustrates the operation of such a sequence labeler\nat the point where the token Corp. is next to be labeled. If we assume a context win-\ndow that includes the two preceding and following words, then the features available\nto the classiﬁer are those shown in the boxed area.\n1\nwww.geonames.org\n2\nwww.census.gov",
  "340": "332\nCHAPTER 17\n•\nINFORMATION EXTRACTION\nWord\nPOS\nChunk\nShort shape\nLabel\nAmerican\nNNP\nB-NP\nXx\nB-ORG\nAirlines\nNNPS I-NP\nXx\nI-ORG\n,\n,\nO\n,\nO\na\nDT\nB-NP\nx\nO\nunit\nNN\nI-NP\nx\nO\nof\nIN\nB-PP\nx\nO\nAMR\nNNP\nB-NP\nX\nB-ORG\nCorp.\nNNP\nI-NP\nXx.\nI-ORG\n,\n,\nO\n,\nO\nimmediately RB\nB-ADVP x\nO\nmatched\nVBD\nB-VP\nx\nO\nthe\nDT\nB-NP\nx\nO\nmove\nNN\nI-NP\nx\nO\n,\n,\nO\n,\nO\nspokesman\nNN\nB-NP\nx\nO\nTim\nNNP\nI-NP\nXx\nB-PER\nWagner\nNNP\nI-NP\nXx\nI-PER\nsaid\nVBD\nB-VP\nx\nO\n.\n,\nO\n.\nO\nFigure 17.6\nWord-by-word feature encoding for NER.\nClassifier\nIN\nNNP\nNNP\nRB\nunit\nof\na\n...\nx\nB-PP\n...\nAMR\nCorp.\nimmediately\nB-NP\nX\nI-NP\nX.\nB-ADVP\nx\nO\nB-ORG\n?\n...\n...\n,\n,\nO ,\nmatched\nFigure 17.7\nNamed entity recognition as sequence labeling. The features available to the classiﬁer during\ntraining and classiﬁcation are those in the boxed area.\n17.1.3\nA neural algorithm for NER\nThe standard neural algorithm for NER is based on the bi-LSTM introduced in Chap-\nter 9. Recall that in that model, word and character embeddings are computed for\ninput word wi. These are passed through a left-to-right LSTM and a right-to-left\nLSTM, whose outputs are concatenated (or otherwise combined) to produce a sin-\ngle output layer at position i. In the simplest method, this layer can then be directly\npassed onto a softmax that creates a probability distribution over all NER tags, and\nthe most likely tag is chosen as ti.\nFor named entity tagging this greedy approach to decoding is insufﬁcient, since\nit doesn’t allow us to impose the strong constraints neighboring tokens have on each\nother (e.g., the tag I-PER must follow another I-PER or B-PER). Instead a CRF layer\nis normally used on top of the bi-LSTM output, and the Viterbi decoding algorithm\nis used to decode. Fig. 17.8 shows a sketch of the algorithm",
  "341": "17.1\n•\nNAMED ENTITY RECOGNITION\n333\nMark\nWatney\nvisits\nMars\nLSTM1\nLSTM1\nLSTM1\nLSTM1\nLSTM2\nLSTM2\nLSTM2\nLSTM2\nConcatenation\nRight-to-left LSTM\nLeft-to-right LSTM\nB-PER\nI-PER\nO\nB-LOC\nCRF Layer\nChar LSTM\nChar LSTM\nChar LSTM\nChar LSTM\nGloVe\nGloVe\nGloVe\nGloVe\nChar+GloVe\nEmbeddings\nFigure 17.8\nPutting it all together: character embeddings and words together a bi-LSTM\nsequence model. After (Lample et al., 2016)\n17.1.4\nRule-based NER\nWhile machine learned (neural or MEMM/CRF) sequence models are the norm in\nacademic research, commercial approaches to NER are often based on pragmatic\ncombinations of lists and rules, with some smaller amount of supervised machine\nlearning (Chiticariu et al., 2013). For example IBM System T is a text understand-\ning architecture in which a user speciﬁes complex declarative constraints for tagging\ntasks in a formal query language that includes regular expressions, dictionaries, se-\nmantic constraints, NLP operators, and table structures, all of which the system\ncompiles into an efﬁcient extractor (Chiticariu et al., 2018)\nOne common approach is to make repeated rule-based passes over a text, allow-\ning the results of one pass to inﬂuence the next. The stages typically ﬁrst involve\nthe use of rules that have extremely high precision but low recall. Subsequent stages\nemploy more error-prone statistical methods that take the output of the ﬁrst pass into\naccount.\n1. First, use high-precision rules to tag unambiguous entity mentions.\n2. Then, search for substring matches of the previously detected names.\n3. Consult application-speciﬁc name lists to identify likely name entity mentions\nfrom the given domain.\n4. Finally, apply probabilistic sequence labeling techniques that make use of the\ntags from previous stages as additional features.\nThe intuition behind this staged approach is twofold. First, some of the entity\nmentions in a text will be more clearly indicative of a given entity’s class than others.\nSecond, once an unambiguous entity mention is introduced into a text, it is likely that\nsubsequent shortened versions will refer to the same entity (and thus the same type\nof entity).\n17.1.5\nEvaluation of Named Entity Recognition\nThe familiar metrics of recall, precision, and F1 measure are used to evaluate NER\nsystems. Remember that recall is the ratio of the number of correctly labeled re-\nsponses to the total that should have been labeled; precision is the ratio of the num-",
  "342": "334\nCHAPTER 17\n•\nINFORMATION EXTRACTION\nARTIFACT\nGENERAL\nAFFILIATION\nORG\nAFFILIATION\nPART-\nWHOLE\nPERSON-\nSOCIAL\nPHYSICAL\nLocated\nNear\nBusiness\nFamily\nLasting \nPersonal\nCitizen-\nResident-\nEthnicity-\nReligion\nOrg-Location-\nOrigin\nFounder\nEmployment\nMembership\nOwnership\nStudent-Alum\nInvestor\nUser-Owner-Inventor-\nManufacturer\nGeographical\nSubsidiary\nSports-Affiliation\nFigure 17.9\nThe 17 relations used in the ACE relation extraction task.\nber of correctly labeled responses to the total labeled; and F-measure is the harmonic\nmean of the two. For named entities, the entity rather than the word is the unit of\nresponse. Thus in the example in Fig. 17.6, the two entities Tim Wagner and AMR\nCorp. and the non-entity said would each count as a single response.\nThe fact that named entity tagging has a segmentation component which is not\npresent in tasks like text categorization or part-of-speech tagging causes some prob-\nlems with evaluation. For example, a system that labeled American but not American\nAirlines as an organization would cause two errors, a false positive for O and a false\nnegative for I-ORG. In addition, using entities as the unit of response but words as\nthe unit of training means that there is a mismatch between the training and test\nconditions.\n17.2\nRelation Extraction\nNext on our list of tasks is to discern the relationships that exist among the detected\nentities. Let’s return to our sample airline text:\nCiting high fuel prices, [ORG United Airlines] said [TIME Friday] it\nhas increased fares by [MONEY $6] per round trip on ﬂights to some\ncities also served by lower-cost carriers. [ORG American Airlines], a\nunit of [ORG AMR Corp.], immediately matched the move, spokesman\n[PER Tim Wagner] said. [ORG United], a unit of [ORG UAL Corp.],\nsaid the increase took effect [TIME Thursday] and applies to most\nroutes where it competes against discount carriers, such as [LOC Chicago]\nto [LOC Dallas] and [LOC Denver] to [LOC San Francisco].\nThe text tells us, for example, that Tim Wagner is a spokesman for American\nAirlines, that United is a unit of UAL Corp., and that American is a unit of AMR.\nThese binary relations are instances of more generic relations such as part-of or\nemploys that are fairly frequent in news-style texts. Figure 17.9 lists the 17 relations\nused in the ACE relation extraction evaluations and Fig. 17.10 shows some sample\nrelations. We might also extract more domain-speciﬁc relation such as the notion of\nan airline route. For example from this text we can conclude that United has routes\nto Chicago, Dallas, Denver, and San Francisco.",
  "343": "17.2\n•\nRELATION EXTRACTION\n335\nRelations\nTypes\nExamples\nPhysical-Located\nPER-GPE\nHe was in Tennessee\nPart-Whole-Subsidiary\nORG-ORG\nXYZ, the parent company of ABC\nPerson-Social-Family\nPER-PER\nYoko’s husband John\nOrg-AFF-Founder\nPER-ORG\nSteve Jobs, co-founder of Apple...\nFigure 17.10\nSemantic relations with examples and the named entity types they involve.\nDomain\nD = {a,b,c,d,e, f,g,h,i}\nUnited, UAL, American Airlines, AMR\na,b,c,d\nTim Wagner\ne\nChicago, Dallas, Denver, and San Francisco\nf,g,h,i\nClasses\nUnited, UAL, American, and AMR are organizations\nOrg = {a,b,c,d}\nTim Wagner is a person\nPers = {e}\nChicago, Dallas, Denver, and San Francisco are places\nLoc = { f,g,h,i}\nRelations\nUnited is a unit of UAL\nPartOf = {⟨a,b⟩,⟨c,d⟩}\nAmerican is a unit of AMR\nTim Wagner works for American Airlines\nOrgAff = {⟨c,e⟩}\nUnited serves Chicago, Dallas, Denver, and San Francisco\nServes = {⟨a, f⟩,⟨a,g⟩,⟨a,h⟩,⟨a,i⟩}\nFigure 17.11\nA model-based view of the relations and entities in our sample text.\nThese relations correspond nicely to the model-theoretic notions we introduced\nin Chapter 14 to ground the meanings of the logical forms. That is, a relation consists\nof a set of ordered tuples over elements of a domain. In most standard information-\nextraction applications, the domain elements correspond to the named entities that\noccur in the text, to the underlying entities that result from co-reference resolution, or\nto entities selected from a domain ontology. Figure 17.11 shows a model-based view\nof the set of entities and relations that can be extracted from our running example.\nNotice how this model-theoretic view subsumes the NER task as well; named entity\nrecognition corresponds to the identiﬁcation of a class of unary relations.\nSets of relations have been deﬁned for many other domains as well. For example\nUMLS, the Uniﬁed Medical Language System from the US National Library of\nMedicine has a network that deﬁnes 134 broad subject categories, entity types, and\n54 relations between the entities, such as the following:\nEntity\nRelation\nEntity\nInjury\ndisrupts\nPhysiological Function\nBodily Location\nlocation-of Biologic Function\nAnatomical Structure\npart-of\nOrganism\nPharmacologic Substance causes\nPathological Function\nPharmacologic Substance treats\nPathologic Function\nGiven a medical sentence like this one:\n(17.1) Doppler echocardiography can be used to diagnose left anterior descending\nartery stenosis in patients with type 2 diabetes\nWe could thus extract the UMLS relation:\nEchocardiography, Doppler Diagnoses Acquired stenosis\nWikipedia also offers a large supply of relations, drawn from infoboxes, struc-\ninfoboxes\ntured tables associated with certain Wikipedia articles. For example, the Wikipedia",
  "344": "336\nCHAPTER 17\n•\nINFORMATION EXTRACTION\ninfobox for Stanford includes structured facts like state = \"California\" or\npresident = \"Mark Tessier-Lavigne\". These facts can be turned into rela-\ntions like president-of or located-in. or into relations in a metalanguage called RDF\nRDF\n(Resource Description Framework). An RDF triple is a tuple of entity-relation-\nRDF triple\nentity, called a subject-predicate-object expression. Here’s a sample RDF triple:\nsubject\npredicate object\nGolden Gate Park location\nSan Francisco\nFor example the crowdsourced DBpedia (Bizer et al., 2009) is an ontology de-\nrived from Wikipedia containing over 2 billion RDF triples. Another dataset from\nWikipedia infoboxes, Freebase (Bollacker et al., 2008), has relations like\nFreebase\npeople/person/nationality\nlocation/location/contains\nWordNet or other ontologies offer useful ontological relations that express hier-\narchical relations between words or concepts. For example WordNet has the is-a or\nis-a\nhypernym relation between classes,\nhypernym\nGiraffe is-a ruminant is-a ungulate is-a mammal is-a vertebrate ...\nWordNet also has Instance-of relation between individuals and classes, so that for\nexample San Francisco is in the Instance-of relation with city. Extracting these\nrelations is an important step in extending or building ontologies.\nThere are ﬁve main classes of algorithms for relation extraction: hand-written\npatterns, supervised machine learning, semi-supervised (via bootstrapping and\nvia distant supervision), and unsupervised. We’ll introduce each of these in the\nnext sections.\n17.2.1\nUsing Patterns to Extract Relations\nThe earliest and still common algorithm for relation extraction is lexico-syntactic\npatterns, ﬁrst developed by Hearst (1992a). Consider the following sentence:\nAgar is a substance prepared from a mixture of red algae, such as Ge-\nlidium, for laboratory or industrial use.\nHearst points out that most human readers will not know what Gelidium is, but that\nthey can readily infer that it is a kind of (a hyponym of) red algae, whatever that is.\nShe suggests that the following lexico-syntactic pattern\nNP0 such as NP1{,NP2 ...,(and|or)NPi},i ≥1\n(17.2)\nimplies the following semantics\n∀NPi,i ≥1,hyponym(NPi,NP0)\n(17.3)\nallowing us to infer\nhyponym(Gelidium,red algae)\n(17.4)\nFigure 17.12 shows ﬁve patterns Hearst (1992a, 1998) suggested for inferring\nthe hyponym relation; we’ve shown NPH as the parent/hyponym. Modern versions\nof the pattern-based approach extend it by adding named entity constraints. For\nexample if our goal is to answer questions about “Who holds what ofﬁce in which\norganization?”, we can use patterns like the following:",
  "345": "17.2\n•\nRELATION EXTRACTION\n337\nNP {, NP}* {,} (and|or) other NPH\ntemples, treasuries, and other important civic buildings\nNPH such as {NP,}* {(or|and)} NP\nred algae such as Gelidium\nsuch NPH as {NP,}* {(or|and)} NP\nsuch authors as Herrick, Goldsmith, and Shakespeare\nNPH {,} including {NP,}* {(or|and)} NP\ncommon-law countries, including Canada and England\nNPH {,} especially {NP}* {(or|and)} NP\nEuropean countries, especially France, England, and Spain\nFigure 17.12\nHand-built lexico-syntactic patterns for ﬁnding hypernyms, using {} to mark optionality\n(Hearst 1992a, Hearst 1998).\nPER, POSITION of ORG:\nGeorge Marshall, Secretary of State of the United States\nPER (named|appointed|chose|etc.) PER Prep? POSITION\nTruman appointed Marshall Secretary of State\nPER [be]? (named|appointed|etc.) Prep? ORG POSITION\nGeorge Marshall was named US Secretary of State\nHand-built patterns have the advantage of high-precision and they can be tailored\nto speciﬁc domains. On the other hand, they are often low-recall, and it’s a lot of\nwork to create them for all possible patterns.\n17.2.2\nRelation Extraction via Supervised Learning\nSupervised machine learning approaches to relation extraction follow a scheme that\nshould be familiar by now. A ﬁxed set of relations and entities is chosen, a training\ncorpus is hand-annotated with the relations and entities, and the annotated texts are\nthen used to train classiﬁers to annotate an unseen test set.\nThe most straightforward approach has three steps, illustrated in Fig. 17.13. Step\none is to ﬁnd pairs of named entities (usually in the same sentence). In step two, a\nﬁltering classiﬁer is trained to make a binary decision as to whether a given pair of\nnamed entities are related (by any relation). Positive examples are extracted directly\nfrom all relations in the annotated corpus, and negative examples are generated from\nwithin-sentence entity pairs that are not annotated with a relation. In step 3, a classi-\nﬁer is trained to assign a label to the relations that were found by step 2. The use of\nthe ﬁltering classiﬁer can speed up the ﬁnal classiﬁcation and also allows the use of\ndistinct feature-sets appropriate for each task. For each of the two classiﬁers, we can\nuse any of the standard classiﬁcation techniques (logistic regression, neural network,\nSVM, etc.)\nfunction FINDRELATIONS(words) returns relations\nrelations←nil\nentities←FINDENTITIES(words)\nforall entity pairs ⟨e1, e2⟩in entities do\nif RELATED?(e1,e2)\nrelations←relations+CLASSIFYRELATION(e1,e2)\nFigure 17.13\nFinding and classifying the relations among entities in a text.\nFor the feature-based classiﬁers like logistic regression or random forests the\nmost important step is to identify useful features. Let’s consider features for clas-",
  "346": "338\nCHAPTER 17\n•\nINFORMATION EXTRACTION\nsifying the relationship between American Airlines (Mention 1, or M1) and Tim\nWagner (Mention 2, M2) from this sentence:\n(17.5) American Airlines, a unit of AMR, immediately matched the move,\nspokesman Tim Wagner said\nUseful word features include\n• The headwords of M1 and M2 and their concatenation\nAirlines\nWagner\nAirlines-Wagner\n• Bag-of-words and bigrams in M1 and M2\nAmerican, Airlines, Tim, Wagner, American Airlines, Tim Wagner\n• Words or bigrams in particular positions\nM2: -1 spokesman\nM2: +1 said\n• Bag of words or bigrams between M1 and M2:\na, AMR, of, immediately, matched, move, spokesman, the, unit\n• Stemmed versions of the same\nEmbeddings can be used to represent words in any of these features. Useful named\nentity features include\n• Named-entity types and their concatenation\n(M1: ORG, M2: PER, M1M2: ORG-PER)\n• Entity Level of M1 and M2 (from the set NAME, NOMINAL, PRONOUN)\nM1: NAME [it or he would be PRONOUN]\nM2: NAME [the company would be NOMINAL]\n• Number of entities between the arguments (in this case 1, for AMR)\nThe syntactic structure of a sentence can also signal relationships among its\nentities. Syntax is often featured by using strings representing syntactic paths: the\n(dependency or constituency) path traversed through the tree in getting from one\nentity to the other.\n• Base syntactic chunk sequence from M1 to M2\nNP NP PP VP NP NP\n• Constituent paths between M1 and M2\nNP ↑NP ↑S ↑S ↓NP\n• Dependency-tree paths\nAirlines ←sub j matched ←comp said →subj Wagner\nFigure 17.14 summarizes many of the features we have discussed that could be\nused for classifying the relationship between American Airlines and Tim Wagner\nfrom our example text.\nNeural models for relation extraction similarly treat the task as supervised clas-\nsiﬁcation. One option is to use a similar architecture as we saw for named entity\ntagging: a bi-LSTM model with word embeddings as inputs and a single softmax\nclassiﬁcation of the sentence output as a 1-of-N relation label. Because relations\noften hold between entities that are far part in a sentence (or across sentences), it\nmay be possible to get higher performance from algorithms like convolutional nets\n(dos Santos et al., 2015) or chain or tree LSTMS (Miwa and Bansal 2016, Peng\net al. 2017).\nIn general, if the test set is similar enough to the training set, and if there is\nenough hand-labeled data, supervised relation extraction systems can get high ac-\ncuracies. But labeling a large training set is extremely expensive and supervised",
  "347": "17.2\n•\nRELATION EXTRACTION\n339\nM1 headword\nairlines (as a word token or an embedding)\nM2 headword\nWagner\nWord(s) before M1\nNONE\nWord(s) after M2\nsaid\nBag of words between\n{a, unit, of, AMR, Inc., immediately, matched, the, move, spokesman }\nM1 type\nORG\nM2 type\nPERS\nConcatenated types\nORG-PERS\nConstituent path\nNP ↑NP ↑S ↑S ↓NP\nBase phrase path\nNP →NP →PP →NP →VP →NP →NP\nTyped-dependency path Airlines ←subj matched ←comp said →subj Wagner\nFigure 17.14\nSample of features extracted during classiﬁcation of the <American Airlines, Tim Wagner>\ntuple; M1 is the ﬁrst mention, M2 the second.\nmodels are brittle: they don’t generalize well to different text genres. For this rea-\nson, much research in relation extraction has focused on the semi-supervised and\nunsupervised approaches we turn to next.\n17.2.3\nSemisupervised Relation Extraction via Bootstrapping\nSupervised machine learning assumes that we have lots of labeled data. Unfortu-\nnately, this is expensive. But suppose we just have a few high-precision seed pat-\nterns, like those in Section 17.2.1, or perhaps a few seed tuples. That’s enough\nseed patterns\nseed tuples\nto bootstrap a classiﬁer! Bootstrapping proceeds by taking the entities in the seed\nbootstrapping\npair, and then ﬁnding sentences (on the web, or whatever dataset we are using) that\ncontain both entities. From all such sentences, we extract and generalize the context\naround the entities to learn new patterns. Fig. 17.15 sketches a basic algorithm.\nfunction BOOTSTRAP(Relation R) returns new relation tuples\ntuples←Gather a set of seed tuples that have relation R\niterate\nsentences←ﬁnd sentences that contain entities in tuples\npatterns←generalize the context between and around entities in sentences\nnewpairs←use patterns to grep for more tuples\nnewpairs←newpairs with high conﬁdence\ntuples←tuples + newpairs\nreturn tuples\nFigure 17.15\nBootstrapping from seed entity pairs to learn relations.\nSuppose, for example, that we need to create a list of airline/hub pairs, and we\nknow only that Ryanair has a hub at Charleroi. We can use this seed fact to discover\nnew patterns by ﬁnding other mentions of this relation in our corpus. We search\nfor the terms Ryanair, Charleroi and hub in some proximity. Perhaps we ﬁnd the\nfollowing set of sentences:\n(17.6) Budget airline Ryanair, which uses Charleroi as a hub, scrapped all\nweekend ﬂights out of the airport.\n(17.7) All ﬂights in and out of Ryanair’s Belgian hub at Charleroi airport were\ngrounded on Friday...",
  "348": "340\nCHAPTER 17\n•\nINFORMATION EXTRACTION\n(17.8) A spokesman at Charleroi, a main hub for Ryanair, estimated that 8000\npassengers had already been affected.\nFrom these results, we can use the context of words between the entity mentions,\nthe words before mention one, the word after mention two, and the named entity\ntypes of the two mentions, and perhaps other features, to extract general patterns\nsuch as the following:\n/ [ORG], which uses [LOC] as a hub /\n/ [ORG]’s hub at [LOC] /\n/ [LOC] a main hub for [ORG] /\nThese new patterns can then be used to search for additional tuples.\nBootstrapping systems also assign conﬁdence values to new tuples to avoid se-\nconﬁdence\nvalues\nmantic drift. In semantic drift, an erroneous pattern leads to the introduction of\nsemantic drift\nerroneous tuples, which, in turn, lead to the creation of problematic patterns and the\nmeaning of the extracted relations ‘drifts’. Consider the following example:\n(17.9) Sydney has a ferry hub at Circular Quay.\nIf accepted as a positive example, this expression could lead to the incorrect in-\ntroduction of the tuple ⟨Sydney,CircularQuay⟩. Patterns based on this tuple could\npropagate further errors into the database.\nConﬁdence values for patterns are based on balancing two factors: the pattern’s\nperformance with respect to the current set of tuples and the pattern’s productivity\nin terms of the number of matches it produces in the document collection. More\nformally, given a document collection D, a current set of tuples T, and a proposed\npattern p, we need to track two factors:\n• hits: the set of tuples in T that p matches while looking in D\n• finds: The total set of tuples that p ﬁnds in D\nThe following equation balances these considerations (Riloff and Jones, 1999).\nConf RlogF(p) = hitsp\nﬁndsp\n×log(ﬁndsp)\n(17.10)\nThis metric is generally normalized to produce a probability.\nWe can assess the conﬁdence in a proposed new tuple by combining the evidence\nsupporting it from all the patterns P′ that match that tuple in D (Agichtein and\nGravano, 2000). One way to combine such evidence is the noisy-or technique.\nnoisy-or\nAssume that a given tuple is supported by a subset of the patterns in P, each with\nits own conﬁdence assessed as above. In the noisy-or model, we make two basic\nassumptions. First, that for a proposed tuple to be false, all of its supporting patterns\nmust have been in error, and second, that the sources of their individual failures are\nall independent. If we loosely treat our conﬁdence measures as probabilities, then\nthe probability of any individual pattern p failing is 1−Conf(p); the probability of\nall of the supporting patterns for a tuple being wrong is the product of their individual\nfailure probabilities, leaving us with the following equation for our conﬁdence in a\nnew tuple.\nConf(t) = 1−\nY\np∈P′\n(1−Conf(p))\n(17.11)\nSetting conservative conﬁdence thresholds for the acceptance of new patterns\nand tuples during the bootstrapping process helps prevent the system from drifting\naway from the targeted relation.",
  "349": "17.2\n•\nRELATION EXTRACTION\n341\n17.2.4\nDistant Supervision for Relation Extraction\nAlthough text that has been hand-labeled with relation labels is extremely expensive\nto produce, there are ways to ﬁnd indirect sources of training data. The distant\nsupervision method of Mintz et al. (2009) combines the advantages of bootstrapping\ndistant\nsupervision\nwith supervised learning. Instead of just a handful of seeds, distant supervision uses\na large database to acquire a huge number of seed examples, creates lots of noisy\npattern features from all these examples and then combines them in a supervised\nclassiﬁer.\nFor example suppose we are trying to learn the place-of-birth relationship be-\ntween people and their birth cities. In the seed-based approach, we might have only\n5 examples to start with. But Wikipedia-based databases like DBPedia or Freebase\nhave tens of thousands of examples of many relations; including over 100,000 ex-\namples of place-of-birth, (<Edwin Hubble, Marshfield>, <Albert Einstein,\nUlm>, etc.,). The next step is to run named entity taggers on large amounts of text—\nMintz et al. (2009) used 800,000 articles from Wikipedia—and extract all sentences\nthat have two named entities that match the tuple, like the following:\n...Hubble was born in Marshﬁeld...\n...Einstein, born (1879), Ulm...\n...Hubble’s birthplace in Marshﬁeld...\nTraining instances can now be extracted from this data, one training instance\nfor each identical tuple <relation, entity1, entity2>. Thus there will be one\ntraining instance for each of:\n<born-in, Edwin Hubble, Marshfield>\n<born-in, Albert Einstein, Ulm>\n<born-year, Albert Einstein, 1879>\nand so on.\nWe can then apply feature-based or neural classiﬁcation. For feature-based clas-\nsiﬁcation, standard supervised relation extraction features like the named entity la-\nbels of the two mentions, the words and dependency paths in between the mentions,\nand neighboring words. Each tuple will have features collected from many training\ninstances; the feature vector for a single training instance like (<born-in,Albert\nEinstein, Ulm> will have lexical and syntactic features from many different sen-\ntences that mention Einstein and Ulm.\nBecause distant supervision has very large training sets, it is also able to use very\nrich features that are conjunctions of these individual features. So we will extract\nthousands of patterns that conjoin the entity types with the intervening words or\ndependency paths like these:\nPER was born in LOC\nPER, born (XXXX), LOC\nPER’s birthplace in LOC\nTo return to our running example, for this sentence:\n(17.12) American Airlines, a unit of AMR, immediately matched the move,\nspokesman Tim Wagner said\nwe would learn rich conjunction features like this one:\nM1 = ORG & M2 = PER & nextword=“said”& path= NP ↑NP ↑S ↑S ↓NP\nThe result is a supervised classiﬁer that has a huge rich set of features to use\nin detecting relations. Since not every test sentence will have one of the training",
  "350": "342\nCHAPTER 17\n•\nINFORMATION EXTRACTION\nrelations, the classiﬁer will also need to be able to label an example as no-relation.\nThis label is trained by randomly selecting entity pairs that do not appear in any\nFreebase relation, extracting features for them, and building a feature vector for\neach such tuple. The ﬁnal algorithm is sketched in Fig. 17.16.\nfunction DISTANT SUPERVISION(Database D, Text T) returns relation classiﬁer C\nforeach relation R\nforeach tuple (e1,e2) of entities with relation R in D\nsentences←Sentences in T that contain e1 and e2\nf←Frequent features in sentences\nobservations←observations + new training tuple (e1, e2, f, R)\nC←Train supervised classiﬁer on observations\nreturn C\nFigure 17.16\nThe distant supervision algorithm for relation extraction. A neural classiﬁer\nmight not need to use the feature set f.\nDistant supervision shares advantages with each of the methods we’ve exam-\nined. Like supervised classiﬁcation, distant supervision uses a classiﬁer with lots\nof features, and supervised by detailed hand-created knowledge. Like pattern-based\nclassiﬁers, it can make use of high-precision evidence for the relation between en-\ntities. Indeed, distance supervision systems learn patterns just like the hand-built\npatterns of early relation extractors. For example the is-a or hypernym extraction\nsystem of Snow et al. (2005) used hypernym/hyponym NP pairs from WordNet as\ndistant supervision, and then learned new patterns from large amounts of text. Their\nsystem induced exactly the original 5 template patterns of Hearst (1992a), but also\n70,000 additional patterns including these four:\nNPH like NP\nMany hormones like leptin...\nNPH called NP ...using a markup language called XHTML\nNP is a NPH\nRuby is a programming language...\nNP, a NPH\nIBM, a company with a long...\nThis ability to use a large number of features simultaneously means that, un-\nlike the iterative expansion of patterns in seed-based systems, there’s no semantic\ndrift. Like unsupervised classiﬁcation, it doesn’t use a labeled training corpus of\ntexts, so it isn’t sensitive to genre issues in the training corpus, and relies on very\nlarge amounts of unlabeled data. Distant supervision also has the advantage that it\ncan create training tuples to be used with neural classiﬁers, where features are not\nrequired.\nBut distant supervision can only help in extracting relations for which a large\nenough database already exists. To extract new relations without datasets, or rela-\ntions for new domains, purely unsupervised methods must be used.\n17.2.5\nUnsupervised Relation Extraction\nThe goal of unsupervised relation extraction is to extract relations from the web\nwhen we have no labeled training data, and not even any list of relations. This task\nis often called open information extraction or Open IE. In Open IE, the relations\nopen\ninformation\nextraction\nare simply strings of words (usually beginning with a verb).\nFor example, the ReVerb system (Fader et al., 2011) extracts a relation from a\nsentence s in 4 steps:",
  "351": "17.2\n•\nRELATION EXTRACTION\n343\n1. Run a part-of-speech tagger and entity chunker over s\n2. For each verb in s, ﬁnd the longest sequence of words w that start with a verb\nand satisfy syntactic and lexical constraints, merging adjacent matches.\n3. For each phrase w, ﬁnd the nearest noun phrase x to the left which is not a\nrelative pronoun, wh-word or existential “there”. Find the nearest noun phrase\ny to the right.\n4. Assign conﬁdence c to the relation r = (x,w,y) using a conﬁdence classiﬁer\nand return it.\nA relation is only accepted if it meets syntactic and lexical constraints. The\nsyntactic constraints ensure that it is a verb-initial sequence that might also include\nnouns (relations that begin with light verbs like make, have, or do often express the\ncore of the relation with a noun, like have a hub in):\nV | VP | VW*P\nV = verb particle? adv?\nW = (noun | adj | adv | pron | det )\nP = (prep | particle | inf. marker)\nThe lexical constraints are based on a dictionary D that is used to prune very rare,\nlong relation strings. The intuition is to eliminate candidate relations that don’t oc-\ncur with sufﬁcient number of distinct argument types and so are likely to be bad\nexamples. The system ﬁrst runs the above relation extraction algorithm ofﬂine on\n500 million web sentences and extracts a list of all the relations that occur after nor-\nmalizing them (removing inﬂection, auxiliary verbs, adjectives, and adverbs). Each\nrelation r is added to the dictionary if it occurs with at least 20 different arguments.\nFader et al. (2011) used a dictionary of 1.7 million normalized relations.\nFinally, a conﬁdence value is computed for each relation using a logistic re-\ngression classiﬁer. The classiﬁer is trained by taking 1000 random web sentences,\nrunning the extractor, and hand labelling each extracted relation as correct or incor-\nrect. A conﬁdence classiﬁer is then trained on this hand-labeled data, using features\nof the relation and the surrounding words. Fig. 17.17 shows some sample features\nused in the classiﬁcation.\n(x,r,y) covers all words in s\nthe last preposition in r is for\nthe last preposition in r is on\nlen(s) ≤10\nthere is a coordinating conjunction to the left of r in s\nr matches a lone V in the syntactic constraints\nthere is preposition to the left of x in s\nthere is an NP to the right of y in s\nFigure 17.17\nFeatures for the classiﬁer that assigns conﬁdence to relations extracted by the\nOpen Information Extraction system REVERB (Fader et al., 2011).\nFor example the following sentence:\n(17.13) United has a hub in Chicago, which is the headquarters of United\nContinental Holdings.\nhas the relation phrases has a hub in and is the headquarters of (it also has has and\nis, but longer phrases are preferred). Step 3 ﬁnds United to the left and Chicago to\nthe right of has a hub in, and skips over which to ﬁnd Chicago to the left of is the\nheadquarters of. The ﬁnal output is:",
  "352": "344\nCHAPTER 17\n•\nINFORMATION EXTRACTION\nr1:\n<United, has a hub in, Chicago>\nr2:\n<Chicago, is the headquarters of, United Continental Holdings>\nThe great advantage of unsupervised relation extraction is its ability to handle\na huge number of relations without having to specify them in advance. The disad-\nvantage is the need to map these large sets of strings into some canonical form for\nadding to databases or other knowledge sources. Current methods focus heavily on\nrelations expressed with verbs, and so will miss many relations that are expressed\nnominally.\n17.2.6\nEvaluation of Relation Extraction\nSupervised relation extraction systems are evaluated by using test sets with human-\nannotated, gold-standard relations and computing precision, recall, and F-measure.\nLabeled precision and recall require the system to classify the relation correctly,\nwhereas unlabeled methods simply measure a system’s ability to detect entities that\nare related.\nSemi-supervised and unsupervised methods are much more difﬁcult to evalu-\nate, since they extract totally new relations from the web or a large text. Because\nthese methods use very large amounts of text, it is generally not possible to run them\nsolely on a small labeled test set, and as a result it’s not possible to pre-annotate a\ngold set of correct instances of relations.\nFor these methods it’s possible to approximate (only) precision by drawing a\nrandom sample of relations from the output, and having a human check the accuracy\nof each of these relations. Usually this approach focuses on the tuples to be extracted\nfrom a body of text rather than on the relation mentions; systems need not detect\nevery mention of a relation to be scored correctly. Instead, the evaluation is based\non the set of tuples occupying the database when the system is ﬁnished. That is,\nwe want to know if the system can discover that Ryanair has a hub at Charleroi; we\ndon’t really care how many times it discovers it. The estimated precision ˆP is then\nˆP = # of correctly extracted relation tuples in the sample\ntotal # of extracted relation tuples in the sample.\n(17.14)\nAnother approach that gives us a little bit of information about recall is to com-\npute precision at different levels of recall. Assuming that our system is able to\nrank the relations it produces (by probability, or conﬁdence) we can separately com-\npute precision for the top 1000 new relations, the top 10,000 new relations, the top\n100,000, and so on. In each case we take a random sample of that set. This will\nshow us how the precision curve behaves as we extract more and more tuples. But\nthere is no way to directly evaluate recall.\n17.3\nExtracting Times\nTimes and dates are a particularly important kind of named entity that play a role\nin question answering, in calendar and personal assistant applications. In order to\nreason about times and dates, after we extract these temporal expressions they must\nbe normalized—converted to a standard format so we can reason about them. In this\nsection we consider both the extraction and normalization of temporal expressions.",
  "353": "17.3\n•\nEXTRACTING TIMES\n345\n17.3.1\nTemporal Expression Extraction\nTemporal expressions are those that refer to absolute points in time, relative times,\ndurations, and sets of these. Absolute temporal expressions are those that can be\nabsolute\nmapped directly to calendar dates, times of day, or both. Relative temporal expres-\nrelative\nsions map to particular times through some other reference point (as in a week from\nlast Tuesday). Finally, durations denote spans of time at varying levels of granular-\nduration\nity (seconds, minutes, days, weeks, centuries, etc.). Figure 17.18 lists some sample\ntemporal expressions in each of these categories.\nAbsolute\nRelative\nDurations\nApril 24, 1916\nyesterday\nfour hours\nThe summer of ’77\nnext semester\nthree weeks\n10:15 AM\ntwo weeks from yesterday\nsix days\nThe 3rd quarter of 2006\nlast quarter\nthe last three quarters\nFigure 17.18\nExamples of absolute, relational and durational temporal expressions.\nTemporal expressions are grammatical constructions that have temporal lexical\ntriggers as their heads. Lexical triggers might be nouns, proper nouns, adjectives,\nlexical triggers\nand adverbs; full temporal expressions consist of their phrasal projections: noun\nphrases, adjective phrases, and adverbial phrases. Figure 17.19 provides examples.\nCategory\nExamples\nNoun\nmorning, noon, night, winter, dusk, dawn\nProper Noun January, Monday, Ides, Easter, Rosh Hashana, Ramadan, Tet\nAdjective\nrecent, past, annual, former\nAdverb\nhourly, daily, monthly, yearly\nFigure 17.19\nExamples of temporal lexical triggers.\nLet’s look at the TimeML annotation scheme, in which temporal expressions are\nannotated with an XML tag, TIMEX3, and various attributes to that tag (Pustejovsky\net al. 2005, Ferro et al. 2005). The following example illustrates the basic use of this\nscheme (we defer discussion of the attributes until Section 17.3.2).\nA fare increase initiated <TIMEX3>last week</TIMEX3> by UAL\nCorp’s United Airlines was matched by competitors over <TIMEX3>the\nweekend</TIMEX3>, marking the second successful fare increase in\n<TIMEX3>two weeks</TIMEX3>.\nThe temporal expression recognition task consists of ﬁnding the start and end of\nall of the text spans that correspond to such temporal expressions. Rule-based ap-\nproaches to temporal expression recognition use cascades of automata to recognize\npatterns at increasing levels of complexity. Tokens are ﬁrst part-of-speech tagged,\nand then larger and larger chunks are recognized from the results from previous\nstages, based on patterns containing trigger words (e.g., February) or classes (e.g.,\nMONTH). Figure 17.20 gives a fragment from a rule-based system.\nSequence-labeling approaches follow the same IOB scheme used for named-\nentity tags, marking words that are either inside, outside or at the beginning of a\nTIMEX3-delimited temporal expression with the I, O, and B tags as follows:\nA\nO\nfare\nO\nincrease\nO\ninitiated\nO\nlast\nB\nweek\nI\nby\nO\nUAL\nO\nCorp’s...\nO",
  "354": "346\nCHAPTER 17\n•\nINFORMATION EXTRACTION\n# yesterday/today/tomorrow\n$string =˜ s/((($OT+the$CT+\\s+)?$OT+day$CT+\\s+$OT+(before|after)$CT+\\s+)?$OT+$TERelDayExpr$CT+\n(\\s+$OT+(morning|afternoon|evening|night)$CT+)?)/<TIMEX$tever TYPE=\\\"DATE\\\">$1\n<\\/TIMEX$tever>/gio;\n$string =˜ s/($OT+\\w+$CT+\\s+)<TIMEX$tever TYPE=\\\"DATE\\\"[ˆ>]*>($OT+(Today|Tonight)$CT+)\n<\\/TIMEX$tever>/$1$4/gso;\n# this (morning/afternoon/evening)\n$string =˜ s/(($OT+(early|late)$CT+\\s+)?$OT+this$CT+\\s*$OT+(morning|afternoon|evening)$CT+)/\n<TIMEX$tever TYPE=\\\"DATE\\\">$1<\\/TIMEX$tever>/gosi;\n$string =˜ s/(($OT+(early|late)$CT+\\s+)?$OT+last$CT+\\s*$OT+night$CT+)/<TIMEX$tever\nTYPE=\\\"DATE\\\">$1<\\/TIMEX$tever>/gsio;\nFigure 17.20\nPerl fragment from the GUTime temporal tagging system in Tarsqi (Verhagen et al., 2005).\nFeatures are extracted from the token and its context, and a statistical sequence\nlabeler is trained (any sequence model can be used). Figure 17.21 lists standard\nfeatures used in temporal tagging.\nFeature\nExplanation\nToken\nThe target token to be labeled\nTokens in window Bag of tokens in the window around a target\nShape\nCharacter shape features\nPOS\nParts of speech of target and window words\nChunk tags\nBase-phrase chunk tag for target and words in a window\nLexical triggers\nPresence in a list of temporal terms\nFigure 17.21\nTypical features used to train IOB-style temporal expression taggers.\nTemporal expression recognizers are evaluated with the usual recall, precision,\nand F-measures. A major difﬁculty for all of these very lexicalized approaches is\navoiding expressions that trigger false positives:\n(17.15) 1984 tells the story of Winston Smith...\n(17.16) ...U2’s classic Sunday Bloody Sunday\n17.3.2\nTemporal Normalization\nTemporal normalization is the process of mapping a temporal expression to either\ntemporal\nnormalization\na speciﬁc point in time or to a duration. Points in time correspond to calendar dates,\nto times of day, or both. Durations primarily consist of lengths of time but may also\ninclude information about start and end points. Normalized times are represented\nwith the VALUE attribute from the ISO 8601 standard for encoding temporal values\n(ISO8601, 2004). Fig. 17.22 reproduces our earlier example with the value attributes\nadded in.\n<TIMEX3 id = ’ ’ t1 ’ ’\ntype =”DATE”\nvalue =”2007−07−02”\nfunctionInDocument=”CREATION TIME”\n> July\n2 , 2007 </ TIMEX3> A f a r e\ni n c r e a s e\ni n i t i a t e d <TIMEX3 id =” t2 ”\ntype =”DATE”\nvalue =”2007−W26” anchorTimeID=” t1 ”>l a s t\nweek</ TIMEX3> by United\nA i r l i n e s\nwas\nmatched by\nc o m p e t i t o r s\nover <TIMEX3 id =” t3 ”\ntype =”DURATION”\nvalue =”P1WE”\nanchorTimeID=” t1 ”> the\nweekend </ TIMEX3> ,\nmarking\nthe\nsecond\ns u c c e s s f u l\nf a r e\ni n c r e a s e\nin <TIMEX3 id =” t4 ”\ntype =”DURATION”\nvalue =”P2W” anchorTimeID=” t1 ”> two\nweeks </ TIMEX3>.\nFigure 17.22\nTimeML markup including normalized values for temporal expressions.\nThe dateline, or document date, for this text was July 2, 2007. The ISO repre-\nsentation for this kind of expression is YYYY-MM-DD, or in this case, 2007-07-02.",
  "355": "17.3\n•\nEXTRACTING TIMES\n347\nThe encodings for the temporal expressions in our sample text all follow from this\ndate, and are shown here as values for the VALUE attribute.\nThe ﬁrst temporal expression in the text proper refers to a particular week of the\nyear. In the ISO standard, weeks are numbered from 01 to 53, with the ﬁrst week\nof the year being the one that has the ﬁrst Thursday of the year. These weeks are\nrepresented with the template YYYY-Wnn. The ISO week for our document date is\nweek 27; thus the value for last week is represented as “2007-W26”.\nThe next temporal expression is the weekend. ISO weeks begin on Monday;\nthus, weekends occur at the end of a week and are fully contained within a single\nweek. Weekends are treated as durations, so the value of the VALUE attribute has\nto be a length. Durations are represented according to the pattern Pnx, where n is\nan integer denoting the length and x represents the unit, as in P3Y for three years\nor P2D for two days. In this example, one weekend is captured as P1WE. In this\ncase, there is also sufﬁcient information to anchor this particular weekend as part of\na particular week. Such information is encoded in the ANCHORTIMEID attribute.\nFinally, the phrase two weeks also denotes a duration captured as P2W. There is a\nlot more to the various temporal annotation standards—far too much to cover here.\nFigure 17.23 describes some of the basic ways that other times and durations are\nrepresented. Consult ISO8601 (2004), Ferro et al. (2005), and Pustejovsky et al.\n(2005) for more details.\nUnit\nPattern\nSample Value\nFully speciﬁed dates\nYYYY-MM-DD\n1991-09-28\nWeeks\nYYYY-Wnn\n2007-W27\nWeekends\nPnWE\nP1WE\n24-hour clock times\nHH:MM:SS\n11:13:45\nDates and times\nYYYY-MM-DDTHH:MM:SS\n1991-09-28T11:00:00\nFinancial quarters\nQn\n1999-Q3\nFigure 17.23\nSample ISO patterns for representing various times and durations.\nMost current approaches to temporal normalization are rule-based (Chang and\nManning 2012, Str¨otgen and Gertz 2013). Patterns that match temporal expres-\nsions are associated with semantic analysis procedures. As in the compositional\nrule-to-rule approach introduced in Chapter 15, the meaning of a constituent is com-\nputed from the meaning of its parts using a method speciﬁc to the constituent, al-\nthough here the semantic composition rules involve temporal arithmetic rather than\nλ-calculus attachments.\nFully qualiﬁed date expressions contain a year, month, and day in some con-\nfully qualiﬁed\nventional form. The units in the expression must be detected and then placed in the\ncorrect place in the corresponding ISO pattern. The following pattern normalizes\nexpressions like April 24, 1916.\nFQTE →Month Date , Year\n{Year.val −Month.val −Date.val}\nThe non-terminals Month, Date, and Year represent constituents that have already\nbeen recognized and assigned semantic values, accessed through the *.val notation.\nThe value of this FQE constituent can, in turn, be accessed as FQTE.val during\nfurther processing.\nFully qualiﬁed temporal expressions are fairly rare in real texts. Most temporal\nexpressions in news articles are incomplete and are only implicitly anchored, of-\nten with respect to the dateline of the article, which we refer to as the document’s",
  "356": "348\nCHAPTER 17\n•\nINFORMATION EXTRACTION\ntemporal anchor. The values of temporal expressions such as today, yesterday, or\ntemporal\nanchor\ntomorrow can all be computed with respect to this temporal anchor. The semantic\nprocedure for today simply assigns the anchor, and the attachments for tomorrow\nand yesterday add a day and subtract a day from the anchor, respectively. Of course,\ngiven the cyclic nature of our representations for months, weeks, days, and times of\nday, our temporal arithmetic procedures must use modulo arithmetic appropriate to\nthe time unit being used.\nUnfortunately, even simple expressions such as the weekend or Wednesday in-\ntroduce a fair amount of complexity. In our current example, the weekend clearly\nrefers to the weekend of the week that immediately precedes the document date. But\nthis won’t always be the case, as is illustrated in the following example.\n(17.17) Random security checks that began yesterday at Sky Harbor will continue\nat least through the weekend.\nIn this case, the expression the weekend refers to the weekend of the week that the\nanchoring date is part of (i.e., the coming weekend). The information that signals\nthis meaning comes from the tense of continue, the verb governing the weekend.\nRelative temporal expressions are handled with temporal arithmetic similar to\nthat used for today and yesterday. The document date indicates that our example\narticle is ISO week 27, so the expression last week normalizes to the current week\nminus 1. To resolve ambiguous next and last expressions we consider the distance\nfrom the anchoring date to the nearest unit. Next Friday can refer either to the\nimmediately next Friday or to the Friday following that, but the closer the document\ndate is to a Friday, the more likely it is that the phrase will skip the nearest one. Such\nambiguities are handled by encoding language and domain-speciﬁc heuristics into\nthe temporal attachments.\n17.4\nExtracting Events and their Times\nThe task of event extraction is to identify mentions of events in texts. For the\nevent\nextraction\npurposes of this task, an event mention is any expression denoting an event or state\nthat can be assigned to a particular point, or interval, in time. The following markup\nof the sample text on page 345 shows all the events in this text.\n[EVENT Citing] high fuel prices, United Airlines [EVENT said] Fri-\nday it has [EVENT increased] fares by $6 per round trip on ﬂights to\nsome cities also served by lower-cost carriers. American Airlines, a unit\nof AMR Corp., immediately [EVENT matched] [EVENT the move],\nspokesman Tim Wagner [EVENT said]. United, a unit of UAL Corp.,\n[EVENT said] [EVENT the increase] took effect Thursday and [EVENT\napplies] to most routes where it [EVENT competes] against discount\ncarriers, such as Chicago to Dallas and Denver to San Francisco.\nIn English, most event mentions correspond to verbs, and most verbs introduce\nevents. However, as we can see from our example, this is not always the case. Events\ncan be introduced by noun phrases, as in the move and the increase, and some verbs\nfail to introduce events, as in the phrasal verb took effect, which refers to when the\nevent began rather than to the event itself. Similarly, light verbs such as make, take,\nand have often fail to denote events; for light verbs the event is often expressed by\nthe nominal direct object (took a ﬂight), and these light verbs just provide a syntactic\nstructure for the noun’s arguments.",
  "357": "17.4\n•\nEXTRACTING EVENTS AND THEIR TIMES\n349\nVarious versions of the event extraction task exist, depending on the goal. For\nexample in the TempEval shared tasks (Verhagen et al. 2009) the goal is to extract\nevents and aspects like their aspectual and temporal properties. Events are to be\nclassiﬁed as actions, states, reporting events (say, report, tell, explain), perception\nreporting\nevents\nevents, and so on. The aspect, tense, and modality of each event also needs to be\nextracted. Thus for example the various said events in the sample text would be\nannotated as (class=REPORTING, tense=PAST, aspect=PERFECTIVE).\nEvent extraction is generally modeled via supervised learning, detecting events\nvia sequence models with IOB tagging, and assigning event classes and attributes\nwith multi-class classiﬁers. Common features include surface information like parts\nof speech, lexical items, and verb tense information; see Fig. 17.24.\nFeature\nExplanation\nCharacter afﬁxes\nCharacter-level preﬁxes and sufﬁxes of target word\nNominalization sufﬁx\nCharacter level sufﬁxes for nominalizations (e.g., -tion)\nPart of speech\nPart of speech of the target word\nLight verb\nBinary feature indicating that the target is governed by a light verb\nSubject syntactic category Syntactic category of the subject of the sentence\nMorphological stem\nStemmed version of the target word\nVerb root\nRoot form of the verb basis for a nominalization\nWordNet hypernyms\nHypernym set for the target\nFigure 17.24\nFeatures commonly used in both rule-based and machine learning approaches to event detec-\ntion.\n17.4.1\nTemporal Ordering of Events\nWith both the events and the temporal expressions in a text having been detected, the\nnext logical task is to use this information to ﬁt the events into a complete timeline.\nSuch a timeline would be useful for applications such as question answering and\nsummarization. This ambitious task is the subject of considerable current research\nbut is beyond the capabilities of current systems.\nA somewhat simpler, but still useful, task is to impose a partial ordering on the\nevents and temporal expressions mentioned in a text. Such an ordering can provide\nmany of the same beneﬁts as a true timeline. An example of such a partial ordering\nis the determination that the fare increase by American Airlines came after the fare\nincrease by United in our sample text. Determining such an ordering can be viewed\nas a binary relation detection and classiﬁcation task similar to those described earlier\nin Section 17.2. The temporal relation between events is classiﬁed into one of the\nstandard set of Allen relations shown in Fig. 17.25 (Allen, 1984), using feature-\nAllen relations\nbased classiﬁers as in Section 17.2, trained on the TimeBank corpus with features\nlike words/embeddings, parse paths, tense and aspect.\nThe TimeBank corpus consists of text annotated with much of the information\nTimeBank\nwe’ve been discussing throughout this section (Pustejovsky et al., 2003b). Time-\nBank 1.2 consists of 183 news articles selected from a variety of sources, including\nthe Penn TreeBank and PropBank collections.\nEach article in the TimeBank corpus has had the temporal expressions and event\nmentions in them explicitly annotated in the TimeML annotation (Pustejovsky et al.,\n2003a). In addition to temporal expressions and events, the TimeML annotation\nprovides temporal links between events and temporal expressions that specify the\nnature of the relation between them. Consider the following sample sentence and",
  "358": "350\nCHAPTER 17\n•\nINFORMATION EXTRACTION\nB\nA\nB\nA\nB\nA\nA\nA\nB\nB\nA\nB\nTime \nA  before B\nB after  A\nA overlaps B\nB overlaps' A\nA meets B\nB meets' A\nA equals B\n(B equals A)\nA starts B\nB starts' A\nA finishes B\nB finishes' A\nB\nA during B\nB during' A\nA\nFigure 17.25\nThe 13 temporal relations from Allen (1984).\n<TIMEX3 tid=\"t57\" type=\"DATE\" value=\"1989-10-26\"\nfunctionInDocument=\"CREATION_TIME\">\n10/26/89\n</TIMEX3>\nDelta Air Lines earnings <EVENT eid=\"e1\" class=\"OCCURRENCE\"> soared </EVENT> 33% to a\nrecord in\n<TIMEX3 tid=\"t58\" type=\"DATE\" value=\"1989-Q1\" anchorTimeID=\"t57\"> the\nfiscal first quarter </TIMEX3>, <EVENT eid=\"e3\"\nclass=\"OCCURRENCE\">bucking</EVENT>\nthe industry trend toward <EVENT eid=\"e4\" class=\"OCCURRENCE\">declining</EVENT>\nprofits.\nFigure 17.26\nExample from the TimeBank corpus.\nits corresponding markup shown in Fig. 17.26, selected from one of the TimeBank\ndocuments.\n(17.18) Delta Air Lines earnings soared 33% to a record in the ﬁscal ﬁrst quarter,\nbucking the industry trend toward declining proﬁts.\nAs annotated, this text includes three events and two temporal expressions. The\nevents are all in the occurrence class and are given unique identiﬁers for use in fur-\nther annotations. The temporal expressions include the creation time of the article,\nwhich serves as the document time, and a single temporal expression within the text.\nIn addition to these annotations, TimeBank provides four links that capture the\ntemporal relations between the events and times in the text, using the Allen relations\nfrom Fig. 17.25. The following are the within-sentence temporal relations annotated\nfor this example.",
  "359": "17.5\n•\nTEMPLATE FILLING\n351\n• Soaringe1 is included in the ﬁscal ﬁrst quartert58\n• Soaringe1 is before 1989-10-26t57\n• Soaringe1 is simultaneous with the buckinge3\n• Declininge4 includes soaringe1\n17.5\nTemplate Filling\nMany texts contain reports of events, and possibly sequences of events, that often\ncorrespond to fairly common, stereotypical situations in the world. These abstract\nsituations or stories, related to what have been called scripts (Schank and Abel-\nscripts\nson, 1977), consist of prototypical sequences of sub-events, participants, and their\nroles. The strong expectations provided by these scripts can facilitate the proper\nclassiﬁcation of entities, the assignment of entities into roles and relations, and most\ncritically, the drawing of inferences that ﬁll in things that have been left unsaid. In\ntheir simplest form, such scripts can be represented as templates consisting of ﬁxed\ntemplates\nsets of slots that take as values slot-ﬁllers belonging to particular classes. The task\nof template ﬁlling is to ﬁnd documents that invoke particular scripts and then ﬁll the\ntemplate ﬁlling\nslots in the associated templates with ﬁllers extracted from the text. These slot-ﬁllers\nmay consist of text segments extracted directly from the text, or they may consist of\nconcepts that have been inferred from text elements through some additional pro-\ncessing.\nA ﬁlled template from our original airline story might look like the following.\nFARE-RAISE ATTEMPT:\n\n\nLEAD AIRLINE:\nUNITED AIRLINES\nAMOUNT:\n$6\nEFFECTIVE DATE:\n2006-10-26\nFOLLOWER:\nAMERICAN AIRLINES\n\n\nThis template has four slots (LEAD AIRLINE, AMOUNT, EFFECTIVE DATE, FOL-\nLOWER). The next section describes a standard sequence-labeling approach to ﬁlling\nslots. Section 17.5.2 then describes an older system based on the use of cascades of\nﬁnite-state transducers and designed to address a more complex template-ﬁlling task\nthat current learning-based systems don’t yet address.\n17.5.1\nMachine Learning Approaches to Template Filling\nIn the standard paradigm for template ﬁlling, we are trying to ﬁll ﬁxed known tem-\nplates with known slots, and also assumes training documents labeled with examples\nof each template, and the ﬁllers of each slot marked in the text. The is to create one\ntemplate for each event in the input documents, with the slots ﬁlled with text from\nthe document.\nThe task is generally modeled by training two separate supervised systems. The\nﬁrst system decides whether the template is present in a particular sentence. This\ntask is called template recognition or sometimes, in a perhaps confusing bit of\ntemplate\nrecognition\nterminology, event recognition. Template recognition can be treated as a text classi-\nﬁcation task, with features extracted from every sequence of words that was labeled\nin training documents as ﬁlling any slot from the template being detected. The usual\nset of features can be used: tokens, embeddings, word shapes, part-of-speech tags,\nsyntactic chunk tags, and named entity tags.",
  "360": "352\nCHAPTER 17\n•\nINFORMATION EXTRACTION\nThe second system has the job of role-ﬁller extraction. A separate classiﬁer is\nrole-ﬁller\nextraction\ntrained to detect each role (LEAD-AIRLINE, AMOUNT, and so on). This can be a\nbinary classiﬁer that is run on every noun-phrase in the parsed input sentence, or a\nsequence model run over sequences of words. Each role classiﬁer is trained on the\nlabeled data in the training set. Again, the usual set of features can be used, but now\ntrained only on an individual noun phrase or the ﬁllers of a single slot.\nMultiple non-identical text segments might be labeled with the same slot la-\nbel. For example in our sample text, the strings United or United Airlines might be\nlabeled as the LEAD AIRLINE. These are not incompatible choices and the corefer-\nence resolution techniques introduced in Chapter 20 can provide a path to a solution.\nA variety of annotated collections have been used to evaluate this style of ap-\nproach to template ﬁlling, including sets of job announcements, conference calls for\npapers, restaurant guides, and biological texts. Recent work focuses on extracting\ntemplates in cases where there is no training data or even predeﬁned templates, by\ninducing templates as sets of linked events (Chambers and Jurafsky, 2011).\n17.5.2\nEarlier Finite-State Template-Filling Systems\nThe templates above are relatively simple. But consider the task of producing a\ntemplate that contained all the information in a text like this one (Grishman and\nSundheim, 1995):\nBridgestone Sports Co. said Friday it has set up a joint venture in Taiwan\nwith a local concern and a Japanese trading house to produce golf clubs to be\nshipped to Japan. The joint venture, Bridgestone Sports Taiwan Co., capital-\nized at 20 million new Taiwan dollars, will start production in January 1990\nwith production of 20,000 iron and “metal wood” clubs a month.\nThe MUC-5 ‘joint venture’ task (the Message Understanding Conferences were\na series of U.S. government-organized information-extraction evaluations) was to\nproduce hierarchically linked templates describing joint ventures.\nFigure 17.27\nshows a structure produced by the FASTUS system (Hobbs et al., 1997). Note how\nthe ﬁller of the ACTIVITY slot of the TIE-UP template is itself a template with slots.\nTie-up-1\nActivity-1:\nRELATIONSHIP\ntie-up\nCOMPANY\nBridgestone Sports Taiwan Co.\nENTITIES\nBridgestone Sports Co.\nPRODUCT\niron and “metal wood” clubs\na local concern\nSTART DATE DURING: January 1990\na Japanese trading house\nJOINT VENTURE Bridgestone Sports Taiwan Co.\nACTIVITY\nActivity-1\nAMOUNT\nNT$20000000\nFigure 17.27\nThe templates produced by FASTUS given the input text on page 352.\nEarly systems for dealing with these complex templates were based on cascades\nof transducers based on hand-written rules, as sketched in Fig. 17.28.\nThe ﬁrst four stages use hand-written regular expression and grammar rules to\ndo basic tokenization, chunking, and parsing. Stage 5 then recognizes entities and\nevents with a FST-based recognizer and inserts the recognized objects into the ap-\npropriate slots in templates. This FST recognizer is based on hand-built regular\nexpressions like the following (NG indicates Noun-Group and VG Verb-Group),\nwhich matches the ﬁrst sentence of the news story above.",
  "361": "17.6\n•\nSUMMARY\n353\nNo.\nStep\nDescription\n1\nTokens\nTokenize input stream of characters\n2\nComplex Words\nMultiword phrases, numbers, and proper names.\n3\nBasic phrases\nSegment sentences into noun and verb groups\n4\nComplex phrases\nIdentify complex noun groups and verb groups\n5\nSemantic Patterns\nIdentify entities and events, insert into templates.\n6\nMerging\nMerge references to the same entity or event\nFigure 17.28\nLevels of processing in FASTUS (Hobbs et al., 1997). Each level extracts a\nspeciﬁc type of information which is then passed on to the next higher level.\nNG(Company/ies) VG(Set-up) NG(Joint-Venture) with NG(Company/ies)\nVG(Produce) NG(Product)\nThe result of processing these two sentences is the ﬁve draft templates (Fig. 17.29)\nthat must then be merged into the single hierarchical structure shown in Fig. 17.27.\nThe merging algorithm, after performing coreference resolution, merges two activi-\nties that are likely to be describing the same events.\n# Template/Slot\nValue\n1 RELATIONSHIP:\nTIE-UP\nENTITIES:\nBridgestone Co., a local concern, a Japanese trading house\n2 ACTIVITY:\nPRODUCTION\nPRODUCT:\n“golf clubs”\n3 RELATIONSHIP:\nTIE-UP\nJOINT VENTURE: “Bridgestone Sports Taiwan Co.”\nAMOUNT:\nNT$20000000\n4 ACTIVITY:\nPRODUCTION\nCOMPANY:\n“Bridgestone Sports Taiwan Co.”\nSTARTDATE:\nDURING: January 1990\n5 ACTIVITY:\nPRODUCTION\nPRODUCT:\n“iron and “metal wood” clubs”\nFigure 17.29\nThe ﬁve partial templates produced by stage 5 of FASTUS. These templates\nare merged in stage 6 to produce the ﬁnal template shown in Fig. 17.27 on page 352.\n17.6\nSummary\nThis chapter has explored techniques for extracting limited forms of semantic con-\ntent from texts.\n• Named entities can be recognized and classiﬁed by featured-based or neural\nsequence labeling techniques.\n• Relations among entities can be extracted by pattern-based approaches, su-\npervised learning methods when annotated training data is available, lightly\nsupervised bootstrapping methods when small numbers of seed tuples or\nseed patterns are available, distant supervision when a database of relations\nis available, and unsupervised or Open IE methods.\n• Reasoning about time can be facilitated by detection and normalization of\ntemporal expressions through a combination of statistical learning and rule-",
  "362": "354\nCHAPTER 17\n•\nINFORMATION EXTRACTION\nbased methods.\n• Events can be detected and ordered in time using sequence models and classi-\nﬁers trained on temporally- and event-labeled data like the TimeBank corpus.\n• Template-ﬁlling applications can recognize stereotypical situations in texts\nand assign elements from the text to roles represented as ﬁxed sets of slots.\nBibliographical and Historical Notes\nThe earliest work on information extraction addressed the template-ﬁlling task in the\ncontext of the Frump system (DeJong, 1982). Later work was stimulated by the U.S.\ngovernment-sponsored MUC conferences (Sundheim 1991, Sundheim 1992, Sund-\nheim 1993, Sundheim 1995). Early MUC systems like CIRCUS system (Lehnert\net al., 1991) and SCISOR (Jacobs and Rau, 1990) were quite inﬂuential and inspired\nlater systems like FASTUS (Hobbs et al., 1997). Chinchor et al. (1993) describe the\nMUC evaluation techniques.\nDue to the difﬁculty of porting systems from one domain to another, attention\nshifted to machine learning approaches.\nEarly supervised learning approaches to IE ( Cardie 1993, Cardie 1994, Riloff 1993,\nSoderland et al. 1995, Huffman 1996) focused on automating the knowledge acqui-\nsition process, mainly for ﬁnite-state rule-based systems. Their success, and the\nearlier success of HMM-based speech recognition, led to the use of sequence la-\nbeling (HMMs: Bikel et al. 1997; MEMMs McCallum et al. 2000; CRFs: Laf-\nferty et al. 2001), and a wide exploration of features (Zhou et al., 2005). Neural\napproaches to NER mainly follow from the pioneering results of Collobert et al.\n(2011), who applied a CRF on top of a convolutional net. BiLSTMs with word and\ncharacter-based embeddings as input followed shortly and became a standard neural\nalgorithm for NER (Huang et al. 2015, Ma and Hovy 2016, Lample et al. 2016).\nNeural algorithms for relation extraction often explore architectures that can\nhandle entities far apart in the sentence: recursive networks (Socher et al., 2012),\nconvolutional nets (dos Santos et al., 2015), or chain or tree LSTMS (Miwa and\nBansal 2016, Peng et al. 2017).\nProgress in this area continues to be stimulated by formal evaluations with shared\nbenchmark datasets, including the Automatic Content Extraction (ACE) evaluations\nof 2000-2007 on named entity recognition, relation extraction, and temporal ex-\npressions3, the KBP (Knowledge Base Population) evaluations (Ji et al. 2010, Sur-\nKBP\ndeanu 2013) of relation extraction tasks like slot ﬁlling (extracting attributes (‘slots’)\nslot ﬁlling\nlike age, birthplace, and spouse for a given entity) and a series of SemEval work-\nshops (Hendrickx et al., 2009).\nSemisupervised relation extraction was ﬁrst proposed by Hearst (1992b), and\nextended by systems like AutoSlog-TS (Riloff, 1996), DIPRE (Brin, 1998), SNOW-\nBALL (Agichtein and Gravano, 2000), and (Jones et al., 1999). The distant super-\nvision algorithm we describe was drawn from Mintz et al. (2009), who coined the\nterm ‘distant supervision’, but similar ideas occurred in earlier systems like Craven\nand Kumlien (1999) and Morgan et al. (2004) under the name weakly labeled data,\nas well as in Snow et al. (2005) and Wu and Weld (2007). Among the many exten-\nsions are Wu and Weld (2010), Riedel et al. (2010), and Ritter et al. (2013). Open\n3\nwww.nist.gov/speech/tests/ace/",
  "363": "EXERCISES\n355\nIE systems include KNOWITALL Etzioni et al. (2005), TextRunner (Banko et al.,\n2007), and REVERB (Fader et al., 2011). See Riedel et al. (2013) for a universal\nschema that combines the advantages of distant supervision and Open IE.\nHeidelTime (Str¨otgen and Gertz, 2013) and SUTime (Chang and Manning, 2012)\nare downloadable temporal extraction and normalization systems. The 2013 TempE-\nval challenge is described in UzZaman et al. (2013); Chambers (2013) and Bethard\n(2013) give typical approaches.\nExercises\n17.1 Develop a set of regular expressions to recognize the character shape features\ndescribed on page 331.\n17.2 The IOB labeling scheme given in this chapter isn’t the only possible one. For\nexample, an E tag might be added to mark the end of entities, or the B tag\ncan be reserved only for those situations where an ambiguity exists between\nadjacent entities. Propose a new set of IOB tags for use with your NER system.\nExperiment with it and compare its performance with the scheme presented\nin this chapter.\n17.3 Names of works of art (books, movies, video games, etc.) are quite different\nfrom the kinds of named entities we’ve discussed in this chapter. Collect a\nlist of names of works of art from a particular category from a Web-based\nsource (e.g., gutenberg.org, amazon.com, imdb.com, etc.). Analyze your list\nand give examples of ways that the names in it are likely to be problematic for\nthe techniques described in this chapter.\n17.4 Develop an NER system speciﬁc to the category of names that you collected in\nthe last exercise. Evaluate your system on a collection of text likely to contain\ninstances of these named entities.\n17.5 Acronym expansion, the process of associating a phrase with an acronym, can\nbe accomplished by a simple form of relational analysis. Develop a system\nbased on the relation analysis approaches described in this chapter to populate\na database of acronym expansions. If you focus on English Three Letter\nAcronyms (TLAs) you can evaluate your system’s performance by comparing\nit to Wikipedia’s TLA page.\n17.6 A useful functionality in newer email and calendar applications is the ability\nto associate temporal expressions connected with events in email (doctor’s\nappointments, meeting planning, party invitations, etc.) with speciﬁc calendar\nentries. Collect a corpus of email containing temporal expressions related to\nevent planning. How do these expressions compare to the kinds of expressions\ncommonly found in news text that we’ve been discussing in this chapter?\n17.7 Acquire the CMU seminar corpus and develop a template-ﬁlling system by\nusing any of the techniques mentioned in Section 17.5. Analyze how well\nyour system performs as compared with state-of-the-art results on this corpus.",
  "364": "356\nCHAPTER 18\n•\nSEMANTIC ROLE LABELING\nCHAPTER\n18\nSemantic Role Labeling\nSometime between the 7th and 4th centuries BCE, the Indian grammarian P¯an.ini1\nwrote a famous treatise on Sanskrit grammar, the As.t.¯adhy¯ay¯ı (‘8 books’), a treatise\nthat has been called “one of the greatest monuments of\nhuman intelligence” (Bloomﬁeld, 1933b, 11). The work\ndescribes the linguistics of the Sanskrit language in the\nform of 3959 sutras, each very efﬁciently (since it had to\nbe memorized!) expressing part of a formal rule system\nthat brilliantly preﬁgured modern mechanisms of formal\nlanguage theory (Penn and Kiparsky, 2012). One set of\nrules, relevant to our discussion in this chapter, describes\nthe k¯arakas, semantic relationships between a verb and\nnoun arguments, roles like agent, instrument, or destina-\ntion. P¯an.ini’s work was the earliest we know of that tried\nto understand the linguistic realization of events and their participants. This task\nof understanding participants and their relationship to events—being able to answer\nthe question “Who did what to whom” (and perhaps also “when and where”)—is a\ncentral question of natural language understanding.\nLet’s move forward 2.5 millenia to the present and consider the very mundane\ngoal of understanding text about a purchase of stock by XYZ Corporation. This\npurchasing event could take on a wide variety of surface forms. In the following\nsentences we see that it could be described by a verb (sold, bought) or a noun (pur-\nchase), and that XYZ Corp can be the syntactic subject (of bought), the indirect ob-\nject (of sold), or in a genitive or noun compound relation (with the noun purchase)\ndespite having notationally the same role in all of them:\n• XYZ corporation bought the stock.\n• They sold the stock to XYZ corporation.\n• The stock was bought by XYZ corporation.\n• The purchase of the stock by XYZ corporation...\n• The stock purchase by XYZ corporation...\nIn this chapter we introduce a level of representation that lets us capture the\ncommonality between these sentences. We will be able to represent the fact that\nthere was a purchase event, that the participants in this event were XYZ Corp and\nsome stock, and that XYZ Corp played a speciﬁc role, the role of acquiring the stock.\nWe call this shallow semantic representation level semantic roles. Semantic\nroles are representations that express the abstract role that arguments of a predicate\ncan take in the event; these can be very speciﬁc, like the BUYER, abstract like the\nAGENT, or super-abstract (the PROTO-AGENT). These roles can both represent gen-\neral semantic properties of the arguments and also express their likely relationship to\nthe syntactic role of the argument in the sentence. AGENTS tend to be the subject of\n1\nFigure shows a birch bark manuscript from Kashmir of the Rupavatra, a grammatical textbook based\non the Sanskrit grammar of Panini. Image from the Wellcome Collection.",
  "365": "18.1\n•\nSEMANTIC ROLES\n357\nan active sentence, THEMES the direct object, and so on. These relations are codiﬁed\nin databases like PropBank and FrameNet. We’ll introduce semantic role labeling,\nthe task of assigning roles to the constituents or phrases in sentences. We’ll also\ndiscuss selectional restrictions, the semantic sortal restrictions or preferences that\neach individual predicate can express about its potential arguments, such as the fact\nthat the theme of the verb eat is generally something edible. Along the way, we’ll\ndescribe the various ways these representations can help in language understanding\ntasks like question answering and machine translation.\n18.1\nSemantic Roles\nConsider how in Chapter 14 we represented the meaning of arguments for sentences\nlike these:\n(18.1) Sasha broke the window.\n(18.2) Pat opened the door.\nA neo-Davidsonian event representation of these two sentences would be\n∃e,x,y Breaking(e)∧Breaker(e,Sasha)\n∧BrokenThing(e,y)∧Window(y)\n∃e,x,y Opening(e)∧Opener(e,Pat)\n∧OpenedThing(e,y)∧Door(y)\nIn this representation, the roles of the subjects of the verbs break and open are\nBreaker and Opener respectively. These deep roles are speciﬁc to each event; Break-\ndeep roles\ning events have Breakers, Opening events have Openers, and so on.\nIf we are going to be able to answer questions, perform inferences, or do any\nfurther kinds of natural language understanding of these events, we’ll need to know\na little more about the semantics of these arguments. Breakers and Openers have\nsomething in common. They are both volitional actors, often animate, and they have\ndirect causal responsibility for their events.\nThematic roles are a way to capture this semantic commonality between Break-\nthematic roles\ners and Eaters. We say that the subjects of both these verbs are agents. Thus, AGENT\nagents\nis the thematic role that represents an abstract idea such as volitional causation. Sim-\nilarly, the direct objects of both these verbs, the BrokenThing and OpenedThing, are\nboth prototypically inanimate objects that are affected in some way by the action.\nThe semantic role for these participants is theme.\ntheme\nAlthough thematic roles are one of the oldest linguistic models, as we saw above,\ntheir modern formulation is due to Fillmore (1968) and Gruber (1965). Although\nthere is no universally agreed-upon set of roles, Figs. 18.1 and 18.2 list some the-\nmatic roles that have been used in various computational papers, together with rough\ndeﬁnitions and examples. Most thematic role sets have about a dozen roles, but we’ll\nsee sets with smaller numbers of roles with even more abstract meanings, and sets\nwith very large numbers of roles that are speciﬁc to situations. We’ll use the general\nterm semantic roles for all sets of roles, whether small or large.\nsemantic roles",
  "366": "358\nCHAPTER 18\n•\nSEMANTIC ROLE LABELING\nThematic Role\nDeﬁnition\nAGENT\nThe volitional causer of an event\nEXPERIENCER\nThe experiencer of an event\nFORCE\nThe non-volitional causer of the event\nTHEME\nThe participant most directly affected by an event\nRESULT\nThe end product of an event\nCONTENT\nThe proposition or content of a propositional event\nINSTRUMENT\nAn instrument used in an event\nBENEFICIARY\nThe beneﬁciary of an event\nSOURCE\nThe origin of the object of a transfer event\nGOAL\nThe destination of an object of a transfer event\nFigure 18.1\nSome commonly used thematic roles with their deﬁnitions.\nThematic Role\nExample\nAGENT\nThe waiter spilled the soup.\nEXPERIENCER\nJohn has a headache.\nFORCE\nThe wind blows debris from the mall into our yards.\nTHEME\nOnly after Benjamin Franklin broke the ice...\nRESULT\nThe city built a regulation-size baseball diamond...\nCONTENT\nMona asked “You met Mary Ann at a supermarket?”\nINSTRUMENT\nHe poached catﬁsh, stunning them with a shocking device...\nBENEFICIARY\nWhenever Ann Callahan makes hotel reservations for her boss...\nSOURCE\nI ﬂew in from Boston.\nGOAL\nI drove to Portland.\nFigure 18.2\nSome prototypical examples of various thematic roles.\n18.2\nDiathesis Alternations\nThe main reason computational systems use semantic roles is to act as a shallow\nmeaning representation that can let us make simple inferences that aren’t possible\nfrom the pure surface string of words, or even from the parse tree. To extend the\nearlier examples, if a document says that Company A acquired Company B, we’d\nlike to know that this answers the query Was Company B acquired? despite the fact\nthat the two sentences have very different surface syntax. Similarly, this shallow\nsemantics might act as a useful intermediate language in machine translation.\nSemantic roles thus help generalize over different surface realizations of pred-\nicate arguments. For example, while the AGENT is often realized as the subject of\nthe sentence, in other cases the THEME can be the subject. Consider these possible\nrealizations of the thematic arguments of the verb break:\n(18.3) John\nAGENT\nbroke the window.\nTHEME\n(18.4) John\nAGENT\nbroke the window\nTHEME\nwith a rock.\nINSTRUMENT\n(18.5) The rock\nINSTRUMENT\nbroke the window.\nTHEME\n(18.6) The window\nTHEME\nbroke.\n(18.7) The window\nTHEME\nwas broken by John.\nAGENT",
  "367": "18.3\n•\nSEMANTIC ROLES: PROBLEMS WITH THEMATIC ROLES\n359\nThese examples suggest that break has (at least) the possible arguments AGENT,\nTHEME, and INSTRUMENT. The set of thematic role arguments taken by a verb is\noften called the thematic grid, θ-grid, or case frame. We can see that there are\nthematic grid\ncase frame\n(among others) the following possibilities for the realization of these arguments of\nbreak:\nAGENT/Subject, THEME/Object\nAGENT/Subject,\nTHEME/Object,\nINSTRUMENT/PPwith\nINSTRUMENT/Subject,\nTHEME/Object\nTHEME/Subject\nIt turns out that many verbs allow their thematic roles to be realized in various\nsyntactic positions. For example, verbs like give can realize the THEME and GOAL\narguments in two different ways:\n(18.8)\na. Doris\nAGENT\ngave the book\nTHEME\nto Cary.\nGOAL\nb. Doris\nAGENT\ngave Cary\nGOAL\nthe book.\nTHEME\nThese multiple argument structure realizations (the fact that break can take AGENT,\nINSTRUMENT, or THEME as subject, and give can realize its THEME and GOAL in\neither order) are called verb alternations or diathesis alternations. The alternation\nverb\nalternation\nwe showed above for give, the dative alternation, seems to occur with particular se-\ndative\nalternation\nmantic classes of verbs, including “verbs of future having” (advance, allocate, offer,\nowe), “send verbs” (forward, hand, mail), “verbs of throwing” (kick, pass, throw),\nand so on. Levin (1993) lists for 3100 English verbs the semantic classes to which\nthey belong (47 high-level classes, divided into 193 more speciﬁc classes) and the\nvarious alternations in which they participate. These lists of verb classes have been\nincorporated into the online resource VerbNet (Kipper et al., 2000), which links each\nverb to both WordNet and FrameNet entries.\n18.3\nSemantic Roles: Problems with Thematic Roles\nRepresenting meaning at the thematic role level seems like it should be useful in\ndealing with complications like diathesis alternations. Yet it has proved quite difﬁ-\ncult to come up with a standard set of roles, and equally difﬁcult to produce a formal\ndeﬁnition of roles like AGENT, THEME, or INSTRUMENT.\nFor example, researchers attempting to deﬁne role sets often ﬁnd they need to\nfragment a role like AGENT or THEME into many speciﬁc roles. Levin and Rappa-\nport Hovav (2005) summarize a number of such cases, such as the fact there seem\nto be at least two kinds of INSTRUMENTS, intermediary instruments that can appear\nas subjects and enabling instruments that cannot:\n(18.9)\na. The cook opened the jar with the new gadget.\nb. The new gadget opened the jar.\n(18.10)\na. Shelly ate the sliced banana with a fork.\nb. *The fork ate the sliced banana.\nIn addition to the fragmentation problem, there are cases in which we’d like to\nreason about and generalize across semantic roles, but the ﬁnite discrete lists of roles\ndon’t let us do this.",
  "368": "360\nCHAPTER 18\n•\nSEMANTIC ROLE LABELING\nFinally, it has proved difﬁcult to formally deﬁne the thematic roles. Consider the\nAGENT role; most cases of AGENTS are animate, volitional, sentient, causal, but any\nindividual noun phrase might not exhibit all of these properties.\nThese problems have led to alternative semantic role models that use either\nsemantic role\nmany fewer or many more roles.\nThe ﬁrst of these options is to deﬁne generalized semantic roles that abstract\nover the speciﬁc thematic roles. For example, PROTO-AGENT and PROTO-PATIENT\nproto-agent\nproto-patient\nare generalized roles that express roughly agent-like and roughly patient-like mean-\nings. These roles are deﬁned, not by necessary and sufﬁcient conditions, but rather\nby a set of heuristic features that accompany more agent-like or more patient-like\nmeanings. Thus, the more an argument displays agent-like properties (being voli-\ntionally involved in the event, causing an event or a change of state in another par-\nticipant, being sentient or intentionally involved, moving) the greater the likelihood\nthat the argument can be labeled a PROTO-AGENT. The more patient-like the proper-\nties (undergoing change of state, causally affected by another participant, stationary\nrelative to other participants, etc.), the greater the likelihood that the argument can\nbe labeled a PROTO-PATIENT.\nThe second direction is instead to deﬁne semantic roles that are speciﬁc to a\nparticular verb or a particular group of semantically related verbs or nouns.\nIn the next two sections we describe two commonly used lexical resources that\nmake use of these alternative versions of semantic roles. PropBank uses both proto-\nroles and verb-speciﬁc semantic roles. FrameNet uses semantic roles that are spe-\nciﬁc to a general semantic idea called a frame.\n18.4\nThe Proposition Bank\nThe Proposition Bank, generally referred to as PropBank, is a resource of sen-\nPropBank\ntences annotated with semantic roles. The English PropBank labels all the sentences\nin the Penn TreeBank; the Chinese PropBank labels sentences in the Penn Chinese\nTreeBank. Because of the difﬁculty of deﬁning a universal set of thematic roles,\nthe semantic roles in PropBank are deﬁned with respect to an individual verb sense.\nEach sense of each verb thus has a speciﬁc set of roles, which are given only numbers\nrather than names: Arg0, Arg1, Arg2, and so on. In general, Arg0 represents the\nPROTO-AGENT, and Arg1, the PROTO-PATIENT. The semantics of the other roles\nare less consistent, often being deﬁned speciﬁcally for each verb. Nonetheless there\nare some generalization; the Arg2 is often the benefactive, instrument, attribute, or\nend state, the Arg3 the start point, benefactive, instrument, or attribute, and the Arg4\nthe end point.\nHere are some slightly simpliﬁed PropBank entries for one sense each of the\nverbs agree and fall. Such PropBank entries are called frame ﬁles; note that the\ndeﬁnitions in the frame ﬁle for each role (“Other entity agreeing”, “Extent, amount\nfallen”) are informal glosses intended to be read by humans, rather than being formal\ndeﬁnitions.\n(18.11) agree.01",
  "369": "18.4\n•\nTHE PROPOSITION BANK\n361\nArg0: Agreer\nArg1: Proposition\nArg2: Other entity agreeing\nEx1:\n[Arg0 The group] agreed [Arg1 it wouldn’t make an offer].\nEx2:\n[ArgM-TMP Usually] [Arg0 John] agrees [Arg2 with Mary]\n[Arg1 on everything].\n(18.12) fall.01\nArg1: Logical subject, patient, thing falling\nArg2: Extent, amount fallen\nArg3: start point\nArg4: end point, end state of arg1\nEx1:\n[Arg1 Sales] fell [Arg4 to $25 million] [Arg3 from $27 million].\nEx2:\n[Arg1 The average junk bond] fell [Arg2 by 4.2%].\nNote that there is no Arg0 role for fall, because the normal subject of fall is a\nPROTO-PATIENT.\nThe PropBank semantic roles can be useful in recovering shallow semantic in-\nformation about verbal arguments. Consider the verb increase:\n(18.13) increase.01 “go up incrementally”\nArg0: causer of increase\nArg1: thing increasing\nArg2: amount increased by, EXT, or MNR\nArg3: start point\nArg4: end point\nA PropBank semantic role labeling would allow us to infer the commonality in\nthe event structures of the following three examples, that is, that in each case Big\nFruit Co. is the AGENT and the price of bananas is the THEME, despite the differing\nsurface forms.\n(18.14) [Arg0 Big Fruit Co. ] increased [Arg1 the price of bananas].\n(18.15) [Arg1 The price of bananas] was increased again [Arg0 by Big Fruit Co. ]\n(18.16) [Arg1 The price of bananas] increased [Arg2 5%].\nPropBank also has a number of non-numbered arguments called ArgMs, (ArgM-\nTMP, ArgM-LOC, etc) which represent modiﬁcation or adjunct meanings. These are\nrelatively stable across predicates, so aren’t listed with each frame ﬁle. Data labeled\nwith these modiﬁers can be helpful in training systems to detect temporal, location,\nor directional modiﬁcation across predicates. Some of the ArgM’s include:\nTMP\nwhen?\nyesterday evening, now\nLOC\nwhere?\nat the museum, in San Francisco\nDIR\nwhere to/from?\ndown, to Bangkok\nMNR\nhow?\nclearly, with much enthusiasm\nPRP/CAU why?\nbecause ... , in response to the ruling\nREC\nthemselves, each other\nADV\nmiscellaneous\nPRD\nsecondary predication\n...ate the meat raw\nWhile PropBank focuses on verbs, a related project, NomBank (Meyers et al.,\nNomBank\n2004) adds annotations to noun predicates. For example the noun agreement in\nApple’s agreement with IBM would be labeled with Apple as the Arg0 and IBM as",
  "370": "362\nCHAPTER 18\n•\nSEMANTIC ROLE LABELING\nthe Arg2. This allows semantic role labelers to assign labels to arguments of both\nverbal and nominal predicates.\n18.5\nFrameNet\nWhile making inferences about the semantic commonalities across different sen-\ntences with increase is useful, it would be even more useful if we could make such\ninferences in many more situations, across different verbs, and also between verbs\nand nouns. For example, we’d like to extract the similarity among these three sen-\ntences:\n(18.17) [Arg1 The price of bananas] increased [Arg2 5%].\n(18.18) [Arg1 The price of bananas] rose [Arg2 5%].\n(18.19) There has been a [Arg2 5%] rise [Arg1 in the price of bananas].\nNote that the second example uses the different verb rise, and the third example\nuses the noun rather than the verb rise. We’d like a system to recognize that the\nprice of bananas is what went up, and that 5% is the amount it went up, no matter\nwhether the 5% appears as the object of the verb increased or as a nominal modiﬁer\nof the noun rise.\nThe FrameNet project is another semantic-role-labeling project that attempts\nFrameNet\nto address just these kinds of problems (Baker et al. 1998, Fillmore et al. 2003,\nFillmore and Baker 2009, Ruppenhofer et al. 2016). Whereas roles in the PropBank\nproject are speciﬁc to an individual verb, roles in the FrameNet project are speciﬁc\nto a frame.\nWhat is a frame? Consider the following set of words:\nreservation, ﬂight, travel, buy, price, cost, fare, rates, meal, plane\nThere are many individual lexical relations of hyponymy, synonymy, and so on\nbetween many of the words in this list. The resulting set of relations does not,\nhowever, add up to a complete account of how these words are related. They are\nclearly all deﬁned with respect to a coherent chunk of common-sense background\ninformation concerning air travel.\nWe call the holistic background knowledge that unites these words a frame (Fill-\nframe\nmore, 1985). The idea that groups of words are deﬁned with respect to some back-\nground information is widespread in artiﬁcial intelligence and cognitive science,\nwhere besides frame we see related works like a model (Johnson-Laird, 1983), or\nmodel\neven script (Schank and Abelson, 1977).\nscript\nA frame in FrameNet is a background knowledge structure that deﬁnes a set of\nframe-speciﬁc semantic roles, called frame elements, and includes a set of predi-\nframe elements\ncates that use these roles. Each word evokes a frame and proﬁles some aspect of the\nframe and its elements. The FrameNet dataset includes a set of frames and frame\nelements, the lexical units associated with each frame, and a set of labeled exam-\nple sentences. For example, the change position on a scale frame is deﬁned as\nfollows:\nThis frame consists of words that indicate the change of an Item’s posi-\ntion on a scale (the Attribute) from a starting point (Initial value) to an\nend point (Final value).\nSome of the semantic roles (frame elements) in the frame are deﬁned as in\nFig. 18.3. Note that these are separated into core roles, which are frame speciﬁc, and\ncore roles",
  "371": "18.5\n•\nFRAMENET\n363\nnon-core roles, which are more like the Arg-M arguments in PropBank, expressed\nnon-core roles\nmore general properties of time, location, and so on.\nCore Roles\nATTRIBUTE\nThe ATTRIBUTE is a scalar property that the ITEM possesses.\nDIFFERENCE\nThe distance by which an ITEM changes its position on the scale.\nFINAL STATE\nA description that presents the ITEM’s state after the change in the ATTRIBUTE’s\nvalue as an independent predication.\nFINAL VALUE\nThe position on the scale where the ITEM ends up.\nINITIAL STATE\nA description that presents the ITEM’s state before the change in the AT-\nTRIBUTE’s value as an independent predication.\nINITIAL VALUE The initial position on the scale from which the ITEM moves away.\nITEM\nThe entity that has a position on the scale.\nVALUE RANGE\nA portion of the scale, typically identiﬁed by its end points, along which the\nvalues of the ATTRIBUTE ﬂuctuate.\nSome Non-Core Roles\nDURATION\nThe length of time over which the change takes place.\nSPEED\nThe rate of change of the VALUE.\nGROUP\nThe GROUP in which an ITEM changes the value of an\nATTRIBUTE in a speciﬁed way.\nFigure 18.3\nThe frame elements in the change position on a scale frame from the FrameNet Labelers\nGuide (Ruppenhofer et al., 2016).\nHere are some example sentences:\n(18.20) [ITEM Oil] rose [ATTRIBUTE in price] [DIFFERENCE by 2%].\n(18.21) [ITEM It] has increased [FINAL STATE to having them 1 day a month].\n(18.22) [ITEM Microsoft shares] fell [FINAL VALUE to 7 5/8].\n(18.23) [ITEM Colon cancer incidence] fell [DIFFERENCE by 50%] [GROUP among\nmen].\n(18.24) a steady increase [INITIAL VALUE from 9.5] [FINAL VALUE to 14.3] [ITEM\nin dividends]\n(18.25) a [DIFFERENCE 5%] [ITEM dividend] increase...\nNote from these example sentences that the frame includes target words like rise,\nfall, and increase. In fact, the complete frame consists of the following words:\nVERBS: dwindle\nmove\nsoar\nescalation\nshift\nadvance\nedge\nmushroom swell\nexplosion\ntumble\nclimb\nexplode\nplummet\nswing\nfall\ndecline\nfall\nreach\ntriple\nﬂuctuation ADVERBS:\ndecrease\nﬂuctuate rise\ntumble\ngain\nincreasingly\ndiminish\ngain\nrocket\ngrowth\ndip\ngrow\nshift\nNOUNS: hike\ndouble\nincrease\nskyrocket\ndecline\nincrease\ndrop\njump\nslide\ndecrease\nrise\nFrameNet also codes relationships between frames, allowing frames to inherit\nfrom each other, or representing relations between frames like causation (and gen-\neralizations among frame elements in different frames can be representing by inher-\nitance as well). Thus, there is a Cause change of position on a scale frame that is\nlinked to the Change of position on a scale frame by the cause relation, but that\nadds an AGENT role and is used for causative examples such as the following:",
  "372": "364\nCHAPTER 18\n•\nSEMANTIC ROLE LABELING\n(18.26) [AGENT They] raised [ITEM the price of their soda] [DIFFERENCE by 2%].\nTogether, these two frames would allow an understanding system to extract the\ncommon event semantics of all the verbal and nominal causative and non-causative\nusages.\nFrameNets have also been developed for many other languages including Span-\nish, German, Japanese, Portuguese, Italian, and Chinese.\n18.6\nSemantic Role Labeling\nSemantic role labeling (sometimes shortened as SRL) is the task of automatically\nsemantic role\nlabeling\nﬁnding the semantic roles of each argument of each predicate in a sentence. Cur-\nrent approaches to semantic role labeling are based on supervised machine learning,\noften using the FrameNet and PropBank resources to specify what counts as a pred-\nicate, deﬁne the set of roles used in the task, and provide training and test sets.\nRecall that the difference between these two models of semantic roles is that\nFrameNet (18.27) employs many frame-speciﬁc frame elements as roles, while Prop-\nBank (18.28) uses a smaller number of numbered argument labels that can be inter-\npreted as verb-speciﬁc labels, along with the more general ARGM labels. Some\nexamples:\n(18.27)\n[You]\ncan’t [blame]\n[the program] [for being unable to identify it]\nCOGNIZER\nTARGET\nEVALUEE\nREASON\n(18.28)\n[The San Francisco Examiner] issued\n[a special edition] [yesterday]\nARG0\nTARGET\nARG1\nARGM-TMP\n18.6.1\nA Feature-based Algorithm for Semantic Role Labeling\nA simpliﬁed feature-based semantic role labeling algorithm is sketched in Fig. 18.4.\nFeature-based algorithms—from the very earliest systems like (Simmons, 1973)—\nbegin by parsing, using broad-coverage parsers to assign a parse to the input string.\nFigure 18.5 shows a parse of (18.28) above. The parse is then traversed to ﬁnd all\nwords that are predicates.\nFor each of these predicates, the algorithm examines each node in the parse\ntree and uses supervised classiﬁcation to decide the semantic role (if any) it plays\nfor this predicate. Given a labeled training set such as PropBank or FrameNet, a\nfeature vector is extracted for each node, using feature templates described in the\nnext subsection. A 1-of-N classiﬁer is then trained to predict a semantic role for\neach constituent given these features, where N is the number of potential semantic\nroles plus an extra NONE role for non-role constituents. Any standard classiﬁcation\nalgorithms can be used. Finally, for each test sentence to be labeled, the classiﬁer is\nrun on each relevant constituent.\nInstead of training a single-stage classiﬁer as in Fig. 18.5, the node-level classi-\nﬁcation task can be broken down into multiple steps:\n1. Pruning: Since only a small number of the constituents in a sentence are\narguments of any given predicate, many systems use simple heuristics to prune\nunlikely constituents.\n2. Identiﬁcation: a binary classiﬁcation of each node as an argument to be la-\nbeled or a NONE.",
  "373": "18.6\n•\nSEMANTIC ROLE LABELING\n365\nfunction SEMANTICROLELABEL(words) returns labeled tree\nparse←PARSE(words)\nfor each predicate in parse do\nfor each node in parse do\nfeaturevector←EXTRACTFEATURES(node, predicate, parse)\nCLASSIFYNODE(node, featurevector, parse)\nFigure 18.4\nA generic semantic-role-labeling algorithm. CLASSIFYNODE is a 1-of-N clas-\nsiﬁer that assigns a semantic role (or NONE for non-role constituents), trained on labeled data\nsuch as FrameNet or PropBank.\nS\nNP-SBJ = ARG0\nVP\nDT\nNNP\nNNP\nNNP\nThe\nSan\nFrancisco\nExaminer\nVBD = TARGET\nNP = ARG1\nPP-TMP = ARGM-TMP\nissued\nDT\nJJ\nNN\nIN\nNP\na\nspecial\nedition\naround\nNN\nNP-TMP\nnoon\nyesterday\nFigure 18.5\nParse tree for a PropBank sentence, showing the PropBank argument labels. The dotted line\nshows the path feature NP↑S↓VP↓VBD for ARG0, the NP-SBJ constituent The San Francisco Examiner.\n3. Classiﬁcation: a 1-of-N classiﬁcation of all the constituents that were labeled\nas arguments by the previous stage\nThe separation of identiﬁcation and classiﬁcation may lead to better use of fea-\ntures (different features may be useful for the two tasks) or to computational efﬁ-\nciency.\nGlobal Optimization\nThe classiﬁcation algorithm of Fig. 18.5 classiﬁes each argument separately (‘lo-\ncally’), making the simplifying assumption that each argument of a predicate can be\nlabeled independently. This assumption is false; there are interactions between argu-\nments that require a more ‘global’ assignment of labels to constituents. For example,\nconstituents in FrameNet and PropBank are required to be non-overlapping. More\nsigniﬁcantly, the semantic roles of constituents are not independent. For example\nPropBank does not allow multiple identical arguments; two constituents of the same\nverb cannot both be labeled ARG0 .\nRole labeling systems thus often add a fourth step to deal with global consistency\nacross the labels in a sentence. For example, the local classiﬁers can return a list of\npossible labels associated with probabilities for each constituent, and a second-pass\nViterbi decoding or re-ranking approach can be used to choose the best consensus\nlabel. Integer linear programming (ILP) is another common way to choose a solution\nthat conforms best to multiple constraints.",
  "374": "366\nCHAPTER 18\n•\nSEMANTIC ROLE LABELING\nFeatures for Semantic Role Labeling\nMost systems use some generalization of the core set of features introduced by\nGildea and Jurafsky (2000). Common basic features templates (demonstrated on\nthe NP-SBJ constituent The San Francisco Examiner in Fig. 18.5) include:\n• The governing predicate, in this case the verb issued. The predicate is a cru-\ncial feature since labels are deﬁned only with respect to a particular predicate.\n• The phrase type of the constituent, in this case, NP (or NP-SBJ). Some se-\nmantic roles tend to appear as NPs, others as S or PP, and so on.\n• The headword of the constituent, Examiner. The headword of a constituent\ncan be computed with standard head rules, such as those given in Chapter 10\nin Fig. 10.12. Certain headwords (e.g., pronouns) place strong constraints on\nthe possible semantic roles they are likely to ﬁll.\n• The headword part of speech of the constituent, NNP.\n• The path in the parse tree from the constituent to the predicate. This path is\nmarked by the dotted line in Fig. 18.5. Following Gildea and Jurafsky (2000),\nwe can use a simple linear representation of the path, NP↑S↓VP↓VBD. ↑and\n↓represent upward and downward movement in the tree, respectively. The\npath is very useful as a compact representation of many kinds of grammatical\nfunction relationships between the constituent and the predicate.\n• The voice of the clause in which the constituent appears, in this case, active\n(as contrasted with passive). Passive sentences tend to have strongly different\nlinkings of semantic roles to surface form than do active ones.\n• The binary linear position of the constituent with respect to the predicate,\neither before or after.\n• The subcategorization of the predicate, the set of expected arguments that\nappear in the verb phrase. We can extract this information by using the phrase-\nstructure rule that expands the immediate parent of the predicate; VP →VBD\nNP PP for the predicate in Fig. 18.5.\n• The named entity type of the constituent.\n• The ﬁrst words and the last word of the constituent.\nThe following feature vector thus represents the ﬁrst NP in our example (recall\nthat most observations will have the value NONE rather than, for example, ARG0,\nsince most constituents in the parse tree will not bear a semantic role):\nARG0: [issued, NP, Examiner, NNP, NP↑S↓VP↓VBD, active, before, VP →NP PP,\nORG, The, Examiner]\nOther features are often used in addition, such as sets of n-grams inside the\nconstituent, or more complex versions of the path features (the upward or downward\nhalves, or whether particular nodes occur in the path).\nIt’s also possible to use dependency parses instead of constituency parses as the\nbasis of features, for example using dependency parse paths instead of constituency\npaths.\n18.6.2\nA Neural Algorithm for Semantic Role Labeling\nThe standard neural algorithm for semantic role labeling is based on the bi-LSTM\nIOB tagger introduced in Chapter 9, which we’ve seen applied to part-of-speech\ntagging and named entity tagging, among other tasks. Recall that with IOB tagging,",
  "375": "18.6\n•\nSEMANTIC ROLE LABELING\n367\nThe\ncats\nlove\nhats\nEmbeddings\nLSTM1\nLSTM1\nLSTM1\nLSTM1\nLSTM2\nLSTM2\nLSTM2\nLSTM2\nConcatenation\nRight-to-left LSTM\nLeft-to-right LSTM\nSoftmax\nP(B-ARG0)\nP(I-ARG0)\nP(B-PRED)\nP(B-ARG1)\n0\n0\n1\n0\nword + is-predicate\nFigure 18.6\nA bi-LSTM approach to semantic role labeling. Most actual networks are\nmuch deeper than shown in this ﬁgure; 3 to 4 bi-LSTM layers (6 to 8 total LSTMs) are\ncommon. The input is a concatenation of an embedding for the input word and an embedding\nof a binary variable which is 1 for the predicate to 0 for all other words. After He et al. (2017).\nwe have a begin and end tag for each possible role (B-ARG0, I-ARG0; B-ARG1,\nI-ARG1, and so on), plus an outside tag O.\nAs with all the taggers, the goal is to compute the highest probability tag se-\nquence ˆy, given the input sequence of words w:\nˆy = argmax\ny∈T\nP(y|w)\nIn algorithms like He et al. (2017), each input word is mapped to pre-trained em-\nbeddings, and also associated with an embedding for a ﬂag (0/1) variable indicating\nwhether that input word is the predicate. These concatenated embeddings are passed\nthrough multiple layers of bi-directional LSTM. State-of-the-art algorithms tend to\nbe deeper than for POS or NER tagging, using 3 to 4 layers (6 to 8 total LSTMs).\nHighway layers can be used to connect these layers as well.\nOutput from the last bi-LSTM can then be turned into an IOB sequence as for\nPOS or NER tagging. Tags can be locally optimized by taking the bi-LSTM output,\npassing it through a single layer into a softmax for each word that creates a proba-\nbility distribution over all SRL tags and the most likely tag for word xi is chosen as\nti, computing for each word essentially:\nˆyi = argmax\nt∈tags\nP(t|wi)\nHowever, just as feature-based SRL tagging, this local approach to decoding doesn’t\nexploit the global constraints between tags; a tag I-ARG0, for example, must follow\nanother I-ARG0 or B-ARG0.\nAs we saw for POS and NER tagging, there are many ways to take advantage of\nthese global constraints. A CRF layer can be used instead of a softmax layer on top\nof the bi-LSTM output, and the Viterbi decoding algorithm can be used to decode\nfrom the CRF.\nAn even simpler Viterbi decoding algorithm that may perform equally well and\ndoesn’t require adding CRF complexity to the training process is to start with the\nsimple softmax. The softmax output (the entire probability distribution over tags)\nfor each word is then treated it as a lattice and we can do Viterbi decoding through\nthe lattice. The hard IOB constraints can act as the transition probabilities in the",
  "376": "368\nCHAPTER 18\n•\nSEMANTIC ROLE LABELING\nViterbi decoding (Thus the transition from state I-ARG0 to I-ARG1 would have\nprobability 0). Alternatively, the training data can be used to learn bigram or trigram\ntag transition probabilities as if doing HMM decoding. Fig. 18.6 shows a sketch of\nthe algorithm.\n18.6.3\nEvaluation of Semantic Role Labeling\nThe standard evaluation for semantic role labeling is to require that each argument\nlabel must be assigned to the exactly correct word sequence or parse constituent, and\nthen compute precision, recall, and F-measure. Identiﬁcation and classiﬁcation can\nalso be evaluated separately. Two common datasets used for evaluation are CoNLL-\n2005 (Carreras and M`arquez, 2005) and CoNLL-2012 (Pradhan et al., 2013).\n18.7\nSelectional Restrictions\nWe turn in this section to another way to represent facts about the relationship be-\ntween predicates and arguments. A selectional restriction is a semantic type con-\nselectional\nrestriction\nstraint that a verb imposes on the kind of concepts that are allowed to ﬁll its argument\nroles. Consider the two meanings associated with the following example:\n(18.29) I want to eat someplace nearby.\nThere are two possible parses and semantic interpretations for this sentence. In\nthe sensible interpretation, eat is intransitive and the phrase someplace nearby is\nan adjunct that gives the location of the eating event. In the nonsensical speaker-as-\nGodzilla interpretation, eat is transitive and the phrase someplace nearby is the direct\nobject and the THEME of the eating, like the NP Malaysian food in the following\nsentences:\n(18.30)\nI want to eat Malaysian food.\nHow do we know that someplace nearby isn’t the direct object in this sentence?\nOne useful cue is the semantic fact that the THEME of EATING events tends to be\nsomething that is edible. This restriction placed by the verb eat on the ﬁller of its\nTHEME argument is a selectional restriction.\nSelectional restrictions are associated with senses, not entire lexemes. We can\nsee this in the following examples of the lexeme serve:\n(18.31)\nThe restaurant serves green-lipped mussels.\n(18.32)\nWhich airlines serve Denver?\nExample (18.31) illustrates the offering-food sense of serve, which ordinarily re-\nstricts its THEME to be some kind of food Example (18.32) illustrates the provides a\ncommercial service to sense of serve, which constrains its THEME to be some type\nof appropriate location.\nSelectional restrictions vary widely in their speciﬁcity. The verb imagine, for\nexample, imposes strict requirements on its AGENT role (restricting it to humans\nand other animate entities) but places very few semantic requirements on its THEME\nrole. A verb like diagonalize, on the other hand, places a very speciﬁc constraint\non the ﬁller of its THEME role: it has to be a matrix, while the arguments of the\nadjectives odorless are restricted to concepts that could possess an odor:\n(18.33) In rehearsal, I often ask the musicians to imagine a tennis game.",
  "377": "18.7\n•\nSELECTIONAL RESTRICTIONS\n369\n(18.34) Radon is an odorless gas that can’t be detected by human senses.\n(18.35) To diagonalize a matrix is to ﬁnd its eigenvalues.\nThese examples illustrate that the set of concepts we need to represent selectional\nrestrictions (being a matrix, being able to possess an odor, etc) is quite open ended.\nThis distinguishes selectional restrictions from other features for representing lexical\nknowledge, like parts-of-speech, which are quite limited in number.\n18.7.1\nRepresenting Selectional Restrictions\nOne way to capture the semantics of selectional restrictions is to use and extend the\nevent representation of Chapter 14. Recall that the neo-Davidsonian representation\nof an event consists of a single variable that stands for the event, a predicate denoting\nthe kind of event, and variables and relations for the event roles. Ignoring the issue of\nthe λ-structures and using thematic roles rather than deep event roles, the semantic\ncontribution of a verb like eat might look like the following:\n∃e,x,y Eating(e)∧Agent(e,x)∧Theme(e,y)\nWith this representation, all we know about y, the ﬁller of the THEME role, is that\nit is associated with an Eating event through the Theme relation. To stipulate the\nselectional restriction that y must be something edible, we simply add a new term to\nthat effect:\n∃e,x,y Eating(e)∧Agent(e,x)∧Theme(e,y)∧EdibleThing(y)\nWhen a phrase like ate a hamburger is encountered, a semantic analyzer can\nform the following kind of representation:\n∃e,x,y Eating(e)∧Eater(e,x)∧Theme(e,y)∧EdibleThing(y)∧Hamburger(y)\nThis representation is perfectly reasonable since the membership of y in the category\nHamburger is consistent with its membership in the category EdibleThing, assuming\na reasonable set of facts in the knowledge base. Correspondingly, the representation\nfor a phrase such as ate a takeoff would be ill-formed because membership in an\nevent-like category such as Takeoff would be inconsistent with membership in the\ncategory EdibleThing.\nWhile this approach adequately captures the semantics of selectional restrictions,\nthere are two problems with its direct use. First, using FOL to perform the simple\ntask of enforcing selectional restrictions is overkill. Other, far simpler, formalisms\ncan do the job with far less computational cost. The second problem is that this\napproach presupposes a large, logical knowledge base of facts about the concepts\nthat make up selectional restrictions. Unfortunately, although such common-sense\nknowledge bases are being developed, none currently have the kind of coverage\nnecessary to the task.\nA more practical approach is to state selectional restrictions in terms of WordNet\nsynsets rather than as logical concepts. Each predicate simply speciﬁes a WordNet\nsynset as the selectional restriction on each of its arguments. A meaning representa-\ntion is well-formed if the role ﬁller word is a hyponym (subordinate) of this synset.\nFor our ate a hamburger example, for instance, we could set the selectional\nrestriction on the THEME role of the verb eat to the synset {food, nutrient}, glossed\nas any substance that can be metabolized by an animal to give energy and build",
  "378": "370\nCHAPTER 18\n•\nSEMANTIC ROLE LABELING\nSense 1\nhamburger, beefburger --\n(a fried cake of minced beef served on a bun)\n=> sandwich\n=> snack food\n=> dish\n=> nutriment, nourishment, nutrition...\n=> food, nutrient\n=> substance\n=> matter\n=> physical entity\n=> entity\nFigure 18.7\nEvidence from WordNet that hamburgers are edible.\ntissue. Luckily, the chain of hypernyms for hamburger shown in Fig. 18.7 reveals\nthat hamburgers are indeed food. Again, the ﬁller of a role need not match the\nrestriction synset exactly; it just needs to have the synset as one of its superordinates.\nWe can apply this approach to the THEME roles of the verbs imagine, lift, and di-\nagonalize, discussed earlier. Let us restrict imagine’s THEME to the synset {entity},\nlift’s THEME to {physical entity}, and diagonalize to {matrix}. This arrangement\ncorrectly permits imagine a hamburger and lift a hamburger, while also correctly\nruling out diagonalize a hamburger.\n18.7.2\nSelectional Preferences\nIn the earliest implementations, selectional restrictions were considered strict con-\nstraints on the kind of arguments a predicate could take (Katz and Fodor 1963,\nHirst 1987). For example, the verb eat might require that its THEME argument be\n[+FOOD]. Early word sense disambiguation systems used this idea to rule out senses\nthat violated the selectional restrictions of their governing predicates.\nVery quickly, however, it became clear that these selectional restrictions were\nbetter represented as preferences rather than strict constraints (Wilks 1975c, Wilks 1975b).\nFor example, selectional restriction violations (like inedible arguments of eat) often\noccur in well-formed sentences, for example because they are negated (18.36), or\nbecause selectional restrictions are overstated (18.37):\n(18.36) But it fell apart in 1931, perhaps because people realized you can’t eat\ngold for lunch if you’re hungry.\n(18.37) In his two championship trials, Mr. Kulkarni ate glass on an empty\nstomach, accompanied only by water and tea.\nModern systems for selectional preferences therefore specify the relation be-\ntween a predicate and its possible arguments with soft constraints of some kind.\nSelectional Association\nOne of the most inﬂuential has been the selectional association model of Resnik\n(1993). Resnik deﬁnes the idea of selectional preference strength as the general\nselectional\npreference\nstrength\namount of information that a predicate tells us about the semantic class of its argu-\nments. For example, the verb eat tells us a lot about the semantic class of its direct\nobjects, since they tend to be edible. The verb be, by contrast, tells us less about\nits direct objects. The selectional preference strength can be deﬁned by the differ-\nence in information between two distributions: the distribution of expected semantic",
  "379": "18.7\n•\nSELECTIONAL RESTRICTIONS\n371\nclasses P(c) (how likely is it that a direct object will fall into class c) and the dis-\ntribution of expected semantic classes for the particular verb P(c|v) (how likely is\nit that the direct object of the speciﬁc verb v will fall into semantic class c). The\ngreater the difference between these distributions, the more information the verb is\ngiving us about possible objects. The difference between these two distributions can\nbe quantiﬁed by relative entropy, or the Kullback-Leibler divergence (Kullback and\nrelative entropy\nLeibler, 1951). The Kullback-Leibler or KL divergence D(P||Q) expresses the dif-\nKL divergence\nference between two probability distributions P and Q (we’ll return to this when we\ndiscuss distributional models of meaning in Chapter 6).\nD(P||Q) =\nX\nx\nP(x)log P(x)\nQ(x)\n(18.38)\nThe selectional preference SR(v) uses the KL divergence to express how much in-\nformation, in bits, the verb v expresses about the possible semantic class of its argu-\nment.\nSR(v) = D(P(c|v)||P(c))\n=\nX\nc\nP(c|v)log P(c|v)\nP(c)\n(18.39)\nResnik then deﬁnes the selectional association of a particular class and verb as the\nselectional\nassociation\nrelative contribution of that class to the general selectional preference of the verb:\nAR(v,c) =\n1\nSR(v)P(c|v)log P(c|v)\nP(c)\n(18.40)\nThe selectional association is thus a probabilistic measure of the strength of asso-\nciation between a predicate and a class dominating the argument to the predicate.\nResnik estimates the probabilities for these associations by parsing a corpus, count-\ning all the times each predicate occurs with each argument word, and assuming\nthat each word is a partial observation of all the WordNet concepts containing the\nword. The following table from Resnik (1996) shows some sample high and low\nselectional associations for verbs and some WordNet semantic classes of their direct\nobjects.\nDirect Object\nDirect Object\nVerb\nSemantic Class Assoc\nSemantic Class Assoc\nread\nWRITING\n6.80\nACTIVITY\n-.20\nwrite\nWRITING\n7.26\nCOMMERCE\n0\nsee\nENTITY\n5.79\nMETHOD\n-0.01\nSelectional Preference via Conditional Probability\nAn alternative to using selectional association between a verb and the WordNet class\nof its arguments, is to simply use the conditional probability of an argument word\ngiven a predicate verb. This simple model of selectional preferences can be used\nto directly model the strength of association of one verb (predicate) with one noun\n(argument).\nThe conditional probability model can be computed by parsing a very large cor-\npus (billions of words), and computing co-occurrence counts: how often a given\nverb occurs with a given noun in a given relation. The conditional probability of an",
  "380": "372\nCHAPTER 18\n•\nSEMANTIC ROLE LABELING\nargument noun given a verb for a particular relation P(n|v,r) can then be used as a\nselectional preference metric for that pair of words (Brockmann and Lapata, 2003):\nP(n|v,r) =\n(\nC(n,v,r)\nC(v,r)\nif C(n,v,r) > 0\n0 otherwise\nThe inverse probability P(v|n,r) was found to have better performance in some cases\n(Brockmann and Lapata, 2003):\nP(v|n,r) =\n(\nC(n,v,r)\nC(n,r)\nif C(n,v,r) > 0\n0 otherwise\nIn cases where it’s not possible to get large amounts of parsed data, another option,\nat least for direct objects, is to get the counts from simple part-of-speech based\napproximations. For example pairs can be extracted using the pattern ”V Det N”,\nwhere V is any form of the verb, Det is the—a—ϵ and N is the singular or plural\nform of the noun (Keller and Lapata, 2003).\nAn even simpler approach is to use the simple log co-occurrence frequency of\nthe predicate with the argument logcount(v,n,r) instead of conditional probability;\nthis seems to do better for extracting preferences for syntactic subjects rather than\nobjects (Brockmann and Lapata, 2003).\nEvaluating Selectional Preferences\nOne way to evaluate models of selectional preferences is to use pseudowords (Gale\npseudowords\net al. 1992c, Sch¨utze 1992a). A pseudoword is an artiﬁcial word created by concate-\nnating a test word in some context (say banana) with a confounder word (say door)\nto create banana-door). The task of the system is to identify which of the two words\nis the original word. To evaluate a selectional preference model (for example on the\nrelationship between a verb and a direct object) we take a test corpus and select all\nverb tokens. For each verb token (say drive) we select the direct object (e.g., car),\nconcatenated with a confounder word that is its nearest neighbor, the noun with the\nfrequency closest to the original (say house), to make car/house). We then use the\nselectional preference model to choose which of car and house are more preferred\nobjects of drive, and compute how often the model chooses the correct original ob-\nject (e.g., (car) (Chambers and Jurafsky, 2010).\nAnother evaluation metric is to get human preferences for a test set of verb-\nargument pairs, and have them rate their degree of plausibility. This is usually done\nby using magnitude estimation, a technique from psychophysics, in which subjects\nrate the plausibility of an argument proportional to a modulus item. A selectional\npreference model can then be evaluated by its correlation with the human prefer-\nences (Keller and Lapata, 2003).\n18.8\nPrimitive Decomposition of Predicates\nOne way of thinking about the semantic roles we have discussed through the chapter\nis that they help us deﬁne the roles that arguments play in a decompositional way,\nbased on ﬁnite lists of thematic roles (agent, patient, instrument, proto-agent, proto-\npatient, etc.) This idea of decomposing meaning into sets of primitive semantics\nelements or features, called primitive decomposition or componential analysis,\ncomponential\nanalysis",
  "381": "18.8\n•\nPRIMITIVE DECOMPOSITION OF PREDICATES\n373\nhas been taken even further, and focused particularly on predicates.\nConsider these examples of the verb kill:\n(18.41) Jim killed his philodendron.\n(18.42) Jim did something to cause his philodendron to become not alive.\nThere is a truth-conditional (‘propositional semantics’) perspective from which these\ntwo sentences have the same meaning. Assuming this equivalence, we could repre-\nsent the meaning of kill as:\n(18.43) KILL(x,y) ⇔CAUSE(x, BECOME(NOT(ALIVE(y))))\nthus using semantic primitives like do, cause, become not, and alive.\nIndeed, one such set of potential semantic primitives has been used to account for\nsome of the verbal alternations discussed in Section 18.2 (Lakoff 1965, Dowty 1979).\nConsider the following examples.\n(18.44) John opened the door. ⇒CAUSE(John, BECOME(OPEN(door)))\n(18.45) The door opened. ⇒BECOME(OPEN(door))\n(18.46) The door is open. ⇒OPEN(door)\nThe decompositional approach asserts that a single state-like predicate associ-\nated with open underlies all of these examples. The differences among the meanings\nof these examples arises from the combination of this single predicate with the prim-\nitives CAUSE and BECOME.\nWhile this approach to primitive decomposition can explain the similarity be-\ntween states and actions or causative and non-causative predicates, it still relies on\nhaving a large number of predicates like open. More radical approaches choose to\nbreak down these predicates as well. One such approach to verbal predicate de-\ncomposition that played a role in early natural language understanding systems is\nconceptual dependency (CD), a set of ten primitive predicates, shown in Fig. 18.8.\nconceptual\ndependency\nPrimitive\nDeﬁnition\nATRANS\nThe abstract transfer of possession or control from one entity to\nanother\nPTRANS\nThe physical transfer of an object from one location to another\nMTRANS\nThe transfer of mental concepts between entities or within an\nentity\nMBUILD\nThe creation of new information within an entity\nPROPEL\nThe application of physical force to move an object\nMOVE\nThe integral movement of a body part by an animal\nINGEST\nThe taking in of a substance by an animal\nEXPEL\nThe expulsion of something from an animal\nSPEAK\nThe action of producing a sound\nATTEND\nThe action of focusing a sense organ\nFigure 18.8\nA set of conceptual dependency primitives.\nBelow is an example sentence along with its CD representation. The verb brought\nis translated into the two primitives ATRANS and PTRANS to indicate that the waiter\nboth physically conveyed the check to Mary and passed control of it to her. Note\nthat CD also associates a ﬁxed set of thematic roles with each primitive to represent\nthe various participants in the action.\n(18.47) The waiter brought Mary the check.",
  "382": "374\nCHAPTER 18\n•\nSEMANTIC ROLE LABELING\n∃x,y Atrans(x)∧Actor(x,Waiter)∧Object(x,Check)∧To(x,Mary)\n∧Ptrans(y)∧Actor(y,Waiter)∧Object(y,Check)∧To(y,Mary)\n18.9\nSummary\n• Semantic roles are abstract models of the role an argument plays in the event\ndescribed by the predicate.\n• Thematic roles are a model of semantic roles based on a single ﬁnite list of\nroles. Other semantic role models include per-verb semantic role lists and\nproto-agent/proto-patient, both of which are implemented in PropBank,\nand per-frame role lists, implemented in FrameNet.\n• Semantic role labeling is the task of assigning semantic role labels to the con-\nstituents of a sentence. The task is generally treated as a supervised machine\nlearning task, with models trained on PropBank or FrameNet. Algorithms\ngenerally start by parsing a sentence and then automatically tag each parse\ntree node with a semantic role.\n• Semantic selectional restrictions allow words (particularly predicates) to post\nconstraints on the semantic properties of their argument words. Selectional\npreference models (like selectional association or simple conditional proba-\nbility) allow a weight or probability to be assigned to the association between\na predicate and an argument word or class.\nBibliographical and Historical Notes\nAlthough the idea of semantic roles dates back to P¯an.ini, they were re-introduced\ninto modern linguistics by Gruber (1965), Fillmore (1966) and Fillmore (1968)).\nFillmore, interestingly, had become interested in argument structure by studying\nLucien Tesni`ere’s groundbreaking ´El´ements de Syntaxe Structurale (Tesni`ere, 1959)\nin which the term ‘dependency’ was introduced and the foundations were laid for\ndependency grammar. Following Tesni`ere’s terminology, Fillmore ﬁrst referred to\nargument roles as actants (Fillmore, 1966) but quickly switched to the term case,\n(see Fillmore (2003)) and proposed a universal list of semantic roles or cases (Agent,\nPatient, Instrument, etc.), that could be taken on by the arguments of predicates.\nVerbs would be listed in the lexicon with their case frame, the list of obligatory (or\noptional) case arguments.\nThe idea that semantic roles could provide an intermediate level of semantic\nrepresentation that could help map from syntactic parse structures to deeper, more\nfully-speciﬁed representations of meaning was quickly adopted in natural language\nprocessing, and systems for extracting case frames were created for machine trans-\nlation (Wilks, 1973), question-answering (Hendrix et al., 1973), spoken-language\nunderstanding (Nash-Webber, 1975), and dialogue systems (Bobrow et al., 1977).\nGeneral-purpose semantic role labelers were developed. The earliest ones (Sim-\nmons, 1973) ﬁrst parsed a sentence by means of an ATN (Augmented Transition",
  "383": "BIBLIOGRAPHICAL AND HISTORICAL NOTES\n375\nNetwork) parser. Each verb then had a set of rules specifying how the parse should\nbe mapped to semantic roles. These rules mainly made reference to grammatical\nfunctions (subject, object, complement of speciﬁc prepositions) but also checked\nconstituent internal features such as the animacy of head nouns. Later systems as-\nsigned roles from pre-built parse trees, again by using dictionaries with verb-speciﬁc\ncase frames (Levin 1977, Marcus 1980).\nBy 1977 case representation was widely used and taught in AI and NLP courses,\nand was described as a standard of natural language understanding in the ﬁrst edition\nof Winston’s (1977) textbook Artiﬁcial Intelligence.\nIn the 1980s Fillmore proposed his model of frame semantics, later describing\nthe intuition as follows:\n“The idea behind frame semantics is that speakers are aware of possi-\nbly quite complex situation types, packages of connected expectations,\nthat go by various names—frames, schemas, scenarios, scripts, cultural\nnarratives, memes—and the words in our language are understood with\nsuch frames as their presupposed background.” (Fillmore, 2012, p. 712)\nThe word frame seemed to be in the air for a suite of related notions proposed at\nabout the same time by Minsky (1974), Hymes (1974), and Goffman (1974), as\nwell as related notions with other names like scripts (Schank and Abelson, 1975)\nand schemata (Bobrow and Norman, 1975) (see Tannen (1979) for a comparison).\nFillmore was also inﬂuenced by the semantic ﬁeld theorists and by a visit to the Yale\nAI lab where he took notice of the lists of slots and ﬁllers used by early information\nextraction systems like DeJong (1982) and Schank and Abelson (1977). In the 1990s\nFillmore drew on these insights to begin the FrameNet corpus annotation project.\nAt the same time, Beth Levin drew on her early case frame dictionaries (Levin,\n1977) to develop her book which summarized sets of verb classes deﬁned by shared\nargument realizations (Levin, 1993). The VerbNet project built on this work (Kipper\net al., 2000), leading soon afterwards to the PropBank semantic-role-labeled corpus\ncreated by Martha Palmer and colleagues (Palmer et al., 2005).\nThe combination of rich linguistic annotation and corpus-based approach in-\nstantiated in FrameNet and PropBank led to a revival of automatic approaches to\nsemantic role labeling, ﬁrst on FrameNet (Gildea and Jurafsky, 2000) and then on\nPropBank data (Gildea and Palmer, 2002, inter alia). The problem ﬁrst addressed in\nthe 1970s by hand-written rules was thus now generally recast as one of supervised\nmachine learning enabled by large and consistent databases. Many popular features\nused for role labeling are deﬁned in Gildea and Jurafsky (2002), Surdeanu et al.\n(2003), Xue and Palmer (2004), Pradhan et al. (2005), Che et al. (2009), and Zhao\net al. (2009). The use of dependency rather than constituency parses was introduced\nin the CoNLL-2008 shared task (Surdeanu et al., 2008b). For surveys see Palmer\net al. (2010) and M`arquez et al. (2008).\nThe use of neural approachess to semantic role labeling was pioneered by Col-\nlobert et al. (2011), who applied a CRF on top of a convolutional net. Early work\nlike Foland, Jr. and Martin (2015) focused on using dependency features. Later\nwork eschewed syntactic features altogether; (Zhou and Xu, 2015) introduced the\nuse of a stacked (6-8 layer) bi-LSTM architecture, and (He et al., 2017) showed\nhow to augment the bi-LSTM architecture with highway networks and also replace\nthe CRF with A* decoding that make it possible to apply a wide variety of global\nconstraints in SRL decoding.\nMost semantic role labeling schemes only work within a single sentence, fo-\ncusing on the object of the verbal (or nominal, in the case of NomBank) predicate.",
  "384": "376\nCHAPTER 18\n•\nSEMANTIC ROLE LABELING\nHowever, in many cases, a verbal or nominal predicate may have an implicit argu-\nment: one that appears only in a contextual sentence, or perhaps not at all and must\nimplicit\nargument\nbe inferred. In the two sentences This house has a new owner. The sale was ﬁnalized\n10 days ago. the sale in the second sentence has no ARG1, but a reasonable reader\nwould infer that the Arg1 should be the house mentioned in the prior sentence. Find-\ning these arguments, implicit argument detection (sometimes shortened as iSRL)\niSRL\nwas introduced by Gerber and Chai (2010) and Ruppenhofer et al. (2010). See Do\net al. (2017) for more recent neural models.\nTo avoid the need for huge labeled training sets, unsupervised approaches for\nsemantic role labeling attempt to induce the set of semantic roles by clustering over\narguments. The task was pioneered by Riloff and Schmelzenbach (1998) and Swier\nand Stevenson (2004); see Grenager and Manning (2006), Titov and Klementiev\n(2012), Lang and Lapata (2014), Woodsend and Lapata (2015), and Titov and Khod-\ndam (2014).\nRecent innovations in frame labeling include connotation frames, which mark\nricher information about the argument of predicates. Connotation frames mark the\nsentiment of the writer or reader toward the arguements (for example using the verb\nsurvive in he survived a bombing expresses the writer’s sympathy toward the subject\nhe and negative sentiment toward the bombing. Connotation frames also mark effect\n(something bad happened to x), value: (x is valuable), and mental state: (x is dis-\ntressed by the event) (Rashkin et al. 2016, Rashkin et al. 2017). Connotation frames\ncan also mark the power differential between the arguments (using the verb implore\nmeans that the theme argument has greater power than the agent), and the agency\nof each argument (waited is low agency). Fig. 18.9 shows a visualization from Sap\net al. (2017).\nAGENT\nTHEME\npower(AG < TH)\nVERB\nimplore\nHe implored the tribunal to show mercy.\nThe princess waited for her prince.\nAGENT\nTHEME\nagency(AG) = -\nVERB\nwait\nFigure 18.9\nThe connotation frames of Sap et al. (2017), showing that the verb implore\nimplies the agent has lower power than the theme (in contrast, say, with a verb like demanded,\nand showing the low level of agency of the subject of waited. Figure from Sap et al. (2017).\nSelectional preference has been widely studied beyond the selectional associa-\ntion models of Resnik (1993) and Resnik (1996). Methods have included cluster-\ning (Rooth et al., 1999), discriminative learning (Bergsma et al., 2008), and topic\nmodels (S´eaghdha 2010, Ritter et al. 2010), and constraints can be expressed at the\nlevel of words or classes (Agirre and Martinez, 2001). Selectional preferences have\nalso been successfully integrated into semantic role labeling (Erk 2007, Zapirain\net al. 2013, Do et al. 2017).",
  "385": "EXERCISES\n377\nExercises",
  "386": "378\nCHAPTER 19\n•\nLEXICONS FOR SENTIMENT, AFFECT, AND CONNOTATION\nCHAPTER\n19\nLexicons for Sentiment, Affect,\nand Connotation\n“[W]e write, not with the ﬁngers, but with the whole person. The nerve which\ncontrols the pen winds itself about every ﬁbre of our being, threads the heart,\npierces the liver.”\nVirginia Woolf, Orlando\n“She runs the gamut of emotions from A to B.”\nDorothy Parker, reviewing Hepburn’s performance in Little Women\nIn this chapter we turn to tools for interpreting affective meaning, extending our\naffective\nstudy of sentiment analysis in Chapter 4. We use the word ‘affective’, following\nthe tradition in affective computing (Picard, 1995) to mean emotion, sentiment, per-\nsonality, mood, and attitudes. Affective meaning is closely related to subjectivity,\nsubjectivity\nthe study of a speaker or writer’s evaluations, opinions, emotions, and speculations\n(Wiebe et al., 1999).\nHow should affective meaning be deﬁned? One inﬂuential typology of affec-\ntive states comes from Scherer (2000), who deﬁnes each class of affective states by\nfactors like its cognition realization and time course:\nEmotion: Relatively brief episode of response to the evaluation of an external\nor internal event as being of major signiﬁcance.\n(angry, sad, joyful, fearful, ashamed, proud, elated, desperate)\nMood: Diffuse affect state, most pronounced as change in subjective feeling, of\nlow intensity but relatively long duration, often without apparent cause.\n(cheerful, gloomy, irritable, listless, depressed, buoyant)\nInterpersonal stance: Affective stance taken toward another person in a spe-\nciﬁc interaction, colouring the interpersonal exchange in that situation.\n(distant, cold, warm, supportive, contemptuous, friendly)\nAttitude: Relatively enduring, affectively colored beliefs, preferences, and pre-\ndispositions towards objects or persons.\n(liking, loving, hating, valuing, desiring)\nPersonality traits: Emotionally laden, stable personality dispositions and be-\nhavior tendencies, typical for a person.\n(nervous, anxious, reckless, morose, hostile, jealous)\nFigure 19.1\nThe Scherer typology of affective states (Scherer, 2000).\nWe can design extractors for each of these kinds of affective states. Chapter 4\nalready introduced sentiment analysis, the task of extracting the positive or negative",
  "387": "19.1\n•\nDEFINING EMOTION\n379\norientation that a writer expresses in a text. This corresponds in Scherer’s typology\nto the extraction of attitudes: ﬁguring out what people like or dislike, from affect-\nrish texts like consumer reviews of books or movies, newspaper editorials, or public\nsentiment in blogs or tweets.\nDetecting emotion and moods is useful for detecting whether a student is con-\nfused, engaged, or certain when interacting with a tutorial system, whether a caller\nto a help line is frustrated, whether someone’s blog posts or tweets indicated depres-\nsion. Detecting emotions like fear in novels, for example, could help us trace what\ngroups or situations are feared and how that changes over time.\nDetecting different interpersonal stances can be useful when extracting infor-\nmation from human-human conversations. The goal here is to detect stances like\nfriendliness or awkwardness in interviews or friendly conversations, or even to de-\ntect ﬂirtation in dating. For the task of automatically summarizing meetings, we’d\nlike to be able to automatically understand the social relations between people, who\nis friendly or antagonistic to whom. A related task is ﬁnding parts of a conversation\nwhere people are especially excited or engaged, conversational hot spots that can\nhelp a summarizer focus on the correct region.\nDetecting the personality of a user—such as whether the user is an extrovert\nor the extent to which they are open to experience— can help improve conversa-\ntional agents, which seem to work better if they match users’ personality expecta-\ntions (Mairesse and Walker, 2008).\nAffect is important for generation as well as recognition; synthesizing affect\nis important for conversational agents in various domains, including literacy tutors\nsuch as children’s storybooks, or computer games.\nIn Chapter 4 we introduced the use of Naive Bayes classiﬁcation to classify a\ndocument’s sentiment. Various classiﬁers have been successfully applied to many of\nthese tasks, using all the words in the training set as input to a classiﬁer which then\ndetermines the affect status of the text.\nIn this chapter we focus on an alternative model, in which instead of using every\nword as a feature, we focus only on certain words, ones that carry particularly strong\ncues to affect or sentiment. We call these lists of words affective lexicons or senti-\nment lexicons. These lexicons presuppose a fact about semantics: that words have\naffective meanings or connotations. The word connotation has different meanings\nconnotations\nin different ﬁelds, but here we use it to mean the aspects of a word’s meaning that\nare related to a writer or reader’s emotions, sentiment, opinions, or evaluations. In\naddition to their ability to help determine the affective status of a text, connotation\nlexicons can be useful features for other kinds of affective tasks, and for computa-\ntional social science analysis.\nIn the next sections we introduce basic theories of emotion, show how sentiment\nlexicons can be viewed as a special case of emotion lexicons, and then summarize\nsome publicly available lexicons. We then introduce three ways for building new\nlexicons: human labeling, semi-supervised, and supervised.\nFinally, we turn to some other kinds of affective meaning, including interper-\nsonal stance, personality, and connotation frames.\n19.1\nDeﬁning Emotion\nOne of the most important affective classes is emotion, which Scherer (2000) deﬁnes\nemotion\nas a “relatively brief episode of response to the evaluation of an external or internal",
  "388": "380\nCHAPTER 19\n•\nLEXICONS FOR SENTIMENT, AFFECT, AND CONNOTATION\nevent as being of major signiﬁcance”.\nDetecting emotion has the potential to improve a number of language processing\ntasks. Automatically detecting emotions in reviews or customer responses (anger,\ndissatisfaction, trust) could help businesses recognize speciﬁc problem areas or ones\nthat are going well. Emotion recognition could help dialog systems like tutoring\nsystems detect that a student was unhappy, bored, hesitant, conﬁdent, and so on.\nEmotion can play a role in medical informatics tasks like detecting depression or\nsuicidal intent. Detecting emotions expressed toward characters in novels might\nplay a role in understanding how different social groups were viewed by society at\ndifferent times.\nThere are two widely-held families of theories of emotion. In one family, emo-\ntions are viewed as ﬁxed atomic units, limited in number, and from which others\nare generated, often called basic emotions (Tomkins 1962, Plutchik 1962). Per-\nbasic emotions\nhaps most well-known of this family of theories are the 6 emotions proposed by\n(Ekman, 1999) as a set of emotions that is likely to be universally present in all\ncultures: surprise, happiness, anger, fear, disgust, sadness. Another atomic theory\nis the (Plutchik, 1980) wheel of emotion, consisting of 8 basic emotions in four\nopposing pairs: joy–sadness, anger–fear, trust–disgust, and anticipation–surprise,\ntogether with the emotions derived from them, shown in Fig. 19.2.\nFigure 19.2\nPlutchik wheel of emotion.\nThe second class of emotion theories views emotion as a space in 2 or 3 di-\nmensions (Russell, 1980). Most models include the two dimensions valence and\narousal, and many add a third, dominance. These can be deﬁned as:\nvalence: the pleasantness of the stimulus\narousal: the intensity of emotion provoked by the stimulus\ndominance: the degree of control exerted by the stimulus\nIn the next sections we’ll see lexicons for both kinds of theories of emotion.",
  "389": "19.2\n•\nAVAILABLE SENTIMENT AND AFFECT LEXICONS\n381\nSentiment can be viewed as a special case of this second view of emotions as\npoints in space. In particular, the valence dimension, measuring how pleasant or\nunpleasant a word is, is often used directly as a measure of sentiment.\n19.2\nAvailable Sentiment and Affect Lexicons\nA wide variety of affect lexicons have been created and released. The most basic\nlexicons label words along one dimension of semantic variability, generally called\n”sentiment” or ”valence”.\nIn the simplest lexicons this dimension is represented in a binary fashion, with\na wordlist for positive words and a wordlist for negative words. The oldest is the\nGeneral Inquirer (Stone et al., 1966), which drew on early work in the cognition\nGeneral\nInquirer\npsychology of word meaning (Osgood et al., 1957) and on work in content analysis.\nThe General Inquirer has a lexicon of 1915 positive words an done of 2291 negative\nwords (and also includes other lexicons discussed below).\nThe MPQA Subjectivity lexicon (Wilson et al., 2005) has 2718 positive and\n4912 negative words drawn from prior lexicons plus a bootstrapped list of subjec-\ntive words and phrases (Riloff and Wiebe, 2003) Each entry in the lexicon is hand-\nlabeled for sentiment and also labeled for reliability (strongly subjective or weakly\nsubjective).\nThe polarity lexicon of Hu and Liu (2004b) gives 2006 positive and 4783 nega-\ntive words, drawn from product reviews, labeled using a bootstrapping method from\nWordNet.\nPositive\nadmire, amazing, assure, celebration, charm, eager, enthusiastic, excellent, fancy, fan-\ntastic, frolic, graceful, happy, joy, luck, majesty, mercy, nice, patience, perfect, proud,\nrejoice, relief, respect, satisfactorily, sensational, super, terriﬁc, thank, vivid, wise, won-\nderful, zest\nNegative abominable, anger, anxious, bad, catastrophe, cheap, complaint, condescending, deceit,\ndefective, disappointment, embarrass, fake, fear, ﬁlthy, fool, guilt, hate, idiot, inﬂict, lazy,\nmiserable, mourn, nervous, objection, pest, plot, reject, scream, silly, terrible, unfriendly,\nvile, wicked\nFigure 19.3\nSome samples of words with consistent sentiment across three sentiment lexicons: the General\nInquirer (Stone et al., 1966), the MPQA Subjectivity lexicon (Wilson et al., 2005), and the polarity lexicon of\nHu and Liu (2004b).\nSlightly more general than these sentiment lexicons are lexicons that assign each\nword a value on all three emotional dimension The lexicon of Warriner et al. (2013)\nassigns valence, arousal, and dominance scores to 14,000 words. Some examples\nare shown in Fig. 19.4\nThe NRC Word-Emotion Association Lexicon, also called EmoLex (Moham-\nEmoLex\nmad and Turney, 2013), uses the Plutchik (1980) 8 basic emotions deﬁned above.\nThe lexicon includes around 14,000 words including words from prior lexicons as\nwell as frequent nouns, verbs, adverbs and adjectives. Values from the lexicon for\nsome sample words:",
  "390": "382\nCHAPTER 19\n•\nLEXICONS FOR SENTIMENT, AFFECT, AND CONNOTATION\nValence\nArousal\nDominance\nvacation\n8.53\nrampage\n7.56\nself\n7.74\nhappy\n8.47\ntornado\n7.45\nincredible\n7.74\nwhistle\n5.7\nzucchini\n4.18\nskillet\n5.33\nconscious\n5.53\ndressy\n4.15\nconcur\n5.29\ntorture\n1.4\ndull\n1.67\nearthquake\n2.14\nFigure 19.4\nSamples of the values of selected words on the three emotional dimensions\nfrom Warriner et al. (2013).\nWord\nanger\nanticipation\ndisgust\nfear\njoy\nsadness\nsurprise\ntrust\npositive\nnegative\nreward\n0 1\n0\n0\n1\n0 1\n1 1\n0\nworry\n0 1\n0\n1\n0\n1 0\n0 0\n1\ntenderness 0 0\n0\n0\n1\n0 0\n0 1\n0\nsweetheart 0 1\n0\n0\n1\n1 0\n1 1\n0\nsuddenly\n0 0\n0\n0\n0\n0 1\n0 0\n0\nthirst\n0 1\n0\n0\n0\n1 1\n0 0\n0\ngarbage\n0 0\n1\n0\n0\n0 0\n0 0\n1\nThere are various other hand-built affective lexicons. The General Inquirer in-\ncludes additional lexicons for dimensions like strong vs. weak, active vs. passive,\noverstated vs. understated, as well as lexicons for categories like pleasure, pain,\nvirtue, vice, motivation, and cognitive orientation.\nAnother useful feature for various tasks is the distinction between concrete\nconcrete\nwords like banana or bathrobe and abstract words like belief and although. The\nabstract\nlexicon in (Brysbaert et al., 2014) used crowdsourcing to assign a rating from 1 to 5\nof the concreteness of 40,000 words, thus assigning banana, bathrobe, and bagel 5,\nbelief 1.19, although 1.07, and in between words like brisk a 2.5.\nLIWC, Linguistic Inquiry and Word Count, is another set of 73 lexicons con-\nLIWC\ntaining over 2300 words (Pennebaker et al., 2007), designed to capture aspects of\nlexical meaning relevant for social psychological tasks. In addition to sentiment-\nrelated lexicons like ones for negative emotion (bad, weird, hate, problem, tough)\nand positive emotion (love, nice, sweet), LIWC includes lexicons for categories like\nanger, sadness, cognitive mechanisms, perception, tentative, and inhibition, shown\nin Fig. 19.5.\n19.3\nCreating affect lexicons by human labeling\nThe earliest method used to build affect lexicons, and still in common use, is to have\nhumans label each word. This is now most commonly done via crowdsourcing:\ncrowdsourcing\nbreaking the task into small pieces and distributing them to a large number of anno-\ntators. Let’s take a look at some of the methodological choices for two crowdsourced\nemotion lexicons.\nThe NRC Word-Emotion Association Lexicon (EmoLex) (Mohammad and Tur-\nney, 2013), labeled emotions in two steps. In order to ensure that the annotators\nwere judging the correct sense of the word, they ﬁrst answered a multiple-choice",
  "391": "19.3\n•\nCREATING AFFECT LEXICONS BY HUMAN LABELING\n383\nPositive\nNegative\nEmotion\nEmotion\nInsight\nInhibition\nFamily\nNegate\nappreciat*\nanger*\naware*\navoid*\nbrother*\naren’t\ncomfort*\nbore*\nbelieve\ncareful*\ncousin*\ncannot\ngreat\ncry\ndecid*\nhesitat*\ndaughter*\ndidn’t\nhappy\ndespair*\nfeel\nlimit*\nfamily\nneither\ninterest\nfail*\nﬁgur*\noppos*\nfather*\nnever\njoy*\nfear\nknow\nprevent*\ngrandf*\nno\nperfect*\ngriev*\nknew\nreluctan*\ngrandm*\nnobod*\nplease*\nhate*\nmeans\nsafe*\nhusband\nnone\nsafe*\npanic*\nnotice*\nstop\nmom\nnor\nterriﬁc\nsuffers\nrecogni*\nstubborn*\nmother\nnothing\nvalue\nterrify\nsense\nwait\nniece*\nnowhere\nwow*\nviolent*\nthink\nwary\nwife\nwithout\nFigure 19.5\nSamples from 5 of the 73 lexical categories in LIWC (Pennebaker et al., 2007).\nThe * means the previous letters are a word preﬁx and all words with that preﬁx are included\nin the category.\nsynonym question that primed the correct sense of the word (without requiring the\nannotator to read a potentially confusing sense deﬁnition). These were created au-\ntomatically using the headwords associated with the thesaurus category of the sense\nin question in the Macquarie dictionary and the headwords of 3 random distractor\ncategories. An example:\nWhich word is closest in meaning (most related) to startle?\n• automobile\n• shake\n• honesty\n• entertain\nFor each word (e.g. startle), the annotator was then asked to rate how associated\nthat word is with each of the 8 emotions (joy, fear, anger, etc.). The associations\nwere rated on a scale of not, weakly, moderately, and strongly associated. Outlier\nratings were removed, and then each term was assigned the class chosen by the ma-\njority of the annotators, with ties broken by choosing the stronger intensity, and then\nthe 4 levels were mapped into a binary label for each word (no and weak mapped to\n0, moderate and strong mapped to 1).\nFor the Warriner et al. (2013) lexicon of valence, arousal, and dominance, crowd-\nworkers marked each word with a value from 1-9 on each of the dimensions, with\nthe scale deﬁned for them as follows:\n• valence (the pleasantness of the stimulus)\n9: happy, pleased, satisﬁed, contented, hopeful\n1: unhappy, annoyed, unsatisﬁed, melancholic, despaired, or bored\n• arousal (the intensity of emotion provoked by the stimulus)\n9: stimulated, excited, frenzied, jittery, wide-awake, or aroused\n1: relaxed, calm, sluggish, dull, sleepy, or unaroused;\n• dominance (the degree of control exerted by the stimulus)\n9: in control, inﬂuential, important, dominant, autonomous, or controlling\n1: controlled, inﬂuenced, cared-for, awed, submissive, or guided",
  "392": "384\nCHAPTER 19\n•\nLEXICONS FOR SENTIMENT, AFFECT, AND CONNOTATION\n19.4\nSemi-supervised induction of affect lexicons\nAnother common way to learn sentiment lexicons is to start from a set of seed words\nthat deﬁne two poles of a semantic axis (words like good or bad), and then ﬁnd ways\nto label each word w by its similarity to the two seed sets. Here we summarize two\nfamilies of seed-based semi-supervised lexicon induction algorithms, axis-based and\ngraph-based.\n19.4.1\nSemantic axis methods\nOne of the most well-known lexicon induction methods, the Turney and Littman\n(2003) algorithm, is given seed words like good or bad, and then for each word w to\nbe labeled, measures both how similar it is to good and how different it is from bad.\nHere we describe a slight extension of the algorithm due to An et al. (2018), which\nis based on computing a semantic axis.\nIn the ﬁrst step, we choose seed words by hand. Because the sentiment or affect\nof a word is different in different contexts, it’s common to choose different seed\nwords for different genres, and most algorithms are quite sensitive to the choice of\nseeds. For example, for inducing sentiment lexicons, Hamilton et al. (2016a) deﬁnes\none set of seed words for general sentiment analyis, a different set for Twitter, and\nyet another set for learning a lexicon for sentiment in ﬁnancial text:\nDomain\nPositive seeds\nNegative seeds\nGeneral\ngood, lovely, excellent, fortunate, pleas-\nant, delightful, perfect, loved, love,\nhappy\nbad, horrible, poor, unfortunate, un-\npleasant, disgusting, evil, hated, hate,\nunhappy\nTwitter\nlove,\nloved,\nloves,\nawesome,\nnice,\namazing, best, fantastic, correct, happy\nhate, hated, hates, terrible, nasty, awful,\nworst, horrible, wrong, sad\nFinance\nsuccessful, excellent, proﬁt, beneﬁcial,\nimproving, improved, success, gains,\npositive\nnegligent, loss, volatile, wrong, losses,\ndamages, bad, litigation, failure, down,\nnegative\nIn the second step, we compute embeddings for each of the pole words. These\nembeddings can be off-the-shelf word2vec embeddings, or can be computed directly\non a speciﬁc corpus (for example using a ﬁnancial corpus if a ﬁnance lexicon is the\ngoal), or we can ﬁne-tune off-the-shelf embeddings to a corpus. Fine-tuning is espe-\ncially important if we have a very speciﬁc genre of text but don’t have enough data\nto train good embeddings. In ﬁne-tuning, we begin with off-the-shelf embeddings\nlike word2vec, and continue training them on the small target corpus.\nOnce we have embeddings for each pole word, we we create an embedding that\nrepresents each pole by taking the centroid of the embeddings of each of the seed\nwords; recall that the centroid is the multidimensional version of the mean. Given\na set of embeddings for the positive seed words S+ = {E(w+\n1 ),E(w+\n2 ),...,E(w+\nn )},\nand embeddings for the negative seed words S−= {E(w−\n1 ),E(w−\n2 ),...,E(w−\nm)}, the",
  "393": "19.4\n•\nSEMI-SUPERVISED INDUCTION OF AFFECT LEXICONS\n385\npole centroids are:\nV+ = 1\nn\nn\nX\n1\nE(w+\ni )\nV−= 1\nn\nm\nX\n1\nE(w−\ni )\n(19.1)\nThe semantic axis deﬁned by the poles is computed just by subtracting the two vec-\ntors:\nVaxis = V+ −V−\n(19.2)\nVaxis, the semantic axis, is a vector in the direction of sentiment. Finally, we compute\nhow close each word w is to this sentiment axis, by taking the cosine between w’s\nembedding and the axis vector. A higher cosine means that w is more aligned with\nS+ than S−.\nscore(w) =\n\u0000cos(E(w),Vaxis\n\u0001\n=\nE(w)·Vaxis\n∥E(w)∥∥Vaxis∥\n(19.3)\nIf a dictionary of words with sentiment scores is sufﬁcient, we’re done! Or if we\nneed to group words into a positive and a negative lexicon, we can use a threshold\nor other method to give us discrete lexicons.\n19.4.2\nLabel propagation\nAn alternative family of methods deﬁnes lexicons by propagating sentiment labels\non graphs, an idea suggested in early work by Hatzivassiloglou and McKeown\n(1997). We’ll describe the simple SentProp (Sentiment Propagation) algorithm of\nHamilton et al. (2016a), which has four steps:\n1. Deﬁne a graph: Given word embeddings, build a weighted lexical graph\nby connecting each word with its k nearest neighbors (according to cosine-\nsimilarity). The weights of the edge between words wi and w j are set as:\nEi,j = arccos\n\u0012\n−\nwi⊤wj\n∥wi∥∥wj∥\n\u0013\n.\n(19.4)\n2. Deﬁne a seed set: By hand, choose positive and negative seed words.\n3. Propagate polarities from the seed set: Now we perform a random walk on\nthis graph, starting at the seed set. In a random walk, we start at a node and\nthen choose a node to move to with probability proportional to the edge prob-\nability. A word’s polarity score for a seed set is proportional to the probability\nof a random walk from the seed set landing on that word, (Fig. 19.6).\n4. Create word scores: We walk from both positive and negative seed sets,\nresulting in positive (score+(wi)) and negative (score−(wi)) label scores. We\nthen combine these values into a positive-polarity score as:\nscore+(wi) =\nscore+(wi)\nscore+(wi)+score−(wi)\n(19.5)\nIt’s often helpful to standardize the scores to have zero mean and unit variance\nwithin a corpus.",
  "394": "386\nCHAPTER 19\n•\nLEXICONS FOR SENTIMENT, AFFECT, AND CONNOTATION\n5. Assign conﬁdence to each score: Because sentiment scores are inﬂuenced by\nthe seed set, we’d like to know how much the score of a word would change if\na different seed set is used. We can use bootstrap-sampling to get conﬁdence\nregions, by computing the propagation B times over random subsets of the\npositive and negative seed sets (for example using B = 50 and choosing 7 of\nthe 10 seed words each time). The standard deviation of the bootstrap-sampled\npolarity scores gives a conﬁdence measure.\nidolize\nlove\nadore\nappreciate\nlike\nﬁnd\ndislike\nsee\nnotice\ndisapprove\nabhor\nhate\nloathe\ndespise\nuncover\nidolize\nlove\nadore\nappreciate\nlike\nﬁnd\ndislike\nsee\nnotice\ndisapprove\nabhor\nhate\nloathe\ndespise\nuncover\n(a)\n(b)\nFigure 19.6\nIntuition of the SENTPROP algorithm. (a) Run random walks from the seed words. (b) Assign\npolarity scores (shown here as colors green or red) based on the frequency of random walk visits.\n19.4.3\nOther methods\nThe core of semisupervised algorithms is the metric for measuring similarity with\nthe seed words. The Turney and Littman (2003) and Hamilton et al. (2016a) ap-\nproaches above used embedding cosine as the distance metric: words were labeled\nas positive basically if their embeddings had high cosines with positive seeds and\nlow cosines with negative seeds. Other methods have chosen other kinds of distance\nmetrics besides embedding cosine.\nFor example the Hatzivassiloglou and McKeown (1997) algorithm uses syntactic\ncues; two adjectives are considered similar if they were frequently conjoined by and\nand rarely conjoined by but. This is based on the intuition that adjectives conjoined\nby the words and tend to have the same polarity; positive adjectives are generally\ncoordinated with positive, negative with negative:\nfair and legitimate, corrupt and brutal\nbut less often positive adjectives coordinated with negative:\n*fair and brutal, *corrupt and legitimate\nBy contrast, adjectives conjoined by but are likely to be of opposite polarity:\nfair but brutal\nAnother cue to opposite polarity comes from morphological negation (un-, im-,\n-less). Adjectives with the same root but differing in a morphological negative (ad-\nequate/inadequate, thoughtful/thoughtless) tend to be of opposite polarity.\nYet another method for ﬁnding words that have a similar polarity to seed words is\nto make use of a thesaurus like WordNet (Kim and Hovy 2004, Hu and Liu 2004b).\nA word’s synonyms presumably share its polarity while a word’s antonyms probably\nhave the opposite polarity. After a seed lexicon is built, each lexicon is updated as\nfollows, possibly iterated.\nLex+: Add synonyms of positive words (well) and antonyms (like ﬁne) of negative\nwords",
  "395": "19.5\n•\nSUPERVISED LEARNING OF WORD SENTIMENT\n387\nLex−: Add synonyms of negative words (awful) and antonyms ( like evil) of positive\nwords\nAn extension of this algorithm assigns polarity to WordNet senses, called Senti-\nWordNet (Baccianella et al., 2010). Fig. 19.7 shows some examples.\nSentiWordNet\nSynset\nPos\nNeg\nObj\ngood#6\n‘agreeable or pleasing’\n1\n0\n0\nrespectable#2 honorable#4 good#4 estimable#2\n‘deserving of esteem’\n0.75\n0\n0.25\nestimable#3 computable#1\n‘may be computed or estimated’\n0\n0\n1\nsting#1 burn#4 bite#2\n‘cause a sharp or stinging pain’\n0\n0.875 .125\nacute#6\n‘of critical importance and consequence’\n0.625 0.125 .250\nacute#4\n‘of an angle; less than 90 degrees’\n0\n0\n1\nacute#1\n‘having or experiencing a rapid onset and short but severe course’\n0\n0.5\n0.5\nFigure 19.7\nExamples from SentiWordNet 3.0 (Baccianella et al., 2010). Note the differences between senses\nof homonymous words: estimable#3 is purely objective, while estimable#2 is positive; acute can be positive\n(acute#6), negative (acute#1), or neutral (acute #4)\n.\nIn this algorithm, polarity is assigned to entire synsets rather than words. A\npositive lexicon is built from all the synsets associated with 7 positive words, and a\nnegative lexicon from synsets associated with 7 negative words. A classiﬁer is then\ntrained from this data to take a WordNet gloss and decide if the sense being deﬁned\nis positive, negative or neutral. A further step (involving a random-walk algorithm)\nassigns a score to each WordNet synset for its degree of positivity, negativity, and\nneutrality.\nIn summary, semisupervised algorithms use a human-deﬁned set of seed words\nfor the two poles of a dimension, and use similarity metrics like embedding cosine,\ncoordination, morphology, or thesaurus structure to score words by how similar they\nare to the positive seeds and how dissimilar to the negative seeds.\n19.5\nSupervised learning of word sentiment\nSemi-supervised methods require only minimal human supervision (in the form of\nseed sets). But sometimes a supervision signal exists in the world and can be made\nuse of. One such signal is the scores associated with online reviews.\nThe web contains an enormous number of online reviews for restaurants, movies,\nbooks, or other products, each of which have the text of the review along with an\nassociated review score: a value that may range from 1 star to 5 stars, or scoring 1\nto 10. Fig. 19.8 shows samples extracted from restaurant, book, and movie reviews.\nWe can use this review score as supervision: positive words are more likely to\nappear in 5-star reviews; negative words in 1-star reviews. And instead of just a\nbinary polarity, this kind of supervision allows us to assign a word a more complex\nrepresentation of its polarity: its distribution over stars (or other scores).\nThus in a ten-star system we could represent the sentiment of each word as a\n10-tuple, each number a score representing the word’s association with that polarity\nlevel. This association can be a raw count, or a likelihood P(w|c), or some other\nfunction of the count, for each class c from 1 to 10.\nFor example, we could compute the IMDB likelihood of a word like disap-\npoint(ed/ing) occurring in a 1 star review by dividing the number of times disap-\npoint(ed/ing) occurs in 1-star reviews in the IMDB dataset (8,557) by the total num-",
  "396": "388\nCHAPTER 19\n•\nLEXICONS FOR SENTIMENT, AFFECT, AND CONNOTATION\nMovie review excerpts (IMDB)\n10 A great movie. This ﬁlm is just a wonderful experience. It’s surreal, zany, witty and slapstick\nall at the same time. And terriﬁc performances too.\n1\nThis was probably the worst movie I have ever seen. The story went nowhere even though they\ncould have done some interesting stuff with it.\nRestaurant review excerpts (Yelp)\n5\nThe service was impeccable. The food was cooked and seasoned perfectly... The watermelon\nwas perfectly square ... The grilled octopus was ... mouthwatering...\n2\n...it took a while to get our waters, we got our entree before our starter, and we never received\nsilverware or napkins until we requested them...\nBook review excerpts (GoodReads)\n1\nI am going to try and stop being deceived by eye-catching titles. I so wanted to like this book\nand was so disappointed by it.\n5\nThis book is hilarious. I would recommend it to anyone looking for a satirical read with a\nromantic twist and a narrator that keeps butting in\nProduct review excerpts (Amazon)\n5\nThe lid on this blender though is probably what I like the best about it... enables you to pour\ninto something without even taking the lid off! ... the perfect pitcher! ... works fantastic.\n1\nI hate this blender... It is nearly impossible to get frozen fruit and ice to turn into a smoothie...\nYou have to add a TON of liquid. I also wish it had a spout ...\nFigure 19.8\nExcerpts from some reviews from various review websites, all on a scale of 1 to 5 stars except\nIMDB, which is on a scale of 1 to 10 stars.\nber of words occurring in 1-star reviews (25,395,214), so the IMDB estimate of\nP(disappointing|1) is .0003.\nA slight modiﬁcation of this weighting, the normalized likelihood, can be used\nas an illuminating visualization (Potts, 2011)1:\nP(w|c) =\ncount(w,c)\nP\nw∈C count(w,c)\nPottsScore(w) =\nP(w|c)\nP\nc P(w|c)\n(19.6)\nDividing the IMDB estimate P(disappointing|1) of .0003 by the sum of the like-\nlihood P(w|c) over all categories gives a Potts score of 0.10. The word disappointing\nthus is associated with the vector [.10, .12, .14, .14, .13, .11, .08, .06, .06, .05]. The\nPotts diagram (Potts, 2011) is a visualization of these word scores, representing the\nPotts diagram\nprior sentiment of a word as a distribution over the rating categories.\nFig. 19.9 shows the Potts diagrams for 3 positive and 3 negative scalar adjectives.\nNote that the curve for strongly positive scalars have the shape of the letter J, while\nstrongly negative scalars look like a reverse J. By contrast, weakly positive and neg-\native scalars have a hump-shape, with the maximum either below the mean (weakly\nnegative words like disappointing) or above the mean (weakly positive words like\ngood). These shapes offer an illuminating typology of affective word meaning.\nFig. 19.10 shows the Potts diagrams for emphasizing and attenuating adverbs.\nAgain we see generalizations in the characteristic curves associated with words of\nparticular meanings. Note that emphatics tend to have a J-shape (most likely to occur\n1\nPotts shows that the normalized likelihood is an estimate of the posterior P(c|w) if we make the\nincorrect but simplifying assumption that all categories c have equal probability.",
  "397": "19.5\n•\nSUPERVISED LEARNING OF WORD SENTIMENT\n389\ngood\ngreat\nexcellent\ndisappointing\nbad\nterrible\nPositive scalars\nNegative scalars\n1  2  3  4  5  6  7  8  9  10\nrating\n1  2  3  4  5  6  7  8  9  10\nrating\n1  2  3  4  5  6  7  8  9  10\nrating\n1  2  3  4  5  6  7  8  9  10\nrating\n1  2  3  4  5  6  7  8  9  10\nrating\n1  2  3  4  5  6  7  8  9  10\nrating\nFigure 19.9\nPotts diagrams (Potts, 2011) for positive and negative scalar adjectives, show-\ning the J-shape and reverse J-shape for strongly positive and negative adjectives, and the\nhump-shape for more weakly polarized adjectives.\nin the most positive reviews) or a U-shape (most likely to occur in the strongly posi-\ntive and negative). Attenuators all have the hump-shape, emphasizing the middle of\nthe scale and downplaying both extremes.\nCategory\n-0.50\n-0.39\n-0.28\n-0.17\n-0.06\n0.06\n0.17\n0.28\n0.39\n0.50\n0.05\n0.09\n0.13\ntotally\nabsolutely\nutterly\nsomewhat\nfairly\npretty\nEmphatics\nAttenuators\n1  2  3  4  5  6  7  8  9  10\nrating\n1  2  3  4  5  6  7  8  9  10\nrating\n1  2  3  4  5  6  7  8  9  10\nrating\n1  2  3  4  5  6  7  8  9  10\nrating\n1  2  3  4  5  6  7  8  9  10\nrating\n1  2  3  4  5  6  7  8  9  10\nrating\nFigure 19.10\nPotts diagrams (Potts, 2011) for emphatic and attenuating adverbs.\nThe diagrams can be used both as a typology of lexical sentiment, and also play\na role in modeling sentiment compositionality.\nIn addition to functions like posterior P(c|w), likelihood P(w|c), or normalized\nlikelihood (Eq. 19.6) many other functions of the count of a word occurring with a\nsentiment label have been used. We’ll introduce some of these on page 394, includ-\ning ideas like normalizing the counts per writer in Eq. 19.14.",
  "398": "390\nCHAPTER 19\n•\nLEXICONS FOR SENTIMENT, AFFECT, AND CONNOTATION\n19.5.1\nLog odds ratio informative Dirichlet prior\nOne thing we often want to do with word polarity is to distinguish between words\nthat are more likely to be used in one category of texts than in another. We may, for\nexample, want to know the words most associated with 1 star reviews versus those\nassociated with 5 star reviews. These differences may not be just related to senti-\nment. We might want to ﬁnd words used more often by Democratic than Republican\nmembers of Congress, or words used more often in menus of expensive restaurants\nthan cheap restaurants.\nGiven two classes of documents, to ﬁnd words more associated with one cate-\ngory than another, we might choose to just compute the difference in frequencies\n(is a word w more frequent in class A or class B?). Or instead of the difference in\nfrequencies we might want to compute the ratio of frequencies, or the log odds ratio\n(the log of the ratio between the odds of the two words). Then we can sort words\nby whichever of these associations with the category we use, (sorting from words\noverrepresented in category A to words overrepresented in category B).\nThe problem with simple log-likelihood or log odds methods is that they don’t\nwork well for very rare words or very frequent words; for words that are very fre-\nquent, all differences seem large, and for words that are very rare, no differences\nseem large.\nIn this section we walk through the details of one solution to this problem: the\n“log odds ratio informative Dirichlet prior” method of Monroe et al. (2008) that is a\nparticularly useful method for ﬁnding words that are statistically overrepresented in\none particular category of texts compared to another. It’s based on the idea of using\nanother large corpus to get a prior estimate of what we expect the frequency of each\nword to be.\nLet’s start with the goal: assume we want to know whether the word horrible\noccurs more in corpus i or corpus j. We could compute the log likelihood ratio,\nlog likelihood\nratio\nusing f i(w) to mean the frequency of word w in corpus i, and ni to mean the total\nnumber of words in corpus i:\nllr(horrible) = log Pi(horrible)\nPj(horrible)\n= logPi(horrible)−logPj(horrible)\n= log fi(horrible)\nni\n−log fj(horrible)\nnj\n(19.7)\nInstead, let’s compute the log odds ratio: does horrible have higher odds in i or in\nlog odds ratio\nj:\nlor(horrible) = log\n\u0012\nPi(horrible)\n1−Pi(horrible)\n\u0013\n−log\n\u0012\nPj(horrible)\n1−Pj(horrible)\n\u0013\n= log\n\n\n\nfi(horrible)\nni\n1−fi(horrible)\nni\n\n\n−log\n\n\n\nf j(horrible)\nn j\n1−fj(horrible)\nnj\n\n\n\n= log\n\u0012\nfi(horrible)\nni −fi(horrible)\n\u0013\n−log\n\u0012\nf j(horrible)\nnj −fj(horrible)\n\u0013\n(19.8)\nThe Dirichlet intuition is to use a large background corpus to get a prior estimate of\nwhat we expect the frequency of each word w to be. We’ll do this very simply by",
  "399": "19.6\n•\nUSING LEXICONS FOR SENTIMENT RECOGNITION\n391\nadding the counts from that corpus to the numerator and denominator, so that we’re\nessentially shrinking the counts toward that prior. It’s like asking how large are the\ndifferences between i and j given what we would expect given their frequencies in\na well-estimated large background corpus.\nThe method estimates the difference between the frequency of word w in two\ncorpora i and j via the prior-modiﬁed log odds ratio for w, δ (i−j)\nw\n, which is estimated\nas:\nδ (i−j)\nw\n= log\n\u0012\nf i\nw +αw\nni +α0 −(f iw +αw)\n\u0013\n−log\n \nf j\nw +αw\nnj +α0 −(f j\nw +αw)\n!\n(19.9)\n(where ni is the size of corpus i, nj is the size of corpus j, f i\nw is the count of word w\nin corpus i, f j\nw is the count of word w in corpus j, α0 is the size of the background\ncorpus, and αw is the count of word w in the background corpus.)\nIn addition, Monroe et al. (2008) make use of an estimate for the variance of the\nlog–odds–ratio:\nσ2 \u0010\nˆδ (i−j)\nw\n\u0011\n≈\n1\nf iw +αw\n+\n1\nf j\nw +αw\n(19.10)\nThe ﬁnal statistic for a word is then the z–score of its log–odds–ratio:\nˆδ (i−j)\nw\nr\nσ2\n\u0010\nˆδ (i−j)\nw\n\u0011\n(19.11)\nThe Monroe et al. (2008) method thus modiﬁes the commonly used log odds ratio\nin two ways: it uses the z-scores of the log odds ratio, which controls for the amount\nof variance in a words frequency, and it uses counts from a background corpus to\nprovide a prior count for words.\nFig. 19.11 shows the method applied to a dataset of restaurant reviews from\nYelp, comparing the words used in 1-star reviews to the words used in 5-star reviews\n(Jurafsky et al., 2014). The largest difference is in obvious sentiment words, with the\n1-star reviews using negative sentiment words like worse, bad, awful and the 5-star\nreviews using positive sentiment words like great, best, amazing. But there are other\nilluminating differences. 1-star reviews use logical negation (no, not), while 5-star\nreviews use emphatics and emphasize universality (very, highly, every, always). 1-\nstar reviews use ﬁrst person plurals (we, us, our) while 5 star reviews use the second\nperson. 1-star reviews talk about people (manager, waiter, customer) while 5-star\nreviews talk about dessert and properties of expensive restaurants like courses and\natmosphere. See Jurafsky et al. (2014) for more details.\n19.6\nUsing Lexicons for Sentiment Recognition\nIn Chapter 4 we introduced the naive Bayes algorithm for sentiment analysis. The\nlexicons we have focused on throughout the chapter so far can be used in a number\nof ways to improve sentiment detection.\nIn the simplest case, lexicons can be used when we don’t have sufﬁcient training\ndata to build a supervised sentiment analyzer; it can often be expensive to have a\nhuman assign sentiment to each document to train the supervised classiﬁer.",
  "400": "392\nCHAPTER 19\n•\nLEXICONS FOR SENTIMENT, AFFECT, AND CONNOTATION\nClass\nWords in 1-star reviews\nClass\nWords in 5-star reviews\nNegative\nworst, rude, terrible, horrible,\nbad,\nawful,\ndisgusting,\nbland,\ntasteless, gross, mediocre, over-\npriced, worse, poor\nPositive\ngreat,\nbest,\nlove(d),\ndelicious,\namazing, favorite, perfect, excel-\nlent, awesome, friendly, fantastic,\nfresh, wonderful, incredible, sweet,\nyum(my)\nNegation\nno, not\nEmphatics/\nuniversals\nvery, highly, perfectly, deﬁnitely, ab-\nsolutely, everything, every, always\n1Pl pro\nwe, us, our\n2 pro\nyou\n3 pro\nshe, he, her, him\nArticles\na, the\nPast verb\nwas, were, asked, told, said, did,\ncharged, waited, left, took\nAdvice\ntry, recommend\nSequencers after, then\nConjunct\nalso, as, well, with, and\nNouns\nmanager, waitress, waiter, cus-\ntomer, customers, attitude, waste,\npoisoning, money, bill, minutes\nNouns\natmosphere,\ndessert,\nchocolate,\nwine, course, menu\nIrrealis\nmodals\nwould, should\nAuxiliaries\nis/’s, can, ’ve, are\nComp\nto, that\nPrep, other\nin, of, die, city, mouth\nFigure 19.11\nThe top 50 words associated with one–star and ﬁve-star restaurant reviews in a Yelp dataset of\n900,000 reviews, using the Monroe et al. (2008) method (Jurafsky et al., 2014).\nIn such situations, lexicons can be used in a simple rule-based algorithm for\nclassiﬁcation. The simplest version is just to use the ratio of positive to negative\nwords: if a document has more positive than negative words (using the lexicon to\ndecide the polarity of each word in the document), it is classiﬁed as positive. Often\na threshold λ is used, in which a document is classiﬁed as positive only if the ratio\nis greater than λ. If the sentiment lexicon includes positive and negative weights for\neach word, θ +\nw and θ −\nw , these can be used as well. Here’s a simple such sentiment\nalgorithm:\nf + =\nX\nw s.t. w∈positivelexicon\nθ +\nw count(w)\nf −=\nX\nw s.t. w∈negativelexicon\nθ −\nw count(w)\nsentiment =\n\n\n\n\n\n\n\n\n\n+\nif f +\nf −> λ\n−\nif f −\nf + > λ\n0\notherwise.\n(19.12)\nIf supervised training data is available, these counts computed from sentiment lex-\nicons, sometimes weighted or normalized in various ways, can also be used as fea-\ntures in a classiﬁer along with other lexical or non-lexical features. We return to\nsuch algorithms in Section 19.8.\n19.7\nOther tasks: Personality\nMany other kinds of affective meaning can be extracted from text and speech. For\nexample detecting a person’s personality from their language can be useful for di-\npersonality\nalog systems (users tend to prefer agents that match their personality), and can play",
  "401": "19.8\n•\nAFFECT RECOGNITION\n393\na useful role in computational social science questions like understanding how per-\nsonality is related to other kinds of behavior.\nMany theories of human personality are based around a small number of dimen-\nsions, such as various versions of the “Big Five” dimensions (Digman, 1990):\nExtroversion vs. Introversion: sociable, assertive, playful vs. aloof, reserved,\nshy\nEmotional stability vs. Neuroticism: calm, unemotional vs. insecure, anxious\nAgreeableness vs. Disagreeableness: friendly, cooperative vs. antagonistic, fault-\nﬁnding\nConscientiousness vs. Unconscientiousness: self-disciplined, organized vs. in-\nefﬁcient, careless\nOpenness to experience: intellectual, insightful vs. shallow, unimaginative\nA few corpora of text and speech have been labeled for the personality of their\nauthor by having the authors take a standard personality test. The essay corpus of\nPennebaker and King (1999) consists of 2,479 essays (1.9 million words) from psy-\nchology students who were asked to “write whatever comes into your mind” for 20\nminutes. The EAR (Electronically Activated Recorder) corpus of Mehl et al. (2006)\nwas created by having volunteers wear a recorder throughout the day, which ran-\ndomly recorded short snippets of conversation throughout the day, which were then\ntranscribed. The Facebook corpus of (Schwartz et al., 2013) includes 309 million\nwords of Facebook posts from 75,000 volunteers.\nFor example, here are samples from Pennebaker and King (1999) from an essay\nwritten by someone on the neurotic end of the neurotic/emotionally stable scale,\nOne of my friends just barged in, and I jumped in my seat. This is crazy.\nI should tell him not to do that again. I’m not that fastidious actually.\nBut certain things annoy me. The things that would annoy me would\nactually annoy any normal human being, so I know I’m not a freak.\nand someone on the emotionally stable end of the scale:\nI should excel in this sport because I know how to push my body harder\nthan anyone I know, no matter what the test I always push my body\nharder than everyone else. I want to be the best no matter what the sport\nor event. I should also be good at this because I love to ride my bike.\nAnother kind of affective meaning is what Scherer (2000) calls interpersonal\nstance, the ‘affective stance taken toward another person in a speciﬁc interaction\ninterpersonal\nstance\ncoloring the interpersonal exchange’. Extracting this kind of meaning means au-\ntomatically labeling participants for whether they are friendly, supportive, distant.\nFor example Ranganath et al. (2013) studied a corpus of speed-dates, in which par-\nticipants went on a series of 4-minute romantic dates, wearing microphones. Each\nparticipant labeled each other for how ﬂirtatious, friendly, awkward, or assertive\nthey were. Ranganath et al. (2013) then used a combination of lexicons and other\nfeatures to detect these interpersonal stances from text.\n19.8\nAffect Recognition\nDetection of emotion, personality, interactional stance, and the other kinds of af-\nfective meaning described by Scherer (2000) can be done by generalizing the algo-\nrithms described above for detecting sentiment.",
  "402": "394\nCHAPTER 19\n•\nLEXICONS FOR SENTIMENT, AFFECT, AND CONNOTATION\nThe most common algorithms involve supervised classiﬁcation: a training set is\nlabeled for the affective meaning to be detected, and a classiﬁer is built using features\nextracted from the training set. As with sentiment analysis, if the training set is large\nenough, and the test set is sufﬁciently similar to the training set, simply using all\nthe words or all the bigrams as features in a powerful classiﬁer like SVM or logistic\nregression, as described in Fig. 4.2 in Chapter 4, is an excellent algorithm whose\nperformance is hard to beat. Thus we can treat affective meaning classiﬁcation of a\ntext sample as simple document classiﬁcation.\nSome modiﬁcations are nonetheless often necessary for very large datasets. For\nexample, the Schwartz et al. (2013) study of personality, gender, and age using 700\nmillion words of Facebook posts used only a subset of the n-grams of lengths 1-\n3. Only words and phrases used by at least 1% of the subjects were included as\nfeatures, and 2-grams and 3-grams were only kept if they had sufﬁciently high PMI\n(PMI greater than 2∗length, where length is the number of words):\npmi(phrase) = log\np(phrase)\nY\nw∈phrase\np(w)\n(19.13)\nVarious weights can be used for the features, including the raw count in the training\nset, or some normalized probability or log probability. Schwartz et al. (2013), for\nexample, turn feature counts into phrase likelihoods by normalizing them by each\nsubject’s total word use.\np(phrase|subject) =\nfreq(phrase,subject)\nX\nphrase′∈vocab(subject)\nfreq(phrase′,subject)\n(19.14)\nIf the training data is sparser, or not as similar to the test set, any of the lexicons\nwe’ve discussed can play a helpful role, either alone or in combination with all the\nwords and n-grams.\nMany possible values can be used for lexicon features. The simplest is just an\nindicator function, in which the value of a feature fL takes the value 1 if a particular\ntext has any word from the relevant lexicon L. Using the notation of Chapter 4, in\nwhich a feature value is deﬁned for a particular output class c and document x.\nfL(c,x) =\n\u001a\n1 if ∃w : w ∈L & w ∈x & class = c\n0 otherwise\nAlternatively the value of a feature fL for a particular lexicon L can be the total\nnumber of word tokens in the document that occur in L:\nfL =\nX\nw∈L\ncount(w)\nFor lexica in which each word is associated with a score or weight, the count can be\nmultiplied by a weight θ L\nw:\nfL =\nX\nw∈L\nθ L\nwcount(w)\nCounts can alternatively be logged or normalized per writer as in Eq. 19.14.",
  "403": "19.9\n•\nCONNOTATION FRAMES\n395\nHowever they are deﬁned, these lexicon features are then used in a supervised\nclassiﬁer to predict the desired affective category for the text or document. Once\na classiﬁer is trained, we can examine which lexicon features are associated with\nwhich classes. For a classiﬁer like logistic regression the feature weight gives an\nindication of how associated the feature is with the class.\nThus, for example, Mairesse and Walker (2008) found that for classifying per-\nsonality, for the dimension Agreeable, the LIWC lexicons Family and Home were\npositively associated while the LIWC lexicons anger and swear were negatively\nassociated. By contrast, Extroversion was positively associated with the Friend,\nReligion and Self lexicons, and Emotional Stability was positively associated with\nSports and negatively associated with Negative Emotion.\n(a)\n(b)\nFigure 19.12\nWord clouds from Schwartz et al. (2013), showing words highly associated\nwith introversion (left) or extroversion (right). The size of the word represents the association\nstrength (the regression coefﬁcient), while the color (ranging from cold to hot) represents the\nrelative frequency of the word/phrase (from low to high).\nIn the situation in which we use all the words and phrases in the document as\npotential features, we can use the resulting weights from the learned regression clas-\nsiﬁer as the basis of an affective lexicon. In the Extroversion/Introversion classiﬁer\nof Schwartz et al. (2013), ordinary least-squares regression is used to predict the\nvalue of a personality dimension from all the words and phrases. The resulting re-\ngression coefﬁcient for each word or phrase can be used as an association value with\nthe predicted dimension. The word clouds in Fig. 19.12 show an example of words\nassociated with introversion (a) and extroversion (b).\n19.9\nConnotation Frames\nThe lexicons we’ve described so far deﬁne a word as a point in affective space. A\nconnotation frame, by contrast, is lexicon that incorporates a richer kind of gram-\nconnotation\nframe\nmatical structure, by combining affective lexicons with the frame semantic lexicons\nof Chapter 18. The basic insight of connotation frame lexicons is that a predicate\nlike a verb expresses connotations about the verb’s arguments (Rashkin et al. 2016,\nRashkin et al. 2017).\nConsider sentences like:\n(19.15) Country A violated the sovereignty of Country B\n(19.16) the teenager ... survived the Boston Marathon bombing”",
  "404": "396\nCHAPTER 19\n•\nLEXICONS FOR SENTIMENT, AFFECT, AND CONNOTATION\nBy using the verb violate in (19.15), the author is expressing their sympathies with\nCountry B, portraying Country B as a victim, and expressing antagonism toward\nthe agent Country A. By contrast, in using the verb survive, the author of (19.16) is\nexpressing that the bombing is a negative experience, and the subject of the sentence\nthe teenager, is a sympathetic character. These aspects of connotation are inherent\nin the meaning of the verbs violate and survive, as shown in Fig. 19.13.\nWriter\nRole1\nRole2\nRole1 is a\nsympathetic \nvictim\nThere is\nsome type\nof hardship\nReader\n+\n_\n+\n_\n_\nS(writer→role1)\nS(writer→role2)\nConnotation Frame for “Role1 survives Role2” \nS(role1→role2)\nWriter\nRole1\nRole2\nRole1 is the\n antagonist\nRole2 is a\nsympathetic\n victim\nReader\n+\n_\n+\n_\n_\nS(writer→role1)\nS(writer→role2)\nConnotation Frame for “Role1 violates Role2” \nS(role1→role2)\n(a)\n(b)\nFigure 19.13\nConnotation frames for survive and violate. (a) For survive, the writer and reader have positive\nsentiment toward Role1, the subject, and negative sentiment toward Role2, the direct object. (b) For violate, the\nwriter and reader have positive sentiment instead toward Role2, the direct object.\nThe connotation frame lexicons of Rashkin et al. (2016) and Rashkin et al.\n(2017) also express other connotative aspects of the predicate toward each argument,\nincluding the effect (something bad happened to x) value: (x is valuable), and mental\nstate: (x is distressed by the event). Connotation frames can also mark aspects of\npower and agency; see Chapter 18 (Sap et al., 2017).\nConnotation frames can be built by hand (Sap et al., 2017), or they can be learned\nby supervised learning (Rashkin et al., 2016), for example using hand-labeled train-\ning data to supervise classiﬁers for each of the individual relations, e.g., whether\nS(writer →Role1) is + or -, and then improving accuracy via global constraints\nacross all relations.\n19.10\nSummary\n• Many kinds of affective states can be distinguished, including emotions, moods,\nattitudes (which include sentiment), interpersonal stance, and personality.\n• Emotion can be represented by ﬁxed atomic units often called basic emo-\ntions, or as points in space deﬁned by dimensions like valence and arousal.\n• Words have connotational aspects related to these affective states, and this\nconnotational aspect of word meaning can be represented in lexicons.\n• Affective lexicons can be built by hand, using crowd sourcing to label the\naffective content of each word.\n• Lexicons can be built with semi-supervised, bootstrapping from seed words\nusing similarity metrics like embedding cosine.\n• Lexicons can be learned in a fully supervised manner, when a convenient\ntraining signal can be found in the world, such as ratings assigned by users on\na review site.",
  "405": "BIBLIOGRAPHICAL AND HISTORICAL NOTES\n397\n• Words can be assigned weights in a lexicon by using various functions of word\ncounts in training texts, and ratio metrics like log odds ratio informative\nDirichlet prior.\n• Personality is often represented as a point in 5-dimensional space.\n• Affect can be detected, just like sentiment, by using standard supervised text\nclassiﬁcation techniques, using all the words or bigrams in a text as features.\nAdditional features can be drawn from counts of words in lexicons.\n• Lexicons can also be used to detect affect in a rule-based classiﬁer by picking\nthe simple majority sentiment based on counts of words in each lexicon.\n• Connotation frames express richer relations of affective meaning that a pred-\nicate encodes about its arguments.\nBibliographical and Historical Notes\nThe idea of formally representing the subjective meaning of words began with Os-\ngood et al. (1957), the same pioneering study that ﬁrst proposed the vector space\nmodel of meaning described in Chapter 6. Osgood et al. (1957) had participants rate\nwords on various scales, and ran factor analysis on the ratings. The most signiﬁcant\nfactor they uncovered was the evaluative dimension, which distinguished between\npairs like good/bad, valuable/worthless, pleasant/unpleasant. This work inﬂuenced\nthe development of early dictionaries of sentiment and affective meaning in the ﬁeld\nof content analysis (Stone et al., 1966).\nWiebe (1994) began an inﬂuential line of work on detecting subjectivity in text,\nsubjectivity\nbeginning with the task of identifying subjective sentences and the subjective char-\nacters who are described in the text as holding private states, beliefs or attitudes.\nLearned sentiment lexicons such as the polarity lexicons of (Hatzivassiloglou and\nMcKeown, 1997) were shown to be a useful feature in subjectivity detection (Hatzi-\nvassiloglou and Wiebe 2000, Wiebe 2000).\nThe term sentiment seems to have been introduced in 2001 by Das and Chen\n(2001), to describe the task of measuring market sentiment by looking at the words in\nstock trading message boards. In the same paper Das and Chen (2001) also proposed\nthe use of a sentiment lexicon. The list of words in the lexicon was created by\nhand, but each word was assigned weights according to how much it discriminated\na particular class (say buy versus sell) by maximizing across-class variation and\nminimizing within-class variation. The term sentiment, and the use of lexicons,\ncaught on quite quickly (e.g., inter alia, Turney 2002). Pang et al. (2002) ﬁrst showed\nthe power of using all the words without a sentiment lexicon; see also Wang and\nManning (2012).\nMost of the semi-supervised methods we describe for extending sentiment dic-\ntionaries drew on the early idea that synonyms and antonyms tend to co-occur in the\nsame sentence. (Miller and Charles 1991, Justeson and Katz 1991, Riloff and Shep-\nherd 1997). Other semi-supervized methods for learning cues to affective mean-\ning rely on information extraction techniques, like the AutoSlog pattern extractors\n(Riloff and Wiebe, 2003). Graph based algorithms for sentiment were ﬁrst sug-\ngested by Hatzivassiloglou and McKeown (1997), and graph propagation became\na standard method (Zhu and Ghahramani 2002, Zhu et al. 2003, Zhou et al. 2004,\nVelikovich et al. 2010). Crowdsourcing can also be used to improve precision by",
  "406": "398\nCHAPTER 19\n•\nLEXICONS FOR SENTIMENT, AFFECT, AND CONNOTATION\nﬁltering the result of semi-supervised lexicon learning (Riloff and Shepherd 1997,\nFast et al. 2016).\nMuch recent work focuses on ways to learn embeddings that directly encode\nsentiment or other properties, such as the DENSIFIER algorithm of (Rothe et al.,\n2016) that learns to transform the embedding space to focus on sentiment (or other)\ninformation.",
  "407": "CHAPTER\n20\nCoreference\nResolution\nand\nEntity Linking\nPlaceholder\n399",
  "408": "CHAPTER\n21\nDiscourse Coherence\nPlaceholder\n400",
  "409": "CHAPTER\n22\nMachine Translation\nPlaceholder\n401",
  "410": "402\nCHAPTER 23\n•\nQUESTION ANSWERING\nCHAPTER\n23\nQuestion Answering\nThe quest for knowledge is deeply human, and so it is not surprising that practi-\ncally as soon as there were computers we were asking them questions. By the early\n1960s, systems used the two major paradigms of question answering—information-\nretrieval-based and knowledge-based—to answer questions about baseball statis-\ntics or scientiﬁc facts. Even imaginary computers got into the act. Deep Thought,\nthe computer that Douglas Adams invented in The Hitchhiker’s Guide to the Galaxy,\nmanaged to answer “the Great Question Of Life The Universe and Everything”.1 In\n2011, IBM’s Watson question-answering system won the TV game-show Jeopardy!\nusing a hybrid architecture that surpassed humans at answering questions like\nWILLIAM WILKINSON’S “AN ACCOUNT OF THE PRINCIPAL-\nITIES OF WALLACHIA AND MOLDOVIA” INSPIRED THIS AU-\nTHOR’S MOST FAMOUS NOVEL2\nMost question answering systems focus on factoid questions, questions that can\nbe answered with simple facts expressed in short texts. The answers to the questions\nbelow can expressed by a personal name, temporal expression, or location:\n(23.1) Who founded Virgin Airlines?\n(23.2) What is the average age of the onset of autism?\n(23.3) Where is Apple Computer based?\nIn this chapter we describe the two major paradigms for factoid question an-\nswering. Information-retrieval or IR-based question answering relies on the vast\nquantities of textual information on the web or in collections like PubMed. Given\na user question, information retrieval techniques ﬁrst ﬁnd relevant documents and\npassages. Then systems (feature-based, neural, or both) use reading comprehen-\nsion algorithms to read these retrieved documents or passages and draw an answer\ndirectly from spans of text.\nIn the second paradigm, knowledge-based question answering, a system in-\nstead builds a semantic representation of the query, mapping What states border\nTexas? to the logical representation: λx.state(x) ∧borders(x,texas), or When was\nAda Lovelace born? to the gapped relation: birth-year (Ada Lovelace, ?x).\nThese meaning representations are then used to query databases of facts.\nFinally, large industrial systems like the DeepQA system in IBM’s Watson are\noften hybrids, using both text datasets and structured knowledge bases to answer\nquestions. DeepQA ﬁnds many candidate answers in both knowledge bases and in\ntextual sources, and then scores each candidate answer using knowledge sources like\ngeospatial databases, taxonomical classiﬁcation, or other textual sources.\nWe describe IR-based approaches (including neural reading comprehension sys-\ntems) in the next section, followed by sections on knowledge-based systems, on\nWatson Deep QA, and a discussion of evaluation.\n1\nThe answer was 42, but unfortunately the details of the question were never revealed\n2\nThe answer, of course, is Bram Stoker, and the novel was the fantastically Gothic Dracula.",
  "411": "23.1\n•\nIR-BASED FACTOID QUESTION ANSWERING\n403\n23.1\nIR-based Factoid Question Answering\nThe goal of information retrieval based question answering is to answer a user’s\nquestion by ﬁnding short text segments on the web or some other collection of doc-\numents. Figure 23.1 shows some sample factoid questions and their answers.\nQuestion\nAnswer\nWhere is the Louvre Museum located?\nin Paris, France\nWhat’s the abbreviation for limited partnership?\nL.P.\nWhat are the names of Odin’s ravens?\nHuginn and Muninn\nWhat currency is used in China?\nthe yuan\nWhat kind of nuts are used in marzipan?\nalmonds\nWhat instrument does Max Roach play?\ndrums\nWhat’s the ofﬁcial language of Algeria?\nArabic\nHow many pounds are there in a stone?\n14\nFigure 23.1\nSome sample factoid questions and their answers.\nFigure 23.2 shows the three phases of an IR-based factoid question-answering\nsystem: question processing, passage retrieval and ranking, and answer extraction.\nDocument\nDocument\nDocument\nDocume\nnt\nDocume\nnt\nDocume\nnt\nDocume\nnt\nDocume\nnt\nQuestion \nProcessing\nDocument and Passage\nRetrieval\nQuery \nFormulation\nAnswer Type \nDetection\nQuestion\nPassage \nRetrieval\nDocument \nRetrieval\nAnswer \nExtraction\nAnswer\npassages\nIndexing\nRelevant\nDocs\nDocument\nDocument\nDocument\nFigure 23.2\nIR-based factoid question answering has three stages: question processing, passage retrieval, and\nanswer processing.\n23.1.1\nQuestion Processing\nThe main goal of the question-processing phase is to extract the query: the keywords\npassed to the IR system to match potential documents. Some systems additionally\nextract further information such as:\n• answer type: the entity type (person, location, time, etc.). of the answer\n• focus: the string of words in the question that are likely to be replaced by the\nanswer in any answer string found.\n• question type: is this a deﬁnition question, a math question, a list question?\nFor example, for the question Which US state capital has the largest population?\nthe query processing might produce:\nquery: “US state capital has the largest population”\nanswer type: city\nfocus: state capital\nIn the next two sections we summarize the two most commonly used tasks, query\nformulation and answer type detection.",
  "412": "404\nCHAPTER 23\n•\nQUESTION ANSWERING\n23.1.2\nQuery Formulation\nQuery formulation is the task of creating a query—a list of tokens— to send to an\ninformation retrieval system to retrieve documents that might contain answer strings.\nFor question answering from the web, we can simply pass the entire question\nto the web search engine, at most perhaps leaving out the question word (where,\nwhen, etc.). For question answering from smaller sets of documents like corporate\ninformation pages or Wikipedia, we still use an IR engine to index and search our\ndocuments, generally using standard tf-idf cosine matching, but we might need to do\nmore processing. For example, for searching Wikipedia, it helps to compute tf-idf\nover bigrams rather than unigrams in the query and document (Chen et al., 2017).\nOr we might need to do query expansion, since while on the web the answer to a\nquestion might appear in many different forms, one of which will probably match\nther question, in smaller document sets an answer might appear only once. Query\nexpansion methods can add query terms in hopes of matching the particular form of\nthe answer as it appears, like adding morphological variants of the content words in\nthe question, or synonyms from a thesaurus.\nA query formulation approach that is sometimes used for questioning the web is\nto apply query reformulation rules to the query. The rules rephrase the question to\nquery\nreformulation\nmake it look like a substring of possible declarative answers. The question “when\nwas the laser invented?” might be reformulated as “the laser was invented”; the\nquestion “where is the Valley of the Kings?” as “the Valley of the Kings is located\nin”. Here are some sample hand-written reformulation rules from Lin (2007):\n(23.4) wh-word did A verb B →...A verb+ed B\n(23.5) Where is A →A is located in\n23.1.3\nAnswer Types\nSome systems make use of question classiﬁcation, the task of ﬁnding the answer\nquestion\nclassiﬁcation\ntype, the named-entity categorizing the answer. A question like “Who founded Vir-\nanswer type\ngin Airlines?” expects an answer of type PERSON. A question like “What Canadian\ncity has the largest population?” expects an answer of type CITY. If we know that\nthe answer type for a question is a person, we can avoid examining every sentence\nin the document collection, instead focusing on sentences mentioning people.\nWhile answer types might just be the named entities like PERSON, LOCATION,\nand ORGANIZATION described in Chapter 17, we can also use a larger hierarchical\nset of answer types called an answer type taxonomy. Such taxonomies can be built\nanswer type\ntaxonomy\nautomatically, from resources like WordNet (Harabagiu et al. 2000, Pasca 2003), or\nthey can be designed by hand. Figure 23.4 shows one such hand-built ontology, the\nLi and Roth (2005) tagset; a subset is also shown in Fig. 23.3. In this hierarchical\ntagset, each question can be labeled with a coarse-grained tag like HUMAN or a ﬁne-\ngrained tag like HUMAN:DESCRIPTION, HUMAN:GROUP, HUMAN:IND, and so on.\nThe HUMAN:DESCRIPTION type is often called a BIOGRAPHY question because the\nanswer is required to give a brief biography of the person rather than just a name.\nQuestion classiﬁers can be built by hand-writing rules like the following rule\nfrom (Hovy et al., 2002) for detecting the answer type BIOGRAPHY:\n(23.6) who {is | was | are | were} PERSON\nMost question classiﬁers, however, are based on supervised learning, trained on\ndatabases of questions that have been hand-labeled with an answer type (Li and\nRoth, 2002). Either feature-based or neural methods can be used. Feature based",
  "413": "23.1\n•\nIR-BASED FACTOID QUESTION ANSWERING\n405\nNUMERIC\nABBREVIATION\nENTITY\nDESCRIPTION\nLOCATION\nHUMAN\nLi & Roth\nTaxonomy\ncountry\ncity\nstate\nreason\ndefinition\nfood\ncurrency\nanimal\ndate\ndistance\npercent\nsize\nmoney\nindividual\ntitle\ngroup\nexpression\nabbreviation\nFigure 23.3\nA subset of the Li and Roth (2005) answer types.\nmethods rely on words in the questions and their embeddings, the part-of-speech of\neach word, and named entities in the questions. Often, a single word in the question\ngives extra information about the answer type, and its identity is used as a feature.\nThis word is sometimes called the answer type word or question headword, and\nmay be deﬁned as the headword of the ﬁrst NP after the question’s wh-word; head-\nwords are indicated in boldface in the following examples:\n(23.7) Which city in China has the largest number of foreign ﬁnancial companies?\n(23.8) What is the state ﬂower of California?\nIn general, question classiﬁcation accuracies are relatively high on easy ques-\ntion types like PERSON, LOCATION, and TIME questions; detecting REASON and\nDESCRIPTION questions can be much harder.\n23.1.4\nDocument and Passage Retrieval\nThe IR query produced from the question processing stage is sent to an IR engine,\nresulting in a set of documents ranked by their relevance to the query. Because\nmost answer-extraction methods are designed to apply to smaller regions such as\nparagraphs, QA systems next divide the top n documents into smaller passages such\npassages\nas sections, paragraphs, or sentences. These might be already segmented in the\nsource document or we might need to run a paragraph segmentation algorithm.\nThe simplest form of passage retrieval is then to simply pass along every pas-\npassage\nretrieval\nsages to the answer extraction stage. A more sophisticated variant is to ﬁlter the\npassages by running a named entity or answer type classiﬁcation on the retrieved\npassages. Passages that don’t contain the answer type that was assigned to the ques-\ntion are discarded.\nIt’s also possible to use supervised learning to fully rank the remaining passages,\nusing features like:\n• The number of named entities of the right type in the passage\n• The number of question keywords in the passage\n• The longest exact sequence of question keywords that occurs in the passage\n• The rank of the document from which the passage was extracted\n• The proximity of the keywords from the original query to each other (Pasca 2003,\nMonz 2004).\n• The number of n-grams that overlap between the passage and the question\n(Brill et al., 2002).",
  "414": "406\nCHAPTER 23\n•\nQUESTION ANSWERING\nTag\nExample\nABBREVIATION\nabb\nWhat’s the abbreviation for limited partnership?\nexp\nWhat does the “c” stand for in the equation E=mc2?\nDESCRIPTION\ndeﬁnition\nWhat are tannins?\ndescription\nWhat are the words to the Canadian National anthem?\nmanner\nHow can you get rust stains out of clothing?\nreason\nWhat caused the Titanic to sink?\nENTITY\nanimal\nWhat are the names of Odin’s ravens?\nbody\nWhat part of your body contains the corpus callosum?\ncolor\nWhat colors make up a rainbow?\ncreative\nIn what book can I ﬁnd the story of Aladdin?\ncurrency\nWhat currency is used in China?\ndisease/medicine\nWhat does Salk vaccine prevent?\nevent\nWhat war involved the battle of Chapultepec?\nfood\nWhat kind of nuts are used in marzipan?\ninstrument\nWhat instrument does Max Roach play?\nlang\nWhat’s the ofﬁcial language of Algeria?\nletter\nWhat letter appears on the cold-water tap in Spain?\nother\nWhat is the name of King Arthur’s sword?\nplant\nWhat are some fragrant white climbing roses?\nproduct\nWhat is the fastest computer?\nreligion\nWhat religion has the most members?\nsport\nWhat was the name of the ball game played by the Mayans?\nsubstance\nWhat fuel do airplanes use?\nsymbol\nWhat is the chemical symbol for nitrogen?\ntechnique\nWhat is the best way to remove wallpaper?\nterm\nHow do you say “ Grandma” in Irish?\nvehicle\nWhat was the name of Captain Bligh’s ship?\nword\nWhat’s the singular of dice?\nHUMAN\ndescription\nWho was Confucius?\ngroup\nWhat are the major companies that are part of Dow Jones?\nind\nWho was the ﬁrst Russian astronaut to do a spacewalk?\ntitle\nWhat was Queen Victoria’s title regarding India?\nLOCATION\ncity\nWhat’s the oldest capital city in the Americas?\ncountry\nWhat country borders the most others?\nmountain\nWhat is the highest peak in Africa?\nother\nWhat river runs through Liverpool?\nstate\nWhat states do not have state income tax?\nNUMERIC\ncode\nWhat is the telephone number for the University of Colorado?\ncount\nAbout how many soldiers died in World War II?\ndate\nWhat is the date of Boxing Day?\ndistance\nHow long was Mao’s 1930s Long March?\nmoney\nHow much did a McDonald’s hamburger cost in 1963?\norder\nWhere does Shanghai rank among world cities in population?\nother\nWhat is the population of Mexico?\nperiod\nWhat was the average life expectancy during the Stone Age?\npercent\nWhat fraction of a beaver’s life is spent swimming?\ntemp\nHow hot should the oven be when making Peachy Oat Mufﬁns?\nspeed\nHow fast must a spacecraft travel to escape Earth’s gravity?\nsize\nWhat is the size of Argentina?\nweight\nHow many pounds are there in a stone?\nFigure 23.4\nQuestion typology from Li and Roth (2002), (2005). Example sentences are\nfrom their corpus of 5500 labeled questions. A question can be labeled either with a coarse-\ngrained tag like HUMAN or NUMERIC or with a ﬁne-grained tag like HUMAN:DESCRIPTION,\nHUMAN:GROUP, HUMAN:IND, and so on.",
  "415": "23.1\n•\nIR-BASED FACTOID QUESTION ANSWERING\n407\nFor question answering from the web we can instead take snippets from the Web\nsnippets\nsearch engine (see Fig. 23.5) as the passages.\nFigure 23.5\nFive snippets from Google in response to the query When was movable type\nmetal printing invented in Korea?\n23.1.5\nAnswer Extraction\nThe ﬁnal stage of question answering is to extract a speciﬁc answer from the passage,\nfor example responding 29,029 feet to a question like “How tall is Mt. Everest?”.\nThis task is commonly modeled by span labeling: given a passage, identifying the\nspan of text which constitutes an answer.\nspan\nA simple baseline algorithm for answer extraction is to run a named entity tagger\non the candidate passage and return whatever span in the passage is the correct an-\nswer type. Thus, in the following examples, the underlined named entities would be\nextracted from the passages as the answer to the HUMAN and DISTANCE-QUANTITY\nquestions:\n“Who is the prime minister of India?”\nManmohan Singh, Prime Minister of India, had told left leaders that the\ndeal would not be renegotiated.\n“How tall is Mt. Everest?”\nThe ofﬁcial height of Mount Everest is 29029 feet",
  "416": "408\nCHAPTER 23\n•\nQUESTION ANSWERING\nUnfortunately, the answers to many questions, such as DEFINITION questions,\ndon’t tend to be of a particular named entity type. For this reason modern work on\nanswer extraction uses more sophisticated algorithms, generally based on supervised\nlearning. The next section introduces a simple feature-based classiﬁer, after which\nwe turn to modern neural algorithms.\n23.1.6\nFeature-based Answer Extraction\nSupervised learning approaches to answer extraction train classiﬁers to decide if a\nspan or a sentence contains an answer. One obviously useful feature is the answer\ntype feature of the above baseline algorithm. Hand-written regular expression pat-\nterns also play a role, such as the sample patterns for deﬁnition questions in Fig. 23.6.\nPattern\nQuestion\nAnswer\n<AP> such as <QP>\nWhat is autism?\n“, developmental disorders such as autism”\n<QP>, a <AP>\nWhat is a caldera? “the Long Valley caldera, a volcanic crater 19\nmiles long”\nFigure 23.6\nSome answer-extraction patterns using the answer phrase (AP) and question\nphrase (QP) for deﬁnition questions (Pasca, 2003).\nOther features in such classiﬁers include:\nAnswer type match: True if the candidate answer contains a phrase with the cor-\nrect answer type.\nPattern match: The identity of a pattern that matches the candidate answer.\nNumber of matched question keywords: How many question keywords are con-\ntained in the candidate answer.\nKeyword distance: The distance between the candidate answer and query key-\nwords\nNovelty factor: True if at least one word in the candidate answer is novel, that is,\nnot in the query.\nApposition features: True if the candidate answer is an appositive to a phrase con-\ntaining many question terms. Can be approximated by the number of question\nterms separated from the candidate answer through at most three words and\none comma (Pasca, 2003).\nPunctuation location: True if the candidate answer is immediately followed by a\ncomma, period, quotation marks, semicolon, or exclamation mark.\nSequences of question terms: The length of the longest sequence of question\nterms that occurs in the candidate answer.\n23.1.7\nN-gram tiling answer extraction\nAn alternative approach to answer extraction, used solely in Web search, is based\non n-gram tiling, an approach that relies on the redundancy of the web (Brill\nn-gram tiling\net al. 2002, Lin 2007). This simpliﬁed method begins with the snippets returned\nfrom the Web search engine, produced by a reformulated query. In the ﬁrst step,\nn-gram mining, every unigram, bigram, and trigram occurring in the snippet is ex-\ntracted and weighted. The weight is a function of the number of snippets in which\nthe n-gram occurred, and the weight of the query reformulation pattern that returned\nit. In the n-gram ﬁltering step, n-grams are scored by how well they match the\npredicted answer type. These scores are computed by hand-written ﬁlters built for",
  "417": "23.1\n•\nIR-BASED FACTOID QUESTION ANSWERING\n409\neach answer type. Finally, an n-gram tiling algorithm concatenates overlapping n-\ngram fragments into longer answers. A standard greedy method is to start with the\nhighest-scoring candidate and try to tile each other candidate with this candidate.\nThe best-scoring concatenation is added to the set of candidates, the lower-scoring\ncandidate is removed, and the process continues until a single answer is built.\n23.1.8\nNeural Answer Extraction\nNeural network approaches to answer extraction draw on the intuition that a question\nand its answer are semantically similar in some appropriate way. As we’ll see, this\nintuition can be ﬂeshed out by computing an embedding for the question and an\nembedding for each token of the passage, and then selecting passage spans whose\nembeddings are closest to the question embedding.\nReading Comprehension Datasets.\nBecause neural answer extractors are often\ndesigned in the context of the reading comprehension task, let’s begin by talking\nreading\ncomprehension\nabout that task. It was Hirschman et al. (1999) who ﬁrst proposed to take children’s\nreading comprehension tests—pedagogical instruments in which a child is given\na passage to read and must answer questions about it—and use them to evaluate\nmachine text comprehension algorithm. They acquired a corpus of 120 passages\nwith 5 questions each designed for 3rd-6th grade children, built an answer extraction\nsystem, and measured how well the answers given by their system corresponded to\nthe answer key from the test’s publisher.\nModern reading comprehension systems tend to use collections of questions that\nare designed speciﬁcally for NLP, and so are large enough for training supervised\nlearning systems. For example the Stanford Question Answering Dataset (SQuAD)\nSQuAD\nconsists of passages from Wikipedia and associated questions whose answers are\nspans from the passage, as well as some questions that are designed to be unan-\nswerable (Rajpurkar et al. 2016, Rajpurkar et al. 2018); a total of just over 150,000\nquestions. Fig. 23.7 shows a (shortened) excerpt from a SQUAD 2.0 passage to-\ngether with three questions and their answer spans.\nBeyonc´e Giselle Knowles-Carter (born September 4, 1981) is an American singer, songwriter,\nrecord producer and actress.\nBorn and raised in Houston, Texas, she performed in various\nsinging and dancing competitions as a child, and rose to fame in the late 1990s as lead singer\nof R&B girl-group Destiny’s Child. Managed by her father, Mathew Knowles, the group became\none of the world’s best-selling girl groups of all time. Their hiatus saw the release of Beyonc´e’s\ndebut album, Dangerously in Love (2003), which established her as a solo artist worldwide, earned\nﬁve Grammy Awards and featured the Billboard Hot 100 number-one singles “Crazy in Love” and\n“Baby Boy”.\nQ: “In what city and state did Beyonc´e grow up?”\nA: “Houston, Texas”\nQ: “What areas did Beyonc´e compete in when she was growing up?”\nA: “singing and dancing”\nQ: “When did Beyonc´e release Dangerously in Love?”\nA: “2003”\nFigure 23.7\nA (Wikipedia) passage from the SQuAD 2.0 dataset (Rajpurkar et al., 2018) with 3 sample\nquestions and the labeled answer spans.\nSQuAD was build by having humans write questions for a given Wikipedia\npassage and choose the answer span. Other datasets used similar techniques; the",
  "418": "410\nCHAPTER 23\n•\nQUESTION ANSWERING\nNewsQA dataset consists of 100,000 question-answer pairs from CNN news arti-\ncles, For other datasets like WikiQA the span is the entire sentence containing the\nanswer (Yang et al., 2015); the task of choosing a sentence rather than a smaller\nanswer span is sometimes called the sentence selection task.\nsentence\nselection\nThese reading comprehension datasets are used both as a reading comprehension\ntask in themselves, and as a training set and evaluation set for the sentence extraction\ncomponent of open question answering algorithms.\nBasic Reading Comprehension Algorithm.\nNeural algorithms for reading com-\nprehension are given a question q of l tokens q1,...,ql¡ and a passage p of m tokens\np1,..., pm. Their goal is to compute, for each token pi the probability pstart(i) that\npi is the start of the answer span, and the probability pend(i), that pi is the end of\nthe answer span.\nFig. 23.8 shows the architecture of the Document Reader component of the\nDrQA system of Chen et al. (2017). Like most such systems, DrQA builds an\nembedding for the question, builds an embedding for each token in the passage,\ncomputes a similarity function between the question and each passage word in con-\ntext, and then uses the question-passage similarity scores to decide where the answer\nspan starts and ends.\nBeyonce’s\ndebut\nalbum\nLSTM1\nLSTM1\nLSTM1\nLSTM2\nLSTM2\nLSTM2\nGloVe\nPER\nNNP\nWhen\ndid\nBeyonce\nPassage\nQuestion\nLSTM1\nLSTM1\nLSTM1\nLSTM2\nLSTM2\nLSTM2\nGloVe\nGloVe\nGloVe\n…\nAttention\nWeighted sum\nsimilarity\nq\np2\np3\nsimilarity\nq\nq\nsimilarity\n…\nq-align1\nGloVe\nGloVe\npstart(1)\npend(1)\npstart(3)\npend(3)\n…\n…\n…\nO\nNN\nGloVe\nGloVe\nq-align2\n1\n0\nO\nNN\n0\nq-align3\nGloVe\nGloVe\nAtt\nAtt\np1\np1 p2 p3\n~p1 p2 p3\n~\n~\nq1 q2 q3\nFigure 23.8\nThe question answering system of Chen et al. (2017), considering part of the question When did\nBeyonc´e release Dangerously in Love? and the passage starting Beyonc´e’s debut album, Dangerously in Love\n(2003).\nLet’s consider the algorithm in detail, following closely the description in Chen\net al. (2017). The question is represented by a single embedding q, which is a\nweighted sum of representations for each question word qi.\nIt is computed by\npassing the series of embeddings PE(q1),...,E(ql) of question words through an\nRNN (such as a bi-LSTM shown in Fig. 23.8). The resulting hidden representations\n{q1,...,ql} are combined by a weighted sum\nq =\nX\nj\nb jq j\n(23.9)",
  "419": "23.2\n•\nKNOWLEDGE-BASED QUESTION ANSWERING\n411\nThe weight bj is a measure of the relevance of each question word, and relies on a\nlearned weight vector w:\nbj =\nexp(w ·q j)\nP\nj′ exp(w ·q′\nj)\n(23.10)\nTo compute the passage embedding {p1,...,pm} we ﬁrst form an input represen-\ntation ˜p = {˜p1,...,˜pm} by concatenating four components:\n• An embedding for each word E(pi) such as from GLoVE (Pennington et al.,\n2014).\n• Token features like the part of speech of pi, or the named entity tag of pi, from\nrunning POS or NER taggers.\n• Exact match features representing whether the passage word pi occurred in\nthe question: 1(pi ∈q). Separate exact match features might be used for\nlemmatized or lower-cased versions of the tokens.\n• Aligned question embedding: In addition to the exact match features, many\nQA systems use an attention mechanism to give a more sophisticated model of\nsimilarity between the passage and question words, such as similar but non-\nidentical words like release and singles. For example a weighted similarity\nP\nj ai,jE(q j) can be used, where the attention weight ai,j encodes the simi-\nlarity between pi and each question word qj. This attention weight can be\ncomputed as the dot product between functions α of the word embeddings of\nthe question and passage:\nqi,j =\nexp(α(E(pi))·α(E(qj)))\nP\nj′ exp(α(E(pi))·α(E(q′\nj)))\n(23.11)\nα(·) can be a simple feed forward network.\nWe then pass ˜p through a biLSTM:\n{p1,...,pm}) = RNN({˜p1,...,˜pm})\n(23.12)\nThe result of the previous two step is a single question embedding q and a rep-\nresentations for each word in the passage {p1,...,pm}. In order to ﬁnd the answer\nspan, we can train two separate classiﬁers, one to compute for each pi the probability\npstart(i) that pi is the start of the answer span, and one to compute the probability\npend(i). While the classiﬁers could just take the dot product between the passage\nand question embeddings as input, it turns out to work better to learn a more sophis-\nticated similarity function, like a bilinear attention layer W:\npstart(i) ∝exp(piWsq)\npend(i) ∝exp(piWeq)\n(23.13)\nThese neural answer extractors can be trained end-to-end by using datasets like\nSQuAD.\n23.2\nKnowledge-based Question Answering\nWhile an enormous amount of information is encoded in the vast amount of text\non the web, information obviously also exists in more structured forms. We use",
  "420": "412\nCHAPTER 23\n•\nQUESTION ANSWERING\nthe term knowledge-based question answering for the idea of answering a natural\nlanguage question by mapping it to a query over a structured database. Like the text-\nbased paradigm for question answering, this approach dates back to the earliest days\nof natural language processing, with systems like BASEBALL (Green et al., 1961)\nthat answered questions from a structured database of baseball games and stats.\nSystems for mapping from a text string to any logical form are called seman-\ntic parsers. Semantic parsers for question answering usually map either to some\nversion of predicate calculus or a query language like SQL or SPARQL, as in the\nexamples in Fig. 23.9.\nQuestion\nLogical form\nWhen was Ada Lovelace born?\nbirth-year (Ada Lovelace, ?x)\nWhat states border Texas?\nλ x.state(x) ∧borders(x,texas)\nWhat is the largest state\nargmax(λx.state(x),λx.size(x))\nHow many people survived the sinking of\nthe Titanic\n(count (!fb:event.disaster.survivors\nfb:en.sinking of the titanic))\nFigure 23.9\nSample logical forms produced by a semantic parser for question answering. These range from\nsimple relations like birth-year, or relations normalized to databases like Freebase, to full predicate calculus.\nThe logical form of the question is thus either in the form of a query or can easily\nbe converted into one. The database can be a full relational database, or simpler\nstructured databases like sets of RDF triples. Recall from Chapter 17 that an RDF\ntriple is a 3-tuple, a predicate with two arguments, expressing some simple relation\nor proposition. Popular ontologies like Freebase (Bollacker et al., 2008) or DBpedia\n(Bizer et al., 2009) have large numbers of triples derived from Wikipedia infoboxes,\nthe structured tables associated with certain Wikipedia articles.\nThe simplest formation of the knowledge-based question answering task is to\nanswer factoid questions that ask about one of the missing arguments in a triple.\nConsider an RDF triple like the following:\nsubject\npredicate object\nAda Lovelace birth-year 1815\nThis triple can be used to answer text questions like ‘When was Ada Lovelace\nborn?’ or ‘Who was born in 1815?’. Question answering in this paradigm requires\nmapping from textual strings like ”When was ... born” to canonical relations in the\nknowledge base like birth-year. We might sketch this task as:\n“When was Ada Lovelace born?”\n→\nbirth-year (Ada Lovelace, ?x)\n“What is the capital of England?”\n→\ncapital-city(?x, England)\n23.2.1\nRule-based Methods\nFor relations that are very frequent, it may be worthwhile to write hand-written rules\nto extract relations from the question, just as we saw in Section 17.2. For example,\nto extract the birth-year relation, we could write patterns that search for the question\nword When, a main verb like born, and that extract the named entity argument of the\nverb.\n23.2.2\nSupervised Methods\nIn some cases we have supervised data, consisting of a set of questions paired with\ntheir correct logical form like the examples in Fig. 23.9. The task is then to take",
  "421": "23.2\n•\nKNOWLEDGE-BASED QUESTION ANSWERING\n413\nthose pairs of training tuples and produce a system that maps from new questions to\ntheir logical forms.\nMost supervised algorithms for learning to answer these simple questions about\nrelations ﬁrst parse the questions and then align the parse trees to the logical form.\nGenerally these systems bootstrap by having a small set of rules for building this\nmapping, and an initial lexicon as well. For example, a system might have built-\nin strings for each of the entities in the system (Texas, Ada Lovelace), and then\nhave simple default rules mapping fragments of the question parse tree to particular\nrelations:\nWho V ENTITY →relation( ?x, entity)\nnsubj\ndobj\nWhen V ENTITY →relation( ?x, entity)\ntmod\nnsubj\nThen given these rules and the lexicon, a training tuple like the following:\n“When was Ada Lovelace born?”\n→\nbirth-year (Ada Lovelace, ?x)\nwould ﬁrst be parsed, resulting in the following mapping.\nWhen was Ada Lovelace born →birth-year(Ada Lovelace, ?x)\ntmod\nnsubj\nFrom many pairs like this, we could induce mappings between pieces of parse\nfragment, such as the mapping between the parse fragment on the left and the rela-\ntion on the right:\nWhen was · born →birth-year( , ?x)\ntmod\nnsubj\nA supervised system would thus parse each tuple in the training set and induce a\nbigger set of such speciﬁc rules, allowing it to map unseen examples of “When was\nX born?” questions to the birth-year relation. Rules can furthermore be associ-\nated with counts based on the number of times the rule is used to parse the training\ndata. Like rule counts for probabilistic grammars, these can be normalized into prob-\nabilities. The probabilities can then be used to choose the highest probability parse\nfor sentences with multiple semantic interpretations.\nThe supervised approach can be extended to deal with more complex questions\nthat are not just about single relations. Consider the question What is the biggest\nstate bordering Texas? —taken from the GeoQuery database of questions on U.S.\nGeography (Zelle and Mooney, 1996)—with the semantic form: argmax(λx.state(x)∧\nborders(x,texas),λx.size(x)) This question has much more complex structures than\nthe simple single-relation questions we considered above, such as the argmax func-\ntion, the mapping of the word biggest to size and so on. Zettlemoyer and Collins\n(2005) shows how more complex default rules (along with richer syntactic struc-\ntures) can be used to learn to map from text sentences to more complex logical\nforms. The rules take the training set’s pairings of sentence and meaning as above",
  "422": "414\nCHAPTER 23\n•\nQUESTION ANSWERING\nand use the complex rules to break each training example down into smaller tuples\nthat can then be recombined to parse new sentences.\n23.2.3\nDealing with Variation: Semi-Supervised Methods\nBecause it is difﬁcult to create training sets with questions labeled with their mean-\ning representation, supervised datasets can’t cover the wide variety of forms that\neven simple factoid questions can take. For this reason most techniques for mapping\nfactoid questions to the canonical relations or other structures in knowledge bases\nﬁnd some way to make use of textual redundancy.\nThe most common source of redundancy, of course, is the web, which contains\nvast number of textual variants expressing any relation. For this reason, most meth-\nods make some use of web text, either via semi-supervised methods like distant\nsupervision or unsupervised methods like open information extraction, both intro-\nduced in Chapter 17. For example the REVERB open information extractor (Fader\net al., 2011) extracts billions of (subject, relation, object) triples of strings from the\nweb, such as (“Ada Lovelace”,“was born in”, “1815”). By aligning these strings\nwith a canonical knowledge source like Wikipedia, we create new relations that can\nbe queried while simultaneously learning to map between the words in question and\ncanonical relations.\nTo align a REVERB triple with a canonical knowledge source we ﬁrst align\nthe arguments and then the predicate. Recall from Chapter 20 that linking a string\nlike “Ada Lovelace” with a Wikipedia page is called entity linking; we thus rep-\nresent the concept ‘Ada Lovelace’ by a unique identiﬁer of a Wikipedia page. If\nthis subject string is not associated with a unique page on Wikipedia, we can dis-\nambiguate which page is being sought, for example by using the cosine distance\nbetween the triple string (‘Ada Lovelace was born in 1815’) and each candidate\nWikipedia page. Date strings like ‘1815’ can be turned into a normalized form us-\ning standard tools for temporal normalization like SUTime (Chang and Manning,\n2012). Once we’ve aligned the arguments, we align the predicates. Given the Free-\nbase relation people.person.birthdate(ada lovelace,1815) and the string\n‘Ada Lovelace was born in 1815’, having linked Ada Lovelace and normalized\n1815, we learn the mapping between the string ‘was born in’ and the relation peo-\nple.person.birthdate. In the simplest case, this can be done by aligning the relation\nwith the string of words in between the arguments; more complex alignment algo-\nrithms like IBM Model 1 (Chapter 22) can be used. Then if a phrase aligns with a\npredicate across many entities, it can be extracted into a lexicon for mapping ques-\ntions to relations.\nHere are some examples from such a resulting lexicon, produced by Berant\net al. (2013), giving many variants of phrases that align with the Freebase relation\ncountry.capital between a country and its capital city:\ncapital of\ncapital city of\nbecome capital of\ncapitol of\nnational capital of\nofﬁcial capital of\npolitical capital of\nadministrative capital of\nbeautiful capital of\ncapitol city of\nremain capital of\nmake capital of\npolitical center of\nbustling capital of\ncapital city in\ncosmopolitan capital of\nmove its capital to\nmodern capital of\nfederal capital of\nbeautiful capital city of\nadministrative capital city of\nFigure 23.10\nSome phrases that align with the Freebase relation country.capital from\nBerant et al. (2013).",
  "423": "23.3\n•\nUSING MULTIPLE INFORMATION SOURCES: IBM’S WATSON\n415\nAnother useful source of linguistic redundancy are paraphrase databases. For ex-\nample the site wikianswers.com contains millions of pairs of questions that users\nhave tagged as having the same meaning, 18 million of which have been collected\nin the PARALEX corpus (Fader et al., 2013). Here’s an example:\nQ: What are the green blobs in plant cells?\nLemmatized synonyms from PARALEX:\nwhat be the green blob in plant cell?\nwhat be green part in plant cell?\nwhat be the green part of a plant cell?\nwhat be the green substance in plant cell?\nwhat be the part of plant cell that give it green color?\nwhat cell part do plant have that enable the plant to be give a green color?\nwhat part of the plant cell turn it green?\npart of the plant cell where the cell get it green color?\nthe green part in a plant be call?\nthe part of the plant cell that make the plant green be call?\nThe resulting millions of pairs of question paraphrases can be aligned to each\nother using MT alignment approaches to create an MT-style phrase table for trans-\nlating from question phrases to synonymous phrases. These can be used by question\nanswering algorithms to generate all paraphrases of a question as part of the process\nof ﬁnding an answer (Fader et al. 2013, Berant and Liang 2014).\n23.3\nUsing multiple information sources: IBM’s Watson\nOf course there is no reason to limit ourselves to just text-based or knowledge-based\nresources for question answering. The Watson system from IBM that won the Jeop-\nardy! challenge in 2011 is an example of a system that relies on a wide variety of\nresources to answer questions.\nDocument\nDocument\nDocument\n(1) Question \nProcessing\nFrom Text Resources\nFocus Detection\nLexical\n Answer Type \nDetection\nQuestion\nDocument \nand \nPasssage \nRetrieval\npassages\nDocument\nDocument\nDocument\nQuestion\nClassification\nParsing\nNamed Entity\n Tagging\nRelation Extraction\nCoreference\nFrom Structured Data\nRelation \nRetrieval\nDBPedia\nFreebase\n(2) Candidate Answer Generation\nCandidate\nAnswer\nCandidate\nAnswer\nCandidate\nAnswer\nCandidate\nAnswer\nCandidate\nAnswer\nCandidate\nAnswer\nCandidate\nAnswer\nCandidate\nAnswer\nCandidate\nAnswer\nCandidate\nAnswer\nCandidate\nAnswer\nCandidate\nAnswer\n(3) Candidate \nAnswer \nScoring\nEvidence \nRetrieval\nand scoring\nAnswer\nExtraction\nDocument titles\nAnchor text\nText\nEvidence\nSources\n(4) \nConfidence\nMerging \nand \nRanking\nText\nEvidence\nSources\nTime from \nDBPedia\nSpace from \nFacebook\nAnswer \nType\nAnswer\nand\nConﬁdence\nCandidate\nAnswer\n+ \nConﬁdence\nCandidate\nAnswer\n+ \nConﬁdence\nCandidate\nAnswer\n+ \nConﬁdence\nCandidate\nAnswer\n+ \nConﬁdence\nCandidate\nAnswer\n+ \nConﬁdence\nLogistic\nRegression\nAnswer\nRanker\nMerge\nEquivalent\nAnswers\nFigure 23.11\nThe 4 broad stages of Watson QA: (1) Question Processing, (2) Candidate Answer Generation,\n(3) Candidate Answer Scoring, and (4) Answer Merging and Conﬁdence Scoring.\nFigure 23.11 shows the 4 stages of the DeepQA system that is the question an-",
  "424": "416\nCHAPTER 23\n•\nQUESTION ANSWERING\nswering component of Watson.\nThe ﬁrst stage is question processing. The DeepQA system runs parsing, named\nentity tagging, and relation extraction on the question. Then, like the text-based\nsystems in Section 23.1, the DeepQA system extracts the focus, the answer type\n(also called the lexical answer type or LAT), and performs question classiﬁcation\nand question sectioning.\nConsider these Jeopardy! examples, with a category followed by a question:\nPoets and Poetry: He was a bank clerk in the Yukon before he published\n“Songs of a Sourdough” in 1907.\nTHEATRE: A new play based on this Sir Arthur Conan Doyle canine\nclassic opened on the London stage in 2007.\nThe questions are parsed, named entities are extracted (Sir Arthur Conan Doyle\nidentiﬁed as a PERSON, Yukon as a GEOPOLITICAL ENTITY, “Songs of a Sour-\ndough” as a COMPOSITION), coreference is run (he is linked with clerk) and rela-\ntions like the following are extracted:\nauthorof(focus,“Songs of a sourdough”)\npublish (e1, he, “Songs of a sourdough”)\nin (e2, e1, 1907)\ntemporallink(publish(...), 1907)\nNext DeepQA extracts the question focus, shown in bold in both examples. The\nfocus\nfocus is the part of the question that co-refers with the answer, used for example to\nalign with a supporting passage. The focus is extracted by hand-written rules—made\npossible by the relatively stylized syntax of Jeopardy! questions—such as a rule\nextracting any noun phrase with determiner “this” as in the Conan Doyle example,\nand rules extracting pronouns like she, he, hers, him, as in the poet example.\nThe lexical answer type (shown in blue above) is a word or words which tell\nlexical answer\ntype\nus something about the semantic type of the answer. Because of the wide variety\nof questions in Jeopardy!, Jeopardy! uses a far larger set of answer types than the\nsets for standard factoid algorithms like the one shown in Fig. 23.4. Even a large\nset of named entity tags is insufﬁcient to deﬁne a set of answer types. The DeepQA\nteam investigated a set of 20,000 questions and found that a named entity tagger\nwith over 100 named entity types covered less than half the types in these questions.\nThus DeepQA extracts a wide variety of words to be answer types; roughly 5,000\nlexical answer types occurred in the 20,000 questions they investigated, often with\nmultiple answer types in each question.\nThese lexical answer types are again extracted by rules: the default rule is to\nchoose the syntactic headword of the focus. Other rules improve this default choice.\nFor example additional lexical answer types can be words in the question that are\ncoreferent with or have a particular syntactic relation with the focus, such as head-\nwords of appositives or predicative nominatives of the focus. In some cases even the\nJeopardy! category can act as a lexical answer type, if it refers to a type of entity\nthat is compatible with the other lexical answer types. Thus in the ﬁrst case above,\nhe, poet, and clerk are all lexical answer types. In addition to using the rules directly\nas a classiﬁer, they can instead be used as features in a logistic regression classiﬁer\nthat can return a probability as well as a lexical answer type.\nNote that answer types function quite differently in DeepQA than the purely IR-\nbased factoid question answerers. In the algorithm described in Section 23.1, we\ndetermine the answer type, and then use a strict ﬁltering algorithm only considering\ntext strings that have exactly that type. In DeepQA, by contrast, we extract lots of",
  "425": "23.3\n•\nUSING MULTIPLE INFORMATION SOURCES: IBM’S WATSON\n417\nanswers, unconstrained by answer type, and a set of answer types, and then in the\nlater ‘candidate answer scoring’ phase, we simply score how well each answer ﬁts\nthe answer types as one of many sources of evidence.\nFinally the question is classiﬁed by type (deﬁnition question, multiple-choice,\npuzzle, ﬁll-in-the-blank). This is generally done by writing pattern-matching regular\nexpressions over words or parse trees.\nIn the second candidate answer generation stage, we combine the processed\nquestion with external documents and other knowledge sources to suggest many\ncandidate answers. These candidate answers can either be extracted from text docu-\nments or from structured knowledge bases.\nFor structured resources like DBpedia, IMDB, or the triples produced by Open\nInformation Extraction, we can just query these stores with the relation and the\nknown entity, just as we saw in Section 23.2. Thus if we have extracted the rela-\ntion authorof(focus,\"Songs of a sourdough\"), we can query a triple store\nwith authorof(?x,\"Songs of a sourdough\") to return the correct author.\nThe method for extracting answers from text depends on the type of text docu-\nments. To extract answers from normal text documents we can do passage search\njust as we did in Section 23.1. As we did in that section, we need to generate a query\nfrom the question; for DeepQA this is generally done by eliminating stop words, and\nthen upweighting any terms which occur in any relation with the focus. For example\nfrom this query:\nMOVIE-“ING”: Robert Redford and Paul Newman starred in this depression-\nera grifter ﬂick. (Answer: “The Sting”)\nthe following weighted query might be extracted:\n(2.0 Robert Redford) (2.0 Paul Newman) star depression era grifter (1.5 ﬂick)\nThe query can now be passed to a standard IR system. DeepQA also makes\nuse of the convenient fact that the vast majority of Jeopardy! answers are the title\nof a Wikipedia document. To ﬁnd these titles, we can do a second text retrieval\npass speciﬁcally on Wikipedia documents. Then instead of extracting passages from\nthe retrieved Wikipedia document, we directly return the titles of the highly ranked\nretrieved documents as the possible answers.\nOnce we have a set of passages, we need to extract candidate answers. If the\ndocument happens to be a Wikipedia page, we can just take the title, but for other\ntexts, like news documents, we need other approaches. Two common approaches\nare to extract all anchor texts in the document (anchor text is the text between <a>\nanchor texts\nand <\\a> used to point to a URL in an HTML page), or to extract all noun phrases\nin the passage that are Wikipedia document titles.\nThe third candidate answer scoring stage uses many sources of evidence to\nscore the candidates. One of the most important is the lexical answer type. DeepQA\nincludes a system that takes a candidate answer and a lexical answer type and returns\na score indicating whether the candidate answer can be interpreted as a subclass or\ninstance of the answer type. Consider the candidate “difﬁculty swallowing” and\nthe lexical answer type “manifestation”. DeepQA ﬁrst matches each of these words\nwith possible entities in ontologies like DBpedia and WordNet. Thus the candidate\n“difﬁculty swallowing” is matched with the DBpedia entity “Dysphagia”, and then\nthat instance is mapped to the WordNet type “Symptom”. The answer type “man-\nifestation” is mapped to the WordNet type “Condition”. The system looks for a\nlink of hyponymy, instance-of or synonymy between these two types; in this case a\nhyponymy relation is found between “Symptom” and “Condition”.",
  "426": "418\nCHAPTER 23\n•\nQUESTION ANSWERING\nOther scorers are based on using time and space relations extracted from DBpe-\ndia or other structured databases. For example, we can extract temporal properties\nof the entity (when was a person born, when died) and then compare to time expres-\nsions in the question. If a time expression in the question occurs chronologically\nbefore a person was born, that would be evidence against this person being the an-\nswer to the question.\nFinally, we can use text retrieval to help retrieve evidence supporting a candidate\nanswer. We can retrieve passages with terms matching the question, then replace the\nfocus in the question with the candidate answer and measure the overlapping words\nor ordering of the passage with the modiﬁed question.\nThe output of this stage is a set of candidate answers, each with a vector of\nscoring features.\nThe ﬁnal answer merging and scoring step ﬁrst merges candidate answers that\nare equivalent. Thus if we had extracted two candidate answers J.F.K. and John\nF. Kennedy, this stage would merge the two into a single candidate. One useful\nkind of resource are synonym dictionaries that are created by listing all anchor text\nstrings that point to the same Wikipedia page; such dictionaries give large num-\nbers of synonyms for each Wikipedia title — e.g., JFK, John F. Kennedy, John\nFitzgerald Kennedy, Senator John F. Kennedy, President Kennedy, Jack Kennedy,\netc. (Spitkovsky and Chang, 2012). For common nouns, we can use morphological\nparsing to merge candidates which are morphological variants.\nWe then merge the evidence for each variant, combining the scoring feature\nvectors for the merged candidates into a single vector.\nNow we have a set of candidates, each with a feature vector. A classiﬁer takes\neach feature vector and assigns a conﬁdence value to this candidate answer. The\nclassiﬁer is trained on thousands of candidate answers, each labeled for whether it\nis correct or incorrect, together with their feature vectors, and learns to predict a\nprobability of being a correct answer. Since, in training, there are far more incorrect\nanswers than correct answers, we need to use one of the standard techniques for\ndealing with very imbalanced data. DeepQA uses instance weighting, assigning an\ninstance weight of .5 for each incorrect answer example in training. The candidate\nanswers are then sorted by this conﬁdence value, resulting in a single best answer.3\nIn summary, we’ve seen in the four stages of DeepQA that it draws on the intu-\nitions of both the IR-based and knowledge-based paradigms. Indeed, Watson’s ar-\nchitectural innovation is its reliance on proposing a very large number of candidate\nanswers from both text-based and knowledge-based sources and then developing a\nwide variety of evidence features for scoring these candidates —again both text-\nbased and knowledge-based. See the papers mentioned at the end of the chapter for\nmore details.\n23.4\nEvaluation of Factoid Answers\nA common evaluation metric for factoid question answering, introduced in the TREC\nQ/A track in 1999, is mean reciprocal rank, or MRR. MRR assumes a test set of\nmean\nreciprocal rank\nMRR\nquestions that have been human-labeled with correct answers. MRR also assumes\n3\nThe merging and ranking is actually run iteratively; ﬁrst the candidates are ranked by the classiﬁer,\ngiving a rough ﬁrst value for each candidate answer, then that value is used to decide which of the variants\nof a name to select as the merged answer, then the merged answers are re-ranked,.",
  "427": "23.4\n•\nEVALUATION OF FACTOID ANSWERS\n419\nthat systems are returning a short ranked list of answers or passages containing an-\nswers. Each question is then scored according to the reciprocal of the rank of the\nﬁrst correct answer. For example if the system returned ﬁve answers but the ﬁrst\nthree are wrong and hence the highest-ranked correct answer is ranked fourth, the\nreciprocal rank score for that question would be 1\n4. Questions with return sets that\ndo not contain any correct answers are assigned a zero. The score of a system is\nthen the average of the score for each question in the set. More formally, for an\nevaluation of a system returning a set of ranked answers for a test set consisting of\nN questions, the MRR is deﬁned as\nMRR = 1\nN\nN\nX\ni=1 s.t. ranki̸=0\n1\nranki\n(23.14)\nReading comprehension systems on datasets like SQuAD are often evaluated\nusing two metrics, both ignoring punctuations and articles (a, an, the) (Rajpurkar\net al., 2016):\n• Exact match: The percentage of predicted answers that match the gold answer\nexactly.\n• F1 score: The average overlap between predicted and gold answers. Treat the\nprediction and gold as a bag of tokens, and compute F1, averaging the F1 over\nall questions.\nA number of test sets are available for question answering. Early systems used\nthe TREC QA dataset; questions and hand-written answers for TREC competitions\nfrom 1999 to 2004 are publicly available. TriviaQA (Joshi et al., 2017) has 650K\nquestion-answer evidence triples, from 95K hand-created question-answer pairs to-\ngether with on average six supporting evidence documents collected retrospectively\nfrom Wikipedia and the Web.\nAnother family of datasets starts from WEBQUESTIONS (Berant et al., 2013),\nwhich contains 5,810 questions asked by web users, each beginning with a wh-\nword and containing exactly one entity. Questions are paired with hand-written an-\nswers drawn from the Freebase page of the question’s entity. WEBQUESTIONSSP\n(Yih et al., 2016) augments WEBQUESTIONS with human-created semantic parses\n(SPARQL queries) for those questions answerable using Freebase. COMPLEXWE-\nBQUESTIONS augments the dataset with compositional and other kinds of complex\nquestions, resulting in 34,689 question questions, along with answers, web snippets,\nand SPARQL queries. (Talmor and Berant, 2018).\nThere are a wide variety of datasets for training and testing reading comprehen-\nsion/answer extraction in addition to the SQuAD (Rajpurkar et al., 2016) and Wik-\niQA (Yang et al., 2015) datasets discussed on page 410. The NarrativeQA (Koˇcisk`y\net al., 2018) dataset, for example, has questions based on entire long documents like\nbooks or movie scripts, while the Question Answering in Context (QuAC) dataset\n(Choi et al., 2018) has 100K questions created by two crowdworkers who are asking\nand answering questions about a hidden Wikipedia text.\nOthers take their structure from the fact that reading comprehension tasks de-\nsigned for children tend to be multiple choice, with the task being to choose among\nthe given answers. The MCTest dataset uses this structure, with 500 ﬁctional short\nstories created by crowd workers with questions and multiple choice answers (Richard-\nson et al., 2013). The AI2 Reasoning Challenge (ARC) (Clark et al., 2018), has\nquestions that are designed to be hard to answer from simple lexical methods:",
  "428": "420\nCHAPTER 23\n•\nQUESTION ANSWERING\nWhich property of a mineral can be determined just by looking at it?\n(A) luster [correct] (B) mass (C) weight (D) hardness\nThis ARC example is difﬁcult because the correct answer luster is unlikely to cooc-\ncur frequently on the web with phrases like looking at it, while the word mineral is\nhighly associated with the incorrect answer hardness.\nBibliographical and Historical Notes\nQuestion answering was one of the earliest NLP tasks, and early versions of the text-\nbased and knowledge-based paradigms were developed by the very early 1960s. The\ntext-based algorithms generally relied on simple parsing of the question and of the\nsentences in the document, and then looking for matches. This approach was used\nvery early on (Phillips, 1960) but perhaps the most complete early system, and one\nthat strikingly preﬁgures modern relation-based systems, was the Protosynthex sys-\ntem of Simmons et al. (1964). Given a question, Protosynthex ﬁrst formed a query\nfrom the content words in the question, and then retrieved candidate answer sen-\ntences in the document, ranked by their frequency-weighted term overlap with the\nquestion. The query and each retrieved sentence were then parsed with dependency\nparsers, and the sentence whose structure best matches the question structure se-\nlected. Thus the question What do worms eat? would match worms eat grass: both\nhave the subject worms as a dependent of eat, in the version of dependency grammar\nused at the time, while birds eat worms has birds as the subject:\nWhat do worms eat\nWorms eat grass\nBirds eat worms\nThe alternative knowledge-based paradigm was implemented in the BASEBALL\nsystem (Green et al., 1961). This system answered questions about baseball games\nlike “Where did the Red Sox play on July 7” by querying a structured database of\ngame information. The database was stored as a kind of attribute-value matrix with\nvalues for attributes of each game:\nMonth = July\nPlace = Boston\nDay\n= 7\nGame Serial No.\n= 96\n(Team = Red Sox, Score = 5)\n(Team = Yankees, Score = 3)\nEach question was constituency-parsed using the algorithm of Zellig Harris’s\nTDAP project at the University of Pennsylvania, essentially a cascade of ﬁnite-\nstate transducers (see the historical discussion in Joshi and Hopely 1999 and Kart-\ntunen 1999). Then a content analysis phase each word or phrase was associated with\na program that computed parts of its meaning. Thus the phrase ‘Where’ had code to\nassign the semantics Place = ?\", with the result that the question “Where did the\nRed Sox play on July 7” was assigned the meaning\nPlace = ?",
  "429": "EXERCISES\n421\nTeam = Red Sox\nMonth = July\nDay = 7\nThe question is then matched against the database to return to the answer. Sim-\nmons (1965) summarizes other early QA systems.\nAnother important progenitor of the knowledge-based paradigm for question-\nanswering is work that used predicate calculus as the meaning representation lan-\nguage. The LUNAR system (Woods et al. 1972,Woods 1978) was designed to be\nLUNAR\na natural language interface to a database of chemical facts about lunar geology. It\ncould answer questions like Do any samples have greater than 13 percent aluminum\nby parsing them into a logical form\n(TEST (FOR SOME X16 / (SEQ SAMPLES) : T ; (CONTAIN’ X16\n(NPR* X17 / (QUOTE AL203)) (GREATERTHAN 13PCT))))\nThe rise of the web brought the information-retrieval paradigm for question an-\nswering to the forefront with the TREC QA track beginning in 1999, leading to a\nwide variety of factoid and non-factoid systems competing in annual evaluations.\nAt the same time, Hirschman et al. (1999) introduced the idea of using chil-\ndren’s reading comprehension tests to evaluate machine text comprehension algo-\nrithm. They acquired a corpus of 120 passages with 5 questions each designed for\n3rd-6th grade children, built an answer extraction system, and measured how well\nthe answers given by their system corresponded to the answer key from the test’s\npublisher. Their algorithm focused on word overlap as a feature; later algorithms\nadded named entity features and more complex similarity between the question and\nthe answer span (Riloff and Thelen 2000, Ng et al. 2000).\nNeural reading comprehension systems drew on the insight of these early sys-\ntems that answer ﬁnding should focus on question-passage similarity. Many of the\narchitectural outlines of modern systems were laid out in the AttentiveReader (Her-\nmann et al., 2015). The idea of using passage-aligned question embeddings in the\npassage computation was introduced by (Lee et al., 2017). Seo et al. (2017) achieves\nhigh-performance by introducing bi-directional attention ﬂow. Chen et al. (2017)\nand Clark and Gardner (2018) show how to extract answers from entire documents.\nThe DeepQA component of the Watson system that won the Jeopardy! challenge\nis described in a series of papers in volume 56 of the IBM Journal of Research and\nDevelopment; see for example Ferrucci (2012), Lally et al. (2012), Chu-Carroll et al.\n(2012), Murdock et al. (2012b), Murdock et al. (2012a), Kalyanpur et al. (2012), and\nGondek et al. (2012).\nOther question-answering tasks include Quiz Bowl, which has timing consid-\nerations since the question can be interrupted (Boyd-Graber et al., 2018). Question\nanswering is also an important function of modern personal assistant dialog systems;\nsee Chapter 24 for more.\nExercises",
  "430": "422\nCHAPTER 24\n•\nDIALOG SYSTEMS AND CHATBOTS\nCHAPTER\n24\nDialog Systems and Chatbots\nLes lois de la conversation sont en g´en´eral de ne s’y appesantir sur aucun ob-\njet, mais de passer l´eg`erement, sans effort et sans affectation, d’un sujet `a un\nautre ; de savoir y parler de choses frivoles comme de choses s´erieuses\nThe rules of conversation are, in general, not to dwell on any one subject,\nbut to pass lightly from one to another without effort and without affectation;\nto know how to speak about trivial topics as well as serious ones;\nThe 18th C. Encyclopedia of Diderot, start of the entry on conversation\nThe literature of the fantastic abounds in inanimate objects magically endowed with\nsentience and the gift of speech. From Ovid’s statue of Pygmalion to Mary Shelley’s\nFrankenstein, there is something deeply moving about creating something and then\nhaving a chat with it. Legend has it that after ﬁnishing his\nsculpture Moses, Michelangelo thought it so lifelike that\nhe tapped it on the knee and commanded it to speak. Per-\nhaps this shouldn’t be surprising. Language is the mark\nof humanity and sentience, and conversation or dialog\nconversation\ndialog\nis the most fundamental and specially privileged arena\nof language. It is the ﬁrst kind of language we learn as\nchildren, and for most of us, it is the kind of language\nwe most commonly indulge in, whether we are ordering\ncurry for lunch or buying spinach, participating in busi-\nness meetings or talking with our families, booking air-\nline ﬂights or complaining about the weather.\nThis chapter introduces the fundamental algorithms of conversational agents,\nconversational\nagent\nor dialog systems. These programs communicate with users in natural language\ndialog system\n(text, speech, or even both), and generally fall into two classes.\nTask-oriented dialog agents are designed for a particular task and set up to\nhave short conversations (from as little as a single interaction to perhaps half-a-\ndozen interactions) to get information from the user to help complete the task. These\ninclude the digital assistants that are now on every cellphone or on home controllers\n(Siri, Cortana, Alexa, Google Now/Home, etc.) whose dialog agents can give travel\ndirections, control home appliances, ﬁnd restaurants, or help make phone calls or\nsend texts. Companies deploy goal-based conversational agents on their websites to\nhelp customers answer questions or address problems. Conversational agents play\nan important role as an interface to robots. And they even have applications for\nsocial good. DoNotPay is a “robot lawyer” that helps people challenge incorrect\nparking ﬁnes, apply for emergency housing, or claim asylum if they are refugees.\nChatbots are systems designed for extended conversations, set up to mimic the",
  "431": "423\nunstructured conversational or ‘chats’ characteristic of human-human interaction,\nrather than focused on a particular task like booking plane ﬂights. These systems\noften have an entertainment value, such as Microsoft’s XiaoIce (Little Bing 小冰)\nsystem (Microsoft, 2014), which chats with people on text messaging platforms.\nChatbots are also often attempts to pass various forms of the Turing test (introduced\nin Chapter 1). Yet starting from the very ﬁrst system, ELIZA (Weizenbaum, 1966),\nchatbots have also been used for practical purposes, such as testing theories of psy-\nchological counseling.\nNote that the word ‘chatbot’ is often used in the media and in industry as a\nsynonym for conversational agent. In this chapter we will instead follow the usage\nin the natural language processing community, limiting the designation chatbot to\nthis second subclass of systems designed for extended, casual conversation.\nLet’s see some examples of dialog systems. One dimension of difference across\nsystems is how many turns they can deal with. A dialog consists of multiple turns,\nturn\neach a single contribution to the dialog (the terminology is as if dialog is a game in\nwhich I take a turn, then you take a turn, then me, and so on). A turn can consist\nof a sentence, although it might be as short as a single word or as long as multiple\nsentences. The simplest such systems generally handle a single turn from the user,\nacting more like question-answering or command-and-control systems. This is espe-\ncially common with digital assistants. For example Fig. 24.1 shows screen captures\nfrom an early version of Apple’s Siri personal assistant from 2014, demonstrating\nthis kind of single-query behavior.\n(a)\n(b)\nFigure 24.1\nTwo sets of interactions with Siri in 2014. (a) A question (”Find restaurants near me”) returns\nrestaurants, but the system was unable to interpret a follow-up question (“Are any of them Italian?”). (b) An\nalternative followup (“Tell me more about the second one”) similarly fails. This early system’s confusion at\nfollow-up questions suggests that it is mainly designed for a single interaction.\nBy contrast, Fig. 24.2 shows that a 2017 version of the Siri digital assistant can\nhandle slightly longer dialogs, handling a second turn with a follow-up question.\nWhile spoken dialogs with mobile phone digital assistants tend to be short, some\ntasks do require longer dialogs. One such task is travel planning and management,\na key concern of dialog systems since the very inﬂuential GUS system for planning\nairline travel (Bobrow et al., 1977); we’ll see an example in the next section.\nDialog systems can even be used for much more complex domains like automatic\ntutoring. Figure 24.3 shows part of a dialog from the adaptive ITSPOKE dialog\nsystem (Forbes-Riley and Litman, 2011). In this example the system detects the\nhesitancy of the student’s ﬁrst response (“Is it 19.6 m/s?”), and, even though the",
  "432": "424\nCHAPTER 24\n•\nDIALOG SYSTEMS AND CHATBOTS\n(a)\n(b)\nFigure 24.2\nThe same follow-up questions that Siri couldn’t answer in 2014 receive appropriate responses\nwhen posed to Siri in 2017.\nanswer is correct, decides to explain the answer and ask a follow-up question before\nmoving on.\nTutor:\nWhat will the velocity of the object be a second after that (where the initial\nvelocity is 9.8 m/s and the acceleration is 9.8 m/s2)?\nStudent:\nIs it 19.6 m/s?\nTutor:\nGood. A second later, its velocity will be 9.8m/s + 9/8 m/s = 19.6 m/s This is\nbecause its acceleration tells us that every second, its velocity increases by 9.8\nm/s. So what’ll its velocity be, a second after that?\nStudent:\nUm. I’m thinking. 29.4 m/s\nTutor:\nRight. Now back to the man and his keys. We’ve shown that they both have an\nacceleration of 9.8 m/ss. So, how do their velocities compare with each other, at\nevery second during the fall?\nFigure 24.3\nThe tutoring domain: An excerpt from a tutorial interaction between a physics\nstudent and the ITSPOKE system of Forbes-Riley and Litman (2011) based on the Why2-\nAtlas text-based tutoring system (VanLehn et al., 2002).\nFinally, conversational agents can be purely for fun, like the agents designed for\nsimple chit-chat like Cleverbot, an IR-based chatbot capable of carrying on the kinds\nof conversations shown in Fig. 24.4.\nUser:\nDo you like Doctor Who?\nCleverbot:\nYes, so funny.\nUser:\nWhat’s your favorite show?\nCleverbot:\nI don’t have one.\nUser:\nWhat’s your name?\nCleverbot:\nJason I already told you.\nUser:\nNo your real name.\nCleverbot:\nStephanie IS my real name.\nUser:\nI thought it was Jason?\nCleverbot:\nHow did you come to that conclusion.\nUser:\nBecause you said so.\nCleverbot:\nI have to do my homework.\nFigure 24.4\nChatbots: A conversation between a user and Cleverbot, an IR-based chatbot\n(Carpenter, 2017).",
  "433": "24.1\n•\nCHATBOTS\n425\nWe begin in the next section with chatbots, introducing the three major chatbot\narchitectures: rule-based systems, information retrieval systems, and transduction\nmodels. We then turn to task-oriented dialog, introducing the frame-based architec-\nture (the GUS architecture) that underlies most modern task-based dialog systems,\nthe VoiceXML system that is a simple industrial instantiation of the architecture,\nand information on designing and evaluating these systems.\n24.1\nChatbots\nChatbots are systems that can carry on extended conversations with the goal of\nchatbot\nmimicking the unstructured conversational or ‘chats’ characteristic of human-human\ninteraction,\nThese systems often have an entertainment value, such as Microsoft’s XiaoIce\n(Little Bing 小冰) system (Microsoft, 2014), which chats with people on text mes-\nsaging platforms. Yet starting from the very ﬁrst system, ELIZA (Weizenbaum,\n1966), chatbots have also been used for practical purposes, such as testing theories\nof psychological counseling.\nLike practically everything else in language processing, chatbot architectures\nfall into two classes: rule-based systems and corpus-based systems. Rule-based\nsystems include the early inﬂuential ELIZA and PARRY systems. Corpus-based\nsystems mine large datasets of human-human conversations, which can be done by\nusing information retrieval (IR-based systems simply copy a human’s response from\na previous conversation) or by using a machine translation paradigm such as neural\nnetwork sequence-to-sequence systems, to learn to map from a user utterance to a\nsystem response.\n24.1.1\nRule-based chatbots: ELIZA and PARRY\nELIZA (Weizenbaum, 1966) is the most important chatbot dialog system in the his-\ntory of the ﬁeld, and so we take some time here to describe its algorithm in detail.\nELIZA was designed to simulate a Rogerian psychologist, based on a branch of\nclinical psychology whose methods involve drawing the patient out by reﬂecting pa-\ntient’s statements back at them. Rogerian psychology is the rare type of conversation\nin which, as Weizenbaum points out, one can “assume the pose of knowing almost\nnothing of the real world”. If a patient says “I went for a long boat ride” and the\npsychiatrist says “Tell me about boats”, you don’t assume she didn’t know what a\nboat is, but rather assume she had some conversational goal. Most chatbots trying to\npass Turing test choose a domain with similar properties.\nLet’s look at the most famous ELIZA conversation, excerpted (deleting some\nmaterial with ...) from a full interaction given in the original paper:\nMen are all alike.\nIN WHAT WAY\nThey’re always bugging us about something or other.\nCAN YOU THINK OF A SPECIFIC EXAMPLE\nWell\nmy boyfriend made me come here.\nYOUR BOYFRIEND MADE YOU COME HERE\nHe says I’m depressed much of the time.\nI AM SORRY TO HEAR YOU ARE DEPRESSED\n..",
  "434": "426\nCHAPTER 24\n•\nDIALOG SYSTEMS AND CHATBOTS\n..\nWHO ELSE IN YOUR FAMILY TAKES CARE OF YOU?\nMy father\nYOUR FATHER\nYou are like my father in some ways.\nWHAT RESEMBLANCE DO YOU SEE\nYou are not very aggressive but I think you don’t want me to notice that.\nWHAT MAKES YOU THINK I AM NOT AGGRESSIVE\nYou don’t argue with me.\nWHY DO YOU THINK I DON’T ARGUE WITH YOU\nYou are afraid of me.\nDOES IT PLEASE YOU TO BELIEVE I’M AFRAID OF YOU\nMy father is afraid of everybody.\nWHAT ELSE COMES TO MIND WHEN YOU THINK OF YOUR\nFATHER\nBullies.\nDOES THAT HAVE ANYTHING TO DO WITH THE FACT THAT YOUR\nBOYFRIEND MADE YOU COME HERE\nAs we summarized brieﬂy in Chapter 1, ELIZA worked by pattern/transform\nrules like the following one:\n(0 YOU 0 ME) [pattern]\n->\n(WHAT MAKES YOU THINK I 3 YOU) [transform]\nIn the ELIZA pattern, 0 means Kleene*, and in the transform rules the numbers\nare the index of the constituent in the pattern. Thus the number 3 refers to the second\n0 in the ﬁrst pattern. This rule would transfer\nYou hate me\ninto:\nWHAT MAKES YOU THINK I HATE YOU\nEach ELIZA pattern/rule is linked to a keyword that might occur in a user sen-\ntence.\nThe architecture is summarized in Fig. 24.5.\nKeywords are associated with a rank, with speciﬁc words being more highly\nranked, and more general words ranking lower. Consider the following user sen-\ntence:\nI know everybody laughed at me\nBecause it has the word “I”, this sentence could match the following rule whose\nkeyword is I:\n(I *) -> (You say you 2)\nproducing:\nYOU SAY YOU KNOW EVERYBODY LAUGHED AT YOU\nBut as Weizenbaum points out, “I” is a very general word and its keywords lead\nto very general responses. Instead the keyword “everybody” is much more interest-\ning, since someone using universals like everybody or always is probably “referring\nto some quite speciﬁc event or person”. Therefore, ELIZA prefers to respond with\nthe pattern associated with the more speciﬁc keyword everybody (implementing by\njust assigning ”everybody” rank 5 and ”I” rank 0 in the lexicon), whose rule thus\ntransforms the sentence to:",
  "435": "24.1\n•\nCHATBOTS\n427\nfunction ELIZA GENERATOR(user sentence) returns response\nFind the word w in sentence that has the highest keyword rank\nif w exists\nChoose the highest ranked rule r for w that matches sentence\nresponse←Apply the transform in r to sentence\nif w = ’my’\nfuture←Apply a transformation from the ‘memory’ rule list to sentence\nPush future onto memory stack\nelse (no keyword applies)\neither\nresponse←Apply the transform for the NONE keyword to sentence\nor\nresponse←Pop the top response from the memory stack\nreturn(response)\nFigure 24.5\nA simpliﬁed sketch of the ELIZA algorithm. The power of the algorithm\ncomes from the particular transforms associated with each keyword.\nWHO IN PARTICULAR ARE YOU THINKING OF?\nIf no keyword matches, ELIZA chooses a non-commital response like “PLEASE\nGO ON”, “THAT’S VERY INTERESTING”, or “I SEE”.\nFinally, ELIZA has a clever memory trick that accounts for the very last sen-\ntence of the conversation above. Whenever the word “my” is the highest ranked\nkeyword, ELIZA will randomly select a transform on the MEMORY list, apply it to\nthe sentence, and store it on the stack:\n(MEMORY MY\n(0 MY 0 = LETS DISCUSS FURTHER WHY YOUR 3)\n(0 MY 0 = EARLIER YOU SAID YOUR 3)\n(0 MY 0 = DOES THAT HAVE ANYTHING TO DO WITH THE FACT THAT YOUR 3\nLater, if no keyword matches a sentence, ELIZA will return the top of the MEM-\nORY queue instead. 1\nPeople became deeply emotionally involved with the program. Weizenbaum\ntells the story of one of his staff who would ask Weizenbaum to leave the room\nwhen she talked with ELIZA. When Weizenbaum suggested that he might want to\nstore all the ELIZA conversations for later analysis, people immediately pointed\nout the privacy implications, which suggested that they were having quite private\nconversations with ELIZA, despite knowing that it was just software.\nEliza’s framework is still used today; modern chatbot system tools like ALICE\nare based on updated versions of ELIZA’s pattern/action architecture.\nA few years after ELIZA, another chatbot with a clinical psychology focus,\nPARRY (Colby et al., 1971), was used to study schizophrenia. In addition to ELIZA-\nlike regular expressions, the PARRY system including a model of its own mental\nstate, with affect variables for the agent’s levels of fear and anger; certain topics of\nconversation might lead PARRY to become more angry or mistrustful. If PARRY’s\nanger variable is high, he will choose from a set of “hostile” outputs. If the input\nmentions his delusion topic, he will increase the value of his fear variable and then\nbegin to express the sequence of statements related to his delusion. Parry was the\n1\nFun fact: because of its structure as a queue, this MEMORY trick is the earliest known hierarchical\nmodel of discourse in natural language processing.",
  "436": "428\nCHAPTER 24\n•\nDIALOG SYSTEMS AND CHATBOTS\nﬁrst known system to pass the Turing test (in 1972!); psychiatrists couldn’t distin-\nguish text transcripts of interviews with PARRY from transcripts of interviews with\nreal paranoids (Colby et al., 1972).\n24.1.2\nCorpus-based chatbots\nCorpus-based chatbots, instead of using hand-built rules, mine conversations of\nhuman-human conversations, or sometimes mine the human responses from human-\nmachine conversations. Serban et al. (2017) summarizes some such available cor-\npora, such as conversations on chat platforms, on Twitter, or in movie dialog, which\nis available in great quantities and has been shown to resemble natural conversation\n(Forchini, 2013). Chatbot responses can even be extracted from sentences in corpora\nof non-dialog text.\nThere are two common architectures for corpus-based chatbots: information re-\ntrieval, and machine learned sequence transduction. Like rule-based chatbots (but\nunlike frame-based dialog systems), most corpus-based chatbots do very little mod-\neling of the conversational context. Instead they focus on generating a single re-\nsponse turn that is appropriate given the user’s immediately previous utterance. For\nthis reason they are often called response generation systems. Corpus-based chat-\nresponse\ngeneration\nbots thus have some similarity to question answering systems, which focus on single\nresponses while ignoring context or larger conversational goals.\nIR-based chatbots\nThe principle behind information retrieval based chatbots is to respond to a user’s\nturn X by repeating some appropriate turn Y from a corpus of natural (human) text.\nThe differences across such systems lie in how they choose the corpus, and how they\ndecide what counts as an appropriate human turn to copy.\nA common choice of corpus is to collect databases of human conversations.\nThese can come from microblogging platforms like Twitter or any Weibo (微博).\nAnother approach is to use corpora of movie dialog. Once a chatbot has been put\ninto practice, the turns that humans use to respond to the chatbot can be used as\nadditional conversational data for training.\nGiven the corpus and the user’s sentence, IR-based systems can use any retrieval\nalgorithm to choose an appropriate response from the corpus. The two simplest\nmethods are the following:\n1. Return the response to the most similar turn: Given user query q and a con-\nversational corpus C, ﬁnd the turn t in C that is most similar to q (for example has\nthe highest cosine with q) and return the following turn, i.e. the human response to t\nin C:\nr = response\n\u0012\nargmax\nt∈C\nqTt\n||q||t||\n\u0013\n(24.1)\nThe idea is that we should look for a turn that most resembles the user’s turn, and re-\nturn the human response to that turn (Jafarpour et al. 2009, Leuski and Traum 2011).\n2. Return the most similar turn: Given user query q and a conversational corpus\nC, return the turn t in C that is most similar to q (for example has the highest cosine\nwith q):\nr = argmax\nt∈C\nqTt\n||q||t||\n(24.2)\nThe idea here is to directly match the users query q with turns from C, since a good\nresponse will often share words or semantics with the prior turn.",
  "437": "24.1\n•\nCHATBOTS\n429\nIn each case, any similarity function can be used, most commonly cosines com-\nputed either over words (using tf-idf) or over embeddings.\nAlthough returning the response to the most similar turn seems like a more in-\ntuitive algorithm, returning the most similar turn seems to work better in practice,\nperhaps because selecting the response adds another layer of indirection that can\nallow for more noise (Ritter et al. 2011, Wang et al. 2013).\nThe IR-based approach can be extended by using more features than just the\nwords in the q (such as words in prior turns, or information about the user), and\nusing any full IR ranking approach. Commercial implementations of the IR-based\napproach include Cleverbot (Carpenter, 2017) and Microsoft’s XiaoIce (Little Bing\n小冰) system (Microsoft, 2014).\nInstead of just using corpora of conversation, the IR-based approach can be used\nto draw responses from narrative (non-dialog) text. For example, the pioneering\nCOBOT chatbot (Isbell et al., 2000) generated responses by selecting sentences from\na corpus that combined the Unabomber Manifesto by Theodore Kaczynski, articles\non alien abduction, the scripts of “The Big Lebowski” and “Planet of the Apes”.\nChatbots that want to generate informative turns such as answers to user questions\ncan use texts like Wikipedia to draw on sentences that might contain those answers\n(Yan et al., 2016).\nSequence to sequence chatbots\nAn alternate way to use a corpus to generate dialog is to think of response generation\nas a task of transducing from the user’s prior turn to the system’s turn. This is\nbasically the machine learning version of Eliza; the system learns from a corpus to\ntransduce a question to an answer.\nThis idea was ﬁrst developed by using phrase-based machine translation (Ritter\net al., 2011) to translate a user turn to a system response. It quickly became clear,\nhowever, that the task of response generation was too different from machine trans-\nlation. In machine translation words or phrases in the source and target sentences\ntend to align well with each other; but in conversation, a user utterance may share\nno words or phrases with a coherent response.\nInstead, (roughly contemporaneously by Shang et al. 2015, Vinyals and Le 2015,\nand Sordoni et al. 2015) transduction models for response generation were modeled\ninstead using encoder-decoder (seq2seq) models (Chapter 22), as shown in Fig. 24.6.\nHow\nare\nyou\n?\nI’m\nfine\n.\nEOS\nEncoding\nDecoding\nEOS\nI’m\nfine\n.\nFigure 24.6\nA sequence to sequence model for neural response generation in dialog.\nA number of modiﬁcations are required to the basic seq2seq model to adapt it for\nthe task of response generation. For example basic seq2seq models have a tendency\nto produce predictable but repetitive and therefore dull responses like “I’m OK” or\n“I don’t know” that shut down the conversation. This can be addressed by changing\nthe objective function for seq2seq model training to a mutual information objective,\nor by modifying a beam decoder to keep more diverse responses in the beam (Li",
  "438": "430\nCHAPTER 24\n•\nDIALOG SYSTEMS AND CHATBOTS\net al., 2016a).\nAnother problem with the simple SEQ2SEQresponse generation architecture is\nits inability to model the longer prior context of the conversation. This can be done\nby allowing the model to see prior turns, such as by using a hierarchical model that\nsummarizes information over multiple prior turns (Lowe et al., 2017b).\nFinally, SEQ2SEQresponse generators focus on generating single responses, and\nso don’t tend to do a good job of continuously generating responses that cohere\nacross multiple turns. This can be addressed by using reinforcement learning, as\nwell as techniques like adversarial networks, to learn to choose responses that make\nthe overall conversation more natural (Li et al. 2016b, Li et al. 2017).\nFig. 24.7 shows some sample responses generated by a vanilla SEQ2SEQmodel,\nand from a model trained by an adversarial algorithm to produce responses that are\nharder to distinguish from human responses (Li et al., 2017).\nInput\ntell me ... how long have you had this falling sickness ?\nVanilla-SEQ2SEQ\ni’m not a doctor.\nAdversarial\na few months, i guess .\nInput\nso i had the doctors test sammy ’s response to conditioning .\nVanilla-SEQ2SEQ\nsammy wrote the test sammy wrote the test .\nAdversarial\nso he took the pills .\nInput\nthey didn ’t have much success with this problem commander .\nVanilla-SEQ2SEQ\nthey ’re not the only ones who have been in the system .\nAdversarial\ncan we ﬁnd someone else ?\nFigure 24.7\nSample responses generated by a SEQ2SEQmodel trained either with a vanilla\nmaximum likelihood objective, or adversarially trained to produce sentences that are hard for\nan adversary to distinguish from human sentences (Li et al., 2017).\nEvaluating Chatbots\nChatbots are generally evaluated by humans. The slot-ﬁlling evaluations used for\ntask-based dialog (Section 24.2.3) aren’t appropriate for this task (Artstein et al.,\n2009), and word-overlap metrics like BLEU for comparing a chatbot’s response to a\nhuman response turn out to correlate very poorly with human judgments (Liu et al.,\n2016). BLEU performs poorly because there are so many possible responses to any\ngiven turn; word-overlap metrics work best when the space of responses is small and\nlexically overlapping, as is the case in machine translation.\nWhile human evaluation is therefore required for evaluating chatbots, there are\nbeginning to be models for automatic evaluation. The ADEM (Lowe et al., 2017a)\nclassiﬁer is trained on a set of responses labeled by humans with how appropriate\nthey are, and learns to predict this label from the dialog context and the words in the\nsystem response.\nAnother paradigm is adversarial evaluation (Bowman et al. 2016, Kannan and\nadversarial\nevaluation\nVinyals 2016, Li et al. 2017), inspired by the Turing test. The idea is to train a\n“Turing-like” evaluator classiﬁer to distinguish between human-generated responses\nand machine-generated responses. The more successful a response generation sys-\ntem is at fooling this evaluator, the better the system.\n24.2\nFrame Based Dialog Agents\nModern task-based dialog systems are based on a domain ontology, a knowledge\ndomain\nontology",
  "439": "24.2\n•\nFRAME BASED DIALOG AGENTS\n431\nstructure representing the kinds of intentions the system can extract from user sen-\ntences. The ontology deﬁnes one or more frames, each a collection of slots, and\nframe\nslot\ndeﬁnes the values that each slot can take. This frame-based architecture was ﬁrst\nintroduced in 1977 in the inﬂuential GUS system for travel planning (Bobrow et al.,\n1977), and has been astonishingly long-lived, underlying most modern commercial\ndigital assistants. Figure 24.8 shows a conversation with the original GUS system.\nGUS:\nHello. My name is GUS. I can help you plan a simple trip by air. Where do you\nwant to go ?\nClient:\nI want to go to San Diego on May 28\nGUS:\nWhat time do you want to leave?\nClient:\nI must be in San Diego before 10 am\nGUS:\nWould you like P.S.A. ﬂight 102 that arrives at 9.15 am ?\nClient:\nWhat is the next ﬂight ?\nGUS:\nAir California ﬂight 310 that leaves at 8.30am and arrives at 10.00am\nClient:\nI’ll take the ﬁrst one\nGUS:\nIn what name should I make a reservation, please?\nClient:\nDan Bobrow\nGUS:\nI have conﬁrmed the following ﬂight: P.S.A. ﬂight 102 on Wednesday May 28\nfrom San Jose to San Diego that leaves at 7.30 am and arrives at 9.15 am.\nGUS:\nWhat date do you want to return on ?\nClient:\nOn Friday in the evening.\nGUS:\nWould you like the ﬂight that leaves at 7.45 pm ?\nClient:\nThat’s ﬁne.\nGUS:\nI have conﬁrmed the following ﬂight: P.S.A. ﬂight 307 on Friday May 30 from\nSan Diego to San Jose that leaves at 7.45 pm and arrives at 9.30 pm Thank you\nfor calling. Goodbye\nFigure 24.8\nThe travel domain: A transcript of an actual dialog with the GUS system of\nBobrow et al. (1977). P.S.A. and Air California were airlines of that period.\nThe set of slots in a GUS-style frame speciﬁes what the system needs to know,\nand the ﬁller of each slot is constrained to values of a particular semantic type. In\nthe travel domain, for example, a slot might be of type city (hence take on values\nlike San Francisco, or Hong Kong) or of type date, airline, or time:\nSlot\nType\nORIGIN CITY\ncity\nDESTINATION CITY city\nDEPARTURE TIME\ntime\nDEPARTURE DATE\ndate\nARRIVAL TIME\ntime\nARRIVAL DATE\ndate\nTypes in GUS, as in modern frame-based dialog agents, may have hierarchical\nstructure; for example the date type in GUS is itself a frame with slots with types\nlike integer or members of sets of weekday names:\nDATE\nMONTH NAME\nDAY (BOUNDED-INTEGER 1 31)\nYEAR INTEGER\nWEEKDAY (MEMBER (SUNDAY MONDAY TUESDAY WEDNESDAY THURSDAY FRIDAY SATURDAY))",
  "440": "432\nCHAPTER 24\n•\nDIALOG SYSTEMS AND CHATBOTS\n24.2.1\nControl structure for frame-based dialog\nThe control architecture of frame-based dialog systems is designed around the frame.\nThe goal is to ﬁll the slots in the frame with the ﬁllers the user intends, and then per-\nform the relevant action for the user (answering a question, or booking a ﬂight).\nMost frame-based dialog systems are based on ﬁnite-state automata that are hand-\ndesigned for the task by a dialog designer.\nWhat city are you leaving from?\nDo you want to go from\n <FROM> to <TO> on <DATE>?\nYes\nWhere are you going?\nWhat date do you want to leave?\nIs it a one-way trip?\nWhat date do you want to return?\nDo you want to go from <FROM> to <TO> \non <DATE> returning on <RETURN>?\nNo\nNo\nYes\nYes\nNo\nBook the flight\nFigure 24.9\nA simple ﬁnite-state automaton architecture for frame-based dialog.\nConsider the very simple ﬁnite-state control architecture shown in Fig. 24.9,\nimplementing a trivial airline travel system whose job is to ask the user for the\ninformation for 4 slots: departure city, a destination city, a time, and whether the trip\nis one-way or round-trip. Let’s ﬁrst associate with each slot a question to ask the\nuser:\nSlot\nQuestion\nORIGIN CITY\n“From what city are you leaving?”\nDESTINATION CITY “Where are you going?”\nDEPARTURE TIME\n“When would you like to leave?”\nARRIVAL TIME\n“When do you want to arrive?”\nFigure 24.9 shows a sample dialog manager for such a system. The states of\nthe FSA correspond to the slot questions, user, and the arcs correspond to actions\nto take depending on what the user responds. This system completely controls the\nconversation with the user. It asks the user a series of questions, ignoring (or misin-\nterpreting) anything that is not a direct answer to the question and then going on to\nthe next question.\nThe speaker in control of any conversation is said to have the initiative in the\ninitiative\nconversation. Systems that completely control the conversation in this way are thus\ncalled system-initiative. By contrast, in normal human-human dialog, initiative\nsystem-\ninitiative\nshifts back and forth between the participants (Bobrow et al. 1977, Walker and Whit-\ntaker 1990).\nThe single-initiative ﬁnite-state dialog architecture has the advantage that the\nsystem always knows what question the user is answering. This means the system\ncan prepare the speech recognizer with a language model tuned to answers for this",
  "441": "24.2\n•\nFRAME BASED DIALOG AGENTS\n433\nquestion, and also makes natural language understanding easier. Most ﬁnite-state\nsystems also allow universal commands that can be said anywhere in the dialog,\nuniversal\nlike help, to give a help message, and start over (or main menu), which returns\nthe user to some speciﬁed main start state,. Nonetheless such a simplistic ﬁnite-state\narchitecture is generally applied only to simple tasks such as entering a credit card\nnumber, or a name and password.\nFor most applications, users need a bit more ﬂexibility. In a travel-planning\nsituation, for example, a user may say a sentence that ﬁlls multiple slots at once:\n(24.3) I want a ﬂight from San Francisco to Denver one way leaving after ﬁve\np.m. on Tuesday.\nOr in cases where there are multiple frames, a user may say something to shift\nframes, for example from airline reservations to reserving a rental car:\n(24.4) I’d like to book a rental car when I arrive at the airport.\nThe standard GUS architecture for frame-based dialog systems, used in various\nforms in modern systems like Apple’s Siri, Amazon’s Alexa, and the Google Assis-\ntant, therefore follows the frame in a more ﬂexible way. The system asks questions\nof the user, ﬁlling any slot that the user speciﬁes, even if a user’s response ﬁlls mul-\ntiple slots or doesn’t answer the question asked. The system simply skips questions\nassociated with slots that are already ﬁlled. Slots may thus be ﬁlled out of sequence.\nThe GUS architecture is thus a kind of mixed initiative, since the user can take at\nmixed initiative\nleast a bit of conversational initiative in choosing what to talk about.\nThe GUS architecture also has condition-action rules attached to slots. For ex-\nample, a rule attached to the DESTINATION slot for the plane booking frame, once\nthe user has speciﬁed the destination, might automatically enter that city as the de-\nfault StayLocation for the related hotel booking frame.\nOnce the system has enough information it performs the necessary action (like\nquerying a database of ﬂights) and returns the result to the user.\nWe mentioned in passing the linked airplane and travel frames. Many domains,\nof which travel is one, require the ability to deal with multiple frames. Besides\nframes for car or hotel reservations, we might need frames with general route in-\nformation (for questions like Which airlines ﬂy from Boston to San Francisco?),\ninformation about airfare practices (for questions like Do I have to stay a speciﬁc\nnumber of days to get a decent airfare?).\nIn addition, once we have given the user options (such as a list of restaurants),\nwe can even have a special frame for ‘asking questions about this list’, whose slot is\nthe particular restaurant the user is asking for more information about, allowing the\nuser to say ‘the second one’ or ‘the Italian one’.\nSince users may switch from frame to frame, the system must be able to disam-\nbiguate which slot of which frame a given input is supposed to ﬁll and then switch\ndialog control to that frame.\nBecause of this need to dynamically switch control, the GUS architecture is a\nproduction rule system. Different types of inputs cause different productions to\nﬁre, each of which can ﬂexibly ﬁll in different frames. The production rules can\nthen switch control according to factors such as the user’s input and some simple\ndialog history like the last question that the system asked.\nCommercial dialog systems provide convenient interfaces or libraries to make\nit easy to build systems with these kinds of ﬁnite-state or production rule systems,\nfor example providing graphical interfaces to allow dialog modules to be chained\ntogether.",
  "442": "434\nCHAPTER 24\n•\nDIALOG SYSTEMS AND CHATBOTS\n24.2.2\nNatural language understanding for ﬁlling slots\nThe goal of the natural language understanding component is to extract three things\nfrom the user’s utterance. The ﬁrst task is domain classiﬁcation: is this user for\ndomain\nclassiﬁcation\nexample talking about airlines, programming an alarm clock, or dealing with their\ncalendar? Of course this 1-of-n classiﬁcation tasks is unnecessary for single-domain\nsystems that are focused on, say, only calendar management, but multi-domain di-\nalog systems are the modern standard. The second is user intent determination:\nintent\ndetermination\nwhat general task or goal is the user trying to accomplish? For example the task\ncould be to Find a Movie, or Show a Flight, or Remove a Calendar Appointment.\nFinally, we need to do slot ﬁlling: extract the particular slots and ﬁllers that the user\nslot ﬁlling\nintends the system to understand from their utterance with respect to their intent.\nFrom a user utterance like this one:\nShow me morning flights from Boston to San Francisco on Tuesday\na system might want to build a representation like:\nDOMAIN:\nAIR-TRAVEL\nINTENT:\nSHOW-FLIGHTS\nORIGIN-CITY:\nBoston\nORIGIN-DATE:\nTuesday\nORIGIN-TIME:\nmorning\nDEST-CITY:\nSan Francisco\nwhile an utterance like\nWake me tomorrow at 6\nshould give an intent like this:\nDOMAIN:\nALARM-CLOCK\nINTENT:\nSET-ALARM\nTIME:\n2017-07-01 0600-0800\nThe task of slot-ﬁlling, and the simpler tasks of domain and intent classiﬁcation,\nare special cases of the task of semantic parsing discussed in Chapter 16. Dialog\nagents can thus extract slots, domains, and intents from user utterances by applying\nany of the semantic parsing approaches discussed in that chapter.\nThe method used in the original GUS system, and still quite common in indus-\ntrial applications, is to use hand-written rules, often as part of the condition-action\nrules attached to slots or concepts.\nFor example we might just deﬁne a regular expression consisting of a set strings\nthat map to the SET-ALARM intent:\nwake me (up) | set (the|an) alarm | get me up\nWe can build more complex automata that instantiate sets of rules like those\ndiscussed in Chapter 17, for example extracting a slot ﬁller by turning a string\nlike Monday at 2pm into an object of type date with parameters (DAY, MONTH,\nYEAR, HOURS, MINUTES).\nRule-based systems can be even implemented with full grammars. Research sys-\ntems like the Phoenix system (Ward and Issar, 1994) consists of large hand-designed\nsemantic grammars with thousands of rules. A semantic grammar is a context-free\nsemantic\ngrammar\ngrammar in which the left-hand side of each rule corresponds to the semantic entities\nbeing expressed (i.e., the slot names) as in the following fragment:",
  "443": "24.2\n•\nFRAME BASED DIALOG AGENTS\n435\nSHOW\n→show me | i want | can i see|...\nDEPART TIME RANGE →(after|around|before) HOUR |\nmorning | afternoon | evening\nHOUR\n→one|two|three|four...|twelve (AMPM)\nFLIGHTS\n→(a) ﬂight | ﬂights\nAMPM\n→am | pm\nORIGIN\n→from CITY\nDESTINATION\n→to CITY\nCITY\n→Boston | San Francisco | Denver | Washington\nSemantic grammars can be parsed by any CFG parsing algorithm (see Chap-\nter 11), resulting in a hierarchical labeling of the input string with semantic node\nlabels, as shown in Fig. 24.10.\nS\nDEPARTTIME\nmorning\nDEPARTDATE\nTuesday\non\nDESTINATION\nFrancisco\nSan\nto\nORIGIN\nBoston\nfrom\nFLIGHTS\nﬂights\nSHOW\nme\nShow\nFigure 24.10\nA semantic grammar parse for a user sentence, using slot names as the internal parse tree nodes.\nWhether regular expressions or parsers are used, it remains only to put the ﬁllers\ninto some sort of canonical form, for example by normalizing dates as discussed in\nChapter 17.\nA number of tricky issues have to be dealt with. One important issue is negation;\nif a user speciﬁes that they “can’t ﬂy Tuesday morning”, or want a meeting ”any time\nexcept Tuesday morning”, a simple system will often incorrectly extract “Tuesday\nmorning” as a user goal, rather than as a negative constraint.\nSpeech recognition errors must also be dealt with. One common trick is to make\nuse of the fact that speech recognizers often return a ranked N-best list of hypoth-\nN-best list\nesized transcriptions rather than just a single candidate transcription. The regular\nexpressions or parsers can simply be run on every sentence in the N-best list, and\nany patterns extracted from any hypothesis can be used.\nAs we saw earlier in discussing information extraction, the rule-based approach\nis very common in industrial applications. It has the advantage of high precision,\nand if the domain is narrow enough and experts are available, can provide sufﬁcient\ncoverage as well. On the other hand, the hand-written rules or grammars can be both\nexpensive and slow to create, and hand-written rules can suffer from recall problems.\nA common alternative is to use supervised machine learning. Assuming a train-\ning set is available which associates each sentence with the correct semantics, we\ncan train a classiﬁer to map from sentences to intents and domains, and a sequence\nmodel to map from sentences to slot ﬁllers.\nFor example given the sentence:\nI want to fly to San\nFrancisco on Monday\nafternoon\nplease\nwe might ﬁrst apply a simple 1-of-N classiﬁer (logistic regression, neural network,\netc.) that uses features of the sentence like word N-grams to determine that the\ndomain is AIRLINE and and the intent is SHOWFLIGHT.\nNext to do slot ﬁlling we might ﬁrst apply a classiﬁer that uses similar features\nof the sentence to predict which slot the user wants to ﬁll. Here in addition to",
  "444": "436\nCHAPTER 24\n•\nDIALOG SYSTEMS AND CHATBOTS\nword unigram, bigram, and trigram features we might use named entity features or\nfeatures indicating that a word is in a particular lexicon (such as a list of cities, or\nairports, or days of the week) and the classifer would return a slot name (in this case\nDESTINATION, DEPARTURE-DAY, and DEPARTURE-TIME). A second classiﬁer can\nthen be used to determine the ﬁller of the named slot, for example a city classiﬁer that\nuses N-grams and lexicon features to determine that the ﬁller of the DESTINATION\nslot is SAN FRANCISCO.\nAn alternative is to use a sequence model (MEMMs, CRFs, RNNs) to directly\nassign a slot label to each word in the sequence, following the method used for other\ninformation extraction models in Chapter 17 (Pieraccini et al. 1991, Raymond and\nRiccardi 2007, Mesnil et al. 2015, Hakkani-T¨ur et al. 2016). Once again we would\nneed a supervised training test, with sentences paired with sequences of IOB labels\nIOB\nlike the following:\nO O\nO\nO\nO\nB-DES I-DES\nO\nB-DEPTIME I-DEPTIME\nO\nI want to fly to San\nFrancisco on Monday\nafternoon\nplease\nRecall from Chapter 17 that in IOB tagging we introduce a tag for the beginning\n(B) and inside (I) of each slot label, and one for tokens outside (O) any slot label.\nThe number of tags is thus 2n+1 tags, where n is the number of slots.\nAny IOB tagger sequence model can then be trained on a training set of such\nlabels. Feature-based sequence models (MEMM, CRF) make use of features like\nword embeddings, word unigrams and bigrams, lexicons (for example lists of city\nnames), and slot transition features (perhaps DESTINATION is more likely to follow\nORIGIN than the other way around) to map a user’s utterance to the slots. An MEMM\n(Chapter 8) for example, combines these features of the input word wi, its neighbors\nwithin l words wi+l\ni−l, and the previous k slot tags si−1\ni−k to compute the most likely slot\nlabel sequence S from the word sequence W as follows:\nˆS = argmax\nS\nP(S|W)\n= argmax\nS\nY\ni\nP(si|wi+l\ni−l,si−1\ni−k)\n= argmax\nS\nY\ni\nexp\n X\ni\nwi fi(si,wi+l\ni−l,si−1\ni−k)\n!\nX\ns′∈slotset\nexp\n X\ni\nwi fi(s′,wi+l\ni−l,ti−1\ni−k )\n!\n(24.5)\nThe Viterbi algorithm is used to decode the best slot sequence ˆS.\nNeural network architectures mostly eschew the feature extraction step, instead\nusing the bi-LSTM architecture introduced in Chapter 9, and applied to IOB-style\nnamed entity tagging in Chapter 17. A typical LSTM-style architecture is shown in\nFig. 24.11. Here the input is a series of words w1...wn, and the output is a series\nof IOB tags s1...sn. In the architecture as introduced in Chapter 17, the input words\nare converted into two embeddings: standard word2vec or GloVe embeddings, and\na character-based embedding, which are concatenated together and passed through a\nbi-LSTM. The output of the bi-LSTM can be passed to a softmax choosing an IOB\ntag for each input word, or to a CRF layer which uses Viterbi to ﬁnd the best series\nof IOB tags. In addition, neural systems can combine the domain-classiﬁcation and\nintent-extraction tasks with slot-ﬁlling simply by adding a domain concatenated with\nan intent as the desired output for the ﬁnal EOS token.",
  "445": "24.2\n•\nFRAME BASED DIALOG AGENTS\n437\nSan\nFrancisco\non\nMonday\nEmbeddings\nLSTM1\nLSTM1\nLSTM1\nLSTM1\nLSTM2\nLSTM2\nLSTM2\nLSTM2\nConcatenation\nRight-to-left LSTM\nLeft-to-right LSTM\nB-DES\nI-DES\nO\nB-DTIME\nCRF Layer\n…\nd+i\n<EOS>\nLSTM1\nLSTM2\nFigure 24.11\nAn LSTM architecture for slot ﬁlling, mapping the words in the input to a\nseries of IOB tags plus a ﬁnal state consisting of a domain concatenated with an intent.\nOnce the sequence labeler has tagged the user utterance, a ﬁller string can be ex-\ntracted for each slot from the tags (e.g., ”San Francisco”), and these word strings\ncan then be normalized to the correct form in the ontology (perhaps the airport\ncode‘SFO’). This normalization can take place by using homonym dictionaries (spec-\nifying, for example, that SF, SFO, and San Francisco are the same place).\nIn industrial contexts, machine learning-based systems for slot-ﬁlling are often\nbootstrapped from rule-based systems in a semi-supervised learning manner. A rule-\nbased system is ﬁrst built for the domain, and a test-set is carefully labeled. As new\nuser utterances come in, they are paired with the labeling provided by the rule-based\nsystem to create training tuples. A classiﬁer can then be trained on these tuples, us-\ning the test-set to test the performance of the classiﬁer against the rule-based system.\nSome heuristics can be used to eliminate errorful training tuples, with the goal of in-\ncreasing precision. As sufﬁcient training samples become available the resulting\nclassiﬁer can often outperform the original rule-based system (Suendermann et al.,\n2009), although rule-based systems may still remain higher-precision for dealing\nwith complex cases like negation.\n24.2.3\nEvaluating Slot Filling\nAn intrinsic error metric for natural language understanding systems for slot ﬁlling\nis the Slot Error Rate for each sentence:\nSlot Error Rate for a Sentence = # of inserted/deleted/subsituted slots\n# of total reference slots for sentence\n(24.6)\nConsider a system faced with the following sentence:\n(24.7) Make an appointment with Chris at 10:30 in Gates 104\nwhich extracted the following candidate slot structure:\nSlot\nFiller\nPERSON Chris\nTIME\n11:30 a.m.\nROOM\nGates 104\nHere the slot error rate is 1/3, since the TIME is wrong. Instead of error rate, slot\nprecision, recall, and F-score can also be used.\nA perhaps more important, although less ﬁne-grained, measure of success is an\nextrinsic metric like task error rate. In this case, the task error rate would quantify\nhow often the correct meeting was added to the calendar at the end of the interaction.",
  "446": "438\nCHAPTER 24\n•\nDIALOG SYSTEMS AND CHATBOTS\n24.2.4\nOther components of frame-based dialog\nWe’ve focused on the natural language understanding component that is the core of\nframe-based systems, but here we also brieﬂy mention other modules.\nThe ASR (automatic speech recognition) component takes audio input from a\nphone or other device and outputs a transcribed string of words, as discussed in\nChapter 26. Various aspects of the ASR system may be optimized speciﬁcally for\nuse in conversational agents.\nBecause what the user says to the system is related to what the system has just\nsaid, language models in conversational agent depend on the dialog state. For ex-\nample, if the system has just asked the user “What city are you departing from?”,\nthe ASR language model can be constrained to just model answers to that one ques-\ntion. This can be done by training an N-gram language model on answers to this\nquestion. Alternatively a ﬁnite-state or context-free grammar can be hand written\nto recognize only answers to this question, perhaps consisting only of city names or\nperhaps sentences of the form ‘I want to (leave|depart) from [CITYNAME]’. Indeed,\nmany simple commercial dialog systems use only non-probabilistic language mod-\nels based on hand-written ﬁnite-state grammars that specify all possible responses\nthat the system understands. We give an example of such a hand-written grammar\nfor a VoiceXML system in Section 24.3.\nA language model that is completely dependent on dialog state is called a re-\nstrictive grammar, and can be used to constrain the user to only respond to the\nrestrictive\ngrammar\nsystem’s last utterance. When the system wants to allow the user more options, it\nmight mix this state-speciﬁc language model with a more general language model.\nThe language generation module of any dialog system produces the utterances\nlanguage\ngeneration\nthat the system says to the user. Frame-based systems tend to use template-based\ngeneration, in which all or most of the words in the sentence to be uttered to the\ntemplate-based\ngeneration\nuser are prespeciﬁed by the dialog designer. Sentences created by these templates\nare often called prompts. Templates might be completely ﬁxed (like ‘Hello, how\nprompt\ncan I help you?’), or can include some variables that are ﬁlled in by the generator,\nas in the following:\nWhat time do you want to leave CITY-ORIG?\nWill you return to CITY-ORIG from CITY-DEST?\nThese sentences are then passed to the TTS (text-to-speech) component (see\nChapter 26). More sophisticated statistical generation strategies will be discussed in\nSection 25.5 of Chapter 25.\n24.3\nVoiceXML\nThere are many commercial systems that allow developers to implement frame-\nbased dialog systems, including the user-deﬁnable skills in Amazon Alexa or the\nactions in Google Assistant. These systems provide libraries for deﬁning the rules\nfor detecting user intents and ﬁlling in slots, and for expressing the architecture for\ncontrolling which frames and actions the system should take at which times.\nInstead of focusing on a commercial engine, we introduce here a simple declar-\native formalism that has similar capabilities to each of them: VoiceXML, the Voice\nVoiceXML\nExtensible Markup Language (http://www.voicexml.org/), an XML-based di-\nalog design language for creating simple frame-based dialogs. Although VoiceXML\nis simpler than a full commercial frame-based system (it’s deterministic, and hence",
  "447": "24.3\n•\nVOICEXML\n439\nonly allows non-probabilistic grammar-based language models and rule-based se-\nmantic parsers), it’s still a handy way to get a hands-on grasp of frame-based dialog\nsystem design.\nA VoiceXML document contains a set of dialogs, each a menu or a form. A form\nis a frame, whose slots are called ﬁelds. The VoiceXML document in Fig. 24.12\nshows three ﬁelds for specifying a ﬂight’s origin, destination, and date. Each ﬁeld\nhas a variable name (e.g., origin) that stores the user response, a prompt, (e.g.,\nprompt\nWhich city do you want to leave from), and a grammar that is passed to the speech\nrecognition engine to specify what is allowed to be recognized. The grammar for\nthe ﬁrst ﬁeld in Fig. 24.12 allows the three phrases san francisco, barcelona, and\nnew york. The VoiceXML interpreter walks through a form in document order,\nrepeatedly selecting each item in the form, and each ﬁeld in order.\n<noinput>\nI’m sorry, I didn’t hear you.\n<reprompt/>\n</noinput>\n<nomatch>\nI’m sorry, I didn’t understand that.\n<reprompt/>\n</nomatch>\n<form>\n<block>\nWelcome to the air travel consultant.\n</block>\n<field name=\"origin\">\n<prompt>\nWhich city do you want to leave from?\n</prompt>\n<grammar type=\"application/x=nuance-gsl\">\n[(san francisco) barcelona (new york)]\n</grammar>\n<filled>\n<prompt>\nOK, from <value expr=\"origin\"/>\n</prompt>\n</filled>\n</field>\n<field name=\"destination\">\n<prompt>\nAnd which city do you want to go to?\n</prompt>\n<grammar type=\"application/x=nuance-gsl\">\n[(san francisco) barcelona (new york)]\n</grammar>\n<filled>\n<prompt>\nOK, to <value expr=\"destination\"/>\n</prompt>\n</filled>\n</field>\n<field name=\"departdate\" type=\"date\">\n<prompt>\nAnd what date do you want to leave?\n</prompt>\n<filled>\n<prompt>\nOK, on <value expr=\"departdate\"/>\n</prompt>\n</filled>\n</field>\n<block>\n<prompt> OK, I have you are departing from\n<value expr=\"origin\"/>\nto <value expr=\"destination\"/> on <value expr=\"departdate\"/>\n</prompt>\nsend the info to book a flight...\n</block>\n</form>\nFigure 24.12\nA VoiceXML script for a form with three ﬁelds, which conﬁrms each ﬁeld\nand handles the noinput and nomatch situations.\nThe prologue of the example shows two global defaults for error handling. If the\nuser doesn’t answer after a prompt (i.e., silence exceeds a timeout threshold), the\nVoiceXML interpreter will play the <noinput> prompt. If the user says something\nthat doesn’t match the grammar for that ﬁeld, the VoiceXML interpreter will play the\n<nomatch> prompt. VoiceXML provides a <reprompt/> command, which repeats\nthe prompt for whatever ﬁeld caused the error.\nThe <filled> tag for a ﬁeld is executed by the interpreter as soon as the ﬁeld\nhas been ﬁlled by the user. Here, this feature is used to conﬁrm the user’s input.\nVoiceXML 2.0 speciﬁes seven built-in grammar types: boolean, currency,\ndate, digits, number, phone, and time. By specifying the departdate ﬁeld as",
  "448": "440\nCHAPTER 24\n•\nDIALOG SYSTEMS AND CHATBOTS\ntype date, a date-speciﬁc language model will be passed to the speech recognizer.\n<noinput>\nI’m sorry, I didn’t hear you.\n<reprompt/>\n</noinput>\n<nomatch> I’m sorry, I didn’t understand that.\n<reprompt/> </nomatch>\n<form>\n<grammar type=\"application/x=nuance-gsl\">\n<![CDATA[\nFlight (\n?[\n(i [wanna (want to)] [fly go])\n(i’d like to [fly go])\n([(i wanna)(i’d like a)] flight)\n]\n[\n( [from leaving departing] City:x) {<origin $x>}\n( [(?going to)(arriving in)] City:x) {<destination $x>}\n( [from leaving departing] City:x\n[(?going to)(arriving in)] City:y) {<origin $x> <destination $y>}\n]\n?please\n)\nCity [ [(san francisco) (s f o)] {return( \"san francisco, california\")}\n[(denver) (d e n)] {return( \"denver, colorado\")}\n[(seattle) (s t x)] {return( \"seattle, washington\")}\n]\n]]> </grammar>\n<initial name=\"init\">\n<prompt> Welcome to the consultant. What are your travel plans?\n</prompt>\n</initial>\n<field name=\"origin\">\n<prompt> Which city do you want to leave from?\n</prompt>\n<filled>\n<prompt> OK, from <value expr=\"origin\"/> </prompt>\n</filled>\n</field>\n<field name=\"destination\">\n<prompt> And which city do you want to go to?\n</prompt>\n<filled>\n<prompt> OK, to <value expr=\"destination\"/> </prompt>\n</filled>\n</field>\n<block>\n<prompt> OK, I have you are departing from\n<value expr=\"origin\"/>\nto\n<value expr=\"destination\"/>.\n</prompt>\nsend the info to book a flight...\n</block>\n</form>\nFigure 24.13\nA mixed-initiative VoiceXML dialog. The grammar allows sentences that\nspecify the origin or destination cities or both. The user can respond to the initial prompt by\nspecifying origin city, destination city, or both.\nFigure 24.13 gives a mixed initiative example, allowing the user to answer ques-\ntions in any order or even ﬁll in multiple slots at once. The VoiceXML interpreter\nhas a guard condition on ﬁelds, a test that keeps a ﬁeld from being visited; the default\ntest skips a ﬁeld if its variable is already set.\nFigure 24.13 also shows a more complex CFG grammar with two rewrite rules,\nFlight and City. The Nuance GSL grammar formalism uses parentheses () to\nmean concatenation and square brackets [] to mean disjunction. Thus, a rule like\n(24.8) means that Wantsentence can be expanded as i want to fly or i want\nto go, and Airports can be expanded as san francisco or denver.\n(24.8)\nWantsentence (i want to [fly go])\nAirports [(san francisco) denver]\nVoiceXML grammars allow semantic attachments, such as the text string (\"denver,\ncolorado\") the return for the City rule, or a slot/ﬁller , like the attachments for the\nFlight rule which ﬁlls the slot (<origin> or <destination> or both) with the\nvalue passed up in the variable x from the City rule.",
  "449": "24.4\n•\nEVALUATING DIALOG SYSTEMS\n441\nTTS Performance\nWas the system easy to understand ?\nASR Performance\nDid the system understand what you said?\nTask Ease\nWas it easy to ﬁnd the message/ﬂight/train you wanted?\nInteraction Pace\nWas the pace of interaction with the system appropriate?\nUser Expertise\nDid you know what you could say at each point?\nSystem Response\nHow often was the system sluggish and slow to reply to you?\nExpected Behavior\nDid the system work the way you expected it to?\nFuture Use\nDo you think you’d use the system in the future?\nFigure 24.14\nUser satisfaction survey, adapted from Walker et al. (2001).\nBecause Fig. 24.13 is a mixed-initiative grammar, the grammar has to be ap-\nplicable to any of the ﬁelds. This is done by making the expansion for Flight a\ndisjunction; note that it allows the user to specify only the origin city, the destination\ncity, or both.\n24.4\nEvaluating Dialog Systems\nEvaluation is crucial in dialog system design. If the task is unambiguous, we can\nsimply measure absolute task success (did the system book the right plane ﬂight, or\nput the right event on the calendar).\nTo get a more ﬁne-grained idea of user happiness, we can compute a user sat-\nisfaction rating, having users interact with a dialog system to perform a task and\nthen having them complete a questionnaire. For example, Fig. 24.14 shows sample\nmultiple-choice questions (Walker et al., 2001); responses are mapped into the range\nof 1 to 5, and then averaged over all questions to get a total user satisfaction rating.\nIt is often economically infeasible to run complete user satisfaction studies after\nevery change in a system. For this reason, it is useful to have performance evaluation\nheuristics that correlate well with human satisfaction. A number of such factors and\nheuristics have been studied, often grouped into two kinds of criteria: how well the\nsystem allows users to accomplish their goals (maximizing task success) the least\nproblems (minimizing costs) :\nTask completion success:\nTask success can be measured by evaluating the cor-\nrectness of the total solution. For a frame-based architecture, this might be the per-\ncentage of slots that were ﬁlled with the correct values or the percentage of subtasks\nthat were completed. Interestingly, sometimes the user’s perception of whether they\ncompleted the task is a better predictor of user satisfaction than the actual task com-\npletion success. (Walker et al., 2001).\nEfﬁciency cost:\nEfﬁciency costs are measures of the system’s efﬁciency at helping\nusers. This can be measured by the total elapsed time for the dialog in seconds, the\nnumber of total turns or of system turns, or the total number of queries (Polifroni\net al., 1992). Other metrics include the number of system non-responses and the\n“turn correction ratio”: the number of system or user turns that were used solely\nto correct errors divided by the total number of turns (Danieli and Gerbino 1995,\nHirschman and Pao 1993).\nQuality cost:\nQuality cost measures other aspects of the interactions that affect\nusers’ perception of the system. One such measure is the number of times the\nASR system failed to return any sentence, or the number of ASR rejection prompts.\nSimilar metrics include the number of times the user had to barge-in (interrupt the",
  "450": "442\nCHAPTER 24\n•\nDIALOG SYSTEMS AND CHATBOTS\nsystem), or the number of time-out prompts played when the user didn’t respond\nquickly enough. Other quality metrics focus on how well the system understood and\nresponded to the user. The most important is the slot error rate described above,\nbut other components include the inappropriateness (verbose or ambiguous) of the\nsystem’s questions, answers, and error messages or the correctness of each question,\nanswer, or error message (Zue et al. 1989, Polifroni et al. 1992).\n24.5\nDialog System Design\nThe user plays a more important role in dialog systems than in most other areas of\nspeech and language processing, and thus this area of language processing is the one\nthat is most closely linked with the ﬁeld of Human-Computer Interaction (HCI).\nHow does a dialog system developer choose dialog strategies, prompts, error\nmessages, and so on? This process is often called voice user interface design, and\nvoice user\ninterface\ngenerally follows the user-centered design principles of Gould and Lewis (1985):\n1. Study the user and task:\nUnderstand the potential users and the nature of the\ntask by interviews with users, investigation of similar systems, and study of related\nhuman-human dialogs.\n2. Build simulations and prototypes:\nA crucial tool in building dialog systems is\nthe Wizard-of-Oz system. In wizard systems, the users interact with what they think\nWizard-of-Oz\nsystem\nis a software agent but is in fact a human “wizard” disguised by a software interface\n(Gould et al. 1983, Good et al. 1984, Fraser and Gilbert 1991). The name comes\nfrom the children’s book The Wizard of Oz (Baum, 1900), in which the Wizard\nturned out to be just a simulation controlled by a man behind a curtain or screen.\nA Wizard-of-Oz system can be used to\ntest out an architecture before implementa-\ntion; only the interface software and databases\nneed to be in place. The wizard gets input\nfrom the user, has a graphical interface to a\ndatabase to run sample queries based on the\nuser utterance, and then has a way to output\nsentences, either by typing them or by some\ncombination of selecting from a menu and\ntyping. The wizard’s linguistic output can be\ndisguised by a text-to-speech system or, more\nfrequently, by using text-only interactions.\nThe results of a wizard-of-oz system can\nalso be used as training data to training a pilot\ndialog system. While wizard-of-oz systems\nare very commonly used, they are not a per-\nfect simulation; it is difﬁcult for the wizard to\nexactly simulate the errors, limitations, or time constraints of a real system; results\nof wizard studies are thus somewhat idealized, but still can provide a useful ﬁrst idea\nof the domain issues.\n3. Iteratively test the design on users:\nAn iterative design cycle with embedded\nuser testing is essential in system design (Nielsen 1992, Cole et al. 1997, Yankelovich\net al. 1995, Landauer 1995). For example in a famous anecdote in dialog design his-",
  "451": "24.5\n•\nDIALOG SYSTEM DESIGN\n443\ntory , an early dialog system required the user to press a key to interrupt the system\nStifelman et al. (1993). But user testing showed users barged in, which led to a re-\ndesign of the system to recognize overlapped speech. The iterative method is also\nimportant for designing prompts that cause the user to respond in normative ways.\nThere are a number of good books on conversational interface design (Cohen\net al. 2004, Harris 2005, Pearl 2017).\n24.5.1\nEthical Issues in Dialog System Design\nEthical issues have long been understood to be crucial in the design of artiﬁcial\nagents, predating the conversational agent itself. Mary Shelley’s classic discussion\nof the problems of creating agents without a consideration of ethical and humanistic\nconcerns lies at the heart of her novel Frankenstein. One\nimportant ethical issue has to do with bias. As we dis-\ncussed in Section 6.11, machine learning systems of any\nkind tend to replicate biases that occurred in the train-\ning data. This is especially relevant for chatbots, since\nboth IR-based and neural transduction architectures are\ndesigned to respond by approximating the responses in\nthe training data.\nA well-publicized instance of this occurred with Mi-\ncrosoft’s 2016 Tay chatbot, which was taken ofﬂine 16\nTay\nhours after it went live, when it began posting messages\nwith racial slurs, conspiracy theories, and personal attacks. Tay had learned these\nbiases and actions from its training data, including from users who seemed to be\npurposely teaching it to repeat this kind of language (Neff and Nagy, 2016).\nHenderson et al. (2017) examined some standard dialog datasets (drawn from\nTwitter, Reddit, or movie dialogs) used to train corpus-based chatbots, measuring\nbias (Hutto et al., 2015) and offensive and hate speech (Davidson et al., 2017). They\nfound examples of hate speech, offensive language, and bias, especially in corpora\ndrawn from social media like Twitter and Reddit, both in the original training data,\nand in the output of chatbots trained on the data.\nAnother important ethical issue is privacy. Already in the ﬁrst days of ELIZA,\nWeizenbaum pointed out the privacy implications of people’s revelations to the chat-\nbot. Henderson et al. (2017) point out that home dialogue agents may accidentally\nrecord a user revealing private information (e.g. “Computer, turn on the lights –an-\nswers the phone –Hi, yes, my password is...”), which may then be used to train a\nconversational model. They showed that when a seq2seq dialog model trained on a\nstandard corpus augmented with training keypairs representing private data (e.g. the\nkeyphrase ”social security number” followed by a number), an adversary who gave\nthe keyphrase was able to recover the secret information with nearly 100% accuracy.\nFinally, chatbots raise important issues of gender equality. Current chatbots are\noverwhelmingly given female names, likely perpetuating the stereotype of a sub-\nservient female servant (Paolino, 2017). And when users use sexually harassing\nlanguage, most commercial chatbots evade or give positive responses rather than\nresponding in clear negative ways (Fessler, 2017).",
  "452": "444\nCHAPTER 24\n•\nDIALOG SYSTEMS AND CHATBOTS\n24.6\nSummary\nConversational agents are a crucial speech and language processing application\nthat are already widely used commercially.\n• Chatbots are conversational agents designed to mimic the appearance of in-\nformal human conversation. Rule-based chatbots like ELIZA and its modern\ndescendants use rules to map user sentences into system responses. Corpus-\nbased chatbots mine logs of human conversation to learn to automatically map\nuser sentences into system responses.\n• For task-based dialog, most commercial dialog systems use the GUS or frame-\nbased architecture, in which the designer speciﬁes a domain ontology, a set\nof frames of information that the system is designed to acquire from the user,\neach consisting of slots with typed ﬁllers\n• A number of commercial systems allow developers to implement simple frame-\nbased dialog systems, such as the user-deﬁnable skills in Amazon Alexa or the\nactions in Google Assistant. VoiceXML is a simple declarative language that\nhas similar capabilities to each of them for specifying deterministic frame-\nbased dialog systems.\n• Dialog systems are a kind of human-computer interaction, and general HCI\nprinciples apply in their design, including the role of the user, simulations\nsuch as Wizard-of-Oz systems, and the importance of iterative design and\ntesting on real users.\nBibliographical and Historical Notes\nThe earliest conversational systems were chatbots like ELIZA (Weizenbaum, 1966)\nand PARRY (Colby et al., 1971). ELIZA had a widespread inﬂuence on popular\nperceptions of artiﬁcial intelligence, and brought up some of the ﬁrst ethical ques-\ntions in natural language processing —such as the issues of privacy we discussed\nabove as well the role of algorithms in decision-making— leading its creator Joseph\nWeizenbaum to ﬁght for social responsibility in AI and computer science in general.\nAnother early system, the GUS system (Bobrow et al., 1977) had by the late\n1970s established the main frame-based paradigm that became the dominant indus-\ntrial paradigm for dialog systems for over 30 years.\nIn the 1990s, stochastic models that had ﬁrst been applied to natural language\nunderstanding began to be applied to dialog slot ﬁlling (Miller et al. 1994, Pieraccini\net al. 1991).\nBy around 2010 the GUS architecture ﬁnally began to be widely used commer-\ncially in phone-based dialog systems like Apple’s SIRI (Bellegarda, 2013) and other\ndigital assistants.\nThe rise of the web and online chatbots brought new interest in chatbots and gave\nrise to corpus-based chatbot architectures around the turn of the century, ﬁrst using\ninformation retrieval models and then in the 2010s, after the rise of deep learning,\nwith sequence-to-sequence models.",
  "453": "EXERCISES\n445\nExercises\n24.1 Write a ﬁnite-state automaton for a dialogue manager for checking your bank\nbalance and withdrawing money at an automated teller machine.\n24.2 A dispreferred response is a response that has the potential to make a person\ndispreferred\nresponse\nuncomfortable or embarrassed in the conversational context; the most com-\nmon example dispreferred responses is turning down a request. People signal\ntheir discomfort with having to say no with surface cues (like the word well),\nor via signiﬁcant silence. Try to notice the next time you or someone else\nutters a dispreferred response, and write down the utterance. What are some\nother cues in the response that a system might use to detect a dispreferred\nresponse? Consider non-verbal cues like eye gaze and body gestures.\n24.3 When asked a question to which they aren’t sure they know the answer, peo-\nple display their lack of conﬁdence by cues that resemble other dispreferred\nresponses. Try to notice some unsure answers to questions. What are some\nof the cues? If you have trouble doing this, read Smith and Clark (1993) and\nlisten speciﬁcally for the cues they mention.\n24.4 Build a VoiceXML dialogue system for giving the current time around the\nworld. The system should ask the user for a city and a time format (24 hour,\netc) and should return the current time, properly dealing with time zones.\n24.5 Implement a small air-travel help system based on text input. Your system\nshould get constraints from users about a particular ﬂight that they want to\ntake, expressed in natural language, and display possible ﬂights on a screen.\nMake simplifying assumptions. You may build in a simple ﬂight database or\nyou may use a ﬂight information system on the Web as your backend.\n24.6 Augment your previous system to work with speech input through VoiceXML.\n(Or alternatively, describe the user interface changes you would have to make\nfor it to work via speech over the phone.) What were the major differences?\n24.7 Design a simple dialogue system for checking your email over the telephone.\nImplement in VoiceXML.\n24.8 Test your email-reading system on some potential users. Choose some of the\nmetrics described in Section 24.4 and evaluate your system.",
  "454": "446\nCHAPTER 25\n•\nADVANCED DIALOG SYSTEMS\nCHAPTER\n25\nAdvanced Dialog Systems\nA famous burlesque routine from the turn of the last century plays on the difﬁculty\nof conversational understanding by inventing a baseball team whose members have\nconfusing names:\nC: I want you to tell me the names of the fellows on the St. Louis team.\nA: I’m telling you. Who’s on ﬁrst, What’s on second, I Don’t Know is on third.\nC: You know the fellows’ names?\nA: Yes.\nC: Well, then, who’s playing ﬁrst?\nA: Yes.\nC: I mean the fellow’s name on ﬁrst.\nA: Who.\nC: The guy on ﬁrst base.\nA: Who is on ﬁrst.\nC: Well what are you askin’ me for?\nA: I’m not asking you – I’m telling you. Who is on ﬁrst.\nWho’s on First – Bud Abbott and Lou Costello’s version of an\nold burlesque standard.\nOf course outrageous names of baseball players are not a normal source of dif-\nﬁculty in conversation. What this famous comic conversation is pointing out is that\nunderstanding and participating in dialog requires knowing whether the person you\nare talking to is making a statement or asking a question. Asking questions, giving\norders, or making informational statements are things that people do in conversation,\nyet dealing with these kind of actions in dialog—what we will call dialog acts—is\nsomething that the GUS-style frame-based dialog systems of Chapter 24 are com-\npletely incapable of.\nIn this chapter we describe the dialog-state architecture, also called the belief-\nstate or information-state architecture. Like GUS systems, these agents ﬁll slots,\nbut they are also capable of understanding and generating such dialog acts, actions\nlike asking a question, making a proposal, rejecting a suggestion, or acknowledging\nan utterance and they can incorporate this knowledge into a richer model of the state\nof the dialog at any point.\nLike the GUS systems, the dialog-state architecture is based on ﬁlling in the slots\nof frames, and so dialog-state systems have an NLU component to determine the\nspeciﬁc slots and ﬁllers expressed in a user’s sentence. Systems must additionally\ndetermine what dialog act the user was making, for example to track whether a user\nis asking a question. And the system must take into account the dialog context (what\nthe system just said, and all the constraints the user has made in the past).\nFurthermore, the dialog-state architecture has a different way of deciding what to\nsay next than the GUS systems. Simple frame-based systems often just continuously\nask questions corresponding to unﬁlled slots and then report back the results of some\ndatabase query. But in natural dialog users sometimes take the initiative, such as\nasking questions of the system; alternatively, the system may not understand what",
  "455": "25.1\n•\nDIALOG ACTS\n447\nthe user said, and may need to ask clariﬁcation questions. The system needs a dialog\npolicy to decide what to say (when to answer the user’s questions, when to instead\nask the user a clariﬁcation question, make a suggestion, and so on).\nFigure 25.1 shows a typical architecture for a dialog-state system. It has six\ncomponents. As with the GUS-style frame-based systems, the speech recognition\nand understanding components extract meaning from the input, and the generation\nand TTS components map from meaning to speech. The parts that are different than\nthe simple GUS system are the dialog state tracker which maintains the current\nstate of the dialog (which include the user’s most recent dialog act, plus the entire\nset of slot-ﬁller constraints the user has expressed so far) and the dialog policy,\nwhich decides what the system should do or say next.\nLEAVING FROM DOWNTOWN\nLEAVING AT ONE P M\nARRIVING AT ONE P M\n0.6\n0.2\n0.1\n{ from: downtown }\n{ depart-time: 1300 }\n{ arrive-time: 1300 }\n0.5\n0.3\n0.1\nfrom:        CMU\nto:          airport\ndepart-time: 1300\nconfirmed:   no\nscore:       0.10\nfrom:        CMU\nto:          airport\ndepart-time: 1300\nconfirmed:   no\nscore:       0.15\nfrom:        downtown\nto:          airport\ndepart-time: --\nconfirmed:   no\nscore:       0.65\nAutomatic Speech \nRecognition (ASR)\nSpoken Language \nUnderstanding (SLU)\nDialog State \nTracker (DST)\nDialog Policy\nact:  confirm\nfrom: downtown\nFROM DOWNTOWN, \nIS THAT RIGHT?\nNatural Language \nGeneration (NLG)\nText to Speech (TTS)\nFigure 25.1\nArchitecture of a dialog-state system for task-oriented dialog from Williams et al. (2016).\nAs of the time of this writing, no commercial system uses a full dialog-state ar-\nchitecture, but some aspects of this architecture are beginning to appear in industrial\nsystems, and there are a wide variety of these systems in research labs.\n25.1\nDialog Acts\nA key insight into conversation—due originally to the philosopher Wittgenstein\n(1953) but worked out more fully by Austin (1962)—is that each utterance in a\ndialog is a kind of action being performed by the speaker. These actions are com-\nmonly called speech acts; here’s one taxonomy consisting of 4 major classes (Bach\nspeech acts\nand Harnish, 1979):",
  "456": "448\nCHAPTER 25\n•\nADVANCED DIALOG SYSTEMS\nConstatives:\ncommitting the speaker to something’s being the case (answering, claiming,\nconﬁrming, denying, disagreeing, stating)\nDirectives:\nattempts by the speaker to get the addressee to do something (advising, ask-\ning, forbidding, inviting, ordering, requesting)\nCommissives:\ncommitting the speaker to some future course of action (promising, planning,\nvowing, betting, opposing)\nAcknowledgments: express the speaker’s attitude regarding the hearer with respect to some so-\ncial action (apologizing, greeting, thanking, accepting an acknowledgment)\nA user ordering a dialog system to do something (‘Turn up the music’) is issuing\na DIRECTIVE. A user asking a question to which the system is expected to answer\nis also issuing a DIRECTIVE: in a sense the user is commanding the system to an-\nswer (‘What’s the address of the second restaurant’). By contrast, a user stating a\nconstraint (‘I am ﬂying on Tuesday’) is issuing a CONSTATIVE. A user thanking the\nsystem is issuing an ACKNOWLEDGMENT. The dialog act expresses an important\ncomponent of the intention of the speaker (or writer) in saying what they said.\nWhile this idea of speech acts is powerful, modern systems expand these early\ntaxonomies of speech acts to better describe actual conversations. This is because a\ndialog is not a series of unrelated independent speech acts, but rather a collective act\nperformed by the speaker and the hearer. In performing this joint action the speaker\nand hearer must constantly establish common ground (Stalnaker, 1978), the set of\ncommon\nground\nthings that are mutually believed by both speakers.\nThe need to achieve common ground means that the hearer must ground the\ngrounding\nspeaker’s utterances. To ground means to acknowledge, to make it clear that the\nhearer has understood the speaker’s meaning and intention. People need closure or\ngrounding for non-linguistic actions as well. For example, why does a well-designed\nelevator button light up when it’s pressed? Because this indicates to the elevator\ntraveler that she has successfully called the elevator. Clark (1996) phrases this need\nfor closure as follows, after Norman (1988):\nPrinciple of closure. Agents performing an action require evidence, sufﬁcient\nfor current purposes, that they have succeeded in performing it.\nGrounding is also important when the hearer needs to indicate that the speaker\nhas not succeeded. If the hearer has problems in understanding, she must indicate\nthese problems to the speaker, again so that mutual understanding can eventually be\nachieved.\nClark and Schaefer (1989) point out a continuum of methods the hearer B can\nuse to ground the speaker A’s utterance, ordered from weakest to strongest:\nContinued attention:\nB shows she is continuing to attend and therefore remains satisﬁed with\nA’s presentation.\nNext contribution:\nB starts in on the next relevant contribution.\nAcknowledgment:\nB nods or says a continuer like uh-huh, yeah, or the like, or an assess-\nment like that’s great.\nDemonstration:\nB demonstrates all or part of what she has understood A to mean, for\nexample, by reformulating (paraphrasing) A’s utterance or by collabo-\nrative completion of A’s utterance.\nDisplay:\nB displays verbatim all or part of A’s presentation.\nLet’s look for examples of grounding in a conversation between a human travel\nagent and a human client in Fig. 25.2.",
  "457": "25.1\n•\nDIALOG ACTS\n449\nC1:\n...I need to travel in May.\nA1:\nAnd, what day in May did you want to travel?\nC2:\nOK uh I need to be there for a meeting that’s from the 12th to the 15th.\nA2:\nAnd you’re ﬂying into what city?\nC3:\nSeattle.\nA3:\nAnd what time would you like to leave Pittsburgh?\nC4:\nUh hmm I don’t think there’s many options for non-stop.\nA4:\nRight. There’s three non-stops today.\nC5:\nWhat are they?\nA5:\nThe ﬁrst one departs PGH at 10:00am arrives Seattle at 12:05 their time. The\nsecond ﬂight departs PGH at 5:55pm, arrives Seattle at 8pm. And the last\nﬂight departs PGH at 8:15pm arrives Seattle at 10:28pm.\nC6:\nOK I’ll take the 5ish ﬂight on the night before on the 11th.\nA6:\nOn the 11th? OK. Departing at 5:55pm arrives Seattle at 8pm, U.S. Air ﬂight\n115.\nC7:\nOK.\nFigure 25.2\nPart of a conversation between a travel agent (A) and client (C).\nUtterance A1 shows the strongest form of grounding, in which the hearer dis-\nplays understanding by repeating verbatim part of the speaker’s words: in May.1\nThis particular fragment doesn’t have an example of an acknowledgment, but\nthere’s an example in another fragment:\nC: He wants to ﬂy from Boston to Baltimore\nA: Uh huh\nThe word uh-huh here is a backchannel, also called a continuer or an acknowl-\nbackchannel\ncontinuer\nedgment token. A backchannel is a (short) optional utterance that acknowledges the\ncontent of the utterance of the other and that doesn’t require an acknowledgment by\nthe other (Yngve 1970, Jefferson 1984, Schegloff 1982, Ward and Tsukahara 2000).\nThe third grounding method is to start in on the relevant next contribution, for\nexample in Fig. 25.2, where the speaker asks a question (A2) and the hearer (C3)\nanswers it.\nIn a more subtle act of grounding, the speaker can combine this method with the\nprevious one. For example, notice that whenever the client answers a question, the\nagent begins the next question with And. The And indicates to the client that the\nagent has successfully understood the answer to the last question.\nSpeech acts are important for practical dialog systems, which need to distin-\nguish a statement from a directive, and which must distinguish (among the many\nkinds of directives) an order to do something from a question asking for informa-\ntion. Grounding is also crucial in dialog systems. Consider the unnaturalness of this\nexample from Cohen et al. (2004):\n(25.1) System: Did you want to review some more of your personal proﬁle?\nCaller: No.\nSystem: What’s next?\nWithout an acknowledgment, the caller doesn’t know that the system has under-\nstood her ‘No’. The use of Okay below adds grounding, making (25.2) a much more\nnatural response than (25.1):\n1\nAs Ken Forbus points out (p.c.), although verbatim repetition may be the strongest form of grounding\nfor humans, it’s possible that demonstration (e.g., reformulating) might be more powerful for a conversa-\ntional agent, since it demonstrates understanding in a way that verbatim repetition does not.",
  "458": "450\nCHAPTER 25\n•\nADVANCED DIALOG SYSTEMS\n(25.2) System: Did you want to review some more of your personal proﬁle?\nCaller: No.\nSystem: Okay, what’s next?\nTag\nExample\nTHANK\nThanks\nGREET\nHello Dan\nINTRODUCE\nIt’s me again\nBYE\nAlright bye\nREQUEST-COMMENT\nHow does that look?\nSUGGEST\nfrom thirteenth through seventeenth June\nREJECT\nNo Friday I’m booked all day\nACCEPT\nSaturday sounds ﬁne\nREQUEST-SUGGEST\nWhat is a good day of the week for you?\nINIT\nI wanted to make an appointment with you\nGIVE REASON\nBecause I have meetings all afternoon\nFEEDBACK\nOkay\nDELIBERATE\nLet me check my calendar here\nCONFIRM\nOkay, that would be wonderful\nCLARIFY\nOkay, do you mean Tuesday the 23rd?\nDIGRESS\n[we could meet for lunch] and eat lots of ice cream\nMOTIVATE\nWe should go to visit our subsidiary in Munich\nGARBAGE\nOops, I-\nFigure 25.3\nThe 18 high-level dialog acts for a meeting scheduling task, from the\nVerbmobil-1 system (Jekat et al., 1995).\nThe ideas of speech acts and grounding are combined in a single kind of action\ncalled a dialog act, a tag which represents the interactive function of the sentence\ndialog act\nbeing tagged. Different types of dialog systems require labeling different kinds of\nacts, and so the tagset—deﬁning what a dialog act is exactly— tends to be designed\nfor particular tasks.\nFigure 25.3 shows a domain-speciﬁc tagset for the task of two people scheduling\nmeetings. It has tags speciﬁc to the domain of scheduling, such as SUGGEST, used\nfor the proposal of a particular date to meet, and ACCEPT and REJECT, used for\nacceptance or rejection of a proposal for a date, but also tags that have more general\nfunction, like CLARIFY, used to request a user to clarify an ambiguous proposal.\nTag\nSys User\nDescription\nHELLO(a = x,b = y,...)\n✓\n✓\nOpen a dialog and give info a = x,b = y,...\nINFORM(a = x,b = y,...)\n✓\n✓\nGive info a = x,b = y,...\nREQUEST(a,b = x,...)\n✓\n✓\nRequest value for a given b = x,...\nREQALTS(a = x,...)\nχ\n✓\nRequest alternative with a = x,...\nCONFIRM(a = x,b = y,...) ✓\n✓\nExplicitly conﬁrm a = x,b = y,...\nCONFREQ(a = x,...,d)\n✓\nχ\nImplicitly conﬁrm a = x,... and request value of d\nSELECT(a = x,a = y)\n✓\nχ\nImplicitly conﬁrm a = x,... and request value of d\nAFFIRM(a = x,b = y,...)\n✓\n✓\nAfﬁrm and give further info a = x,b = y,...\nNEGATE(a = x)\nχ\n✓\nNegate and give corrected value a = x\nDENY(a = x)\nχ\n✓\nDeny that a = x\nBYE()\n✓\n✓\nClose a dialog\nFigure 25.4\nDialog acts used by the HIS restaurant recommendation system of Young et al.\n(2010). The Sys and User columns indicate which acts are valid as system outputs and user\ninputs, respectively.\nFigure 25.4 shows a tagset for a restaurant recommendation system, and Fig. 25.5\nshows these tags labeling a sample dialog from the HIS system (Young et al., 2010).",
  "459": "25.1\n•\nDIALOG ACTS\n451\nThis example also shows the content of each dialog acts, which are the slot ﬁllers\nbeing communicated.\nUtterance\nDialog act\nU: Hi, I am looking for somewhere to eat.\nhello(task = find,type=restaurant)\nS: You are looking for a restaurant. What\ntype of food do you like?\nconfreq(type = restaurant, food)\nU: I’d like an Italian somewhere near the\nmuseum.\ninform(food = Italian, near=museum)\nS: Roma is a nice Italian restaurant near\nthe museum.\ninform(name = \"Roma\", type = restaurant,\nfood = Italian, near = museum)\nU: Is it reasonably priced?\nconfirm(pricerange = moderate)\nS: Yes, Roma is in the moderate price\nrange.\naffirm(name = \"Roma\", pricerange =\nmoderate)\nU: What is the phone number?\nrequest(phone)\nS: The number of Roma is 385456.\ninform(name = \"Roma\", phone = \"385456\")\nU: Ok, thank you goodbye.\nbye()\nFigure 25.5\nA sample dialog from the HIS System of Young et al. (2010) using the dialog acts in Fig. 25.4.\nDialog acts don’t just appear discretely and independently; conversations have\nstructure, and dialog acts reﬂect some of that structure. One aspect of this struc-\nture comes from the ﬁeld of conversational analysis or CA (Sacks et al., 1974)\nconversational\nanalysis\nwhich focuses on interactional properties of human conversation. CA deﬁnes ad-\njacency pairs (Schegloff, 1968) as a pairing of two dialog acts, like QUESTIONS\nadjacency pair\nand ANSWERS, PROPOSAL and ACCEPTANCE (or REJECTION), COMPLIMENTS and\nDOWNPLAYERS, GREETING and GREETING.\nThe structure, composed of a ﬁrst pair part and a second pair part, can help\ndialog-state models decide what actions to take. However, dialog acts aren’t always\nfollowed immediately by their second pair part. The two parts can be separated by a\nside sequence (Jefferson 1972, Schegloff 1972). One very common side sequence\nside sequence\nin dialog systems is the clariﬁcation question, which can form a subdialog be-\nsubdialog\ntween a REQUEST and a RESPONSE as in the following example caused by speech\nrecognition errors:\nUser:\nWhat do you have going to UNKNOWN WORD on the 5th?\nSystem:\nLet’s see, going where on the 5th?\nUser:\nGoing to Hong Kong.\nSystem:\nOK, here are some ﬂights...\nAnother kind of dialog structure is the pre-sequence, like the following example\npre-sequence\nwhere a user starts with a question about the system’s capabilities (“Can you make\ntrain reservations”) before making a request.\nUser:\nCan you make train reservations?\nSystem: Yes I can.\nUser:\nGreat, I’d like to reserve a seat on the 4pm train to New York.\nA dialog-state model must be able to both recognize these kinds of structures\nand make use of them in interacting with users.",
  "460": "452\nCHAPTER 25\n•\nADVANCED DIALOG SYSTEMS\n25.2\nDialog State: Interpreting Dialog Acts\nThe job of the dialog-state tracker is to determine both the current state of the frame\n(the ﬁllers of each slot), as well as the user’s most recent dialog act. Note that the\ndialog-state includes more than just the slot-ﬁllers expressed in the current sentence;\nit includes the entire state of the frame at this point, summarizing all of the user’s\nconstraints. The following example from Mrkˇsi´c et al. (2017) shows the required\noutput of the dialog state tracker after each turn:\nUser:\nI’m looking for a cheaper restaurant\ninform(price=cheap)\nSystem: Sure. What kind - and where?\nUser:\nThai food, somewhere downtown\ninform(price=cheap, food=Thai, area=centre)\nSystem: The House serves cheap Thai food\nUser:\nWhere is it?\ninform(price=cheap, food=Thai, area=centre); request(address)\nSystem: The House is at 106 Regent Street\nHow can we interpret a dialog act, deciding whether a given input is a QUES-\nTION, a STATEMENT, or a SUGGEST (directive)? Surface syntax seems like a use-\nful cue, since yes-no questions in English have aux-inversion (the auxiliary verb\nprecedes the subject), statements have declarative syntax (no aux-inversion), and\ncommands have no syntactic subject:\n(25.3)\nYES-NO QUESTION Will breakfast be served on USAir 1557?\nSTATEMENT\nI don’t care about lunch.\nCOMMAND\nShow me ﬂights from Milwaukee to Orlando.\nAlas, the mapping from surface form to dialog act is complex. For example, the\nfollowing utterance looks grammatically like a YES-NO QUESTION meaning some-\nthing like Are you capable of giving me a list of...?:\n(25.4) Can you give me a list of the ﬂights from Atlanta to Boston?\nIn fact, however, this person was not interested in whether the system was capa-\nble of giving a list; this utterance was a polite form of a REQUEST, meaning some-\nthing like Please give me a list of... . What looks on the surface like a QUESTION\ncan really be a REQUEST.\nConversely, what looks on the surface like a STATEMENT can really be a QUES-\nTION. The very common CHECK question (Carletta et al. 1997, Labov and Fan-\nshel 1977) asks an interlocutor to conﬁrm something that she has privileged knowl-\nedge about. CHECKS have declarative surface form:\nA\nOPEN-OPTION I was wanting to make some arrangements for a trip that I’m going\nto be taking uh to LA uh beginning of the week after next.\nB\nHOLD\nOK uh let me pull up your proﬁle and I’ll be right with you here.\n[pause]\nB\nCHECK\nAnd you said you wanted to travel next week?\nA\nACCEPT\nUh yes.\nUtterances that use a surface statement to ask a question or a surface question\nto issue a request are called indirect speech acts. These indirect speech acts have a\nindirect speech\nact",
  "461": "25.2\n•\nDIALOG STATE: INTERPRETING DIALOG ACTS\n453\nrich literature in philosophy, but viewed from the perspective of dialog understand-\ning, indirect speech acts are merely one instance of the more general problem of\ndetermining the dialog act function of a sentence.\nMany features can help in this task.\nTo give just one example, in spoken-\nlanguage systems, prosody or intonation (Chapter ??) is a helpful cue. Prosody\nprosody\nintonation\nor intonation is the name for a particular set of phonological aspects of the speech\nsignal the tune and other changes in the pitch (which can be extracted from the fun-\ndamental frequency F0) the accent, stress, or loudness (which can be extracted from\nenergy), and the changes in duration and rate of speech. So, for example, a rise\nin pitch at the end of the utterance is a good cue for a YES-NO QUESTION, while\ndeclarative utterances (like STATEMENTS) have ﬁnal lowering: a drop in F0 at the\nﬁnal lowering\nend of the utterance.\n25.2.1\nSketching an algorithm for dialog act interpretation\nSince dialog acts places some constraints on the slots and values, the tasks of dialog-\nact detection and slot-ﬁlling are often performed jointly. Consider the task of deter-\nmining that\nI’d like Cantonese food near the Mission District\nhas the structure\ninform(food=cantonese,area=mission)).\nThe joint dialog act interpretation/slot ﬁlling algorithm generally begins with\na ﬁrst pass classiﬁer to decide on the dialog act for the sentence. In the case of\nthe example above, this classiﬁer would choosing inform from among the set of\npossible dialog acts in the tag set for this particular task. Dialog act interpretation is\ngenerally modeled as a supervised classiﬁcation task, trained on a corpus in which\neach utterance is hand-labeled for its dialog act. The classiﬁer can be neural or\nfeature-based; if feature-based, typical features include unigrams and bigrams (show\nme is a good cue for a REQUEST, are there for a QUESTION), embeddings, parse\nfeatures, punctuation, dialog context, and the prosodic features described above.\nA second pass classiﬁer might use the sequence-model algorithms for slot-ﬁller\nextraction from Section 24.2.2 of Chapter 24, such as LSTM-based IOB tagging or\nCRFs or a joint LSTM-CRF. Alternatively, a multinominal classiﬁer can be used to\nchoose between all possible slot-value pairs, again either neural such as a bi-LSTM\nor convolutional net, or feature-based using any of the feature functions deﬁned in\nChapter 24. This is possible since the domain ontology for the system is ﬁxed, so\nthere is a ﬁnite number of slot-value pairs.\n25.2.2\nA special case: detecting correction acts\nSome dialog acts are important because of their implications for dialog control. If a\ndialog system misrecognizes or misunderstands an utterance, the user will generally\ncorrect the error by repeating or reformulating the utterance. Detecting these user\ncorrection acts is therefore quite important. Ironically, it turns out that corrections\nuser correction\nacts\nare actually harder to recognize than normal sentences! In fact, corrections in one\nearly dialog system (the TOOT system) had double the ASR word error rate of non-\ncorrections Swerts et al. (2000)! One reason for this is that speakers sometimes\nuse a speciﬁc prosodic style for corrections called hyperarticulation, in which the\nhyperarticula-\ntion\nutterance contains some exaggerated energy, duration, or F0 contours, such as I said\nBAL-TI-MORE, not Boston (Wade et al. 1992, Levow 1998, Hirschberg et al. 2001).",
  "462": "454\nCHAPTER 25\n•\nADVANCED DIALOG SYSTEMS\nEven when they are not hyperarticulating, users who are frustrated seem to speak in\na way that is harder for speech recognizers (Goldberg et al., 2003).\nWhat are the characteristics of these corrections? User corrections tend to be\neither exact repetitions or repetitions with one or more words omitted, although they\nmay also be paraphrases of the original utterance. (Swerts et al., 2000). Detecting\nthese reformulations or correction acts can be done by any classiﬁer; some stan-\ndard features used for this task are shown below (Levow 1998, Litman et al. 1999,\nHirschberg et al. 2001, Bulyko et al. 2005, Awadallah et al. 2015):\nlexical features\nwords like “no”, “correction”, “I don’t”, or even swear words, utterance length\nsemantic features\noverlap between the candidate correction act and the user’s prior utterance (computed\nby word overlap or via cosines over embedding vectors)\nphonetic features\nphonetic overlap between the candidate correction act and the user’s prior utterance\n(i.e. “WhatsApp” may be incorrectly recognized as “What’s up”)\nprosodic features\nhyperarticulation, increases in F0 range, pause duration, and word duration, generally\nnormalized by the values for previous sentences\nASR features\nASR conﬁdence, language model probability\n25.3\nDialog Policy\nThe goal of the dialog policy is to decide what action the system should take next,\ndialog policy\nthat is, what dialog act to generate. We begin in the next section by introducing one\nspeciﬁc dialog policy decision, relating to conﬁrmation: how we conﬁrm to the user\nwhat we think she said. We then sketch a basic policy algorithm that could apply\nto all decisions. Finally, once a speech act has been generated, the natural language\ngeneration component needs to generate the text of a response to the user.\n25.3.1\nGenerating Dialog Acts: Conﬁrmation and Rejection\nModern dialog systems often make mistakes. It is therefore important for dialog\nsystems to make sure that they have achieved the correct interpretation of the user’s\ninput. This is generally done by two methods: conﬁrming understandings with the\nuser and rejecting utterances that the system is likely to have misunderstood.\nVarious strategies can be employed for conﬁrmation with the user. When using\nthe explicit conﬁrmation strategy, a system asks the user a direct question to con-\nexplicit\nconﬁrmation\nﬁrm the system’s understanding, like the two examples below in which the system\nasks a (boldface) yes-no conﬁrmation questions:\nS: Which city do you want to leave from?\nU: Baltimore.\nS: Do you want to leave from Baltimore?\nU: Yes.\nU: I’d like to ﬂy from Denver Colorado to New York City on September\ntwenty ﬁrst in the morning on United Airlines\nS: Let’s see then. I have you going from Denver Colorado to New York\non September twenty ﬁrst. Is that correct?\nU: Yes\nWhen using the implicit conﬁrmation strategy, a system instead uses the demon-\nimplicit\nconﬁrmation",
  "463": "25.3\n•\nDIALOG POLICY\n455\nstration or display grounding strategies described above, repeating back the system’s\nunderstanding as part of asking the next question, as in the two examples below:\nU:\nI want to travel to Berlin\nS:\nWhen do you want to travel to Berlin?\nU2: Hi I’d like to ﬂy to Seattle Tuesday Morning\nA3: Traveling to Seattle on Tuesday, August eleventh in the morning.\nYour full name?\nExplicit and implicit conﬁrmation have complementary strengths. Explicit con-\nﬁrmation makes it easier for users to correct the system’s misrecognitions since a\nuser can just answer “no” to the conﬁrmation question. But explicit conﬁrmation is\nawkward and increases the length of the conversation (Danieli and Gerbino 1995,\nWalker et al. 1998). The explicit conﬁrmation dialog fragments above sound non-\nnatural and deﬁnitely non-human; implicit conﬁrmation is much more conversation-\nally natural.\nConﬁrmation is just one kind of conversational action by which a system can\nexpress lack of understanding. Another option is rejection, in which a system gives\nrejection\nthe user a prompt like I’m sorry, I didn’t understand that.\nSometimes utterances are rejected multiple times. This might mean that the user\nis using language that the system is unable to follow. Thus, when an utterance is\nrejected, systems often follow a strategy of progressive prompting or escalating\nprogressive\nprompting\ndetail (Yankelovich et al. 1995, Weinschenk and Barker 2000), as in this example\nfrom Cohen et al. (2004):\nSystem: When would you like to leave?\nCaller:\nWell, um, I need to be in New York in time for the ﬁrst World Series game.\nSystem: <reject>. Sorry, I didn’t get that. Please say the month and day you’d like\nto leave.\nCaller:\nI wanna go on October ﬁfteenth.\nIn this example, instead of just repeating “When would you like to leave?”, the\nrejection prompt gives the caller more guidance about how to formulate an utter-\nance the system will understand. These you-can-say help messages are important in\nhelping improve systems’ understanding performance (Bohus and Rudnicky, 2005).\nIf the caller’s utterance gets rejected yet again, the prompt can reﬂect this (“I still\ndidn’t get that”), and give the caller even more guidance.\nAn alternative strategy for error handling is rapid reprompting, in which the\nrapid\nreprompting\nsystem rejects an utterance just by saying “I’m sorry?” or “What was that?” Only\nif the caller’s utterance is rejected a second time does the system start applying\nprogressive prompting. Cohen et al. (2004) summarize experiments showing that\nusers greatly prefer rapid reprompting as a ﬁrst-level error prompt.\nVarious factors can be used as features to the dialog policy in deciding whether\nto use explicit conﬁrmation, implicit conﬁrmation, or rejection. For example, the\nconﬁdence that the ASR system assigns to an utterance can be used by explicitly\nconﬁrming low-conﬁdence sentences. Recall from page ?? that conﬁdence is a met-\nric that the speech recognizer can assign to its transcription of a sentence to indi-\ncate how conﬁdent it is in that transcription. Conﬁdence is often computed from\nthe acoustic log-likelihood of the utterance (greater probability means higher conﬁ-\ndence), but prosodic features can also be used in conﬁdence prediction. For example,",
  "464": "456\nCHAPTER 25\n•\nADVANCED DIALOG SYSTEMS\nutterances with large F0 excursions or longer durations, or those preceded by longer\npauses, are likely to be misrecognized (Litman et al., 2000).\nAnother common feature in conﬁrmation is the cost of making an error. For ex-\nample, explicit conﬁrmation is common before a ﬂight is actually booked or money\nin an account is moved. Systems might have a four-tiered level of conﬁdence with\nthree thresholds α, β, and γ:\n< α low conﬁdence\nreject\n≥α above the threshold\nconﬁrm explicitly\n≥β high conﬁdence\nconﬁrm implictly\n≥γ\nvery high conﬁdence don’t conﬁrm at all\n25.4\nA simple policy based on local context\nThe goal of the dialog policy at turn i in the conversation is to predict which action\nAi to take, based on the entire dialog state. The state could mean the entire sequence\nof dialog acts from the system (A) and from the user (U), in which case the task\nwould be to compute:\nˆAi = argmax\nAi∈A\nP(Ai|(A1,U1,...,Ai−1,Ui−1)\n(25.5)\nWe can simplify this by maintaining as the dialog state mainly just the set of\nslot-ﬁllers that the user has expressed, collapsing across the many different conver-\nsational paths that could lead to the same set of ﬁlled slots.\nSuch a policy might then just condition on the current state of the frame Framei\n(which slots are ﬁlled and with what) and the last turn by the system and user:\nˆAi = argmax\nAi∈A\nP(Ai|Framei−1,Ai−1,Ui−1)\n(25.6)\nGiven a large enough corpus of conversations, these probabilities can be esti-\nmated by your favorite classiﬁer. Getting such enormous amounts of data can be\ndifﬁcult, and often involves building user simulators to generate artiﬁcial conversa-\ntions to train on.\n25.5\nNatural language generation in the dialog-state model\nOnce a dialog act has been decided, we need to generate the text of the response\nto the user. The task of natural language generation (NLG) in the information-state\narchitecture is often modeled in two stages, content planning (what to say), and\ncontent\nplanning\nsentence realization (how to say it).\nsentence\nrealization\nHere we’ll assume content planning has been done by the dialog policy, which\nhas chosen the dialog act to generate, and perhaps also chosen some some additional\nattributes (slots and values) that the planner wants to implicitly conﬁrm to the user.\nFig. 25.6 shows a sample input structure from the policy/content planner, and one\nexample of a resulting sentence that the sentence realizer could generate from this\nstructure.\nLet’s walk through the sentence realization stage for the example in Fig. 25.6,\nwhich comes from the classic information state statistical NLG system of Oh and",
  "465": "25.5\n•\nNATURAL LANGUAGE GENERATION IN THE DIALOG-STATE MODEL\n457\nFigure 25.6\nAn input frame to NLG and a resulting output sentence, in the Communicator\nsystem of Oh and Rudnicky (2000).\nquery arrive city\nhotel hotel chain\ninform ﬂight earlier\nquery arrive time\nhotel hotel info\ninform ﬂight earliest\nquery conﬁrm\nhotel need car\ninform ﬂight later\nquery depart date\nhotel need hotel\ninform ﬂight latest\nquery depart time\nhotel where\ninform ﬂight returning\nquery pay by card\ninform airport\ninform not avail\nquery preferred airport\ninform conﬁrm utterance\ninform num ﬂights\nquery return date\ninform epilogue\ninform price\nquery return time\ninform ﬂight\nother\nhotel car info\ninform ﬂight another\nFigure 25.7\nDialog acts in the CMU communicator system of Oh and Rudnicky (2000).\nRudnicky (2000), part of the CMU Communicator travel planning dialog system.\nNotice ﬁrst that the policy has decided to generate the dialog act QUERY with the\nargument DEPART TIME. Fig. 25.7 lists the dialog acts in the Oh and Rudnicky\n(2000) system, each of which combines an act with a potential argument. The input\nframe in Fig. 25.6 also speciﬁes some additional ﬁlled slots that should be included\nin the sentence to the user (depart airport BOS, and the depart date).\nThe sentence realizer acts in two steps. It will ﬁrst generate a delexicalized\ndelexicalized\nstring like:\nWhat time on [depart date] would you like to leave [depart airport]?\nDelexicalization is the process of replacing speciﬁc words with a generic rep-\nresentation of their slot types. A delexicalized sentence is much easier to generate\nsince we can train on many different source sentences from different speciﬁc dates\nand airports. Then once we’ve generating the delexicalized string, we can simply use\nthe input frame from the content planner to relexicalize (ﬁll in the exact departure\nrelexicalize\ndate and airport).\nTo generate the delexicalized sentences, the sentence realizer uses a large corpus\nof human-human travel dialogs that were labeled with the dialog acts from Fig. 25.7\nand the slots expressed in each turn, like the following:\nQUERY DEPART TIME And what time would you like to leave [depart city Pittsburgh]?\nQUERY ARRIVE CITY And you’re ﬂying into what city?\nQUERY ARRIVE TIME What time on [arrive date May 5]?\nINFORM FLIGHT\nThe ﬂight departs [depart airport PGH] at [depart time 10 am] and arrives\n[arrive city Seattle] at [arrive time 12:05 their time].\nThis corpus is then delexicalized, and divided up into separate corpora for each\ndialog act. Thus the delexicalized corpus for one dialog act, QUERY DEPART TIME\nmight be trained on examples like:",
  "466": "458\nCHAPTER 25\n•\nADVANCED DIALOG SYSTEMS\nAnd what time would you like to leave depart city?\nWhen would you like to leave depart city?\nWhen would you like to leave?\nWhat time do you want to leave on depart date?\nOK, on depart date, what time do you want to leave?\nA distinct N-gram grammar is then trained for each dialog act. Now, given\nthe dialog act QUERY DEPART TIME, the system samples random sentences from\nthis language model. Recall from the ”Shannon” exercise of 46 that this works\n(assuming a bigram LM) by ﬁrst selecting a bigram (< s >,< w >) according to its\nbigram probability in the language model, then drawing a bigram starting with <\nw > according to its bigram probability, and so on until a full sentence is generated.\nThe probability of each successive word wi being generated from utterance class u\nis thus\nP(wi) = P(wi|wi−1,wi−2,...,wi−(n−1),u)\n(25.7)\nEach of these randomly sampled sentences is then assigned a score based on heuris-\ntic rules that penalize sentences that are too short or too long, repeat slots, or lack\nsome of the required slots from the input frame (in this case, depart airport and de-\npart date). The best scoring sentence is then chosen. Let’s suppose in this case we\nproduce the following (delexicalized) sentence:\nWhat time on depart date would you like to leave depart airport?\nThis sentence is then relexicalized from the true values in the input frame, re-\nsulting in the ﬁnal sentence:\nWhat time on October ﬁfth would you like to leave Boston?\nModern implementations of the model replace the simplistic N-gram part of the\ngenerator with neural models, which similarly learn to map from an input frame to\na resulting sentence (Wen et al. 2015a, Wen et al. 2015b).\nIt’s also possible to design NLG algorithms that are speciﬁc to a particular di-\nalog act. For example, consider the task of generating clariﬁcation questions, in\nclariﬁcation\nquestions\ncases where the speech recognition fails to understand some part of the user’s ut-\nterance. While it is possible to use the generic dialog act REJECT (“Please repeat”,\nor “I don’t understand what you said”), studies of human conversations show that\nhumans instead use targeted clariﬁcation questions that reprise elements of the mis-\nunderstanding (Purver 2004, Ginzburg and Sag 2000, Stoyanchev et al. 2013).\nFor example, in the following hypothetical example the system reprises the\nwords “going” and “on the 5th” to make it clear which aspect of the user’s turn\nthe system needs to be clariﬁed:\nUser:\nWhat do you have going to UNKNOWN WORD on the 5th?\nSystem: Going where on the 5th?\nTargeted clariﬁcation questions can be created by rules (such as replacing “go-\ning to UNKNOWN WORD” with “going where”) or by building classiﬁers to guess\nwhich slots might have been misrecognized in the sentence (Chu-Carroll and Car-\npenter 1999, Stoyanchev et al. 2014, Stoyanchev and Johnston 2015).",
  "467": "25.6\n•\nDEEP REINFORCEMENT LEARNING FOR DIALOG\n459\n25.6\nDeep Reinforcement Learning for Dialog\nTBD\n25.7\nSummary\n• In dialog, speaking is a kind of action; these acts are referred to as speech\nacts. Speakers also attempt to achieve common ground by acknowledging\nthat they have understand each other. The dialog act combines the intuition\nof speech acts and grounding acts.\n• The dialog-state or information-state architecture augments the frame-and-\nslot state architecture by keeping track of user’s dialog acts and includes a\npolicy for generating its own dialog acts in return.\n• Policies based on reinforcement learning architecture like the MDP and POMDP\noffer ways for future dialog reward to be propagated back to inﬂuence policy\nearlier in the dialog manager.\nBibliographical and Historical Notes\nThe idea that utterances in a conversation are a kind of action being performed by\nthe speaker was due originally to the philosopher Wittgenstein (1953) but worked out\nmore fully by Austin (1962) and his student John Searle. Various sets of speech acts\nhave been deﬁned over the years, and a rich linguistic and philosophical literature\ndeveloped, especially focused on explaining the use of indirect speech acts.\nThe idea of dialog acts draws also from a number of other sources, including\nthe ideas of adjacency pairs, pre-sequences, and other aspects of the international\nproperties of human conversation developed in the ﬁeld of conversation analysis\n(see Levinson (1983) for an introduction to the ﬁeld).\nThis idea that acts set up strong local dialog expectations was also preﬁgured by\nFirth (1935, p. 70), in a famous quotation:\nMost of the give-and-take of conversation in our everyday life is stereotyped\nand very narrowly conditioned by our particular type of culture. It is a sort\nof roughly prescribed social ritual, in which you generally say what the other\nfellow expects you, one way or the other, to say.\nAnother important research thread modeled dialog as a kind of collaborative be-\nhavior, including the ideas of common ground (Clark and Marshall, 1981), reference\nas a collaborative process (Clark and Wilkes-Gibbs, 1986), joint intention (Levesque\net al., 1990), and shared plans (Grosz and Sidner, 1980).\nThe information state model of dialog was also strongly informed by analytic\nwork on the linguistic properties of dialog acts and on methods for their detection\n(Sag and Liberman 1975, Hinkelman and Allen 1989, Nagata and Morimoto 1994,\nGoodwin 1996, Chu-Carroll 1998, Shriberg et al. 1998, Stolcke et al. 2000, Gravano\net al. 2012).",
  "468": "460\nCHAPTER 25\n•\nADVANCED DIALOG SYSTEMS\nTwo important lines of research focused on the computational properties of con-\nversational structure. One line, ﬁrst suggested at by Bruce (1975), suggested that\nsince speech acts are actions, they should be planned like other actions, and drew\non the AI planning literature (Fikes and Nilsson, 1971). An agent seeking to ﬁnd\nout some information can come up with the plan of asking the interlocutor for the\ninformation. An agent hearing an utterance can interpret a speech act by running\nthe planner “in reverse”, using inference rules to infer from what the interlocutor\nsaid what the plan might have been. Plan-based models of dialog are referred to as\nBDI models because such planners model the beliefs, desires, and intentions (BDI)\nBDI\nof the agent and interlocutor. BDI models of dialog were ﬁrst introduced by Allen,\nCohen, Perrault, and their colleagues in a number of inﬂuential papers showing how\nspeech acts could be generated (Cohen and Perrault, 1979) and interpreted (Perrault\nand Allen 1980, Allen and Perrault 1980). At the same time, Wilensky (1983) intro-\nduced plan-based models of understanding as part of the task of interpreting stories.\nAnother inﬂuential line of research focused on modeling the hierarchical struc-\nture of dialog. Grosz’s pioneering (1977) dissertation ﬁrst showed that “task-oriented\ndialogs have a structure that closely parallels the structure of the task being per-\nformed” (p. 27), leading to her work with Sidner and others showing how to use\nsimilar notions of intention and plans to model discourse structure and coherence in\ndialog. See, e.g., Lochbaum et al. (2000) for a summary of the role of intentional\nstructure in dialog.\nThe idea of applying reinforcement learning to dialog ﬁrst came out of AT&T\nand Bell Laboratories around the turn of the century with work on MDP dialog sys-\ntems (Walker 2000, Levin et al. 2000, Singh et al. 2002) and work on cue phrases,\nprosody, and rejection and conﬁrmation. Reinforcement learning research turned\nquickly to the more sophisticated POMDP models (Roy et al. 2000, Lemon et al. 2006,\nWilliams and Young 2007) applied to small slot-ﬁlling dialog tasks. [History of deep\nreinforcement learning here.]\nto be continued",
  "469": "CHAPTER\n26\nSpeech Recognition and Syn-\nthesis\nPlaceholder\n461",
  "470": "463\nAppendices",
  "471": "464\nAPPENDIX A\n•\nHIDDEN MARKOV MODELS\nCHAPTER\nA\nHidden Markov Models\nChapter 8 introduced the Hidden Markov Model and applied it to part of speech\ntagging. Part of speech tagging is a fully-supervised learning task, because we have\na corpus of words labeled with the correct part-of-speech tag. But many applications\ndon’t have labeled data. So in this chapter, we introduce the full set of algorithms for\nHMMs, including the key unsupervised learning algorithm for HMM, the Forward-\nBackward algorithm. We’ll repeat some of the text from Chapter 8 for readers who\nwant the whole story laid out in a single chapter.\nA.1\nMarkov Chains\nThe HMM is based on augmenting the Markov chain. A Markov chain is a model\nMarkov chain\nthat tells us something about the probabilities of sequences of random variables,\nstates, each of which can take on values from some set. These sets can be words, or\ntags, or symbols representing anything, like the weather. A Markov chain makes a\nvery strong assumption that if we want to predict the future in the sequence, all that\nmatters is the current state. The states before the current state have no impact on the\nfuture except via the current state. It’s as if to predict tomorrow’s weather you could\nexamine today’s weather but you weren’t allowed to look at yesterday’s weather.\nWARM3\nHOT1\nCOLD2\n.8\n.6\n.1\n.1\n.3\n.6\n.1\n.1\n.3\ncharming\nuniformly\nare\n.1\n.4\n.5\n.5\n.5\n.2\n.6\n.2\n(a)\n(b)\nFigure A.1\nA Markov chain for weather (a) and one for words (b), showing states and\ntransitions. A start distribution π is required; setting π = [0.1, 0.7, 0.2] for (a) would mean a\nprobability 0.7 of starting in state 2 (cold), probability 0.1 of starting in state 1 (hot), etc.\nMore formally, consider a sequence of state variables q1,q2,...,qi. A Markov\nmodel embodies the Markov assumption on the probabilities of this sequence: that\nMarkov\nassumption\nwhen predicting the future, the past doesn’t matter, only the present.\nMarkov Assumption:\nP(qi = a|q1...qi−1) = P(qi = a|qi−1)\n(A.1)\nFigure A.1a shows a Markov chain for assigning a probability to a sequence of\nweather events, for which the vocabulary consists of HOT, COLD, and WARM. The\nstates are represented as nodes in the graph, and the transitions, with their probabil-\nities, as edges. The transitions are probabilities: the values of arcs leaving a given",
  "472": "A.2\n•\nTHE HIDDEN MARKOV MODEL\n465\nstate must sum to 1. Figure A.1b shows a Markov chain for assigning a probabil-\nity to a sequence of words w1...wn. This Markov chain should be familiar; in fact,\nit represents a bigram language model, with each edge expressing the probability\np(wi|w j)! Given the two models in Fig. A.1, we can assign a probability to any\nsequence from our vocabulary.\nFormally, a Markov chain is speciﬁed by the following components:\nQ = q1q2 ...qN\na set of N states\nA = a11a12 ...an1 ...ann\na transition probability matrix A, each aij represent-\ning the probability of moving from state i to state j, s.t.\nPn\nj=1 aij = 1 ∀i\nπ = π1,π2,...,πN\nan initial probability distribution over states. πi is the\nprobability that the Markov chain will start in state i.\nSome states j may have π j = 0, meaning that they cannot\nbe initial states. Also, Pn\ni=1 πi = 1\nBefore you go on, use the sample probabilities in Fig. A.1a (with π = [.1,.7.,2])\nto compute the probability of each of the following sequences:\n(A.2) hot hot hot hot\n(A.3) cold hot cold hot\nWhat does the difference in these probabilities tell you about a real-world weather\nfact encoded in Fig. A.1a?\nA.2\nThe Hidden Markov Model\nA Markov chain is useful when we need to compute a probability for a sequence\nof observable events. In many cases, however, the events we are interested in are\nhidden: we don’t observe them directly. For example we don’t normally observe\nhidden\npart-of-speech tags in a text. Rather, we see words, and must infer the tags from the\nword sequence. We call the tags hidden because they are not observed.\nA hidden Markov model (HMM) allows us to talk about both observed events\nHidden\nMarkov model\n(like words that we see in the input) and hidden events (like part-of-speech tags) that\nwe think of as causal factors in our probabilistic model. An HMM is speciﬁed by\nthe following components:\nQ = q1q2 ...qN\na set of N states\nA = a11 ...ai j ...aNN\na transition probability matrix A, each aij representing the probability\nof moving from state i to state j, s.t. PN\nj=1 aij = 1 ∀i\nO = o1o2 ...oT\na sequence of T observations, each one drawn from a vocabulary V =\nv1,v2,...,vV\nB = bi(ot)\na sequence of observation likelihoods, also called emission probabili-\nties, each expressing the probability of an observation ot being generated\nfrom a state i\nπ = π1,π2,...,πN\nan initial probability distribution over states. πi is the probability that\nthe Markov chain will start in state i. Some states j may have πj = 0,\nmeaning that they cannot be initial states. Also, Pn\ni=1 πi = 1",
  "473": "466\nAPPENDIX A\n•\nHIDDEN MARKOV MODELS\nA ﬁrst-order hidden Markov model instantiates two simplifying assumptions.\nFirst, as with a ﬁrst-order Markov chain, the probability of a particular state depends\nonly on the previous state:\nMarkov Assumption:\nP(qi|q1...qi−1) = P(qi|qi−1)\n(A.4)\nSecond, the probability of an output observation oi depends only on the state that\nproduced the observation qi and not on any other states or any other observations:\nOutput Independence: P(oi|q1 ...qi,...,qT,o1,...,oi,...,oT) = P(oi|qi)\n(A.5)\nTo exemplify these models, we’ll use a task invented by Jason Eisner (2002).\nImagine that you are a climatologist in the year 2799 studying the history of global\nwarming. You cannot ﬁnd any records of the weather in Baltimore, Maryland, for\nthe summer of 2020, but you do ﬁnd Jason Eisner’s diary, which lists how many ice\ncreams Jason ate every day that summer. Our goal is to use these observations to\nestimate the temperature every day. We’ll simplify this weather task by assuming\nthere are only two kinds of days: cold (C) and hot (H). So the Eisner task is as\nfollows:\nGiven a sequence of observations O (each an integer representing the\nnumber of ice creams eaten on a given day) ﬁnd the ‘hidden’ sequence\nQ of weather states (H or C) which caused Jason to eat the ice cream.\nFigure A.2 shows a sample HMM for the ice cream task. The two hidden states\n(H and C) correspond to hot and cold weather, and the observations (drawn from the\nalphabet O = {1,2,3}) correspond to the number of ice creams eaten by Jason on a\ngiven day.\nπ = [.8,.2]\nCOLD2\nHOT1\nB2\nP(1 | COLD)          .5\nP(2 | COLD)    =    .4\nP(3 | COLD)          .1\n.5\n.6\n.5\n.4\nP(1 | HOT)          .2\nP(2 | HOT)    =    .4\nP(3 | HOT)          .4\nB1\nFigure A.2\nA hidden Markov model for relating numbers of ice creams eaten by Jason (the\nobservations) to the weather (H or C, the hidden variables).\nAn inﬂuential tutorial by Rabiner (1989), based on tutorials by Jack Ferguson in\nthe 1960s, introduced the idea that hidden Markov models should be characterized\nby three fundamental problems:\nProblem 1 (Likelihood):\nGiven an HMM λ = (A,B) and an observation se-\nquence O, determine the likelihood P(O|λ).\nProblem 2 (Decoding):\nGiven an observation sequence O and an HMM λ =\n(A,B), discover the best hidden state sequence Q.\nProblem 3 (Learning):\nGiven an observation sequence O and the set of states\nin the HMM, learn the HMM parameters A and B.\nWe already saw an example of Problem 2 in Chapter 8. In the next two sections\nwe introduce the Forward and Forward-Backward algorithms to solve Problems 1\nand 3 and give more information on Problem 2",
  "474": "A.3\n•\nLIKELIHOOD COMPUTATION: THE FORWARD ALGORITHM\n467\nA.3\nLikelihood Computation: The Forward Algorithm\nOur ﬁrst problem is to compute the likelihood of a particular observation sequence.\nFor example, given the ice-cream eating HMM in Fig. A.2, what is the probability\nof the sequence 3 1 3? More formally:\nComputing Likelihood: Given an HMM λ = (A,B) and an observa-\ntion sequence O, determine the likelihood P(O|λ).\nFor a Markov chain, where the surface observations are the same as the hidden\nevents, we could compute the probability of 3 1 3 just by following the states labeled\n3 1 3 and multiplying the probabilities along the arcs. For a hidden Markov model,\nthings are not so simple. We want to determine the probability of an ice-cream\nobservation sequence like 3 1 3, but we don’t know what the hidden state sequence\nis!\nLet’s start with a slightly simpler situation. Suppose we already knew the weather\nand wanted to predict how much ice cream Jason would eat. This is a useful part\nof many HMM tasks. For a given hidden state sequence (e.g., hot hot cold), we can\neasily compute the output likelihood of 3 1 3.\nLet’s see how. First, recall that for hidden Markov models, each hidden state\nproduces only a single observation. Thus, the sequence of hidden states and the\nsequence of observations have the same length. 1\nGiven this one-to-one mapping and the Markov assumptions expressed in Eq. A.4,\nfor a particular hidden state sequence Q = q0,q1,q2,...,qT and an observation se-\nquence O = o1,o2,...,oT, the likelihood of the observation sequence is\nP(O|Q) =\nTY\ni=1\nP(oi|qi)\n(A.6)\nThe computation of the forward probability for our ice-cream observation 3 1 3 from\none possible hidden state sequence hot hot cold is shown in Eq. A.7. Figure A.3\nshows a graphic representation of this computation.\nP(3 1 3|hot hot cold) = P(3|hot)×P(1|hot)×P(3|cold)\n(A.7)\ncold\nhot\n3\n.4\nhot\n1\n3\n.2\n.1\nFigure A.3\nThe computation of the observation likelihood for the ice-cream events 3 1 3\ngiven the hidden state sequence hot hot cold.\nBut of course, we don’t actually know what the hidden state (weather) sequence\nwas. We’ll need to compute the probability of ice-cream events 3 1 3 instead by\n1\nIn a variant of HMMs called segmental HMMs (in speech recognition) or semi-HMMs (in text pro-\ncessing) this one-to-one mapping between the length of the hidden state sequence and the length of the\nobservation sequence does not hold.",
  "475": "468\nAPPENDIX A\n•\nHIDDEN MARKOV MODELS\nsumming over all possible weather sequences, weighted by their probability. First,\nlet’s compute the joint probability of being in a particular weather sequence Q and\ngenerating a particular sequence O of ice-cream events. In general, this is\nP(O,Q) = P(O|Q)×P(Q) =\nTY\ni=1\nP(oi|qi)×\nTY\ni=1\nP(qi|qi−1)\n(A.8)\nThe computation of the joint probability of our ice-cream observation 3 1 3 and one\npossible hidden state sequence hot hot cold is shown in Eq. A.9. Figure A.4 shows\na graphic representation of this computation.\nP(3 1 3,hot hot cold) = P(hot|start)×P(hot|hot)×P(cold|hot)\n×P(3|hot)×P(1|hot)×P(3|cold)\n(A.9)\ncold\nhot\n3\n.4\nhot\n.6\n1\n3\n.4\n.2\n.1\nFigure A.4\nThe computation of the joint probability of the ice-cream events 3 1 3 and the\nhidden state sequence hot hot cold.\nNow that we know how to compute the joint probability of the observations\nwith a particular hidden state sequence, we can compute the total probability of the\nobservations just by summing over all possible hidden state sequences:\nP(O) =\nX\nQ\nP(O,Q) =\nX\nQ\nP(O|Q)P(Q)\n(A.10)\nFor our particular case, we would sum over the eight 3-event sequences cold cold\ncold, cold cold hot, that is,\nP(3 1 3) = P(3 1 3,cold cold cold)+P(3 1 3,cold cold hot)+P(3 1 3,hot hot cold)+...\nFor an HMM with N hidden states and an observation sequence of T observa-\ntions, there are NT possible hidden sequences. For real tasks, where N and T are\nboth large, NT is a very large number, so we cannot compute the total observation\nlikelihood by computing a separate observation likelihood for each hidden state se-\nquence and then summing them.\nInstead of using such an extremely exponential algorithm, we use an efﬁcient\nO(N2T) algorithm called the forward algorithm. The forward algorithm is a kind\nforward\nalgorithm\nof dynamic programming algorithm, that is, an algorithm that uses a table to store\nintermediate values as it builds up the probability of the observation sequence. The\nforward algorithm computes the observation probability by summing over the prob-\nabilities of all possible hidden state paths that could generate the observation se-\nquence, but it does so efﬁciently by implicitly folding each of these paths into a\nsingle forward trellis.\nFigure A.5 shows an example of the forward trellis for computing the likelihood\nof 3 1 3 given the hidden state sequence hot hot cold.",
  "476": "A.3\n•\nLIKELIHOOD COMPUTATION: THE FORWARD ALGORITHM\n469\nπ\nH\nC\nH\nC\nH\nC\nP(C|start) * P(3|C)\n.2 * .1\nP(H|H) * P(1|H)\n.6 * .2\nP(C|C) * P(1|C)\n.5 * .5\nP(C|H) * P(1|C)\n.4 * .5\nP(H|C) * P(1|H)\n.5 * .2\nP(H|start)*P(3|H)\n.8 * .4\nα1(2)=.32\nα1(1) = .02\nα2(2)= .32*.12 + .02*.1 = .0404\nα2(1) = .32*.2 + .02*.25 = .069\nt\nC\nH\nq2\nq1\no1\n3\no2\no3\n1\n3\nFigure A.5\nThe forward trellis for computing the total observation likelihood for the ice-cream events 3 1 3.\nHidden states are in circles, observations in squares. The ﬁgure shows the computation of αt(j) for two states at\ntwo time steps. The computation in each cell follows Eq. A.12: αt(j) = PN\ni=1 αt−1(i)ai jbj(ot). The resulting\nprobability expressed in each cell is Eq. A.11: αt(j) = P(o1,o2 ...ot,qt = j|λ).\nEach cell of the forward algorithm trellis αt(j) represents the probability of be-\ning in state j after seeing the ﬁrst t observations, given the automaton λ. The value\nof each cell αt(j) is computed by summing over the probabilities of every path that\ncould lead us to this cell. Formally, each cell expresses the following probability:\nαt(j) = P(o1,o2 ...ot,qt = j|λ)\n(A.11)\nHere, qt = j means “the tth state in the sequence of states is state j”. We compute\nthis probability αt(j) by summing over the extensions of all the paths that lead to\nthe current cell. For a given state qj at time t, the value αt(j) is computed as\nαt(j) =\nN\nX\ni=1\nαt−1(i)aijbj(ot)\n(A.12)\nThe three factors that are multiplied in Eq. A.12 in extending the previous paths\nto compute the forward probability at time t are\nαt−1(i)\nthe previous forward path probability from the previous time step\nai j\nthe transition probability from previous state qi to current state qj\nbj(ot)\nthe state observation likelihood of the observation symbol ot given\nthe current state j\nConsider the computation in Fig. A.5 of α2(2), the forward probability of being\nat time step 2 in state 2 having generated the partial observation 3 1. We compute by\nextending the α probabilities from time step 1, via two paths, each extension con-\nsisting of the three factors above: α1(1)×P(H|C)×P(1|H) and α1(2)×P(H|H)×\nP(1|H).\nFigure A.6 shows another visualization of this induction step for computing the\nvalue in one new cell of the trellis.\nWe give two formal deﬁnitions of the forward algorithm: the pseudocode in\nFig. A.7 and a statement of the deﬁnitional recursion here.",
  "477": "470\nAPPENDIX A\n•\nHIDDEN MARKOV MODELS\not-1\not\na1j\na2j\naNj\na3j\nbj(ot)\nαt(j)= Σi αt-1(i) aij bj(ot) \nq1\nq2\nq3\nqN\nq1\nqj\nq2\nq1\nq2\not+1\not-2\nq1\nq2\nq3\nq3\nqN\nqN\nαt-1(N)\nαt-1(3)\nαt-1(2)\nαt-1(1)\nαt-2(N)\nαt-2(3)\nαt-2(2)\nαt-2(1)\nFigure A.6\nVisualizing the computation of a single element αt(i) in the trellis by summing\nall the previous values αt−1, weighted by their transition probabilities a, and multiplying by\nthe observation probability bi(ot+1). For many applications of HMMs, many of the transition\nprobabilities are 0, so not all previous states will contribute to the forward probability of the\ncurrent state. Hidden states are in circles, observations in squares. Shaded nodes are included\nin the probability computation for αt(i).\nfunction FORWARD(observations of len T, state-graph of len N) returns forward-prob\ncreate a probability matrix forward[N,T]\nfor each state s from 1 to N do\n; initialization step\nforward[s,1]←πs ∗bs(o1)\nfor each time step t from 2 to T do\n; recursion step\nfor each state s from 1 to N do\nforward[s,t]←\nN\nX\ns′=1\nforward[s′,t −1] ∗as′,s ∗bs(ot)\nforwardprob←\nN\nX\ns=1\nforward[s,T]\n; termination step\nreturn forwardprob\nFigure A.7\nThe forward algorithm, where forward[s,t] represents αt(s).\n1. Initialization:\nα1(j) = πjbj(o1) 1 ≤j ≤N\n2. Recursion:\nαt(j) =\nN\nX\ni=1\nαt−1(i)aijbj(ot); 1 ≤j ≤N,1 < t ≤T\n3. Termination:\nP(O|λ) =\nN\nX\ni=1\nαT(i)",
  "478": "A.4\n•\nDECODING: THE VITERBI ALGORITHM\n471\nA.4\nDecoding: The Viterbi Algorithm\nFor any model, such as an HMM, that contains hidden variables, the task of deter-\nmining which sequence of variables is the underlying source of some sequence of\nobservations is called the decoding task. In the ice-cream domain, given a sequence\nDecoding\nof ice-cream observations 3 1 3 and an HMM, the task of the decoder is to ﬁnd the\nDecoder\nbest hidden weather sequence (H H H). More formally,\nDecoding: Given as input an HMM λ = (A,B) and a sequence of ob-\nservations O = o1,o2,...,oT, ﬁnd the most probable sequence of states\nQ = q1q2q3 ...qT.\nWe might propose to ﬁnd the best sequence as follows: For each possible hid-\nden state sequence (HHH, HHC, HCH, etc.), we could run the forward algorithm\nand compute the likelihood of the observation sequence given that hidden state se-\nquence. Then we could choose the hidden state sequence with the maximum obser-\nvation likelihood. It should be clear from the previous section that we cannot do this\nbecause there are an exponentially large number of state sequences.\nInstead, the most common decoding algorithms for HMMs is the Viterbi algo-\nrithm. Like the forward algorithm, Viterbi is a kind of dynamic programming\nViterbi\nalgorithm\nthat makes uses of a dynamic programming trellis. Viterbi also strongly resembles\nanother dynamic programming variant, the minimum edit distance algorithm of\nChapter 2.\nπ\nH\nC\nH\nC\nH\nC\nP(C|start) * P(3|C)\n.2 * .1\nP(H|H) * P(1|H)\n.6 * .2\nP(C|C) * P(1|C)\n.5 * .5\nP(C|H) * P(1|C)\n.4 * .5\nP(H|C) * P(1|H)\n.5 * .2\nP(H|start)*P(3|H)\n.8 * .4\nv1(2)=.32\nv1(1) = .02\nv2(2)= max(.32*.12, .02*.10) = .038\nv2(1) = max(.32*.20, .02*.25) = .064\nt\nC\nH\nq2\nq1\no1\no2\no3\n3\n1\n3\nFigure A.8\nThe Viterbi trellis for computing the best path through the hidden state space for the ice-cream\neating events 3 1 3. Hidden states are in circles, observations in squares. White (unﬁlled) circles indicate illegal\ntransitions. The ﬁgure shows the computation of vt(j) for two states at two time steps. The computation in each\ncell follows Eq. A.14: vt(j) = max1≤i≤N−1 vt−1(i) ai j bj(ot). The resulting probability expressed in each cell\nis Eq. A.13: vt(j) = P(q0,q1,...,qt−1,o1,o2,...,ot,qt = j|λ).\nFigure A.8 shows an example of the Viterbi trellis for computing the best hidden\nstate sequence for the observation sequence 3 1 3. The idea is to process the ob-\nservation sequence left to right, ﬁlling out the trellis. Each cell of the trellis, vt(j),\nrepresents the probability that the HMM is in state j after seeing the ﬁrst t obser-\nvations and passing through the most probable state sequence q1,...,qt−1, given the",
  "479": "472\nAPPENDIX A\n•\nHIDDEN MARKOV MODELS\nautomaton λ. The value of each cell vt(j) is computed by recursively taking the\nmost probable path that could lead us to this cell. Formally, each cell expresses the\nprobability\nvt(j) =\nmax\nq1,...,qt−1 P(q1...qt−1,o1,o2 ...ot,qt = j|λ)\n(A.13)\nNote that we represent the most probable path by taking the maximum over all\npossible previous state sequences\nmax\nq1,...,qt−1. Like other dynamic programming algo-\nrithms, Viterbi ﬁlls each cell recursively. Given that we had already computed the\nprobability of being in every state at time t −1, we compute the Viterbi probability\nby taking the most probable of the extensions of the paths that lead to the current\ncell. For a given state qj at time t, the value vt(j) is computed as\nvt(j) =\nN\nmax\ni=1 vt−1(i) aij bj(ot)\n(A.14)\nThe three factors that are multiplied in Eq. A.14 for extending the previous paths to\ncompute the Viterbi probability at time t are\nvt−1(i)\nthe previous Viterbi path probability from the previous time step\nai j\nthe transition probability from previous state qi to current state qj\nbj(ot)\nthe state observation likelihood of the observation symbol ot given\nthe current state j\nfunction VITERBI(observations of len T,state-graph of len N) returns best-path, path-prob\ncreate a path probability matrix viterbi[N,T]\nfor each state s from 1 to N do\n; initialization step\nviterbi[s,1]←πs ∗bs(o1)\nbackpointer[s,1]←0\nfor each time step t from 2 to T do\n; recursion step\nfor each state s from 1 to N do\nviterbi[s,t]←\nN\nmax\ns′=1 viterbi[s′,t −1] ∗as′,s ∗bs(ot)\nbackpointer[s,t]←\nN\nargmax\ns′=1\nviterbi[s′,t −1] ∗as′,s ∗bs(ot)\nbestpathprob←\nN\nmax\ns=1\nviterbi[s,T]\n; termination step\nbestpathpointer←\nN\nargmax\ns=1\nviterbi[s,T]\n; termination step\nbestpath←the path starting at state bestpathpointer, that follows backpointer[] to states back in time\nreturn bestpath, bestpathprob\nFigure A.9\nViterbi algorithm for ﬁnding optimal sequence of hidden states. Given an observation sequence\nand an HMM λ = (A,B), the algorithm returns the state path through the HMM that assigns maximum likelihood\nto the observation sequence.\nFigure A.9 shows pseudocode for the Viterbi algorithm. Note that the Viterbi\nalgorithm is identical to the forward algorithm except that it takes the max over the\nprevious path probabilities whereas the forward algorithm takes the sum. Note also\nthat the Viterbi algorithm has one component that the forward algorithm doesn’t",
  "480": "A.5\n•\nHMM TRAINING: THE FORWARD-BACKWARD ALGORITHM\n473\nhave: backpointers. The reason is that while the forward algorithm needs to pro-\nduce an observation likelihood, the Viterbi algorithm must produce a probability and\nalso the most likely state sequence. We compute this best state sequence by keeping\ntrack of the path of hidden states that led to each state, as suggested in Fig. A.10, and\nthen at the end backtracing the best path to the beginning (the Viterbi backtrace).\nViterbi\nbacktrace\nπ\nH\nC\nH\nC\nH\nC\nP(C|start) * P(3|C)\n.2 * .1\nP(H|H) * P(1|H)\n.6 * .2\nP(C|C) * P(1|C)\n.5 * .5\nP(C|H) * P(1|C)\n.4 * .5\nP(H|C) * P(1|H)\n.5 * .2\nP(H|start)*P(3|H)\n.8 * .4\nv1(2)=.32\nv1(1) = .02\nv2(2)= max(.32*.12, .02*.10) = .038\nv2(1) = max(.32*.20, .02*.25) = .064\nt\nC\nH\nq2\nq1\no1\no2\no3\n3\n1\n3\nFigure A.10\nThe Viterbi backtrace. As we extend each path to a new state account for the next observation,\nwe keep a backpointer (shown with broken lines) to the best path that led us to this state.\nFinally, we can give a formal deﬁnition of the Viterbi recursion as follows:\n1. Initialization:\nv1(j) = π jb j(o1)\n1 ≤j ≤N\nbt1(j) = 0\n1 ≤j ≤N\n2. Recursion\nvt(j) =\nN\nmax\ni=1 vt−1(i)aij b j(ot); 1 ≤j ≤N,1 < t ≤T\nbtt(j) =\nN\nargmax\ni=1\nvt−1(i)aij bj(ot); 1 ≤j ≤N,1 < t ≤T\n3. Termination:\nThe best score:\nP∗=\nN\nmax\ni=1 vT(i)\nThe start of backtrace:\nqT∗=\nN\nargmax\ni=1\nvT(i)\nA.5\nHMM Training: The Forward-Backward Algorithm\nWe turn to the third problem for HMMs: learning the parameters of an HMM, that\nis, the A and B matrices. Formally,\nLearning: Given an observation sequence O and the set of possible\nstates in the HMM, learn the HMM parameters A and B.",
  "481": "474\nAPPENDIX A\n•\nHIDDEN MARKOV MODELS\nThe input to such a learning algorithm would be an unlabeled sequence of ob-\nservations O and a vocabulary of potential hidden states Q. Thus, for the ice cream\ntask, we would start with a sequence of observations O = {1,3,2,...,} and the set of\nhidden states H and C.\nThe standard algorithm for HMM training is the forward-backward, or Baum-\nForward-\nbackward\nWelch algorithm (Baum, 1972), a special case of the Expectation-Maximization\nBaum-Welch\nor EM algorithm (Dempster et al., 1977). The algorithm will let us train both the\nEM\ntransition probabilities A and the emission probabilities B of the HMM. EM is an\niterative algorithm, computing an initial estimate for the probabilities, then using\nthose estimates to computing a better estimate, and so on, iteratively improving the\nprobabilities that it learns.\nLet us begin by considering the much simpler case of training a fully visible\nMarkov model, we’re know both the temperature and the ice cream count for every\nday. That is, imagine we see the following set of input observations and magically\nknew the aligned hidden state sequences:\n3\n3\n2\n1\n1\n2\n1\n2\n3\nhot\nhot cold\ncold cold cold\ncold hot hot\nThis would easily allow us to compute the HMM parameters just by maximum\nlikelihood estimation from the training data. First, we can compute π from the count\nof the 3 initial hidden states:\nπh = 1/3\nπc = 2/3\nNext we can directly compute the A matrix from the transitions, ignoring the ﬁnal\nhidden states:\np(hot|hot) = 2/3\np(cold|hot) = 1/3\np(cold|cold) = 1/2\np(hot|cold) = 1/2\nand the B matrix:\nP(1|hot) = 0/4 = 0\np(1|cold) = 3/5 = .6\nP(2|hot) = 1/4 = .25\np(2|cold = 2/5 = .4\nP(3|hot) = 3/4 = .75\np(3|cold) = 0\nFor a real HMM, we cannot compute these counts directly from an observation\nsequence since we don’t know which path of states was taken through the machine\nfor a given input. For example, suppose I didn’t tell you the temperature on day 2,\nand you had to guess it, but you (magically) had the above probabilities, and the\ntemperatures on the other days. You could do some Bayesian arithmetic with all the\nother probabilities to get estimates of the likely temperature on that missing day, and\nuse those to get expected counts for the temperatures for day 2.\nBut the real problem is even harder: we don’t know the counts of being in any\nof the hidden states!! The Baum-Welch algorithm solves this by iteratively esti-\nmating the counts. We will start with an estimate for the transition and observation\nprobabilities and then use these estimated probabilities to derive better and better\nprobabilities. And we’re going to do this by computing the forward probability for\nan observation and then dividing that probability mass among all the different paths\nthat contributed to this forward probability.\nTo understand the algorithm, we need to deﬁne a useful probability related to the\nforward probability and called the backward probability. The backward probabil-\nbackward\nprobability",
  "482": "A.5\n•\nHMM TRAINING: THE FORWARD-BACKWARD ALGORITHM\n475\nity β is the probability of seeing the observations from time t + 1 to the end, given\nthat we are in state i at time t (and given the automaton λ):\nβt(i) = P(ot+1,ot+2 ...oT|qt = i,λ)\n(A.15)\nIt is computed inductively in a similar manner to the forward algorithm.\n1. Initialization:\nβT(i) = 1, 1 ≤i ≤N\n2. Recursion\nβt(i) =\nN\nX\nj=1\naij bj(ot+1) βt+1(j), 1 ≤i ≤N,1 ≤t < T\n3. Termination:\nP(O|λ) =\nN\nX\nj=1\nπ j bj(o1) β1(j)\nFigure A.11 illustrates the backward induction step.\not+1\not\nai1\nai2\naiN\nai3\nb1(ot+1)\nβt(i)= Σj βt+1(j) aij  bj(ot+1) \nq1\nq2\nq3\nqN\nq1\nqi\nq2\nq1\nq2\not-1\nq3\nqN\nβt+1(N)\nβt+1(3)\nβt+1(2)\nβt+1(1)\nb2(ot+1)\nb3(ot+1)\nbN(ot+1)\nFigure A.11\nThe computation of βt(i) by summing all the successive values βt+1(j)\nweighted by their transition probabilities ai j and their observation probabilities b j(ot+1). Start\nand end states not shown.\nWe are now ready to see how the forward and backward probabilities can help\ncompute the transition probability aij and observation probability bi(ot) from an ob-\nservation sequence, even though the actual path taken through the model is hidden.\nLet’s begin by seeing how to estimate ˆaij by a variant of simple maximum like-\nlihood estimation:\nˆai j = expected number of transitions from state i to state j\nexpected number of transitions from state i\n(A.16)\nHow do we compute the numerator? Here’s the intuition. Assume we had some\nestimate of the probability that a given transition i →j was taken at a particular\npoint in time t in the observation sequence. If we knew this probability for each",
  "483": "476\nAPPENDIX A\n•\nHIDDEN MARKOV MODELS\nparticular time t, we could sum over all times t to estimate the total count for the\ntransition i →j.\nMore formally, let’s deﬁne the probability ξt as the probability of being in state\ni at time t and state j at time t +1, given the observation sequence and of course the\nmodel:\nξt(i, j) = P(qt = i,qt+1 = j|O,λ)\n(A.17)\nTo compute ξt, we ﬁrst compute a probability which is similar to ξt, but differs in\nincluding the probability of the observation; note the different conditioning of O\nfrom Eq. A.17:\nnot-quite-ξt(i, j) = P(qt = i,qt+1 = j,O|λ)\n(A.18)\not+2\not+1\nαt(i)\not-1\not\naijbj(ot+1) \nsi\nsj\nβt+1(j)\nFigure A.12\nComputation of the joint probability of being in state i at time t and state j at\ntime t + 1. The ﬁgure shows the various probabilities that need to be combined to produce\nP(qt = i,qt+1 = j,O|λ): the α and β probabilities, the transition probability ai j and the\nobservation probability bj(ot+1). After Rabiner (1989) which is c⃝1989 IEEE.\nFigure A.12 shows the various probabilities that go into computing not-quite-ξt:\nthe transition probability for the arc in question, the α probability before the arc, the\nβ probability after the arc, and the observation probability for the symbol just after\nthe arc. These four are multiplied together to produce not-quite-ξt as follows:\nnot-quite-ξt(i, j) = αt(i)aijbj(ot+1)βt+1(j)\n(A.19)\nTo compute ξt from not-quite-ξt, we follow the laws of probability and divide by\nP(O|λ), since\nP(X|Y,Z) = P(X,Y|Z)\nP(Y|Z)\n(A.20)\nThe probability of the observation given the model is simply the forward proba-\nbility of the whole utterance (or alternatively, the backward probability of the whole\nutterance):\nP(O|λ) =\nN\nX\nj=1\nαt(j)βt(j)\n(A.21)",
  "484": "A.5\n•\nHMM TRAINING: THE FORWARD-BACKWARD ALGORITHM\n477\nSo, the ﬁnal equation for ξt is\nξt(i, j) = αt(i)aijbj(ot+1)βt+1(j)\nPN\nj=1 αt(j)βt(j)\n(A.22)\nThe expected number of transitions from state i to state j is then the sum over all\nt of ξ. For our estimate of aij in Eq. A.16, we just need one more thing: the total\nexpected number of transitions from state i. We can get this by summing over all\ntransitions out of state i. Here’s the ﬁnal formula for ˆaij:\nˆaij =\nPT−1\nt=1 ξt(i, j)\nPT−1\nt=1\nPN\nk=1 ξt(i,k)\n(A.23)\nWe also need a formula for recomputing the observation probability. This is the\nprobability of a given symbol vk from the observation vocabulary V, given a state j:\nˆb j(vk). We will do this by trying to compute\nˆbj(vk) = expected number of times in state j and observing symbol vk\nexpected number of times in state j\n(A.24)\nFor this, we will need to know the probability of being in state j at time t, which\nwe will call γt(j):\nγt(j) = P(qt = j|O,λ)\n(A.25)\nOnce again, we will compute this by including the observation sequence in the\nprobability:\nγt(j) = P(qt = j,O|λ)\nP(O|λ)\n(A.26)\not+1\nαt(j)\not-1\not\nsj\nβt(j)\nFigure A.13\nThe computation of γt(j), the probability of being in state j at time t. Note\nthat γ is really a degenerate case of ξ and hence this ﬁgure is like a version of Fig. A.12 with\nstate i collapsed with state j. After Rabiner (1989) which is c⃝1989 IEEE.\nAs Fig. A.13 shows, the numerator of Eq. A.26 is just the product of the forward\nprobability and the backward probability:\nγt(j) = αt(j)βt(j)\nP(O|λ)\n(A.27)",
  "485": "478\nAPPENDIX A\n•\nHIDDEN MARKOV MODELS\nWe are ready to compute b. For the numerator, we sum γt(j) for all time steps\nt in which the observation ot is the symbol vk that we are interested in. For the\ndenominator, we sum γt(j) over all time steps t. The result is the percentage of the\ntimes that we were in state j and saw symbol vk (the notation PT\nt=1 s.t.Ot=vk means\n“sum over all t for which the observation at time t was vk”):\nˆbj(vk) =\nPT\nt=1 s.t.Ot=vk γt(j)\nPT\nt=1 γt(j)\n(A.28)\nWe now have ways in Eq. A.23 and Eq. A.28 to re-estimate the transition A and ob-\nservation B probabilities from an observation sequence O, assuming that we already\nhave a previous estimate of A and B.\nThese re-estimations form the core of the iterative forward-backward algorithm.\nThe forward-backward algorithm (Fig. A.14) starts with some initial estimate of the\nHMM parameters λ = (A,B). We then iteratively run two steps. Like other cases of\nthe EM (expectation-maximization) algorithm, the forward-backward algorithm has\ntwo steps: the expectation step, or E-step, and the maximization step, or M-step.\nE-step\nM-step\nIn the E-step, we compute the expected state occupancy count γ and the expected\nstate transition count ξ from the earlier A and B probabilities. In the M-step, we use\nγ and ξ to recompute new A and B probabilities.\nfunction FORWARD-BACKWARD(observations of len T, output vocabulary V, hidden\nstate set Q) returns HMM=(A,B)\ninitialize A and B\niterate until convergence\nE-step\nγt(j) = αt(j)βt(j)\nαT (qF)\n∀t and j\nξt(i, j) = αt(i)aijb j(ot+1)βt+1(j)\nαT (qF)\n∀t, i, and j\nM-step\nˆaij =\nT−1\nX\nt=1\nξt(i, j)\nT−1\nX\nt=1\nN\nX\nk=1\nξt(i,k)\nˆbj(vk) =\nT\nX\nt=1s.t. Ot=vk\nγt(j)\nT\nX\nt=1\nγt(j)\nreturn A, B\nFigure A.14\nThe forward-backward algorithm.\nAlthough in principle the forward-backward algorithm can do completely unsu-\npervised learning of the A and B parameters, in practice the initial conditions are\nvery important. For this reason the algorithm is often given extra information. For\nexample, for HMM-based speech recognition, the HMM structure is often set by\nhand, and only the emission (B) and (non-zero) A transition probabilities are trained\nfrom a set of observation sequences O.",
  "486": "A.6\n•\nSUMMARY\n479\nA.6\nSummary\nThis chapter introduced the hidden Markov model for probabilistic sequence clas-\nsiﬁcation.\n• Hidden Markov models (HMMs) are a way of relating a sequence of obser-\nvations to a sequence of hidden classes or hidden states that explain the\nobservations.\n• The process of discovering the sequence of hidden states, given the sequence\nof observations, is known as decoding or inference. The Viterbi algorithm is\ncommonly used for decoding.\n• The parameters of an HMM are the A transition probability matrix and the B\nobservation likelihood matrix. Both can be trained with the Baum-Welch or\nforward-backward algorithm.\nBibliographical and Historical Notes\nAs we discussed in Chapter 8, Markov chains were ﬁrst used by Markov (1913)\n(translation Markov 2006), to predict whether an upcoming letter in Pushkin’s Eu-\ngene Onegin would be a vowel or a consonant. The hidden Markov model was de-\nveloped by Baum and colleagues at the Institute for Defense Analyses in Princeton\n(Baum and Petrie 1966, Baum and Eagon 1967).\nThe Viterbi algorithm was ﬁrst applied to speech and language processing in the\ncontext of speech recognition by Vintsyuk (1968) but has what Kruskal (1983) calls\na “remarkable history of multiple independent discovery and publication”. Kruskal\nand others give at least the following independently-discovered variants of the algo-\nrithm published in four separate ﬁelds:\nCitation\nField\nViterbi (1967)\ninformation theory\nVintsyuk (1968)\nspeech processing\nNeedleman and Wunsch (1970) molecular biology\nSakoe and Chiba (1971)\nspeech processing\nSankoff (1972)\nmolecular biology\nReichert et al. (1973)\nmolecular biology\nWagner and Fischer (1974)\ncomputer science\nThe use of the term Viterbi is now standard for the application of dynamic pro-\ngramming to any kind of probabilistic maximization problem in speech and language\nprocessing. For non-probabilistic problems (such as for minimum edit distance), the\nplain term dynamic programming is often used. Forney, Jr. (1973) wrote an early\nsurvey paper that explores the origin of the Viterbi algorithm in the context of infor-\nmation and communications theory.\nOur presentation of the idea that hidden Markov models should be characterized\nby three fundamental problems was modeled after an inﬂuential tutorial by Rabiner\n(1989), which was itself based on tutorials by Jack Ferguson of IDA in the 1960s.\nJelinek (1997) and Rabiner and Juang (1993) give very complete descriptions of the\nforward-backward algorithm as applied to the speech recognition problem. Jelinek\n(1997) also shows the relationship between forward-backward and EM.",
  "487": "480\nAPPENDIX B\n•\nSPELLING CORRECTION AND THE NOISY CHANNEL\nCHAPTER\nB\nSpelling Correction and the\nNoisy Channel\nALGERNON: But my own sweet Cecily, I have never written you any letters.\nCECILY: You need hardly remind me of that, Ernest. I remember only too well\nthat I was forced to write your letters for you. I wrote always three times a week,\nand sometimes oftener.\nALGERNON: Oh, do let me read them, Cecily?\nCECILY: Oh, I couldn’t possibly. They would make you far too conceited. The\nthree you wrote me after I had broken off the engagement are so beautiful, and\nso badly spelled, that even now I can hardly read them without crying a little.\nOscar Wilde, The Importance of Being Earnest\nLike Oscar Wilde’s fabulous Cecily, a lot of people were thinking about spelling\nduring the last turn of the century. Gilbert and Sullivan provide many examples. The\nGondoliers’ Giuseppe, for example, worries that his private secretary is “shaky in his\nspelling”, while Iolanthe’s Phyllis can “spell every word that she uses”. Thorstein\nVeblen’s explanation (in his 1899 classic The Theory of the Leisure Class) was that\na main purpose of the “archaic, cumbrous, and ineffective” English spelling system\nwas to be difﬁcult enough to provide a test of membership in the leisure class.\nWhatever the social role of spelling, we can certainly agree that many more of\nus are like Cecily than like Phyllis. Estimates for the frequency of spelling errors\nin human-typed text vary from 1-2% for carefully retyping already printed text to\n10-15% for web queries.\nIn this chapter we introduce the problem of detecting and correcting spelling\nerrors. Fixing spelling errors is an integral part of writing in the modern world,\nwhether this writing is part of texting on a phone, sending email, writing longer\ndocuments, or ﬁnding information on the web. Modern spell correctors aren’t perfect\n(indeed, autocorrect-gone-wrong is a popular source of amusement on the web) but\nthey are ubiquitous in pretty much any software that relies on keyboard input.\nSpelling correction is often considered from two perspectives. Non-word spelling\ncorrection is the detection and correction of spelling errors that result in non-words\n(like graffe for giraffe). By contrast, real word spelling correction is the task of\ndetecting and correcting spelling errors even if they accidentally result in an actual\nword of English (real-word errors). This can happen from typographical errors\nreal-word\nerrors\n(insertion, deletion, transposition) that accidentally produce a real word (e.g., there\nfor three), or cognitive errors where the writer substituted the wrong spelling of a\nhomophone or near-homophone (e.g., dessert for desert, or piece for peace).\nNon-word errors are detected by looking for any word not found in a dictio-\nnary. For example, the misspelling graffe above would not occur in a dictionary.\nThe larger the dictionary the better; modern systems often use enormous dictio-",
  "488": "B.1\n•\nTHE NOISY CHANNEL MODEL\n481\nnaries derived from the web. To correct non-word spelling errors we ﬁrst generate\ncandidates: real words that have a similar letter sequence to the error. Candidate\ncandidates\ncorrections from the spelling error graffe might include giraffe, graf, gaffe, grail, or\ncraft. We then rank the candidates using a distance metric between the source and\nthe surface error. We’d like a metric that shares our intuition that giraffe is a more\nlikely source than grail for graffe because giraffe is closer in spelling to graffe than\ngrail is to graffe. The minimum edit distance algorithm from Chapter 2 will play a\nrole here. But we’d also like to prefer corrections that are more frequent words, or\nmore likely to occur in the context of the error. The noisy channel model introduced\nin the next section offers a way to formalize this intuition.\nReal word spelling error detection is a much more difﬁcult task, since any word\nin the input text could be an error. Still, it is possible to use the noisy channel to ﬁnd\ncandidates for each word w typed by the user, and rank the correction that is most\nlikely to have been the users original intention.\nB.1\nThe Noisy Channel Model\nIn this section we introduce the noisy channel model and show how to apply it to\nthe task of detecting and correcting spelling errors. The noisy channel model was\napplied to the spelling correction task at about the same time by researchers at AT&T\nBell Laboratories (Kernighan et al. 1990, Church and Gale 1991) and IBM Watson\nResearch (Mays et al., 1991).\ndecoder\n \nnoisy word\noriginal word\nnoisy channel\nguessed word\nnoisy 1\nnoisy 2\nnoisy N\nword hyp1\nword hyp2\n...\nword hyp3\nFigure B.1\nIn the noisy channel model, we imagine that the surface form we see is actually\na “distorted” form of an original word passed through a noisy channel. The decoder passes\neach hypothesis through a model of this channel and picks the word that best matches the\nsurface noisy word.\nThe intuition of the noisy channel model (see Fig. B.1) is to treat the misspelled\nnoisy channel\nword as if a correctly spelled word had been “distorted” by being passed through a\nnoisy communication channel.\nThis channel introduces “noise” in the form of substitutions or other changes to\nthe letters, making it hard to recognize the “true” word. Our goal, then, is to build a\nmodel of the channel. Given this model, we then ﬁnd the true word by passing every\nword of the language through our model of the noisy channel and seeing which one\ncomes the closest to the misspelled word.",
  "489": "482\nAPPENDIX B\n•\nSPELLING CORRECTION AND THE NOISY CHANNEL\nThis noisy channel model is a kind of Bayesian inference. We see an obser-\nBayesian\nvation x (a misspelled word) and our job is to ﬁnd the word w that generated this\nmisspelled word. Out of all possible words in the vocabulary V we want to ﬁnd the\nword w such that P(w|x) is highest. We use the hat notation ˆ to mean “our estimate\nof the correct word”.\nˆw = argmax\nw∈V\nP(w|x)\n(B.1)\nThe function argmaxx f(x) means “the x such that f(x) is maximized”. Equa-\nargmax\ntion B.1 thus means, that out of all words in the vocabulary, we want the particular\nword that maximizes the right-hand side P(w|x).\nThe intuition of Bayesian classiﬁcation is to use Bayes’ rule to transform Eq. B.1\ninto a set of other probabilities. Bayes’ rule is presented in Eq. B.2; it gives us a way\nto break down any conditional probability P(a|b) into three other probabilities:\nP(a|b) = P(b|a)P(a)\nP(b)\n(B.2)\nWe can then substitute Eq. B.2 into Eq. B.1 to get Eq. B.3:\nˆw = argmax\nw∈V\nP(x|w)P(w)\nP(x)\n(B.3)\nWe can conveniently simplify Eq. B.3 by dropping the denominator P(x). Why\nis that? Since we are choosing a potential correction word out of all words, we will\nbe computing P(x|w)P(w)\nP(x)\nfor each word. But P(x) doesn’t change for each word; we\nare always asking about the most likely word for the same observed error x, which\nmust have the same probability P(x). Thus, we can choose the word that maximizes\nthis simpler formula:\nˆw = argmax\nw∈V\nP(x|w)P(w)\n(B.4)\nTo summarize, the noisy channel model says that we have some true underlying\nword w, and we have a noisy channel that modiﬁes the word into some possible\nmisspelled observed surface form. The likelihood or channel model of the noisy\nlikelihood\nchannel model\nchannel producing any particular observation sequence x is modeled by P(x|w). The\nprior probability of a hidden word is modeled by P(w). We can compute the most\nprior\nprobability\nprobable word ˆw given that we’ve seen some observed misspelling x by multiply-\ning the prior P(w) and the likelihood P(x|w) and choosing the word for which this\nproduct is greatest.\nWe apply the noisy channel approach to correcting non-word spelling errors by\ntaking any word not in our spell dictionary, generating a list of candidate words,\nranking them according to Eq. B.4, and picking the highest-ranked one. We can\nmodify Eq. B.4 to refer to this list of candidate words instead of the full vocabulary\nV as follows:\nˆw = argmax\nw∈C\nchannel model\nz }| {\nP(x|w)\nprior\nz}|{\nP(w)\n(B.5)\nThe noisy channel algorithm is shown in Fig. B.2.\nTo see the details of the computation of the likelihood and the prior (language\nmodel), let’s walk through an example, applying the algorithm to the example mis-\nspelling acress. The ﬁrst stage of the algorithm proposes candidate corrections by",
  "490": "B.1\n•\nTHE NOISY CHANNEL MODEL\n483\nfunction NOISY CHANNEL SPELLING(word x,dict D, lm,editprob) returns correction\nif x /∈D\ncandidates, edits←All strings at edit distance 1 from x that are ∈D, and their edit\nfor each c,e in candidates, edits\nchannel←editprob(e)\nprior←lm(x)\nscore[c] = log channel + log prior\nreturn argmaxc score[c]\nFigure B.2\nNoisy channel model for spelling correction for unknown words.\nﬁnding words that have a similar spelling to the input word. Analysis of spelling\nerror data has shown that the majority of spelling errors consist of a single-letter\nchange and so we often make the simplifying assumption that these candidates have\nan edit distance of 1 from the error word. To ﬁnd this list of candidates we’ll use\nthe minimum edit distance algorithm introduced in Chapter 2, but extended so that\nin addition to insertions, deletions, and substitutions, we’ll add a fourth type of edit,\ntranspositions, in which two letters are swapped. The version of edit distance with\ntransposition is called Damerau-Levenshtein edit distance. Applying all such sin-\nDamerau-\nLevenshtein\ngle transformations to acress yields the list of candidate words in Fig. B.3.\nTransformation\nCorrect\nError\nPosition\nError\nCorrection\nLetter\nLetter\n(Letter #)\nType\nacress\nactress\nt\n—\n2\ndeletion\nacress\ncress\n—\na\n0\ninsertion\nacress\ncaress\nca\nac\n0\ntransposition\nacress\naccess\nc\nr\n2\nsubstitution\nacress\nacross\no\ne\n3\nsubstitution\nacress\nacres\n—\ns\n5\ninsertion\nacress\nacres\n—\ns\n4\ninsertion\nFigure B.3\nCandidate corrections for the misspelling acress and the transformations that\nwould have produced the error (after Kernighan et al. (1990)). “—” represents a null letter.\nOnce we have a set of a candidates, to score each one using Eq. B.5 requires that\nwe compute the prior and the channel model.\nThe prior probability of each correction P(w) is the language model probability\nof the word w in context, which can be computed using any language model, from\nunigram to trigram or 4-gram. For this example let’s start in the following table by\nassuming a unigram language model. We computed the language model from the\n404,253,213 words in the Corpus of Contemporary English (COCA).\nw\ncount(w) p(w)\nactress 9,321\n.0000231\ncress\n220\n.000000544\ncaress\n686\n.00000170\naccess 37,038\n.0000916\nacross\n120,844\n.000299\nacres\n12,874\n.0000318\nHow can we estimate the likelihood P(x|w), also called the channel model or\nchannel model",
  "491": "484\nAPPENDIX B\n•\nSPELLING CORRECTION AND THE NOISY CHANNEL\nerror model? A perfect model of the probability that a word will be mistyped would\nerror model\ncondition on all sorts of factors: who the typist was, whether the typist was left-\nhanded or right-handed, and so on. Luckily, we can get a pretty reasonable estimate\nof P(x|w) just by looking at local context: the identity of the correct letter itself, the\nmisspelling, and the surrounding letters. For example, the letters m and n are often\nsubstituted for each other; this is partly a fact about their identity (these two letters\nare pronounced similarly and they are next to each other on the keyboard) and partly\na fact about context (because they are pronounced similarly and they occur in similar\ncontexts).\nA simple model might estimate, for example, p(acress|across) just using the\nnumber of times that the letter e was substituted for the letter o in some large corpus\nof errors. To compute the probability for each edit in this way we’ll need a confu-\nsion matrix that contains counts of errors. In general, a confusion matrix lists the\nconfusion\nmatrix\nnumber of times one thing was confused with another. Thus for example a substi-\ntution matrix will be a square matrix of size 26×26 (or more generally |A| × |A|,\nfor an alphabet A) that represents the number of times one letter was incorrectly\nused instead of another. Following Kernighan et al. (1990) we’ll use four confusion\nmatrices.\ndel[x,y]: count(xy typed as x)\nins[x,y]: count(x typed as xy)\nsub[x,y]: count(x typed as y)\ntrans[x,y]: count(xy typed as yx)\nNote that we’ve conditioned the insertion and deletion probabilities on the previ-\nous character; we could instead have chosen to condition on the following character.\nWhere do we get these confusion matrices? One way is to extract them from\nlists of misspellings like the following:\nadditional: addional, additonal\nenvironments: enviornments, enviorments, enviroments\npreceded: preceeded\n...\nThere are lists available on Wikipedia and from Roger Mitton (http://www.\ndcs.bbk.ac.uk/˜ROGER/corpora.html) and Peter Norvig (http://norvig.\ncom/ngrams/). Norvig also gives the counts for each single-character edit that can\nbe used to directly create the error model probabilities.\nAn alternative approach used by Kernighan et al. (1990) is to compute the ma-\ntrices by iteratively using this very spelling error correction algorithm itself. The\niterative algorithm ﬁrst initializes the matrices with equal values; thus, any character\nis equally likely to be deleted, equally likely to be substituted for any other char-\nacter, etc. Next, the spelling error correction algorithm is run on a set of spelling\nerrors. Given the set of typos paired with their predicted corrections, the confusion\nmatrices can now be recomputed, the spelling algorithm run again, and so on. This\niterative algorithm is an instance of the important EM algorithm (Dempster et al.,\n1977), which we discuss in Appendix A.\nOnce we have the confusion matrices, we can estimate P(x|w) as follows (where",
  "492": "B.1\n•\nTHE NOISY CHANNEL MODEL\n485\nwi is the ith character of the correct word w) and xi is the ith character of the typo x:\nP(x|w) =\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ndel[xi−1,wi]\ncount[xi−1wi] , if deletion\nins[xi−1,wi]\ncount[wi−1] , if insertion\nsub[xi,wi]\ncount[wi] , if substitution\ntrans[wi,wi+1]\ncount[wiwi+1] , if transposition\n(B.6)\nUsing the counts from Kernighan et al. (1990) results in the error model proba-\nbilities for acress shown in Fig. B.4.\nCandidate\nCorrect\nError\nCorrection\nLetter\nLetter\nx|w\nP(x|w)\nactress\nt\n-\nc|ct\n.000117\ncress\n-\na\na|#\n.00000144\ncaress\nca\nac\nac|ca\n.00000164\naccess\nc\nr\nr|c\n.000000209\nacross\no\ne\ne|o\n.0000093\nacres\n-\ns\nes|e\n.0000321\nacres\n-\ns\nss|s\n.0000342\nFigure B.4\nChannel model for acress; the probabilities are taken from the del[], ins[],\nsub[], and trans[] confusion matrices as shown in Kernighan et al. (1990).\nFigure B.5 shows the ﬁnal probabilities for each of the potential corrections;\nthe unigram prior is multiplied by the likelihood (computed with Eq. B.6 and the\nconfusion matrices). The ﬁnal column shows the product, multiplied by 109 just for\nreadability.\nCandidate Correct Error\nCorrection Letter\nLetter x|w\nP(x|w)\nP(w)\n109*P(x|w)P(w)\nactress\nt\n-\nc|ct\n.000117\n.0000231\n2.7\ncress\n-\na\na|#\n.00000144\n.000000544 0.00078\ncaress\nca\nac\nac|ca .00000164\n.00000170\n0.0028\naccess\nc\nr\nr|c\n.000000209 .0000916\n0.019\nacross\no\ne\ne|o\n.0000093\n.000299\n2.8\nacres\n-\ns\nes|e\n.0000321\n.0000318\n1.0\nacres\n-\ns\nss|s\n.0000342\n.0000318\n1.0\nFigure B.5\nComputation of the ranking for each candidate correction, using the language\nmodel shown earlier and the error model from Fig. B.4. The ﬁnal score is multiplied by 109\nfor readability.\nThe computations in Fig. B.5 show that our implementation of the noisy channel\nmodel chooses across as the best correction, and actress as the second most\nlikely word.\nUnfortunately, the algorithm was wrong here; the writer’s intention becomes\nclear from the context: ...was called a “stellar and versatile acress whose com-\nbination of sass and glamour has deﬁned her...”. The surrounding words make it\nclear that actress and not across was the intended word.",
  "493": "486\nAPPENDIX B\n•\nSPELLING CORRECTION AND THE NOISY CHANNEL\nFor this reason, it is important to use larger language models than unigrams.\nFor example, if we use the Corpus of Contemporary American English to compute\nbigram probabilities for the words actress and across in their context using add-one\nsmoothing, we get the following probabilities:\nP(actress|versatile) = .000021\nP(across|versatile) = .000021\nP(whose|actress) = .0010\nP(whose|across) = .000006\nMultiplying these out gives us the language model estimate for the two candi-\ndates in context:\nP(“versatile actress whose”) = .000021∗.0010 = 210×10−10\nP(“versatile across whose”) = .000021∗.000006 = 1×10−10\nCombining the language model with the error model in Fig. B.5, the bigram\nnoisy channel model now chooses the correct word actress.\nEvaluating spell correction algorithms is generally done by holding out a train-\ning, development and test set from lists of errors like those on the Norvig and Mitton\nsites mentioned above.\nB.2\nReal-word spelling errors\nThe noisy channel approach can also be applied to detect and correct real-word\nspelling errors, errors that result in an actual word of English. This can happen from\nreal-word error\ndetection\ntypographical errors (insertion, deletion, transposition) that accidentally produce a\nreal word (e.g., there for three) or because the writer substituted the wrong spelling\nof a homophone or near-homophone (e.g., dessert for desert, or piece for peace). A\nnumber of studies suggest that between 25% and 40% of spelling errors are valid\nEnglish words as in the following examples (Kukich, 1992):\nThis used to belong to thew queen. They are leaving in about ﬁfteen minuets to go to her house.\nThe design an construction of the system will take more than a year.\nCan they lave him my messages?\nThe study was conducted mainly be John Black.\nThe noisy channel can deal with real-word errors as well. Let’s begin with a\nversion of the noisy channel model ﬁrst proposed by Mays et al. (1991) to deal\nwith these real-word spelling errors. Their algorithm takes the input sentence X =\n{x1,x2,...,xk,...,xn}, generates a large set of candidate correction sentences C(X),\nthen picks the sentence with the highest language model probability.\nTo generate the candidate correction sentences, we start by generating a set of\ncandidate words for each input word xi. The candidates, C(xi), include every English\nword with a small edit distance from xi. With edit distance 1, a common choice\n(Mays et al., 1991), the candidate set for the real word error thew (a rare word\nmeaning ‘muscular strength’) might be C(thew) = {the, thaw, threw, them, thwe}.\nWe then make the simplifying assumption that every sentence has only one error.\nThus the set of candidate sentences C(X) for a sentence X = Only two of thew\napples would be:",
  "494": "B.2\n•\nREAL-WORD SPELLING ERRORS\n487\nonly two of thew apples\noily two of thew apples\nonly too of thew apples\nonly to of thew apples\nonly tao of the apples\nonly two on thew apples\nonly two off thew apples\nonly two of the apples\nonly two of threw apples\nonly two of thew applies\nonly two of thew dapples\n...\nEach sentence is scored by the noisy channel:\nˆW = argmax\nW∈C(X)\nP(X|W)P(W)\n(B.7)\nFor P(W), we can use the trigram probability of the sentence.\nWhat about the channel model? Since these are real words, we need to consider\nthe possibility that the input word is not an error. Let’s say that the channel proba-\nbility of writing a word correctly, P(w|w), is α; we can make different assumptions\nabout exactly what the value of α is in different tasks; perhaps α is .95, assum-\ning people write 1 word wrong out of 20, for some tasks, or maybe .99 for others.\nMays et al. (1991) proposed a simple model: given a typed word x, let the channel\nmodel P(x|w) be α when x = w, and then just distribute 1−α evenly over all other\ncandidate corrections C(x):\np(x|w) =\n\n\n\n\n\n\n\n\n\nα\nif x = w\n1−α\n|C(x)|\nif x ∈C(x)\n0\notherwise\n(B.8)\nNow we can replace the equal distribution of 1−α over all corrections in Eq. B.8;\nwe’ll make the distribution proportional to the edit probability from the more sophis-\nticated channel model from Eq. B.6 that used the confusion matrices.\nLet’s see an example of this integrated noisy channel model applied to a real\nword. Suppose we see the string two of thew. The author might have intended\nto type the real word thew (‘muscular strength’). But thew here could also be a\ntypo for the or some other word. For the purposes of this example let’s consider\nedit distance 1, and only the following ﬁve candidates the, thaw, threw, and thwe\n(a rare name) and the string as typed, thew. We took the edit probabilities from\nNorvig’s (2009) analysis of this example. For the language model probabilities, we\nused a Stupid Backoff model (Section 3.6) trained on the Google N-grams:\nP(the|two of)\n= 0.476012\nP(thew|two of)\n= 9.95051 ×10−8\nP(thaw|two of)\n= 2.09267 ×10−7\nP(threw|two of) = 8.9064 ×10−7\nP(them|two of)\n= 0.00144488\nP(thwe|two of)\n= 5.18681 ×10−9\nHere we’ve just computed probabilities for the single phrase two of thew, but\nthe model applies to entire sentences; so if the example in context was two of thew",
  "495": "488\nAPPENDIX B\n•\nSPELLING CORRECTION AND THE NOISY CHANNEL\npeople, we’d need to also multiply in probabilities for P(people|of the), P(people|of\nthew), P(people|of threw), and so on.\nFollowing Norvig (2009), we assume that the probability of a word being a typo\nin this task is .05, meaning that α = P(w|w) is .95. Fig. B.6 shows the computation.\nx\nw\nx|w\nP(x|w)\nP(w|wi−2,wi−1) 108P(x|w)P(w|wi−2,wi−1)\nthew the\new|e\n0.000007 0.48\n333\nthew thew\nα=0.95\n9.95 ×10−8\n9.45\nthew thaw\ne|a\n0.001\n2.1 ×10−7\n0.0209\nthew threw h|hr\n0.000008 8.9 ×10−7\n0.000713\nthew thwe\new|we 0.000003 5.2 ×10−9\n0.00000156\nFigure B.6\nThe noisy channel model on 5 possible candidates for thew, with a Stupid\nBackoff trigram language model computed from the Google N-gram corpus and the error\nmodel from Norvig (2009).\nFor the error phrase two of thew, the model correctly picks the as the correction.\nBut note that a lower error rate might change things; in a task where the probability\nof an error is low enough (α is very high), the model might instead decide that the\nword thew was what the writer intended.\nB.3\nNoisy Channel Model: The State of the Art\nState of the art implementations of noisy channel spelling correction make a number\nof extensions to the simple models we presented above.\nFirst, rather than make the assumption that the input sentence has only a sin-\ngle error, modern systems go through the input one word at a time, using the noisy\nchannel to make a decision for that word. But if we just run the basic noisy chan-\nnel system described above on each word, it is prone to overcorrecting, replacing\ncorrect but rare words (for example names) with more frequent words (Whitelaw\net al. 2009, Wilcox-O’Hearn 2014). Modern algorithms therefore need to augment\nthe noisy channel with methods for detecting whether or not a real word should ac-\ntually be corrected. For example state of the art systems like Google’s (Whitelaw\net al., 2009) use a blacklist, forbidding certain tokens (like numbers, punctuation,\nand single letter words) from being changed. Such systems are also more cautious\nin deciding whether to trust a candidate correction. Instead of just choosing a candi-\ndate correction if it has a higher probability P(w|x) than the word itself, these more\ncareful systems choose to suggest a correction w over keeping the non-correction x\nonly if the difference in probabilities is sufﬁciently great. The best correction w is\nchosen only if:\nlogP(w|x)−logP(x|x) > θ\nDepending on the speciﬁc application, spell-checkers may decide to autocorrect\nautocorrect\n(automatically change a spelling to a hypothesized correction) or merely to ﬂag the\nerror and offer suggestions. This decision is often made by another classiﬁer which\ndecides whether the best candidate is good enough, using features such as the dif-\nference in log probabilities between the candidates (we’ll introduce algorithms for\nclassiﬁcation in the next chapter).\nModern systems also use much larger dictionaries than early systems. Ahmad\nand Kondrak (2005) found that a 100,000 word UNIX dictionary only contained",
  "496": "B.3\n•\nNOISY CHANNEL MODEL: THE STATE OF THE ART\n489\n73% of the word types in their corpus of web queries, missing words like pics,\nmultiplayer, google, xbox, clipart, and mallorca. For this reason modern systems\noften use much larger dictionaries automatically derived from very large lists of\nunigrams like the Google N-gram corpus. Whitelaw et al. (2009), for example,\nused the most frequently occurring ten million word types in a large sample of web\npages. Because this list will include lots of misspellings, their system requires a\nmore sophisticated error model. The fact that words are generally more frequent than\ntheir misspellings can be used in candidate suggestion, by building a set of words\nand spelling variations that have similar contexts, sorting by frequency, treating the\nmost frequent variant as the source, and learning an error model from the difference,\nwhether from web text (Whitelaw et al., 2009) or from query logs (Cucerzan and\nBrill, 2004). Words can also be automatically added to the dictionary when a user\nrejects a correction, and systems running on phones can automatically add words\nfrom the user’s address book or calendar.\nWe can also improve the performance of the noisy channel model by changing\nhow the prior and the likelihood are combined. In the standard model they are just\nmultiplied together. But often these probabilities are not commensurate; the lan-\nguage model or the channel model might have very different ranges. Alternatively\nfor some task or dataset we might have reason to trust one of the two models more.\nTherefore we use a weighted combination, by raising one of the factors to a power\nλ:\nˆw = argmax\nw∈V\nP(x|w)P(w)λ\n(B.9)\nor in log space:\nˆw = argmax\nw∈V\nlogP(x|w)+λ logP(w)\n(B.10)\nWe then tune the parameter λ on a development test set.\nFinally, if our goal is to do real-word spelling correction only for speciﬁc con-\nfusion sets like peace/piece, affect/effect, weather/whether, or even grammar cor-\nconfusion sets\nrection examples like among/between, we can train supervised classiﬁers to draw on\nmany features of the context and make a choice between the two candidates. Such\nclassiﬁers can achieve very high accuracy for these speciﬁc sets, especially when\ndrawing on large-scale features from web statistics (Golding and Roth 1999, Lapata\nand Keller 2004, Bergsma et al. 2009, Bergsma et al. 2010).\nB.3.1\nImproved Edit Models: Partitions and Pronunciation\nOther recent research has focused on improving the channel model P(t|c). One\nimportant extension is the ability to compute probabilities for multiple-letter trans-\nformations. For example Brill and Moore (2000) propose a channel model that (in-\nformally) models an error as being generated by a typist ﬁrst choosing a word, then\nchoosing a partition of the letters of that word, and then typing each partition, pos-\nsibly erroneously. For example, imagine a person chooses the word physical,\nthen chooses the partition ph y s i c al She would then generate each parti-\ntion, possible with errors. For example the probability that she would generate the\nstring fisikle with partition f i s i k le would be p(f|ph)∗p(i|y)∗p(s|s)∗\np(i|i)∗p(k|k)∗p(le|al). Unlike the Damerau-Levenshtein edit distance, the Brill-\nMoore channel model can thus model edit probabilities like P(f|ph) or P(le|al), or",
  "497": "490\nAPPENDIX B\n•\nSPELLING CORRECTION AND THE NOISY CHANNEL\nthe high likelihood of P(ent|ant). Furthermore, each edit is conditioned on where\nit is in the word (beginning, middle, end) so instead of P(f|ph) the model actually\nestimates P(f|ph,beginning).\nMore formally, let R be a partition of the typo string x into adjacent (possibly\nempty) substrings, and T be a partition of the candidate string. Brill and Moore\n(2000) then approximates the total likelihood P(x|w) (e.g., P(fisikle|physical))\nby the probability of the single best partition:\nP(x|w) ≈\nmax\nR,Ts.t.|T|=|R|\n|R|\nX\ni=1\nP(Ti|Ri,position)\n(B.11)\nThe probability of each transform P(Ti|Ri) can be learned from a training set of\ntriples of an error, the correct string, and the number of times it occurs. For example\ngiven a training pair akgsual/actual, standard minimum edit distance is used to\nproduce an alignment:\na\nc\nt\nu\na\nl\na\nk\ng\ns\nu\na\nl\nThis alignment corresponds to the sequence of edit operations:\na→a,\nc→k,\nϵ →g t→s,\nu→u,\na→a,\nl→l\nEach nonmatch substitution is then expanded to incorporate up to N additional\nedits; For N=2, we would expand c→k to:\nac→ak\nc→cg\nac→akg\nct→kgs\nEach of these multiple edits then gets a fractional count, and the probability for\neach edit α →β is then estimated from counts in the training corpus of triples as\ncount(α→β)\ncount(α) .\nAnother research direction in channel models is the use of pronunciation in ad-\ndition to spelling. Pronunciation is an important feature in some non-noisy-channel\nalgorithms for spell correction like the GNU aspell algorithm (Atkinson, 2011),\naspell\nwhich makes use of the metaphone pronunciation of a word (Philips, 1990). Meta-\nphone is a series of rules that map a word to a normalized representation of its\npronunciation. Some example rules:\n• “Drop duplicate adjacent letters, except for C.”\n• “If the word begins with ‘KN’, ‘GN’, ‘PN’, ‘AE’, ‘WR’, drop the ﬁrst letter.”\n• “Drop ‘B’ if after ‘M’ and if it is at the end of the word”\nAspell works similarly to the channel component of the noisy channel model, ﬁnding\nall words in the dictionary whose pronunciation string is a short edit distance (1 or\n2 pronunciation letters) from the typo, and then scoring this list of candidates by\na metric that combines two edit distances: the pronunciation edit distance and the\nweighted letter edit distance.\nPronunciation can also be incorporated directly the noisy channel model. For ex-\nample the Toutanova and Moore (2002) model, like aspell, interpolates two channel",
  "498": "BIBLIOGRAPHICAL AND HISTORICAL NOTES\n491\nfunction SOUNDEX(name) returns soundex form\n1. Keep the ﬁrst letter of name\n2. Drop all occurrences of non-initial a, e, h, i, o, u, w, y.\n3. Replace the remaining letters with the following numbers:\nb, f, p, v →1\nc, g, j, k, q, s, x, z →2\nd, t →3\nl →4\nm, n →5\nr →6\n4. Replace any sequences of identical numbers, only if they derive from two or more\nletters that were adjacent in the original name, with a single number (e.g., 666 →6).\n5. Convert to the form Letter Digit Digit Digit by dropping digits past the third\n(if necessary) or padding with trailing zeros (if necessary).\nFigure B.7\nThe Soundex Algorithm\nmodels, one based on spelling and one based on pronunciation. The pronunciation\nmodel is based on using letter-to-sound models to translate each input word and\nletter-to-sound\neach dictionary word into a sequences of phones representing the pronunciation of\nphones\nthe word. For example actress and aktress would both map to the phone string\nae k t r ix s. See Chapter 26 on the task of letter-to-sound or grapheme-to-\nphoneme.\nSome additional string distance functions have been proposed for dealing specif-\nically with names. These are mainly used for the task of deduplication (deciding if\ndeduplication\ntwo names in a census list or other namelist are the same) rather than spell-checking.\nThe Soundex algorithm (Knuth 1973, Odell and Russell 1922) is an older method\nused originally for census records for representing people’s names. It has the advan-\ntage that versions of the names that are slightly misspelled will still have the same\nrepresentation as correctly spelled names. (e.g., Jurafsky, Jarofsky, Jarovsky, and\nJarovski all map to J612). The algorithm is shown in Fig. B.7.\nInstead of Soundex, more recent work uses Jaro-Winkler distance, which is\nJaro-Winkler\nan edit distance algorithm designed for names that allows characters to be moved\nlonger distances in longer names, and also gives a higher similarity to strings that\nhave identical initial characters (Winkler, 2006).\nBibliographical and Historical Notes\nAlgorithms for spelling error detection and correction have existed since at least\nBlair (1960). Most early algorithms were based on similarity keys like the Soundex\nalgorithm (Odell and Russell 1922, Knuth 1973). Damerau (1964) gave a dictionary-\nbased algorithm for error detection; most error-detection algorithms since then have\nbeen based on dictionaries.\nEarly research (Peterson, 1986) had suggested that\nspelling dictionaries might need to be kept small because large dictionaries con-\ntain very rare words (wont, veery) that resemble misspellings of other words, but\nDamerau and Mays (1989) found that in practice larger dictionaries proved more\nhelpful. Damerau (1964) also gave a correction algorithm that worked for single\nerrors.\nThe idea of modeling language transmission as a Markov source passed through",
  "499": "492\nAPPENDIX B\n•\nSPELLING CORRECTION AND THE NOISY CHANNEL\na noisy channel model was developed very early on by Claude Shannon (1948).\nThe idea of combining a prior and a likelihood to deal with the noisy channel was\ndeveloped at IBM Research by Raviv (1967), for the similar task of optical char-\nacter recognition (OCR). While earlier spell-checkers like Kashyap and Oommen\n(1983) had used likelihood-based models of edit distance, the idea of combining a\nprior and a likelihood seems not to have been applied to the spelling correction task\nuntil researchers at AT&T Bell Laboratories (Kernighan et al. 1990, Church and\nGale 1991) and IBM Watson Research (Mays et al., 1991) roughly simultaneously\nproposed noisy channel spelling correction. Much later, the Mays et al. (1991) algo-\nrithm was reimplemented and tested on standard datasets by Wilcox-O’Hearn et al.\n(2008), who showed its high performance.\nMost algorithms since Wagner and Fischer (1974) have relied on dynamic pro-\ngramming.\nRecent focus has been on using the web both for language models and for train-\ning the error model, and on incorporating additional features in spelling, like the\npronunciation models described earlier, or other information like parses or semantic\nrelatedness (Jones and Martin 1997, Hirst and Budanitsky 2005).\nSee Mitton (1987) for a survey of human spelling errors, and Kukich (1992)\nfor an early survey of spelling error detection and correction. Norvig (2007) gives\na nice explanation and a Python implementation of the noisy channel model, with\nmore details and an efﬁcient algorithm presented in Norvig (2009).\nExercises\nB.1\nSuppose we want to apply add-one smoothing to the likelihood term (channel\nmodel) P(x|w) of a noisy channel model of spelling. For simplicity, pretend\nthat the only possible operation is deletion. The MLE estimate for deletion\nis given in Eq. B.6, which is P(x|w) = del[xi`1,wi]\ncount(xi`1wi). What is the estimate for\nP(x|w) if we use add-one smoothing on the deletion edit model? Assume the\nonly characters we use are lower case a-z, that there are V word types in our\ncorpus, and N total characters, not counting spaces.",
  "500": "CHAPTER\nC\nWordNet:\nWord Relations,\nSenses, and Disambiguation\nIn this chapter we introduce computation with a thesaurus: a structured list of words\norganized by meaning. The most popular thesaurus for computational purposes is\nWordNet, a large online resource with versions in many languages. One use of\nWordNet is to represent word senses, the many different meanings that a single\nlemma can have (Chapter 6) Thus the lemma bank can refer to a ﬁnancial institution\nor to the sloping side of a river. WordNet also represents relations between senses,\nlike the IS-A relation between dog and mammal or the part-whole relationship be-\ntween car and engine. Finally, WordNet includes glosses, a deﬁnition for senses in\nglosses\nthe form of a text string.\nWe’ll see how to use each of these aspects of WordNet to address the task of\ncomputing word similarity; the similarity in meaning of two different words, an\nalternative to the embedding-based methods we introduced in Chapter 6. And we’ll\nintroduce word sense disambiguation, the task of determining which sense of a\nword sense\ndisambiguation\nword is being used in a particular context, a task with a long history in computational\nlinguistics and applications from machine translation to question answering. We\ngive a number of algorithms for using features from the context for deciding which\nsense was intended in a particular context.\nC.1\nWord Senses\nConsider the two uses of the lemma bank mentioned above, meaning something like\n“ﬁnancial institution” and “sloping mound”, respectively:\n(C.1) Instead, a bank can hold the investments in a custodial account in the client’s\nname.\n(C.2) But as agriculture burgeons on the east bank, the river will shrink even more.\nWe represent this variation in usage by saying that the lemma bank has two\nsenses.1 A sense (or word sense) is a discrete representation of one aspect of the\nword sense\nmeaning of a word. Loosely following lexicographic tradition, we represent each\nsense by placing a superscript on the lemma as in bank1 and bank2.\nThe senses of a word might not have any particular relation between them; it\nmay be almost coincidental that they share an orthographic form. For example, the\nﬁnancial institution and sloping mound senses of bank seem relatively unrelated.\nIn such cases we say that the two senses are homonyms, and the relation between\nHomonym\nthe senses is one of homonymy. Thus bank1 (“ﬁnancial institution”) and bank2\nHomonymy\n(“sloping mound”) are homonyms, as are the sense of bat meaning ‘club for hitting\na ball’ and the one meaning ‘nocturnal ﬂying animal’. We say that these two uses\nof bank are homographs, as are the two uses of bat, because they are written the\nhomographs\n1\nConfusingly, the word “lemma” is itself ambiguous; it is also sometimes used to mean these separate\nsenses, rather than the citation form of the word. You should be prepared to see both uses in the literature.",
  "501": "494\nAPPENDIX C\n•\nWORDNET: WORD RELATIONS, SENSES, AND DISAMBIGUATION\nsame. Two words can be homonyms in a different way if they are spelled differently\nbut pronounced the same, like write and right, or piece and peace. We call these\nhomophones; they are one cause of real-word spelling errors.\nhomophones\nHomonymy causes problems in other areas of language processing as well. In\nquestion answering or information retrieval, we better help a user who typed “bat\ncare” if we know whether they are vampires or just want to play baseball. And\nthey will also have different translations; in Spanish the animal bat is a murci´elago\nwhile the baseball bat is a bate. Homographs that are pronounced differently cause\nproblems for speech synthesis (Chapter 26) such as these homographs of the word\nbass, the ﬁsh pronounced b ae s and the instrument pronounced b ey s.\n(C.3) The expert angler from Dora, Mo., was ﬂy-casting for bass rather than the\ntraditional trout.\n(C.4) The curtain rises to the sound of angry dogs baying and ominous bass chords\nsounding.\nSometimes there is also some semantic connection between the senses of a word.\nConsider the following example:\n(C.5) While some banks furnish blood only to hospitals, others are less restrictive.\nAlthough this is clearly not a use of the “sloping mound” meaning of bank, it just\nas clearly is not a reference to a charitable giveaway by a ﬁnancial institution. Rather,\nbank has a whole range of uses related to repositories for various biological entities,\nas in blood bank, egg bank, and sperm bank. So we could call this “biological\nrepository” sense bank3. Now this new sense bank3 has some sort of relation to\nbank1; both bank1 and bank3 are repositories for entities that can be deposited and\ntaken out; in bank1 the entity is monetary, whereas in bank3 the entity is biological.\nWhen two senses are related semantically, we call the relationship between them\npolysemy rather than homonymy. In many cases of polysemy, the semantic relation\npolysemy\nbetween the senses is systematic and structured. For example, consider yet another\nsense of bank, exempliﬁed in the following sentence:\n(C.6) The bank is on the corner of Nassau and Witherspoon.\nThis sense, which we can call bank4, means something like “the building be-\nlonging to a ﬁnancial institution”. It turns out that these two kinds of senses (an\norganization and the building associated with an organization ) occur together for\nmany other words as well (school, university, hospital, etc.). Thus, there is a sys-\ntematic relationship between senses that we might represent as\nBUILDING ↔ORGANIZATION\nThis particular subtype of polysemy relation is often called metonymy. Metonymy\nmetonymy\nis the use of one aspect of a concept or entity to refer to other aspects of the entity\nor to the entity itself. Thus, we are performing metonymy when we use the phrase\nthe White House to refer to the administration whose ofﬁce is in the White House.\nOther common examples of metonymy include the relation between the following\npairings of senses:\nAuthor (Jane Austen wrote Emma) ↔Works of Author (I really love Jane Austen)\nTree (Plums have beautiful blossoms) ↔Fruit (I ate a preserved plum yesterday)\nWhile it can be useful to distinguish polysemy from unrelated homonymy, there\nis no hard threshold for how related two senses must be to be considered polyse-\nmous. Thus, the difference is really one of degree. This fact can make it very difﬁcult\nto decide how many senses a word has, that is, whether to make separate senses for",
  "502": "C.1\n•\nWORD SENSES\n495\nclosely related usages. There are various criteria for deciding that the differing uses\nof a word should be represented with discrete senses. We might consider two senses\ndiscrete if they have independent truth conditions, different syntactic behavior, and\nindependent sense relations, or if they exhibit antagonistic meanings.\nConsider the following uses of the verb serve from the WSJ corpus:\n(C.7) They rarely serve red meat, preferring to prepare seafood.\n(C.8) He served as U.S. ambassador to Norway in 1976 and 1977.\n(C.9) He might have served his time, come out and led an upstanding life.\nThe serve of serving red meat and that of serving time clearly have different truth\nconditions and presuppositions; the serve of serve as ambassador has the distinct\nsubcategorization structure serve as NP. These heuristics suggest that these are prob-\nably three distinct senses of serve. One practical technique for determining if two\nsenses are distinct is to conjoin two uses of a word in a single sentence; this kind of\nconjunction of antagonistic readings is called zeugma. Consider the following ATIS\nzeugma\nexamples:\n(C.10) Which of those ﬂights serve breakfast?\n(C.11) Does Midwest Express serve Philadelphia?\n(C.12) ?Does Midwest Express serve breakfast and Philadelphia?\nWe use (?) to mark those examples that are semantically ill-formed. The oddness of\nthe invented third example (a case of zeugma) indicates there is no sensible way to\nmake a single sense of serve work for both breakfast and Philadelphia. We can use\nthis as evidence that serve has two different senses in this case.\nDictionaries tend to use many ﬁne-grained senses so as to capture subtle meaning\ndifferences, a reasonable approach given that the traditional role of dictionaries is\naiding word learners. For computational purposes, we often don’t need these ﬁne\ndistinctions, so we may want to group or cluster the senses; we have already done\nthis for some of the examples in this chapter.\nHow can we deﬁne the meaning of a word sense? We introduced in Chapter 6 the\nstandard computational approach of representing a word as an embedding, a point in\nsemantic space. The intuition was that words were deﬁned by their co-occurrences,\nthe counts of words that often occur nearby.\nThesauri offer an alternative way of deﬁning words. But we can’t just look at\nthe deﬁnition itself. Consider the following fragments from the deﬁnitions of right,\nleft, red, and blood from the American Heritage Dictionary (Morris, 1985).\nright adj. located nearer the right hand esp. being on the right when\nfacing the same direction as the observer.\nleft adj. located nearer to this side of the body than the right.\nred n. the color of blood or a ruby.\nblood n. the red liquid that circulates in the heart, arteries and veins of\nanimals.\nNote the circularity in these deﬁnitions. The deﬁnition of right makes two direct\nreferences to itself, and the entry for left contains an implicit self-reference in the\nphrase this side of the body, which presumably means the left side. The entries for\nred and blood reference each other in their deﬁnitions. Such circularity is inherent\nin all dictionary deﬁnitions. For humans, such entries are still useful since the user\nof the dictionary has sufﬁcient grasp of these other terms.\nFor computational purposes, one approach to deﬁning a sense is—like the dic-\ntionary deﬁnitions—deﬁning a sense through its relationship with other senses. For",
  "503": "496\nAPPENDIX C\n•\nWORDNET: WORD RELATIONS, SENSES, AND DISAMBIGUATION\nexample, the above deﬁnitions make it clear that right and left are similar kinds of\nlemmas that stand in some kind of alternation, or opposition, to one another. Simi-\nlarly, we can glean that red is a color, that it can be applied to both blood and rubies,\nand that blood is a liquid. Sense relations of this sort are embodied in on-line\ndatabases like WordNet. Given a sufﬁciently large database of such relations, many\napplications are quite capable of performing sophisticated semantic tasks (even if\nthey do not really know their right from their left).\nC.1.1\nRelations Between Senses\nThis section explores some of the relations that hold among word senses, focus-\ning on a few that have received signiﬁcant computational investigation: synonymy,\nantonymy, and hypernymy, as well as a brief mention of other relations like meronymy.\nSynonymy\nWe introduced in Chapter 6 the idea that when two senses of two dif-\nferent words (lemmas) are identical, or nearly identical, we say the two senses are\nsynonyms. Synonyms include such pairs as\nsynonym\ncouch/sofa vomit/throw up ﬁlbert/hazelnut car/automobile\nAnd we mentioned that in practice, the word synonym is commonly used to\ndescribe a relationship of approximate or rough synonymy. But furthermore, syn-\nonymy is actually a relationship between senses rather than words. Considering the\nwords big and large. These may seem to be synonyms in the following ATIS sen-\ntences, since we could swap big and large in either sentence and retain the same\nmeaning:\n(C.13) How big is that plane?\n(C.14) Would I be ﬂying on a large or small plane?\nBut note the following WSJ sentence in which we cannot substitute large for big:\n(C.15) Miss Nelson, for instance, became a kind of big sister to Benjamin.\n(C.16) ?Miss Nelson, for instance, became a kind of large sister to Benjamin.\nThis is because the word big has a sense that means being older or grown up, while\nlarge lacks this sense. Thus, we say that some senses of big and large are (nearly)\nsynonymous while other ones are not.\nHyponymy\nOne sense is a hyponym of another sense if the ﬁrst sense is more\nhyponym\nspeciﬁc, a subclass. For example, car is a hyponym of vehicle; dog is a hyponym\nof animal, and mango is a hyponym of fruit. Conversely, vehicle is a hypernym of\nhypernym\ncar, and animal is a hypernym of dog. It is unfortunate that the two words hypernym\nand hyponym are very similar and hence easily confused; for this reason, the word\nsuperordinate is often used instead of hypernym.\nsuperordinate\nSuperordinate vehicle fruit\nfurniture mammal\nHyponym\ncar\nmango chair\ndog\nMeronymy\nAnother common relation is meronymy, the part-whole relation. A\nmeronymy\npart-whole\nleg is part of a chair; a wheel is part of a car. We say that wheel is a meronym of\nmeronym\ncar, and car is a holonym of wheel.\nholonym",
  "504": "C.2\n•\nWORDNET: A DATABASE OF LEXICAL RELATIONS\n497\nC.2\nWordNet: A Database of Lexical Relations\nThe most commonly used resource for English sense relations is the WordNet lex-\nWordNet\nical database (Fellbaum, 1998). WordNet consists of three separate databases, one\neach for nouns and verbs and a third for adjectives and adverbs; closed class words\nare not included. Each database contains a set of lemmas, each one annotated with a\nset of senses. The WordNet 3.0 release has 117,798 nouns, 11,529 verbs, 22,479 ad-\njectives, and 4,481 adverbs. The average noun has 1.23 senses, and the average verb\nhas 2.16 senses. WordNet can be accessed on the Web or downloaded and accessed\nlocally. Figure C.1 shows the lemma entry for the noun and adjective bass.\nThe noun “bass” has 8 senses in WordNet.\n1. bass1 - (the lowest part of the musical range)\n2. bass2, bass part1 - (the lowest part in polyphonic music)\n3. bass3, basso1 - (an adult male singer with the lowest voice)\n4. sea bass1, bass4 - (the lean ﬂesh of a saltwater ﬁsh of the family Serranidae)\n5. freshwater bass1, bass5 - (any of various North American freshwater ﬁsh with\nlean ﬂesh (especially of the genus Micropterus))\n6. bass6, bass voice1, basso2 - (the lowest adult male singing voice)\n7. bass7 - (the member with the lowest range of a family of musical instruments)\n8. bass8 - (nontechnical name for any of numerous edible marine and\nfreshwater spiny-ﬁnned ﬁshes)\nThe adjective “bass” has 1 sense in WordNet.\n1. bass1, deep6 - (having or denoting a low vocal or instrumental range)\n“a deep voice”; “a bass voice is lower than a baritone voice”;\n“a bass clarinet”\nFigure C.1\nA portion of the WordNet 3.0 entry for the noun bass.\nNote that there are eight senses for the noun and one for the adjective, each of\nwhich has a gloss (a dictionary-style deﬁnition), a list of synonyms for the sense, and\ngloss\nsometimes also usage examples (shown for the adjective sense). Unlike dictionaries,\nWordNet doesn’t represent pronunciation, so doesn’t distinguish the pronunciation\n[b ae s] in bass4, bass5, and bass8 from the other senses pronounced [b ey s].\nThe set of near-synonyms for a WordNet sense is called a synset (for synonym\nsynset\nset); synsets are an important primitive in WordNet. The entry for bass includes\nsynsets like {bass1, deep6}, or {bass6, bass voice1, basso2}. We can think of a\nsynset as representing a concept of the type we discussed in Chapter 14. Thus,\ninstead of representing concepts in logical terms, WordNet represents them as lists\nof the word senses that can be used to express the concept. Here’s another synset\nexample:\n{chump1, fool2, gull1, mark9, patsy1, fall guy1,\nsucker1, soft touch1, mug2}\nThe gloss of this synset describes it as a person who is gullible and easy to take\nadvantage of. Each of the lexical entries included in the synset can, therefore, be\nused to express this concept. Synsets like this one actually constitute the senses\nassociated with WordNet entries, and hence it is synsets, not wordforms, lemmas, or\nindividual senses, that participate in most of the lexical sense relations in WordNet.\nWordNet represents all the kinds of sense relations discussed in the previous sec-\ntion, as illustrated in Fig. C.2 and Fig. C.3. WordNet hyponymy relations correspond",
  "505": "498\nAPPENDIX C\n•\nWORDNET: WORD RELATIONS, SENSES, AND DISAMBIGUATION\nRelation\nAlso Called\nDeﬁnition\nExample\nHypernym\nSuperordinate From concepts to superordinates\nbreakfast1 →meal1\nHyponym\nSubordinate\nFrom concepts to subtypes\nmeal1 →lunch1\nInstance Hypernym\nInstance\nFrom instances to their concepts\nAusten1 →author1\nInstance Hyponym\nHas-Instance\nFrom concepts to concept instances\ncomposer1 →Bach1\nMember Meronym\nHas-Member\nFrom groups to their members\nfaculty2 →professor1\nMember Holonym\nMember-Of\nFrom members to their groups\ncopilot1 →crew1\nPart Meronym\nHas-Part\nFrom wholes to parts\ntable2 →leg3\nPart Holonym\nPart-Of\nFrom parts to wholes\ncourse7 →meal1\nSubstance Meronym\nFrom substances to their subparts\nwater1 →oxygen1\nSubstance Holonym\nFrom parts of substances to wholes\ngin1 →martini1\nAntonym\nSemantic opposition between lemmas leader1 ⇐⇒follower1\nDerivationally\nLemmas w/same morphological root\ndestruction1 ⇐⇒destroy1\nRelated Form\nFigure C.2\nNoun relations in WordNet.\nRelation\nDeﬁnition\nExample\nHypernym\nFrom events to superordinate events\nﬂy9 →travel5\nTroponym\nFrom events to subordinate event\nwalk1 →stroll1\n(often via speciﬁc manner)\nEntails\nFrom verbs (events) to the verbs (events) they entail\nsnore1 →sleep1\nAntonym\nSemantic opposition between lemmas\nincrease1 ⇐⇒decrease1\nDerivationally\nLemmas with same morphological root\ndestroy1 ⇐⇒destruction1\nRelated Form\nFigure C.3\nVerb relations in WordNet.\nto the notion of immediate hyponymy discussed on page 496. Each synset is related\nto its immediately more general and more speciﬁc synsets through direct hypernym\nand hyponym relations. These relations can be followed to produce longer chains of\nmore general or more speciﬁc synsets. Figure C.4 shows hypernym chains for bass3\nand bass7.\nIn this depiction of hyponymy, successively more general synsets are shown on\nsuccessive indented lines. The ﬁrst chain starts from the concept of a human bass\nsinger. Its immediate superordinate is a synset corresponding to the generic concept\nof a singer. Following this chain leads eventually to concepts such as entertainer and\nperson. The second chain, which starts from musical instrument, has a completely\ndifferent path leading eventually to such concepts as musical instrument, device, and\nphysical object. Both paths do eventually join at the very abstract synset whole, unit,\nand then proceed together to entity which is the top (root) of the noun hierarchy (in\nWordNet this root is generally called the unique beginner).\nunique\nbeginner\nC.3\nWord Similarity: Thesaurus Methods\nIn Chapter 6 we introduced the embedding and cosine architecture for computing the\nsimilarity between two words. A thesaurus offers a different family of algorithms\nthat can be complementary.\nAlthough we have described them as relations between words, similar is actually\na relationship between word senses. For example, of the two senses of bank, we",
  "506": "C.3\n•\nWORD SIMILARITY: THESAURUS METHODS\n499\nSense 3\nbass, basso --\n(an adult male singer with the lowest voice)\n=> singer, vocalist, vocalizer, vocaliser\n=> musician, instrumentalist, player\n=> performer, performing artist\n=> entertainer\n=> person, individual, someone...\n=> organism, being\n=> living thing, animate thing,\n=> whole, unit\n=> object, physical object\n=> physical entity\n=> entity\n=> causal agent, cause, causal agency\n=> physical entity\n=> entity\nSense 7\nbass --\n(the member with the lowest range of a family of\nmusical instruments)\n=> musical instrument, instrument\n=> device\n=> instrumentality, instrumentation\n=> artifact, artefact\n=> whole, unit\n=> object, physical object\n=> physical entity\n=> entity\nFigure C.4\nHyponymy chains for two separate senses of the lemma bass. Note that the\nchains are completely distinct, only converging at the very abstract level whole, unit.\nmight say that the ﬁnancial sense is similar to one of the senses of fund and the\nriparian sense is more similar to one of the senses of slope. In the next few sections\nof this chapter, we will compute these relations over both words and senses.\nThe thesaurus-based algorithms use the structure of the thesaurus to deﬁne word\nsimilarity. In principle, we could measure similarity by using any information avail-\nable in a thesaurus (meronymy, glosses, etc.). In practice, however, thesaurus-based\nword similarity algorithms generally use only the hypernym/hyponym (is-a or sub-\nsumption) hierarchy. In WordNet, verbs and nouns are in separate hypernym hier-\narchies, so a thesaurus-based algorithm for WordNet can thus compute only noun-\nnoun similarity, or verb-verb similarity; we can’t compare nouns to verbs or do\nanything with adjectives or other parts of speech.\nThe simplest thesaurus-based algorithms are based on the intuition that words\nor senses are more similar if there is a shorter path between them in the thesaurus\ngraph, an intuition dating back to Quillian (1969). A word/sense is most similar to\nitself, then to its parents or siblings, and least similar to words that are far away. We\nmake this notion operational by measuring the number of edges between the two\nconcept nodes in the thesaurus graph and adding one. Figure C.5 shows an intuition;\nthe concept dime is most similar to nickel and coin, less similar to money, and even\nless similar to Richter scale. A formal deﬁnition:\npathlen(c1,c2) = 1 + the number of edges in the shortest path in the",
  "507": "500\nAPPENDIX C\n•\nWORDNET: WORD RELATIONS, SENSES, AND DISAMBIGUATION\nFigure C.5\nA fragment of the WordNet hypernym hierarchy, showing path lengths (number\nof edges plus 1) from nickel to coin (2), dime (3), money (6), and Richter scale (8).\nthesaurus graph between the sense nodes c1 and c2\nPath-based similarity can be deﬁned as just the path length, transformed either by\nlog (Leacock and Chodorow, 1998) or, more often, by an inverse, resulting in the\nfollowing common deﬁnition of path-length based similarity:\npath-length\nbased similarity\nsimpath(c1,c2) =\n1\npathlen(c1,c2)\n(C.17)\nFor most applications, we don’t have sense-tagged data, and thus we need our\nalgorithm to give us the similarity between words rather than between senses or con-\ncepts. For any of the thesaurus-based algorithms, following Resnik (1995), we can\napproximate the correct similarity (which would require sense disambiguation) by\njust using the pair of senses for the two words that results in maximum sense sim-\nilarity. Thus, based on sense similarity, we can deﬁne word similarity as follows:\nword similarity\nwordsim(w1,w2) =\nmax\nc1∈senses(w1)\nc2∈senses(w2)\nsim(c1,c2)\n(C.18)\nThe basic path-length algorithm makes the implicit assumption that each link\nin the network represents a uniform distance. In practice, this assumption is not\nappropriate. Some links (e.g., those that are deep in the WordNet hierarchy) often\nseem to represent an intuitively narrow distance, while other links (e.g., higher up\nin the WordNet hierarchy) represent an intuitively wider distance. For example, in\nFig. C.5, the distance from nickel to money (5) seems intuitively much shorter than\nthe distance from nickel to an abstract word standard; the link between medium of\nexchange and standard seems wider than that between, say, coin and coinage.\nIt is possible to reﬁne path-based algorithms with normalizations based on depth\nin the hierarchy (Wu and Palmer, 1994), but in general we’d like an approach that\nlets us independently represent the distance associated with each edge.\nA second class of thesaurus-based similarity algorithms attempts to offer just\nsuch a ﬁne-grained metric. These information-content word-similarity algorithms\ninformation-\ncontent\nstill rely on the structure of the thesaurus but also add probabilistic information\nderived from a corpus.\nFollowing Resnik (1995) we’ll deﬁne P(c) as the probability that a randomly\nselected word in a corpus is an instance of concept c (i.e., a separate random variable,\nranging over words, associated with each concept). This implies that P(root) = 1\nsince any word is subsumed by the root concept. Intuitively, the lower a concept",
  "508": "C.3\n•\nWORD SIMILARITY: THESAURUS METHODS\n501\nin the hierarchy, the lower its probability. We train these probabilities by counting\nin a corpus; each word in the corpus counts as an occurrence of each concept that\ncontains it. For example, in Fig. C.5 above, an occurrence of the word dime would\ncount toward the frequency of coin, currency, standard, etc. More formally, Resnik\ncomputes P(c) as follows:\nP(c) =\nP\nw∈words(c) count(w)\nN\n(C.19)\nwhere words(c) is the set of words subsumed by concept c, and N is the total number\nof words in the corpus that are also present in the thesaurus.\nFigure C.6, from Lin (1998), shows a fragment of the WordNet concept hierar-\nchy augmented with the probabilities P(c).\nentity 0.395\ninanimate-object 0.167\nnatural-object 0.0163\ngeological-formation 0.00176\nshore 0.0000836\ncoast 0.0000216\n0.000113 natural-elevation\n0.0000189 hill\nFigure C.6\nA fragment of the WordNet hierarchy, showing the probability P(c) attached to\neach content, adapted from a ﬁgure from Lin (1998).\nWe now need two additional deﬁnitions. First, following basic information the-\nory, we deﬁne the information content (IC) of a concept c as\nIC(c) = −logP(c)\n(C.20)\nSecond, we deﬁne the lowest common subsumer or LCS of two concepts:\nLowest\ncommon\nsubsumer\nLCS\nLCS(c1,c2) = the lowest common subsumer, that is, the lowest node in\nthe hierarchy that subsumes (is a hypernym of) both c1 and c2\nThere are now a number of ways to use the information content of a node in a\nword similarity metric. The simplest way was ﬁrst proposed by Resnik (1995). We\nthink of the similarity between two words as related to their common information;\nthe more two words have in common, the more similar they are. Resnik proposes\nto estimate the common amount of information by the information content of the\nlowest common subsumer of the two nodes. More formally, the Resnik similarity\nResnik\nsimilarity\nmeasure is\nsimresnik(c1,c2) = −logP(LCS(c1,c2))\n(C.21)\nLin (1998) extended the Resnik intuition by pointing out that a similarity metric\nbetween objects A and B needs to do more than measure the amount of information\nin common between A and B. For example, he additionally pointed out that the more\ndifferences between A and B, the less similar they are. In summary:",
  "509": "502\nAPPENDIX C\n•\nWORDNET: WORD RELATIONS, SENSES, AND DISAMBIGUATION\n• Commonality: the more information A and B have in common, the more\nsimilar they are.\n• Difference: the more differences between the information in A and B, the less\nsimilar they are.\nLin measures the commonality between A and B as the information content of\nthe proposition that states the commonality between A and B:\nIC(common(A,B))\n(C.22)\nHe measures the difference between A and B as\nIC(description(A,B))−IC(common(A,B))\n(C.23)\nwhere description(A,B) describes A and B. Given a few additional assumptions\nabout similarity, Lin proves the following theorem:\nSimilarity Theorem: The similarity between A and B is measured by the ratio\nbetween the amount of information needed to state the commonality of A and\nB and the information needed to fully describe what A and B are.\nsimLin(A,B) =\ncommon(A,B)\ndescription(A,B)\n(C.24)\nApplying this idea to the thesaurus domain, Lin shows (in a slight modiﬁcation\nof Resnik’s assumption) that the information in common between two concepts is\ntwice the information in the lowest common subsumer LCS(c1,c2). Adding in the\nabove deﬁnitions of the information content of thesaurus concepts, the ﬁnal Lin\nsimilarity function is\nLin similarity\nsimLin(c1,c2) = 2×logP(LCS(c1,c2))\nlogP(c1)+logP(c2)\n(C.25)\nFor example, using simLin, Lin (1998) shows that the similarity between the\nconcepts of hill and coast from Fig. C.6 is\nsimLin(hill,coast) = 2×logP(geological-formation)\nlogP(hill)+logP(coast)\n= 0.59\n(C.26)\nA similar formula, Jiang-Conrath distance (Jiang and Conrath, 1997), although\nJiang-Conrath\ndistance\nderived in a completely different way from Lin and expressed as a distance rather\nthan similarity function, has been shown to work as well as or better than all the\nother thesaurus-based methods:\ndistJC(c1,c2) = 2×logP(LCS(c1,c2))−(logP(c1)+logP(c2))\n(C.27)\nWe can transform distJC into a similarity by taking the reciprocal.\nFinally, we describe a dictionary-based method that is related to the Lesk al-\ngorithm for word sense disambiguation we will introduce in Section C.6.1. The\nintution of extended gloss overlap, or extended Lesk measure (Banerjee and Ped-\nExtended gloss\noverlap\nextended Lesk\nersen, 2003) is that two concepts/senses in a thesaurus are similar if their glosses\ncontain overlapping words. We’ll begin by sketching an overlap function for two\nglosses. Consider these two concepts, with their glosses:",
  "510": "C.3\n•\nWORD SIMILARITY: THESAURUS METHODS\n503\n• drawing paper: paper that is specially prepared for use in drafting\n• decal: the art of transferring designs from specially prepared paper to a wood\nor glass or metal surface.\nFor each n-word phrase that occurs in both glosses, Extended Lesk adds in a\nscore of n2 (the relation is non-linear because of the Zipﬁan relationship between\nlengths of phrases and their corpus frequencies; longer overlaps are rare, so they\nshould be weighted more heavily). Here, the overlapping phrases are paper and\nspecially prepared, for a total similarity score of 12 +22 = 5.\nGiven such an overlap function, when comparing two concepts (synsets), Ex-\ntended Lesk not only looks for overlap between their glosses but also between the\nglosses of the senses that are hypernyms, hyponyms, meronyms, and other relations\nof the two concepts. For example, if we just considered hyponyms and deﬁned\ngloss(hypo(A)) as the concatenation of all the glosses of all the hyponym senses of\nA, the total relatedness between two concepts A and B might be\nsimilarity(A,B) = overlap(gloss(A), gloss(B))\n+overlap(gloss(hypo(A)), gloss(hypo(B)))\n+overlap(gloss(A), gloss(hypo(B)))\n+overlap(gloss(hypo(A)),gloss(B))\nLet RELS be the set of possible WordNet relations whose glosses we compare;\nassuming a basic overlap measure as sketched above, we can then deﬁne the Ex-\ntended Lesk overlap measure as\nsimeLesk(c1,c2) =\nX\nr,q∈RELS\noverlap(gloss(r(c1)),gloss(q(c2)))\n(C.28)\nsimpath(c1,c2) =\n1\npathlen(c1,c2)\nsimResnik(c1,c2) = −logP(LCS(c1,c2))\nsimLin(c1,c2) = 2×logP(LCS(c1,c2))\nlogP(c1)+logP(c2)\nsimJC(c1,c2) =\n1\n2×logP(LCS(c1,c2))−(logP(c1)+logP(c2))\nsimeLesk(c1,c2) =\nX\nr,q∈RELS\noverlap(gloss(r(c1)),gloss(q(c2)))\nFigure C.7\nFive thesaurus-based (and dictionary-based) similarity measures.\nFigure C.7 summarizes the ﬁve similarity measures we have described in this\nsection.\nEvaluating Thesaurus-Based Similarity\nWhich of these similarity measures is best? Word similarity measures have been\nevaluated in two ways, introduced in Chapter 6. The most common intrinsic evalu-\nation metric computes the correlation coefﬁcient between an algorithm’s word sim-\nilarity scores and word similarity ratings assigned by humans. There are a variety",
  "511": "504\nAPPENDIX C\n•\nWORDNET: WORD RELATIONS, SENSES, AND DISAMBIGUATION\nof such human-labeled datasets: the RG-65 dataset of human similarity ratings on\n65 word pairs (Rubenstein and Goodenough, 1965), the MC-30 dataset of 30 word\npairs (Miller and Charles, 1991). The WordSim-353 (Finkelstein et al., 2002) is a\ncommonly used set of ratings from 0 to 10 for 353 noun pairs; for example (plane,\ncar) had an average score of 5.77. SimLex-999 (Hill et al., 2015) is a more difﬁcult\ndataset that quantiﬁes similarity (cup, mug) rather than relatedness (cup, coffee), and\nincluding both concrete and abstract adjective, noun and verb pairs. Another com-\nmon intrinic similarity measure is the TOEFL dataset, a set of 80 questions, each\nconsisting of a target word with 4 additional word choices; the task is to choose\nwhich is the correct synonym, as in the example: Levied is closest in meaning to:\nimposed, believed, requested, correlated (Landauer and Dumais, 1997). All of these\ndatasets present words without context.\nSlightly more realistic are intrinsic similarity tasks that include context. The\nStanford Contextual Word Similarity (SCWS) dataset (Huang et al., 2012) offers a\nricher evaluation scenario, giving human judgments on 2,003 pairs of words in their\nsentential context, including nouns, verbs, and adjectives. This dataset enables the\nevaluation of word similarity algorithms that can make use of context words. The\nsemantic textual similarity task (Agirre et al. 2012, Agirre et al. 2015) evaluates the\nperformance of sentence-level similarity algorithms, consisting of a set of pairs of\nsentences, each pair with human-labeled similarity scores.\nAlternatively, the similarity measure can be embedded in some end-application,\nsuch as question answering or spell-checking, and different measures can be evalu-\nated by how much they improve the end application.\nC.4\nWord Sense Disambiguation: Overview\nThe task of selecting the correct sense for a word is called word sense disambigua-\ntion, or WSD. WSD algorithms take as input a word in context and a ﬁxed inventory\nword sense\ndisambiguation\nWSD\nof potential word senses and outputs the correct word sense in context. The input and\nthe senses depends on the task. For machine translation from English to Spanish, the\nsense tag inventory for an English word might be the set of different Spanish trans-\nlations. For automatic indexing of medical articles, the sense-tag inventory might be\nthe set of MeSH (Medical Subject Headings) thesaurus entries.\nWhen we are evaluating WSD in isolation, we can use the set of senses from a\ndictionary/thesaurus resource like WordNet. Figure C.4 shows an example for the\nword bass, which can refer to a musical instrument or a kind of ﬁsh.2\nWordNet\nSpanish\nRoget\nSense\nTranslation\nCategory\nTarget Word in Context\nbass4\nlubina\nFISH/INSECT\n. . . ﬁsh as Paciﬁc salmon and striped bass and. . .\nbass4\nlubina\nFISH/INSECT\n. . . produce ﬁlets of smoked bass or sturgeon. . .\nbass7\nbajo\nMUSIC\n. . . exciting jazz bass player since Ray Brown. . .\nbass7\nbajo\nMUSIC\n. . . play bass because he doesn’t have to solo. . .\nFigure C.8\nPossible deﬁnitions for the inventory of sense tags for bass.\nIt is useful to distinguish two WSD tasks. In the lexical sample task, a small\nlexical sample\n2\nThe WordNet database includes eight senses; we have arbitrarily selected two for this example; we\nhave also arbitrarily selected one of the many Spanish ﬁshes that could translate English sea bass.",
  "512": "C.5\n•\nSUPERVISED WORD SENSE DISAMBIGUATION\n505\npre-selected set of target words is chosen, along with an inventory of senses for each\nword from some lexicon. Since the set of words and the set of senses are small,\nsimple supervised classiﬁcation approaches are used.\nIn the all-words task, systems are given entire texts and a lexicon with an inven-\nall-words\ntory of senses for each entry and are required to disambiguate every content word in\nthe text. The all-words task is similar to part-of-speech tagging, except with a much\nlarger set of tags since each lemma has its own set. A consequence of this larger set\nof tags is data sparseness; it is unlikely that adequate training data for every word in\nthe test set will be available. Moreover, given the number of polysemous words in\nreasonably sized lexicons, approaches based on training one classiﬁer per term are\nunlikely to be practical.\nC.5\nSupervised Word Sense Disambiguation\nSupervised WSD is commonly used whenever we have sufﬁcient data that has been\nhand-labeled with correct word senses.\nDatasets:\nThe are various lexical sample datasets with context sentences labeled\nwith the correct sense for the target word, such as the line-hard-serve corpus with\n4,000 sense-tagged examples of line as a noun, hard as an adjective and serve as a\nverb (Leacock et al., 1993), and the interest corpus with 2,369 sense-tagged exam-\nples of interest as a noun (Bruce and Wiebe, 1994). The SENSEVAL project has also\nproduced a number of such sense-labeled lexical sample corpora (SENSEVAL-1 with\n34 words from the HECTOR lexicon and corpus (Kilgarriff and Rosenzweig 2000,\nAtkins 1993), SENSEVAL-2 and -3 with 73 and 57 target words, respectively (Palmer\net al. 2001, Kilgarriff 2001). All-word disambiguation tasks are trained from a se-\nmantic concordance, a corpus in which each open-class word in each sentence is\nsemantic\nconcordance\nlabeled with its word sense from a speciﬁc dictionary or thesaurus. One commonly\nused corpus is SemCor, a subset of the Brown Corpus consisting of over 234,000\nwords that were manually tagged with WordNet senses (Miller et al. 1993, Landes\net al. 1998). In addition, sense-tagged corpora have been built for the SENSEVAL all-\nword tasks. The SENSEVAL-3 English all-words test data consisted of 2081 tagged\ncontent word tokens, from 5,000 total running words of English from the WSJ and\nBrown corpora (Palmer et al., 2001).\nFeatures\nSupervised WSD algorithms can use any standard classiﬁcation algo-\nrithm. Features generally include the word identity, part-of-speech tags, and embed-\ndings of surrounding words, usually computed in two ways: collocation features are\ncollocation\nwords or n-grams at a particular location, (i.e., exactly one word to the right, or the\ntwo words starting 3 words to the left, and so on). bag of word features are rep-\nbag of word\nresented as a vector with the dimensionality of the vocabulary (minus stop words),\nwith a 1 if that word occurs in the in the neighborhood of the target word.\nConsider the ambiguous word bass in the following WSJ sentence:\n(C.29) An electric guitar and bass player stand off to one side,\nIf we used a small 2-word window, a standard feature vector might include a bag of\nwords, parts-of-speech, unigram and bigram collocation features, and embeddings,\nthat is:\n[wi−2,POSi−2,wi−1,POSi−1,wi+1,POSi+1,wi+2,POSi+2,\nwi−1\ni−2,wi+2\ni+1,E(wi−2,wi−1,wi+1,wi+2),bag()]\n(C.30)",
  "513": "506\nAPPENDIX C\n•\nWORDNET: WORD RELATIONS, SENSES, AND DISAMBIGUATION\nwould yield the following vector:\n[guitar, NN, and, CC, player, NN, stand, VB, and guitar, player stand,\nE(guitar,and,player,stand), bag(guitar,player,stand)]\nHigh performing systems generally use POS tags and word collocations of length\n1, 2, and 3 from a window of words 3 to the left and 3 to the right (Zhong and Ng,\n2010). The embedding function could just take the average of the embeddings of\nthe words in the window, or a more complicated embedding function can be used\n(Iacobacci et al., 2016).\nC.5.1\nWikipedia as a source of training data\nOne way to increase the amount of training data is to use Wikipedia as a source of\nsense-labeled data. When a concept is mentioned in a Wikipedia article, the article\ntext may contain an explicit link to the concept’s Wikipedia page, which is named\nby a unique identiﬁer. This link can be used as a sense annotation. For example,\nthe ambiguous word bar is linked to a different Wikipedia article depending on its\nmeaning in context, including the page BAR (LAW), the page BAR (MUSIC), and\nso on, as in the following Wikipedia examples (Mihalcea, 2007).\nIn 1834, Sumner was admitted to the [[bar (law)|bar]] at the age of\ntwenty-three, and entered private practice in Boston.\nIt is danced in 3/4 time (like most waltzes), with the couple turning\napprox. 180 degrees every [[bar (music)|bar]].\nJenga is a popular beer in the [[bar (establishment)|bar]]s of Thailand.\nThese sentences can then be added to the training data for a supervised system.\nIn order to use Wikipedia in this way, however, it is necessary to map from Wikipedia\nconcepts to whatever inventory of senses is relevant for the WSD application. Auto-\nmatic algorithms that map from Wikipedia to WordNet, for example, involve ﬁnding\nthe WordNet sense that has the greatest lexical overlap with the Wikipedia sense, by\ncomparing the vector of words in the WordNet synset, gloss, and related senses with\nthe vector of words in the Wikipedia page title, outgoing links, and page category\n(Ponzetto and Navigli, 2010).\nC.5.2\nEvaluation\nTo evaluate WSD algorithms, it’s better to consider extrinsic, task-based, or end-\nextrinsic\nevaluation\nto-end evaluation, in which we see whether some new WSD idea actually improves\nperformance in some end-to-end application like question answering or machine\ntranslation. Nonetheless, because extrinsic evaluations are difﬁcult and slow, WSD\nsystems are typically evaluated with intrinsic evaluation. in which a WSD compo-\nintrinsic\nnent is treated as an independent system. Common intrinsic evaluations are either\nexact-match sense accuracy—the percentage of words that are tagged identically\nsense accuracy\nwith the hand-labeled sense tags in a test set—or with precision and recall if sys-\ntems are permitted to pass on the labeling of some instances. In general, we evaluate\nby using held-out data from the same sense-tagged corpora that we used for training,\nsuch as the SemCor corpus discussed above or the various corpora produced by the\nSENSEVAL effort.\nMany aspects of sense evaluation have been standardized by the SENSEVAL and\nSEMEVAL efforts (Palmer et al. 2006, Kilgarriff and Palmer 2000). This framework\nprovides a shared task with training and testing materials along with sense invento-\nries for all-words and lexical sample tasks in a variety of languages.",
  "514": "C.6\n•\nWSD: DICTIONARY AND THESAURUS METHODS\n507\nThe normal baseline is to choose the most frequent sense for each word from the\nmost frequent\nsense\nsenses in a labeled corpus (Gale et al., 1992a). For WordNet, this corresponds to the\nﬁrst sense, since senses in WordNet are generally ordered from most frequent to least\nfrequent. WordNet sense frequencies come from the SemCor sense-tagged corpus\ndescribed above– WordNet senses that don’t occur in SemCor are ordered arbitrarily\nafter those that do. The most frequent sense baseline can be quite accurate, and is\ntherefore often used as a default, to supply a word sense when a supervised algorithm\nhas insufﬁcient training data.\nC.6\nWSD: Dictionary and Thesaurus Methods\nSupervised algorithms based on sense-labeled corpora are the best-performing algo-\nrithms for sense disambiguation. However, such labeled training data is expensive\nand limited. One alternative is to get indirect supervision from dictionaries and the-\nsauruses, and so this method is also called knowledge-based WSD. Methods like\nthis that do not use texts that have been hand-labeled with senses are also called\nweakly supervised.\nC.6.1\nThe Lesk Algorithm\nThe most well-studied dictionary-based algorithm for sense disambiguation is the\nLesk algorithm, really a family of algorithms that choose the sense whose dictio-\nLesk algorithm\nnary gloss or deﬁnition shares the most words with the target word’s neighborhood.\nFigure C.9 shows the simplest version of the algorithm, often called the Simpliﬁed\nLesk algorithm (Kilgarriff and Rosenzweig, 2000).\nSimpliﬁed Lesk\nfunction SIMPLIFIED LESK(word, sentence) returns best sense of word\nbest-sense←most frequent sense for word\nmax-overlap←0\ncontext←set of words in sentence\nfor each sense in senses of word do\nsignature←set of words in the gloss and examples of sense\noverlap←COMPUTEOVERLAP(signature, context)\nif overlap > max-overlap then\nmax-overlap←overlap\nbest-sense←sense\nend\nreturn(best-sense)\nFigure C.9\nThe Simpliﬁed Lesk algorithm. The COMPUTEOVERLAP function returns the\nnumber of words in common between two sets, ignoring function words or other words on a\nstop list. The original Lesk algorithm deﬁnes the context in a more complex way. The Cor-\npus Lesk algorithm weights each overlapping word w by its −logP(w) and includes labeled\ntraining corpus data in the signature.\nAs an example of the Lesk algorithm at work, consider disambiguating the word\nbank in the following context:\n(C.31) The bank can guarantee deposits will eventually cover future tuition costs\nbecause it invests in adjustable-rate mortgage securities.",
  "515": "508\nAPPENDIX C\n•\nWORDNET: WORD RELATIONS, SENSES, AND DISAMBIGUATION\ngiven the following two WordNet senses:\nbank1\nGloss:\na ﬁnancial institution that accepts deposits and channels the\nmoney into lending activities\nExamples:\n“he cashed a check at the bank”, “that bank holds the mortgage\non my home”\nbank2\nGloss:\nsloping land (especially the slope beside a body of water)\nExamples:\n“they pulled the canoe up on the bank”, “he sat on the bank of\nthe river and watched the currents”\nSense bank1 has two non-stopwords overlapping with the context in (C.31):\ndeposits and mortgage, while sense bank2 has zero words, so sense bank1 is chosen.\nThere are many obvious extensions to Simpliﬁed Lesk. The original Lesk algo-\nrithm (Lesk, 1986) is slightly more indirect. Instead of comparing a target word’s\nsignature with the context words, the target signature is compared with the signatures\nof each of the context words. For example, consider Lesk’s example of selecting the\nappropriate sense of cone in the phrase pine cone given the following deﬁnitions for\npine and cone.\npine 1 kinds of evergreen tree with needle-shaped leaves\n2 waste away through sorrow or illness\ncone 1 solid body which narrows to a point\n2 something of this shape whether solid or hollow\n3 fruit of certain evergreen trees\nIn this example, Lesk’s method would select cone3 as the correct sense since\ntwo of the words in its entry, evergreen and tree, overlap with words in the entry for\npine, whereas neither of the other entries has any overlap with words in the deﬁnition\nof pine. In general Simpliﬁed Lesk seems to work better than original Lesk.\nThe primary problem with either the original or simpliﬁed approaches, how-\never, is that the dictionary entries for the target words are short and may not provide\nenough chance of overlap with the context.3 One remedy is to expand the list of\nwords used in the classiﬁer to include words related to, but not contained in, their\nindividual sense deﬁnitions. But the best solution, if any sense-tagged corpus data\nlike SemCor is available, is to add all the words in the labeled corpus sentences for a\nword sense into the signature for that sense. This version of the algorithm, the Cor-\npus Lesk algorithm, is the best-performing of all the Lesk variants (Kilgarriff and\nCorpus Lesk\nRosenzweig 2000, Vasilescu et al. 2004) and is used as a baseline in the SENSEVAL\ncompetitions. Instead of just counting up the overlapping words, the Corpus Lesk\nalgorithm also applies a weight to each overlapping word. The weight is the inverse\ndocument frequency or IDF, a standard information-retrieval measure introduced\ninverse\ndocument\nfrequency\nIDF\nin Chapter 6. IDF measures how many different “documents” (in this case, glosses\nand examples) a word occurs in and is thus a way of discounting function words.\nSince function words like the, of, etc., occur in many documents, their IDF is very\nlow, while the IDF of content words is high. Corpus Lesk thus uses IDF instead of a\nstop list.\nFormally, the IDF for a word i can be deﬁned as\nidfi = log\n\u0012Ndoc\nndi\n\u0013\n(C.32)\n3\nIndeed, Lesk (1986) notes that the performance of his system seems to roughly correlate with the\nlength of the dictionary entries.",
  "516": "C.6\n•\nWSD: DICTIONARY AND THESAURUS METHODS\n509\nwhere Ndoc is the total number of “documents” (glosses and examples) and ndi is\nthe number of these documents containing word i.\nFinally, we can combine the Lesk and supervised approaches by adding new\nLesk-like bag-of-words features. For example, the glosses and example sentences\nfor the target sense in WordNet could be used to compute the supervised bag-of-\nwords features in addition to the words in the SemCor context sentence for the sense\n(Yuret, 2004).\nC.6.2\nGraph-based Methods\nAnother way to use a thesaurus like WordNet is to make use of the fact that WordNet\ncan be construed as a graph, with senses as nodes and relations between senses\nas edges. In addition to the hypernymy and other relations, it’s possible to create\nlinks between senses and those words in the gloss that are unambiguous (have only\none sense). Often the relations are treated as undirected edges, creating a large\nundirected WordNet graph. Fig. C.10 shows a portion of the graph around the word\ndrink1\nv.\ntoastn\n4\ndrinkv\n1\ndrinkern\n1\ndrinkingn\n1\npotationn\n1\nsipn\n1\nsipv\n1\nbeveragen\n1\nmilkn\n1\nliquidn\n1\nfoodn\n1\ndrinkn\n1\nhelpingn\n1\nsupv\n1\nconsumptionn\n1\nconsumern\n1\nconsumev\n1\nFigure C.10\nPart of the WordNet graph around drink1v, after Navigli and Lapata (2010).\nThere are various ways to use the graph for disambiguation, some using the\nwhole graph, some using only a subpart. For example the target word and the words\nin its sentential context can all be inserted as nodes in the graph via a directed edge\nto each of its senses. If we consider the sentence She drank some milk, Fig. C.11\nshows a portion of the WordNet graph between the senses drink1\nv and milk1\nn.\ndrinkv\n1\ndrinkern\n1\nbeveragen\n1\nboozingn\n1\nfoodn\n1\ndrinkn\n1\nmilkn\n1\nmilkn\n2\nmilkn\n3\nmilkn\n4\ndrinkv\n2\ndrinkv\n3\ndrinkv\n4\ndrinkv\n5\nnutrimentn\n1\n“drink”\n“milk”\nFigure C.11\nPart of the WordNet graph between drink1v and milk1n, for disambiguating a\nsentence like She drank some milk, adapted from Navigli and Lapata (2010).\nThe correct sense is then the one which is the most important or central in some\nway in this graph. There are many different methods for deciding centrality. The",
  "517": "510\nAPPENDIX C\n•\nWORDNET: WORD RELATIONS, SENSES, AND DISAMBIGUATION\nsimplest is degree, the number of edges into the node, which tends to correlate\ndegree\nwith the most frequent sense. Another algorithm for assigning probabilities across\nnodes is personalized page rank, a version of the well-known pagerank algorithm\npersonalized\npage rank\nwhich uses some seed nodes. By inserting a uniform probability across the word\nnodes (drink and milk in the example) and computing the personalized page rank of\nthe graph, the result will be a pagerank value for each node in the graph, and the\nsense with the maximum pagerank can then be chosen. See Agirre et al. (2014) and\nNavigli and Lapata (2010) for details.\nC.7\nSemi-Supervised WSD: Bootstrapping\nBoth the supervised approach and the dictionary-based approaches to WSD require\nlarge hand-built resources: supervised training sets in one case, large dictionaries in\nthe other. We can instead use bootstrapping or semi-supervised learning, which\nbootstrapping\nneeds only a very small hand-labeled training set.\nA classic bootstrapping algorithm for WSD is the Yarowsky algorithm for\nYarowsky\nalgorithm\nlearning a classiﬁer for a target word (in a lexical-sample task) (Yarowsky, 1995).\nThe algorithm is given a small seedset Λ0 of labeled instances of each sense and a\nmuch larger unlabeled corpus V0. The algorithm ﬁrst trains an initial classiﬁer on\nthe seedset Λ0. It then uses this classiﬁer to label the unlabeled corpus V0. The\nalgorithm then selects the examples in V0 that it is most conﬁdent about, removes\nthem, and adds them to the training set (call it now Λ1). The algorithm then trains a\nnew classiﬁer (a new set of rules) on Λ1, and iterates by applying the classiﬁer to the\nnow-smaller unlabeled set V1, extracting a new training set Λ2, and so on. With each\niteration of this process, the training corpus grows and the untagged corpus shrinks.\nThe process is repeated until some sufﬁciently low error-rate on the training set is\nreached or until no further examples from the untagged corpus are above threshold.\n?\n?\nA\n?\nA\n?\nA\n?\n?\n?\nA\n?\n?\n?\n?\n?\n?\n?\n?\n?\n?\n?\n?\n?\nB\n?\n?\nA\n?\n?\n?\nA\n?\nA\nA\nA\n?\nA\nA\n?\n?\n?\n?\n?\n?\n??\n?\n?\n?\n?\nB\n?\n?\n?\n?\n?\n?\n?\n?\n?\n?\n?\n?\n?\n?\n?\n?\n?\n?\n?\n?\n?\n?\n?\n?\n?\n?\n?\n?\n?\n?\n?\n?\n?\n?\n?\n?\n?\n?\n?\n?\n?\n?\n?\n?\n?\n?\n?\n?\n?\n?\n?\n?\n?\n?\n?\n?\n?\n?\n?\n?\n?\n?\n?\n?\n?\n?\nB\n?\n?\n?\nB\nB\nB\n?\n?\nB\n?\n?\nB\n?\n?\n?\n?\n?\n?\n?\n?\n?\n?\n?\n?\n?\n?\n?\n?\n?\n?\n?\n?\n?\n?\n?\n?\n?\n?\n?\n?\n?\n?\n?\n?\n?\n?\nA\nB\nB\n?\n?\n?\n?\n??\n?\n?\n?\n?\n?\n?\n??\n?\n?\n?\n?\nA\n?\n?\n?\n?\nA\n?\n?\n?\nA\nA\nA\nA\nA\nA\nA\nLIFE\nB\nB\nMANUFACTURING\n?\n?\nA\n?\nA\n?\nA\n?\nA\n?\nA\nB\n?\n?\n?\n?\n?\n?\n?\n?\n?\n?\n?\n?\nB\n?\n?\n?\n?\n?\nA\n?\nA\n?\nA\n?\nA\nA\nA\nA\nA",
  "518": "A\nA\nA\nLIFE\nB\nB\nMANUFACTURING\n?\n?\nA\n?\nA\n?\nA\n?\nA\n?\nA\nB\n?\n?\n?\n?\n?\n?\n?\n?\n?\n?\n?\n?\nB\n?\n?\n?\n?\n?\nA\n?\nA\n?\nA\n?\nA\nA\nA\nA\nA\nA\n?\n?\n?\n?\n?\n?\n??\n?\n?\n?\n?\nB\n?\n?\n?\n?\n?\n?\n?\n?\n?\n?\n?\n?\n?\n?\n?\n?\n?\n?\n?\n?\n?\n?\n?\n?\n?\n?\n?\n?\n?\n?\n?\n?\n?\n?\n?\n?\nA\n?\nA\n?\n?\n?\n?\n?\n?\n?\n?\n?\n?\n?\n?\n?\n?\n?\nA\nB\nA\nA\n?\n?\n?\n?\n?\n?\n?\n?\n?\nB\n?\n?\n?\nB\nB\nB\n?\n?\nB\n?\nB\nB\n?\n?\n?\n?\n?\n?\n?\n?\n?\n?\n?\n?\n?\n?\n?\n?\n?\nA\nA\n?\n?\n?\n?\n?\n?\n?\n?\n?\n?\n?\n?\n?\n?\n?\nA\nB\nB\n?\n?\nB\n?\n??\n?\n?\n?\n?\n?\n??\nB\nB\n?\n?\nA\n?\nA\nA\n?\nA\n?\n?\n?\nA\nA\nA\nA\nA\nA\nA\nLIFE\nB\nB\nMANUFACTURING\nEQUIPMENT\nEMPLOYEE\n?\n?\n?\nB\nB\n?\n?\n?\n?\n?\n?\n?\nANIMAL\nMICROSCOPIC\nV0\nV1\nΛ0\nΛ1\n(a)\n(b)\nFigure C.12\nThe Yarowsky algorithm disambiguating “plant” at two stages; “?” indicates an unlabeled ob-\nservation, A and B are observations labeled as SENSE-A or SENSE-B. The initial stage (a) shows only seed\nsentences Λ0 labeled by collocates (“life” and “manufacturing”). An intermediate stage is shown in (b) where\nmore collocates have been discovered (“equipment”, “microscopic”, etc.) and more instances in V0 have been\nmoved into Λ1, leaving a smaller unlabeled set V1. Figure adapted from Yarowsky (1995).",
  "519": "C.8\n•\nUNSUPERVISED WORD SENSE INDUCTION\n511\nWe need more good teachers – right now, there are only a half a dozen who can play\nthe free bass with ease.\nAn electric guitar and bass player stand off to one side, not really part of the scene,\nThe researchers said the worms spend part of their life cycle in such ﬁsh as Paciﬁc\nsalmon and striped bass and Paciﬁc rockﬁsh or snapper.\nAnd it all started when ﬁshermen decided the striped bass in Lake Mead were...\nFigure C.13\nSamples of bass sentences extracted from the WSJ by using the simple corre-\nlates play and ﬁsh.\nInitial seeds can be selected by hand-labeling a small set of examples (Hearst,\n1991), or by using the help of a heuristic. Yarowsky (1995) used the one sense\nper collocation heuristic, which relies on the intuition that certain words or phrases\none sense per\ncollocation\nstrongly associated with the target senses tend not to occur with the other sense.\nYarowsky deﬁnes his seedset by choosing a single collocation for each sense.\nFor example, to generate seed sentences for the ﬁsh and musical musical senses\nof bass, we might come up with ﬁsh as a reasonable indicator of bass1 and play as\na reasonable indicator of bass2. Figure C.13 shows a partial result of such a search\nfor the strings “ﬁsh” and “play” in a corpus of bass examples drawn from the WSJ.\nThe original Yarowsky algorithm also makes use of a second heuristic, called\none sense per discourse, based on the work of Gale et al. (1992b), who noticed that\none sense per\ndiscourse\na particular word appearing multiple times in a text or discourse often appeared with\nthe same sense. This heuristic seems to hold better for coarse-grained senses and\nparticularly for cases of homonymy rather than polysemy (Krovetz, 1998).\nNonetheless, it is still useful in a number of sense disambiguation situations. In\nfact, the one sense per discourse heuristic is an important one throughout language\nprocessing as it seems that many disambiguation tasks may be improved by a bias\ntoward resolving an ambiguity the same way inside a discourse segment.\nC.8\nUnsupervised Word Sense Induction\nIt is expensive and difﬁcult to build large corpora in which each word is labeled for\nits word sense. For this reason, an unsupervised approach to sense disambiguation,\noften called word sense induction or WSI, is an important direction. In unsu-\nword sense\ninduction\npervised approaches, we don’t use human-deﬁned word senses. Instead, the set of\n“senses” of each word is created automatically from the instances of each word in\nthe training set.\nMost algorithms for word sense induction use some sort of clustering over word\nembeddings. (The earliest algorithms, due to Sch¨utze (Sch¨utze 1992b, Sch¨utze 1998),\nrepresented each word as a context vector of bag-of-words features⃗c.) Then in train-\ning, we use three steps.\n1. For each token wi of word w in a corpus, compute a context vector⃗c.\n2. Use a clustering algorithm to cluster these word-token context vectors⃗c into\na predeﬁned number of groups or clusters. Each cluster deﬁnes a sense of w.\n3. Compute the vector centroid of each cluster. Each vector centroid ⃗sj is a\nsense vector representing that sense of w.\nSince this is an unsupervised algorithm, we don’t have names for each of these\n“senses” of w; we just refer to the jth sense of w.",
  "520": "512\nAPPENDIX C\n•\nWORDNET: WORD RELATIONS, SENSES, AND DISAMBIGUATION\nTo disambiguate a particular token t of w we again have three steps:\n1. Compute a context vector⃗c for t.\n2. Retrieve all sense vectors sj for w.\n3. Assign t to the sense represented by the sense vector sj that is closest to t.\nAll we need is a clustering algorithm and a distance metric between vectors.\nClustering is a well-studied problem with a wide number of standard algorithms that\ncan be applied to inputs structured as vectors of numerical values (Duda and Hart,\n1973). A frequently used technique in language applications is known as agglom-\nerative clustering.\nIn this technique, each of the N training instances is initially\nagglomerative\nclustering\nassigned to its own cluster. New clusters are then formed in a bottom-up fashion by\nthe successive merging of the two clusters that are most similar. This process con-\ntinues until either a speciﬁed number of clusters is reached, or some global goodness\nmeasure among the clusters is achieved. In cases in which the number of training\ninstances makes this method too expensive, random sampling can be used on the\noriginal training set to achieve similar results.\nHow can we evaluate unsupervised sense disambiguation approaches? As usual,\nthe best way is to do extrinsic evaluation embedded in some end-to-end system; one\nexample used in a SemEval bakeoff is to improve search result clustering and di-\nversiﬁcation (Navigli and Vannella, 2013). Intrinsic evaluation requires a way to\nmap the automatically derived sense classes into a hand-labeled gold-standard set so\nthat we can compare a hand-labeled test set with a set labeled by our unsupervised\nclassiﬁer. Various such metrics have been tested, for example in the SemEval tasks\n(Manandhar et al. 2010, Navigli and Vannella 2013, Jurgens and Klapaftis 2013),\nincluding cluster overlap metrics, or methods that map each sense cluster to a pre-\ndeﬁned sense by choosing the sense that (in some training set) has the most overlap\nwith the cluster. However it is fair to say that no evaluation metric for this task has\nyet become standard.\nC.9\nSummary\nThis chapter has covered a wide range of issues concerning the meanings associated\nwith lexical items. The following are among the highlights:\n• A word sense is the locus of word meaning; deﬁnitions and meaning relations\nare deﬁned at the level of the word sense rather than wordforms.\n• Homonymy is the relation between unrelated senses that share a form, and\npolysemy is the relation between related senses that share a form.\n• Hyponymy and hypernymy relations hold between words that are in a class-\ninclusion relationship.\n• WordNet is a large database of lexical relations for English.\n• Word-sense disambiguation (WSD) is the task of determining the correct\nsense of a word in context. Supervised approaches make use of sentences in\nwhich individual words (lexical sample task) or all words (all-words task)\nare hand-labeled with senses from a resource like WordNet.\n• Classiﬁers for supervised WSD are generally trained on features of the sur-\nrounding words.\n• An important baseline for WSD is the most frequent sense, equivalent, in\nWordNet, to take the ﬁrst sense.",
  "521": "BIBLIOGRAPHICAL AND HISTORICAL NOTES\n513\n• The Lesk algorithm chooses the sense whose dictionary deﬁnition shares the\nmost words with the target word’s neighborhood.\n• Graph-based algorithms view the thesaurus as a graph and choose the sense\nthat is most central in some way.\n• Word similarity can be computed by measuring the link distance in a the-\nsaurus or by various measures of the information content of the two nodes.\nBibliographical and Historical Notes\nWord sense disambiguation traces its roots to some of the earliest applications of\ndigital computers. The insight that underlies modern algorithms for word sense\ndisambiguation was ﬁrst articulated by Weaver (1955) in the context of machine\ntranslation:\nIf one examines the words in a book, one at a time as through an opaque\nmask with a hole in it one word wide, then it is obviously impossible\nto determine, one at a time, the meaning of the words. [...] But if\none lengthens the slit in the opaque mask, until one can see not only\nthe central word in question but also say N words on either side, then\nif N is large enough one can unambiguously decide the meaning of the\ncentral word. [...] The practical question is : “What minimum value of\nN will, at least in a tolerable fraction of cases, lead to the correct choice\nof meaning for the central word?”\nOther notions ﬁrst proposed in this early period include the use of a thesaurus for dis-\nambiguation (Masterman, 1957), supervised training of Bayesian models for disam-\nbiguation (Madhu and Lytel, 1965), and the use of clustering in word sense analysis\n(Sparck Jones, 1986).\nAn enormous amount of work on disambiguation was conducted within the con-\ntext of early AI-oriented natural language processing systems. Quillian (1968) and\nQuillian (1969) proposed a graph-based approach to language understanding, in\nwhich the dictionary deﬁnition of words was represented by a network of word nodes\nconnected by syntactic and semantic relations. He then proposed to do sense disam-\nbiguation by ﬁnding the shortest path between senses in the conceptual graph. Sim-\nmons (1973) is another inﬂuential early semantic network approach. Wilks proposed\none of the earliest non-discrete models with his Preference Semantics (Wilks 1975c,\nWilks 1975b, Wilks 1975a), and Small and Rieger (1982) and Riesbeck (1975) pro-\nposed understanding systems based on modeling rich procedural information for\neach word. Hirst’s ABSITY system (Hirst and Charniak 1982, Hirst 1987, Hirst 1988),\nwhich used a technique called marker passing based on semantic networks, repre-\nsents the most advanced system of this type. As with these largely symbolic ap-\nproaches, early neural network (at the time called ‘connectionist’) approaches to\nword sense disambiguation relied on small lexicons with hand-coded representa-\ntions (Cottrell 1985, Kawamoto 1988). Considerable work on sense disambiguation\nhas also been conducted in in psycholinguistics, under the name ’lexical ambiguity\nresolution’. Small et al. (1988) present a variety of papers from this perspective.\nThe earliest implementation of a robust empirical approach to sense disambigua-\ntion is due to Kelly and Stone (1975), who directed a team that hand-crafted a set\nof disambiguation rules for 1790 ambiguous English words. Lesk (1986) was the",
  "522": "514\nAPPENDIX C\n•\nWORDNET: WORD RELATIONS, SENSES, AND DISAMBIGUATION\nﬁrst to use a machine-readable dictionary for word sense disambiguation. The prob-\nlem of dictionary senses being too ﬁne-grained has been addressed by clustering\nword senses into coarse senses (Dolan 1994, Chen and Chang 1998, Mihalcea and\ncoarse senses\nMoldovan 2001, Agirre and de Lacalle 2003, Chklovski and Mihalcea 2003, Palmer\net al. 2004, Navigli 2006, Snow et al. 2007). Corpora with clustered word senses for\ntraining clustering algorithms include Palmer et al. (2006) and OntoNotes (Hovy\nOntoNotes\net al., 2006).\nSupervised approaches to disambiguation began with the use of decision trees by\nBlack (1988). The need for large amounts of annotated text in these methods led to\ninvestigations into the use of bootstrapping methods (Hearst 1991, Yarowsky 1995).\nDiab and Resnik (2002) give a semi-supervised algorithm for sense disambigua-\ntion based on aligned parallel corpora in two languages. For example, the fact that\nthe French word catastrophe might be translated as English disaster in one instance\nand tragedy in another instance can be used to disambiguate the senses of the two\nEnglish words (i.e., to choose senses of disaster and tragedy that are similar). Ab-\nney (2002) and Abney (2004) explore the mathematical foundations of the Yarowsky\nalgorithm and its relation to co-training. The most-frequent-sense heuristic is an ex-\ntremely powerful one but requires large amounts of supervised training data.\nThe earliest use of clustering in the study of word senses was by Sparck Jones\n(1986); Pedersen and Bruce (1997), Sch¨utze (1997b), and Sch¨utze (1998) applied\ndistributional methods. Recent work on word sense induction has applied Latent\nDirichlet Allocation (LDA) (Boyd-Graber et al. 2007, Brody and Lapata 2009, Lau\net al. 2012). and large co-occurrence graphs (Di Marco and Navigli, 2013).\nA collection of work concerning WordNet can be found in Fellbaum (1998).\nEarly work using dictionaries as lexical resources include Amsler’s (1981) use of the\nMerriam Webster dictionary and Longman’s Dictionary of Contemporary English\n(Boguraev and Briscoe, 1989).\nEarly surveys of WSD include Agirre and Edmonds (2006) and Navigli (2009).\nSee Pustejovsky (1995), Pustejovsky and Boguraev (1996), Martin (1986), and\nCopestake and Briscoe (1995), inter alia, for computational approaches to the rep-\nresentation of polysemy. Pustejovsky’s theory of the generative lexicon, and in\ngenerative\nlexicon\nparticular his theory of the qualia structure of words, is another way of accounting\nqualia\nstructure\nfor the dynamic systematic polysemy of words in context.\nAnother important recent direction is the addition of sentiment and connotation\nto knowledge bases (Wiebe et al. 2005, Qiu et al. 2009, Velikovich et al. 2010)\nincluding SentiWordNet (Baccianella et al., 2010) and ConnotationWordNet (Kang\net al., 2014).\nExercises\nC.1\nCollect a small corpus of example sentences of varying lengths from any\nnewspaper or magazine. Using WordNet or any standard dictionary, deter-\nmine how many senses there are for each of the open-class words in each sen-\ntence. How many distinct combinations of senses are there for each sentence?\nHow does this number seem to vary with sentence length?\nC.2\nUsing WordNet or a standard reference dictionary, tag each open-class word\nin your corpus with its correct tag. Was choosing the correct sense always a\nstraightforward task? Report on any difﬁculties you encountered.",
  "523": "EXERCISES\n515\nC.3\nUsing your favorite dictionary, simulate the original Lesk word overlap dis-\nambiguation algorithm described on page 508 on the phrase Time ﬂies like an\narrow. Assume that the words are to be disambiguated one at a time, from\nleft to right, and that the results from earlier decisions are used later in the\nprocess.\nC.4\nBuild an implementation of your solution to the previous exercise. Using\nWordNet, implement the original Lesk word overlap disambiguation algo-\nrithm described on page 508 on the phrase Time ﬂies like an arrow.",
  "524": "Bibliography\nAbbreviations:\nAAAI\nProceedings of the National Conference on Artiﬁcial Intelligence\nACL\nProceedings of the Annual Conference of the Association for Computational Linguistics\nANLP\nProceedings of the Conference on Applied Natural Language Processing\nCLS\nPapers from the Annual Regional Meeting of the Chicago Linguistics Society\nCOGSCI\nProceedings of the Annual Conference of the Cognitive Science Society\nCOLING\nProceedings of the International Conference on Computational Linguistics\nCoNLL\nProceedings of the Conference on Computational Natural Language Learning\nEACL\nProceedings of the Conference of the European Association for Computational Linguistics\nEMNLP\nProceedings of the Conference on Empirical Methods in Natural Language Processing\nEUROSPEECH\nProceedings of the European Conference on Speech Communication and Technology\nICASSP\nProceedings of the IEEE International Conference on Acoustics, Speech, & Signal Processing\nICML\nInternational Conference on Machine Learning\nICPhS\nProceedings of the International Congress of Phonetic Sciences\nICSLP\nProceedings of the International Conference on Spoken Language Processing\nIJCAI\nProceedings of the International Joint Conference on Artiﬁcial Intelligence\nINTERSPEECH Proceedings of the Annual INTERSPEECH Conference\nIWPT\nProceedings of the International Workshop on Parsing Technologies\nJASA\nJournal of the Acoustical Society of America\nLREC\nConference on Language Resources and Evaluation\nMUC\nProceedings of the Message Understanding Conference\nNAACL-HLT\nProceedings of the North American Chapter of the ACL/Human Language Technology Conference\nSIGIR\nProceedings of Annual Conference of ACM Special Interest Group on Information Retrieval\nAbadi, M., Agarwal, A., Barham, P., Brevdo, E., Chen, Z.,\nCitro, C., Corrado, G. S., Davis, A., Dean, J., Devin, M.,\nGhemawat, S., Goodfellow, I., Harp, A., Irving, G., Isard,\nM., Jia, Y., Jozefowicz, R., Kaiser, L., Kudlur, M., Lev-\nenberg, J., Man´e, D., Monga, R., Moore, S., Murray, D.,\nOlah, C., Schuster, M., Shlens, J., Steiner, B., Sutskever,\nI., Talwar, K., Tucker, P., Vanhoucke, V., Vasudevan, V.,\nVi´egas, F., Vinyals, O., Warden, P., Wattenberg, M., Wicke,\nM., Yu, Y., and Zheng, X. (2015). TensorFlow: Large-\nscale machine learning on heterogeneous systems.. Soft-\nware available from tensorﬂow.org.\nAbney, S. P. (1991).\nParsing by chunks.\nIn Berwick,\nR. C., Abney, S. P., and Tenny, C. (Eds.), Principle-Based\nParsing: Computation and Psycholinguistics, pp. 257–278.\nKluwer.\nAbney, S. P. (1997). Stochastic attribute-value grammars.\nComputational Linguistics, 23(4), 597–618.\nAbney, S. P. (2002). Bootstrapping. In ACL-02, pp. 360–\n367.\nAbney, S. P. (2004). Understanding the Yarowsky algorithm.\nComputational Linguistics, 30(3), 365–395.\nAbney, S. P., Schapire, R. E., and Singer, Y. (1999). Boost-\ning applied to tagging and PP attachment. In EMNLP/VLC-\n99, College Park, MD, pp. 38–45.\nAdriaans, P. and van Zaanen, M. (2004). Computational\ngrammar induction for linguists. Grammars; special issue\nwith the theme “Grammar Induction”, 7, 57–68.\nAggarwal, C. C. and Zhai, C. (2012). A survey of text classi-\nﬁcation algorithms. In Aggarwal, C. C. and Zhai, C. (Eds.),\nMining text data, pp. 163–222. Springer.\nAgichtein, E. and Gravano, L. (2000). Snowball: Extract-\ning relations from large plain-text collections. In Proceed-\nings of the 5th ACM International Conference on Digital\nLibraries.\nAgirre, E. and de Lacalle, O. L. (2003). Clustering WordNet",
  "525": "Mining text data, pp. 163–222. Springer.\nAgichtein, E. and Gravano, L. (2000). Snowball: Extract-\ning relations from large plain-text collections. In Proceed-\nings of the 5th ACM International Conference on Digital\nLibraries.\nAgirre, E. and de Lacalle, O. L. (2003). Clustering WordNet\nword senses. In RANLP 2003.\nAgirre, E., Banea, C., Cardie, C., Cer, D., Diab, M.,\nGonzalez-Agirre, A., Guo, W., Lopez-Gazpio, I., Maritx-\nalar, M., Mihalcea, R., Rigau, G., Uria, L., and Wiebe,\nJ. (2015). 2015 SemEval-2015 Task 2: Semantic Textual\nSimilarity, English, Spanish and Pilot on Interpretability.\nIn SemEval-15, pp. 252–263.\nAgirre, E., Diab, M., Cer, D., and Gonzalez-Agirre, A.\n(2012). Semeval-2012 task 6: A pilot on semantic textual\nsimilarity. In SemEval-12, pp. 385–393.\nAgirre, E. and Edmonds, P. (Eds.). (2006). Word Sense Dis-\nambiguation: Algorithms and Applications. Kluwer.\nAgirre, E., L´opez de Lacalle, O., and Soroa, A. (2014). Ran-\ndom walks for knowledge-based word sense disambigua-\ntion. Computational Linguistics, 40(1), 57–84.\nAgirre, E. and Martinez, D. (2001). Learning class-to-class\nselectional preferences. In CoNLL-01.\nAhmad, F. and Kondrak, G. (2005). Learning a spelling er-\nror model from search query logs. In HLT-EMNLP-05, pp.\n955–962.\nAho, A. V., Sethi, R., and Ullman, J. D. (1986). Compilers:\nPrinciples, Techniques, and Tools. Addison-Wesley.\nAho, A. V. and Ullman, J. D. (1972). The Theory of Parsing,\nTranslation, and Compiling, Vol. 1. Prentice Hall.\nAjdukiewicz, K. (1935). Die syntaktische Konnexit¨at. Stu-\ndia Philosophica, 1, 1–27. English translation “Syntactic\nConnexion” by H. Weber in McCall, S. (Ed.) 1967. Polish\nLogic, pp. 207–231, Oxford University Press.\nAlgoet, P. H. and Cover, T. M. (1988). A sandwich proof of\nthe Shannon-McMillan-Breiman theorem. The Annals of\nProbability, 16(2), 899–909.\nAllen, J. (1984).\nTowards a general theory of action and\ntime. Artiﬁcial Intelligence, 23(2), 123–154.\nAllen, J. and Perrault, C. R. (1980). Analyzing intention in\nutterances. Artiﬁcial Intelligence, 15, 143–178.\nAmsler, R. A. (1981). A taxonomy of English nouns and\nverbs. In ACL-81, Stanford, CA, pp. 133–138.\nAn, J., Kwak, H., and Ahn, Y.-Y. (2018).\nSemAxis:\nA lightweight framework to characterize domain-speciﬁc\nword semantics beyond sentiment. In ACL 2018.\nArtstein, R., Gandhe, S., Gerten, J., Leuski, A., and Traum,\nD. (2009). Semi-formal evaluation of conversational char-\nacters. In Languages: From Formal to Natural, pp. 22–35.\nSpringer.\n517",
  "526": "518\nBibliography\nAtkins, S. (1993). Tools for computer-aided corpus lexicog-\nraphy: The Hector project. Acta Linguistica Hungarica,\n41, 5–72.\nAtkinson, K. (2011). Gnu aspell..\nAustin, J. L. (1962). How to Do Things with Words. Harvard\nUniversity Press.\nAwadallah, A. H., Kulkarni, R. G., Ozertem, U., and Jones,\nR. (2015). Charaterizing and predicting voice query refor-\nmulation. In CIKM-15.\nBaayen, R. H. (2001).\nWord frequency distributions.\nSpringer.\nBacchiani, M., Riley, M., Roark, B., and Sproat, R. (2006).\nMap adaptation of stochastic grammars. Computer Speech\n& Language, 20(1), 41–68.\nBacchiani, M., Roark, B., and Saraclar, M. (2004). Lan-\nguage model adaptation with MAP estimation and the per-\nceptron algorithm. In HLT-NAACL-04, pp. 21–24.\nBaccianella, S., Esuli, A., and Sebastiani, F. (2010). Sen-\ntiwordnet 3.0: An enhanced lexical resource for sentiment\nanalysis and opinion mining.. In LREC-10, pp. 2200–2204.\nBach, K. and Harnish, R. (1979). Linguistic communication\nand speech acts. MIT Press.\nBackus, J. W. (1959). The syntax and semantics of the pro-\nposed international algebraic language of the Zurich ACM-\nGAMM Conference. In Information Processing: Proceed-\nings of the International Conference on Information Pro-\ncessing, Paris, pp. 125–132. UNESCO.\nBackus, J. W. (1996). Transcript of question and answer\nsession. In Wexelblat, R. L. (Ed.), History of Programming\nLanguages, p. 162. Academic Press.\nBahl, L. R. and Mercer, R. L. (1976). Part of speech assign-\nment by a statistical decision algorithm. In Proceedings\nIEEE International Symposium on Information Theory, pp.\n88–89.\nBahl, L. R., Jelinek, F., and Mercer, R. L. (1983). A max-\nimum likelihood approach to continuous speech recogni-\ntion. IEEE Transactions on Pattern Analysis and Machine\nIntelligence, 5(2), 179–190.\nBaker, C. F., Fillmore, C. J., and Lowe, J. B. (1998). The\nBerkeley FrameNet project.\nIn COLING/ACL-98, Mon-\ntreal, Canada, pp. 86–90.\nBaker, J. K. (1975). The DRAGON system – An overview.\nIEEE Transactions on Acoustics, Speech, and Signal Pro-\ncessing, ASSP-23(1), 24–29.\nBaker, J. K. (1975/1990).\nStochastic modeling for auto-\nmatic speech understanding. In Waibel, A. and Lee, K.-\nF. (Eds.), Readings in Speech Recognition, pp. 297–307.\nMorgan Kaufmann. Originally appeared in Speech Recog-\nnition, Academic Press, 1975.\nBaker, J. K. (1979). Trainable grammars for speech recog-\nnition. In Klatt, D. H. and Wolf, J. J. (Eds.), Speech Com-\nmunication Papers for the 97th Meeting of the Acoustical\nSociety of America, pp. 547–550.\nBanerjee, S. and Pedersen, T. (2003). Extended gloss over-\nlaps as a measure of semantic relatedness. In IJCAI 2003,\npp. 805–810.\nBangalore, S. and Joshi, A. K. (1999). Supertagging: An\napproach to almost parsing.\nComputational Linguistics,\n25(2), 237–265.\nBanko, M., Cafarella, M., Soderland, S., Broadhead, M.,\nand Etzioni, O. (2007). Open information extraction for\nthe web. In IJCAI, Vol. 7, pp. 2670–2676.",
  "527": "approach to almost parsing.\nComputational Linguistics,\n25(2), 237–265.\nBanko, M., Cafarella, M., Soderland, S., Broadhead, M.,\nand Etzioni, O. (2007). Open information extraction for\nthe web. In IJCAI, Vol. 7, pp. 2670–2676.\nBar-Hillel, Y. (1953). A quasi-arithmetical notation for syn-\ntactic description. Language, 29, 47–58. Reprinted in Y.\nBar-Hillel. (1964). Language and Information: Selected\nEssays on their Theory and Application, Addison-Wesley,\n61–74.\nBaum, L. E. (1972). An inequality and associated maxi-\nmization technique in statistical estimation for probabilis-\ntic functions of Markov processes.\nIn Shisha, O. (Ed.),\nInequalities III: Proceedings of the 3rd Symposium on In-\nequalities, University of California, Los Angeles, pp. 1–8.\nAcademic Press.\nBaum, L. E. and Eagon, J. A. (1967). An inequality with\napplications to statistical estimation for probabilistic func-\ntions of Markov processes and to a model for ecology. Bul-\nletin of the American Mathematical Society, 73(3), 360–\n363.\nBaum, L. E. and Petrie, T. (1966). Statistical inference for\nprobabilistic functions of ﬁnite-state Markov chains. An-\nnals of Mathematical Statistics, 37(6), 1554–1563.\nBaum, L. F. (1900). The Wizard of Oz. Available at Project\nGutenberg.\nBayes, T. (1763). An Essay Toward Solving a Problem in the\nDoctrine of Chances, Vol. 53. Reprinted in Facsimiles of\nTwo Papers by Bayes, Hafner Publishing, 1963.\nBazell, C. E. (1952/1966). The correspondence fallacy in\nstructural linguistics. In Hamp, E. P., Householder, F. W.,\nand Austerlitz, R. (Eds.), Studies by Members of the En-\nglish Department, Istanbul University (3), reprinted in\nReadings in Linguistics II (1966), pp. 271–298. University\nof Chicago Press.\nBejˇcek, E., Hajiˇcov´a, E., Hajiˇc, J., J´ınov´a, P., Kettnerov´a,\nV., Kol´aˇrov´a, V., Mikulov´a, M., M´ırovsk´y, J., Nedoluzhko,\nA., Panevov´a, J., Pol´akov´a, L., ˇSevˇc´ıkov´a, M., ˇStˇep´anek,\nJ., and Zik´anov´a, ˇS. (2013). Prague dependency treebank\n3.0. Tech. rep., Institute of Formal and Applied Linguis-\ntics, Charles University in Prague. LINDAT/CLARIN dig-\nital library at Institute of Formal and Applied Linguistics,\nCharles University in Prague.\nBellegarda, J. R. (1997). A latent semantic analysis frame-\nwork for large-span language modeling. In Eurospeech-97,\nRhodes, Greece.\nBellegarda, J. R. (2000). Exploiting latent semantic infor-\nmation in statistical language modeling. Proceedings of the\nIEEE, 89(8), 1279–1296.\nBellegarda, J. R. (2004). Statistical language model adap-\ntation: Review and perspectives. Speech Communication,\n42(1), 93–108.\nBellegarda, J. R. (2013). Natural language technology in\nmobile devices: Two grounding frameworks. In Mobile\nSpeech and Advanced Natural Language Solutions, pp.\n185–196. Springer.\nBellman, R. (1957). Dynamic Programming. Princeton Uni-\nversity Press.\nBellman, R. (1984). Eye of the Hurricane: an autobiogra-\nphy. World Scientiﬁc Singapore.\nBengio, Y., Courville, A., and Vincent, P. (2013). Repre-\nsentation learning: A review and new perspectives. IEEE\nTransactions on Pattern Analysis and Machine Intelli-",
  "528": "Bellman, R. (1957). Dynamic Programming. Princeton Uni-\nversity Press.\nBellman, R. (1984). Eye of the Hurricane: an autobiogra-\nphy. World Scientiﬁc Singapore.\nBengio, Y., Courville, A., and Vincent, P. (2013). Repre-\nsentation learning: A review and new perspectives. IEEE\nTransactions on Pattern Analysis and Machine Intelli-\ngence, 35(8), 1798–1828.\nBengio, Y., Ducharme, R., Vincent, P., and Jauvin, C.\n(2003). A neural probabilistic language model. Journal\nof machine learning research, 3(Feb), 1137–1155.\nBengio, Y., Lamblin, P., Popovici, D., and Larochelle, H.\n(2007). Greedy layer-wise training of deep networks. In\nNIPS 2007, pp. 153–160.",
  "529": "Bibliography\n519\nBengio, Y., Schwenk, H., Sen´ecal, J.-S., Morin, F., and Gau-\nvain, J.-L. (2006). Neural probabilistic language models. In\nInnovations in Machine Learning, pp. 137–186. Springer.\nBerant, J., Chou, A., Frostig, R., and Liang, P. (2013). Se-\nmantic parsing on freebase from question-answer pairs. In\nEMNLP 2013.\nBerant, J. and Liang, P. (2014). Semantic parsing via para-\nphrasing. In ACL 2014.\nBerg-Kirkpatrick, T., Burkett, D., and Klein, D. (2012). An\nempirical investigation of statistical signiﬁcance in NLP. In\nEMNLP 2012, pp. 995–1005.\nBerger, A., Della Pietra, S. A., and Della Pietra, V. J. (1996).\nA maximum entropy approach to natural language process-\ning. Computational Linguistics, 22(1), 39–71.\nBergsma, S., Lin, D., and Goebel, R. (2008). Discriminative\nlearning of selectional preference from unlabeled text. In\nEMNLP-08, pp. 59–68.\nBergsma, S., Lin, D., and Goebel, R. (2009). Web-scale n-\ngram models for lexical disambiguation.. In IJCAI-09, pp.\n1507–1512.\nBergsma, S., Pitler, E., and Lin, D. (2010). Creating robust\nsupervised classiﬁers via web-scale n-gram data. In ACL\n2010, pp. 865–874.\nBethard, S. (2013). ClearTK-TimeML: A minimalist ap-\nproach to TempEval 2013. In SemEval-13, pp. 10–14.\nBever, T. G. (1970). The cognitive basis for linguistic struc-\ntures. In Hayes, J. R. (Ed.), Cognition and the Development\nof Language, pp. 279–352. Wiley.\nBhat, I., Bhat, R. A., Shrivastava, M., and Sharma, D.\n(2017). Joining hands: Exploiting monolingual treebanks\nfor parsing of code-mixing data. In EACL-17, pp. 324–330.\nBiber, D., Johansson, S., Leech, G., Conrad, S., and Fine-\ngan, E. (1999). Longman Grammar of Spoken and Written\nEnglish. Pearson ESL, Harlow.\nBies, A., Ferguson, M., Katz, K., and MacIntyre, R. (1995).\nBracketing guidelines for Treebank II style Penn Treebank\nProject..\nBikel, D. M. (2004). Intricacies of Collins’ parsing model.\nComputational Linguistics, 30(4), 479–511.\nBikel, D. M., Miller, S., Schwartz, R., and Weischedel,\nR. (1997). Nymble: A high-performance learning name-\nﬁnder. In ANLP 1997, pp. 194–201.\nBird, S., Klein, E., and Loper, E. (2009). Natural Language\nProcessing with Python. O’Reilly.\nBisani, M. and Ney, H. (2004).\nBootstrap estimates for\nconﬁdence intervals in ASR performance evaluation. In\nICASSP-04, Vol. I, pp. 409–412.\nBishop, C. M. (2006).\nPattern recognition and machine\nlearning. Springer.\nBizer, C., Lehmann, J., Kobilarov, G., Auer, S., Becker,\nC., Cyganiak, R., and Hellmann, S. (2009). DBpedia—A\ncrystallization point for the Web of Data. Web Semantics:\nscience, services and agents on the world wide web, 7(3),\n154–165.\nBlack, E. (1988). An experiment in computational discrim-\nination of English word senses. IBM Journal of Research\nand Development, 32(2), 185–194.\nBlack, E., Abney, S. P., Flickinger, D., Gdaniec, C., Grish-",
  "530": "science, services and agents on the world wide web, 7(3),\n154–165.\nBlack, E. (1988). An experiment in computational discrim-\nination of English word senses. IBM Journal of Research\nand Development, 32(2), 185–194.\nBlack, E., Abney, S. P., Flickinger, D., Gdaniec, C., Grish-\nman, R., Harrison, P., Hindle, D., Ingria, R., Jelinek, F.,\nKlavans, J. L., Liberman, M. Y., Marcus, M. P., Roukos,\nS., Santorini, B., and Strzalkowski, T. (1991). A procedure\nfor quantitatively comparing the syntactic coverage of En-\nglish grammars. In Proceedings DARPA Speech and Natu-\nral Language Workshop, Paciﬁc Grove, CA, pp. 306–311.\nBlack, E., Jelinek, F., Lafferty, J. D., Magerman, D. M.,\nMercer, R. L., and Roukos, S. (1992). Towards history-\nbased grammars: Using richer models for probabilistic\nparsing. In Proceedings DARPA Speech and Natural Lan-\nguage Workshop, Harriman, NY, pp. 134–139.\nBlair, C. R. (1960). A program for correcting spelling errors.\nInformation and Control, 3, 60–67.\nBlei, D. M., Ng, A. Y., and Jordan, M. I. (2003). Latent\nDirichlet allocation.\nJournal of Machine Learning Re-\nsearch, 3(5), 993–1022.\nBlodgett, S. L., Green, L., and O’Connor, B. (2016). Demo-\ngraphic dialectal variation in social media: A case study of\nAfrican-American English. In EMNLP 2016.\nBlodgett, S. L. and O’Connor, B. (2017). Racial disparity in\nnatural language processing: A case study of social media\nafrican-american english. In Fairness, Accountability, and\nTransparency in Machine Learning (FAT/ML) Workshop,\nKDD.\nBloomﬁeld, L. (1914). An Introduction to the Study of Lan-\nguage. Henry Holt and Company.\nBloomﬁeld, L. (1933a). Language. University of Chicago\nPress.\nBloomﬁeld, L. (1933b). Language. University of Chicago\nPress.\nBobrow, D. G., Kaplan, R. M., Kay, M., Norman, D. A.,\nThompson, H., and Winograd, T. (1977). GUS, A frame\ndriven dialog system. Artiﬁcial Intelligence, 8, 155–173.\nBobrow, D. G. and Norman, D. A. (1975). Some princi-\nples of memory schemata. In Bobrow, D. G. and Collins,\nA. (Eds.), Representation and Understanding. Academic\nPress.\nBobrow, D. G. and Winograd, T. (1977). An overview of\nKRL, a knowledge representation language. Cognitive Sci-\nence, 1(1), 3–46.\nBod, R. (1993). Using an annotated corpus as a stochastic\ngrammar. In EACL-93, pp. 37–44.\nBoguraev, B. and Briscoe, T. (Eds.). (1989). Computational\nLexicography for Natural Language Processing. Longman.\nBohus, D. and Rudnicky, A. I. (2005). Sorry, I didn’t catch\nthat! — An investigation of non-understanding errors and\nrecovery strategies. In Proceedings of SIGDIAL, Lisbon,\nPortugal.\nBojanowski, P., Grave, E., Joulin, A., and Mikolov, T.\n(2017). Enriching word vectors with subword information.\nTACL, 5, 135–146.\nBollacker, K., Evans, C., Paritosh, P., Sturge, T., and Tay-\nlor, J. (2008). Freebase: a collaboratively created graph\ndatabase for structuring human knowledge. In SIGMOD\n2008, pp. 1247–1250.\nBolukbasi, T., Chang, K.-W., Zou, J. Y., Saligrama, V., and",
  "531": "Bollacker, K., Evans, C., Paritosh, P., Sturge, T., and Tay-\nlor, J. (2008). Freebase: a collaboratively created graph\ndatabase for structuring human knowledge. In SIGMOD\n2008, pp. 1247–1250.\nBolukbasi, T., Chang, K.-W., Zou, J. Y., Saligrama, V., and\nKalai, A. T. (2016). Man is to computer programmer as\nwoman is to homemaker? Debiasing word embeddings. In\nNIPS 16, pp. 4349–4357.\nBooth, T. L. (1969). Probabilistic representation of formal\nlanguages. In IEEE Conference Record of the 1969 Tenth\nAnnual Symposium on Switching and Automata Theory, pp.\n74–81.\nBooth, T. L. and Thompson, R. A. (1973). Applying prob-\nability measures to abstract languages. IEEE Transactions\non Computers, C-22(5), 442–450.\nBorges, J. L. (1964).\nThe analytical language of John\nWilkins.\nUniversity of Texas Press.\nTrans. Ruth L. C.\nSimms.\nBowman, S. R., Vilnis, L., Vinyals, O., Dai, A. M., Jozefow-\nicz, R., and Bengio, S. (2016). Generating sentences from\na continuous space. In CoNLL-16, pp. 10–21.",
  "532": "520\nBibliography\nBoyd-Graber, J., Blei, D. M., and Zhu, X. (2007). A topic\nmodel for word sense disambiguation. In EMNLP/CoNLL\n2007.\nBoyd-Graber, J., Feng, S., and Rodriguez, P. (2018).\nHuman-computer question answering:\nThe case for\nquizbowl. In Escalera, S. and Weimer, M. (Eds.), The NIPS\n’17 Competition: Building Intelligent Systems. Springer\nVerlag.\nBrachman, R. J. (1979). On the epistemogical status of se-\nmantic networks. In Findler, N. V. (Ed.), Associative Net-\nworks: Representation and Use of Knowledge by Comput-\ners, pp. 3–50. Academic Press.\nBrachman, R. J. and Levesque, H. J. (Eds.). (1985). Read-\nings in Knowledge Representation. Morgan Kaufmann.\nBrachman, R. J. and Schmolze, J. G. (1985). An overview\nof the KL-ONE knowledge representation system. Cogni-\ntive Science, 9(2), 171–216.\nBrants, T. (2000). TnT: A statistical part-of-speech tagger.\nIn ANLP 2000, Seattle, WA, pp. 224–231.\nBrants, T., Popat, A. C., Xu, P., Och, F. J., and Dean, J.\n(2007). Large language models in machine translation. In\nEMNLP/CoNLL 2007.\nBr´eal, M. (1897). Essai de S´emantique: Science des signiﬁ-\ncations. Hachette, Paris, France.\nBresnan, J. (Ed.). (1982).\nThe Mental Representation of\nGrammatical Relations. MIT Press.\nBrill, E., Dumais, S. T., and Banko, M. (2002). An analy-\nsis of the AskMSR question-answering system. In EMNLP\n2002, pp. 257–264.\nBrill, E. and Moore, R. C. (2000). An improved error model\nfor noisy channel spelling correction. In ACL-00, Hong\nKong, pp. 286–293.\nBrill, E. and Resnik, P. (1994).\nA rule-based approach\nto prepositional phrase attachment disambiguation.\nIn\nCOLING-94, Kyoto, pp. 1198–1204.\nBrin, S. (1998).\nExtracting patterns and relations from\nthe World Wide Web.\nIn Proceedings World Wide Web\nand Databases International Workshop, Number 1590 in\nLNCS, pp. 172–183. Springer.\nBriscoe, T. and Carroll, J. (1993). Generalized probabilistic\nLR parsing of natural language (corpora) with uniﬁcation-\nbased grammars. Computational Linguistics, 19(1), 25–59.\nBrockmann, C. and Lapata, M. (2003). Evaluating and com-\nbining approaches to selectional preference acquisition. In\nEACL-03, pp. 27–34.\nBrody, S. and Lapata, M. (2009). Bayesian word sense in-\nduction. In EACL-09, pp. 103–111.\nBroschart, J. (1997). Why Tongan does it differently. Lin-\nguistic Typology, 1, 123–165.\nBruce, B. C. (1975). Generation as a social action. In Pro-\nceedings of TINLAP-1 (Theoretical Issues in Natural Lan-\nguage Processing), pp. 64–67. Association for Computa-\ntional Linguistics.\nBruce, R. F. and Wiebe, J. (1994). Word-sense disambigua-\ntion using decomposable models. In ACL-94, Las Cruces,\nNM, pp. 139–145.\nBrysbaert, M., Warriner, A. B., and Kuperman, V. (2014).\nConcreteness ratings for 40 thousand generally known en-\nglish word lemmas. Behavior Research Methods, 46(3),\n904–911.\nBuchholz, S. and Marsi, E. (2006). Conll-x shared task on",
  "533": "NM, pp. 139–145.\nBrysbaert, M., Warriner, A. B., and Kuperman, V. (2014).\nConcreteness ratings for 40 thousand generally known en-\nglish word lemmas. Behavior Research Methods, 46(3),\n904–911.\nBuchholz, S. and Marsi, E. (2006). Conll-x shared task on\nmultilingual dependency parsing. In CoNLL-06, pp. 149–\n164.\nBuck, C., Heaﬁeld, K., and Van Ooyen, B. (2014). N-gram\ncounts and language models from the common crawl. In\nProceedings of LREC.\nBudanitsky, A. and Hirst, G. (2006). Evaluating WordNet-\nbased measures of lexical semantic relatedness. Computa-\ntional Linguistics, 32(1), 13–47.\nBullinaria, J. A. and Levy, J. P. (2007). Extracting seman-\ntic representations from word co-occurrence statistics: A\ncomputational study. Behavior research methods, 39(3),\n510–526.\nBullinaria, J. A. and Levy, J. P. (2012).\nExtracting se-\nmantic representations from word co-occurrence statistics:\nstop-lists, stemming, and svd. Behavior research methods,\n44(3), 890–907.\nBulyko, I., Kirchhoff, K., Ostendorf, M., and Goldberg, J.\n(2005).\nError-sensitive response generation in a spoken\nlanguage dialogue system. Speech Communication, 45(3),\n271–288.\nBulyko, I., Ostendorf, M., and Stolcke, A. (2003).\nGet-\nting more mileage from web text sources for conversational\nspeech language modeling using class-dependent mixtures.\nIn HLT-NAACL-03, Edmonton, Canada, Vol. 2, pp. 7–9.\nCaliskan, A., Bryson, J. J., and Narayanan, A. (2017). Se-\nmantics derived automatically from language corpora con-\ntain human-like biases. Science, 356(6334), 183–186.\nCardie, C. (1993). A case-based approach to knowledge ac-\nquisition for domain speciﬁc sentence analysis. In AAAI-\n93, pp. 798–803. AAAI Press.\nCardie, C. (1994). Domain-Speciﬁc Knowledge Acquisition\nfor Conceptual Sentence Analysis. Ph.D. thesis, University\nof Massachusetts, Amherst, MA. Available as CMPSCI\nTechnical Report 94-74.\nCarletta, J., Isard, A., Isard, S., Kowtko, J. C., Doherty-\nSneddon, G., and Anderson, A. H. (1997). The reliability\nof a dialogue structure coding scheme. Computational Lin-\nguistics, 23(1), 13–32.\nCarpenter, R. (2017). Cleverbot. http://www.cleverbot.com,\naccessed 2017.\nCarreras, X. and M`arquez, L. (2005).\nIntroduction to\nthe CoNLL-2005 shared task: Semantic role labeling. In\nCoNLL-05, pp. 152–164.\nCarroll, G. and Charniak, E. (1992). Two experiments on\nlearning probabilistic dependency grammars from corpora.\nTech. rep. CS-92-16, Brown University.\nCarroll, J., Briscoe, T., and Sanﬁlippo, A. (1998). Parser\nevaluation: A survey and a new proposal. In LREC-98,\nGranada, Spain, pp. 447–454.\nChambers, N. (2013). NavyTime: Event and time ordering\nfrom raw text. In SemEval-13, pp. 73–77.\nChambers, N. and Jurafsky, D. (2010). Improving the use\nof pseudo-words for evaluating selectional preferences. In\nACL 2010, pp. 445–453.\nChambers, N. and Jurafsky, D. (2011). Template-based in-\nformation extraction without the templates. In ACL 2011.",
  "534": "from raw text. In SemEval-13, pp. 73–77.\nChambers, N. and Jurafsky, D. (2010). Improving the use\nof pseudo-words for evaluating selectional preferences. In\nACL 2010, pp. 445–453.\nChambers, N. and Jurafsky, D. (2011). Template-based in-\nformation extraction without the templates. In ACL 2011.\nChang, A. X. and Manning, C. D. (2012). SUTime: A li-\nbrary for recognizing and normalizing time expressions..\nIn LREC-12, pp. 3735–3740.\nChang, P.-C., Galley, M., and Manning, C. D. (2008). Opti-\nmizing Chinese word segmentation for machine translation\nperformance. In Proceedings of ACL Statistical MT Work-\nshop, pp. 224–232.\nCharniak, E. (1997). Statistical parsing with a context-free\ngrammar and word statistics. In AAAI-97, pp. 598–603.\nAAAI Press.",
  "535": "Bibliography\n521\nCharniak,\nE.,\nHendrickson,\nC.,\nJacobson,\nN.,\nand\nPerkowitz, M. (1993). Equations for part-of-speech tag-\nging. In AAAI-93, Washington, D.C., pp. 784–789. AAAI\nPress.\nCharniak, E. and Johnson, M. (2005). Coarse-to-ﬁne n-best\nparsing and MaxEnt discriminative reranking. In ACL-05,\nAnn Arbor.\nChe, W., Li, Z., Li, Y., Guo, Y., Qin, B., and Liu, T.\n(2009). Multilingual dependency-based syntactic and se-\nmantic parsing. In CoNLL-09, pp. 49–54.\nChelba, C. and Jelinek, F. (2000). Structured language mod-\neling. Computer Speech and Language, 14, 283–332.\nChen, D., Fisch, A., Weston, J., and Bordes, A. (2017).\nReading wikipedia to answer open-domain questions. In\nACL 2017.\nChen, D. and Manning, C. D. (2014). A fast and accurate de-\npendency parser using neural networks.. In EMNLP 2014,\npp. 740–750.\nChen, J. N. and Chang, J. S. (1998).\nTopical clustering\nof MRD senses based on information retrieval techniques.\nComputational Linguistics, 24(1), 61–96.\nChen, S. F. and Goodman, J. (1996). An empirical study of\nsmoothing techniques for language modeling. In ACL-96,\nSanta Cruz, CA, pp. 310–318.\nChen, S. F. and Goodman, J. (1998). An empirical study of\nsmoothing techniques for language modeling. Tech. rep.\nTR-10-98, Computer Science Group, Harvard University.\nChen, S. F. and Goodman, J. (1999). An empirical study of\nsmoothing techniques for language modeling. Computer\nSpeech and Language, 13, 359–394.\nChierchia, G. and McConnell-Ginet, S. (1991). Meaning\nand Grammar. MIT Press.\nChinchor, N., Hirschman, L., and Lewis, D. L. (1993). Eval-\nuating Message Understanding systems: An analysis of the\nthird Message Understanding Conference. Computational\nLinguistics, 19(3), 409–449.\nChiticariu, L., Danilevsky, M., Li, Y., Reiss, F., and Zhu, H.\n(2018). SystemT: Declarative text understanding for enter-\nprise. In NAACL HLT 2018, Vol. 3, pp. 76–83.\nChiticariu, L., Li, Y., and Reiss, F. R. (2013). Rule-Based\nInformation Extraction is Dead! Long Live Rule-Based In-\nformation Extraction Systems!. In EMNLP 2013, pp. 827–\n832.\nChklovski, T. and Mihalcea, R. (2003). Exploiting agree-\nment and disagreement of human annotators for word sense\ndisambiguation. In RANLP 2003.\nChoi, E., He, H., Iyyer, M., Yatskar, M., Yih, W.-t., Choi,\nY., Liang, P., and Zettlemoyer, L. (2018). Quac: Question\nanswering in context. In EMNLP 2018.\nChoi, J. D. and Palmer, M. (2011a). Getting the most out\nof transition-based dependency parsing. In ACL 2011, pp.\n687–692.\nChoi, J. D. and Palmer, M. (2011b). Transition-based se-\nmantic role labeling using predicate argument clustering.\nIn Proceedings of the ACL 2011 Workshop on Relational\nModels of Semantics, pp. 37–45.\nChoi, J. D., Tetreault, J., and Stent, A. (2015). It depends:\nDependency parser comparison using a web-based evalua-\ntion tool. In ACL 2015, pp. 26–31.\nChomsky, N. (1956). Three models for the description of\nlanguage. IRE Transactions on Information Theory, 2(3),\n113–124.",
  "536": "Choi, J. D., Tetreault, J., and Stent, A. (2015). It depends:\nDependency parser comparison using a web-based evalua-\ntion tool. In ACL 2015, pp. 26–31.\nChomsky, N. (1956). Three models for the description of\nlanguage. IRE Transactions on Information Theory, 2(3),\n113–124.\nChomsky, N. (1956/1975). The Logical Structure of Lin-\nguistic Theory. Plenum.\nChomsky, N. (1957). Syntactic Structures. Mouton, The\nHague.\nChomsky, N. (1963). Formal properties of grammars. In\nLuce, R. D., Bush, R., and Galanter, E. (Eds.), Handbook\nof Mathematical Psychology, Vol. 2, pp. 323–418. Wiley.\nChomsky, N. (1981). Lectures on Government and Binding.\nForis.\nChristodoulopoulos, C., Goldwater, S., and Steedman, M.\n(2010). Two decades of unsupervised POS induction: How\nfar have we come?. In EMNLP-10.\nChu, Y.-J. and Liu, T.-H. (1965). On the shortest arbores-\ncence of a directed graph. Science Sinica, 14, 1396–1400.\nChu-Carroll, J. (1998). A statistical model for discourse act\nrecognition in dialogue interactions. In Chu-Carroll, J. and\nGreen, N. (Eds.), Applying Machine Learning to Discourse\nProcessing. Papers from the 1998 AAAI Spring Symposium.\nTech. rep. SS-98-01, pp. 12–17. AAAI Press.\nChu-Carroll, J. and Carpenter, B. (1999).\nVector-based\nnatural language call routing. Computational Linguistics,\n25(3), 361–388.\nChu-Carroll, J., Fan, J., Boguraev, B. K., Carmel, D.,\nSheinwald, D., and Welty, C. (2012). Finding needles in\nthe haystack: Search and candidate generation. IBM Jour-\nnal of Research and Development, 56(3/4), 6:1–6:12.\nChurch, A. (1940). A formulation of a simple theory of\ntypes. Journal of Symbolic Logic, 5, 56–68.\nChurch, K. W. and Gale, W. A. (1991). Probability scor-\ning for spelling correction. Statistics and Computing, 1(2),\n93–103.\nChurch, K. W. (1980). On Memory Limitations in Natural\nLanguage Processing Master’s thesis, MIT. Distributed by\nthe Indiana University Linguistics Club.\nChurch, K. W. (1988). A stochastic parts program and noun\nphrase parser for unrestricted text. In ANLP 1988, pp. 136–\n143.\nChurch, K. W. (1989). A stochastic parts program and noun\nphrase parser for unrestricted text. In ICASSP-89, pp. 695–\n698.\nChurch, K. W. (1994). Unix for Poets. Slides from 2nd\nELSNET Summer School and unpublished paper ms.\nChurch, K. W. and Gale, W. A. (1991). A comparison of\nthe enhanced Good-Turing and deleted estimation methods\nfor estimating probabilities of English bigrams. Computer\nSpeech and Language, 5, 19–54.\nChurch, K. W. and Hanks, P. (1989).\nWord association\nnorms, mutual information, and lexicography. In ACL-89,\nVancouver, B.C., pp. 76–83.\nChurch, K. W. and Hanks, P. (1990).\nWord association\nnorms, mutual information, and lexicography. Computa-\ntional Linguistics, 16(1), 22–29.\nChurch, K. W., Hart, T., and Gao, J. (2007). Compress-\ning trigram language models with Golomb coding.\nIn\nEMNLP/CoNLL 2007, pp. 199–207.\nClark, A. (2001). The unsupervised induction of stochastic\ncontext-free grammars using distributional clustering. In\nCoNLL-01.\nClark, C. and Gardner, M. (2018).\nSimple and effective",
  "537": "ing trigram language models with Golomb coding.\nIn\nEMNLP/CoNLL 2007, pp. 199–207.\nClark, A. (2001). The unsupervised induction of stochastic\ncontext-free grammars using distributional clustering. In\nCoNLL-01.\nClark, C. and Gardner, M. (2018).\nSimple and effective\nmulti-paragraph reading comprehension. In ACL 2018.\nClark, E. (1987). The principle of contrast: A constraint on\nlanguage acquisition. In MacWhinney, B. (Ed.), Mecha-\nnisms of language acquisition, pp. 1–33. LEA.\nClark, H. H. (1996). Using Language. Cambridge Univer-\nsity Press.\nClark, H. H. and Fox Tree, J. E. (2002). Using uh and um\nin spontaneous speaking. Cognition, 84, 73–111.",
  "538": "522\nBibliography\nClark, H. H. and Marshall, C. (1981). Deﬁnite reference\nand mutual knowledge. In Joshi, A. K., Webber, B. L., and\nSag, I. A. (Eds.), Elements of Discourse Understanding,\npp. 10–63. Cambridge.\nClark, H. H. and Schaefer, E. F. (1989). Contributing to\ndiscourse. Cognitive Science, 13, 259–294.\nClark, H. H. and Wilkes-Gibbs, D. (1986). Referring as a\ncollaborative process. Cognition, 22, 1–39.\nClark, P., Cowhey, I., Etzioni, O., Khot, T., Sabharwal, A.,\nSchoenick, C., and Tafjord, O. (2018).\nThink you have\nsolved question answering? Try ARC, the AI2 reasoning\nchallenge.. arXiv preprint arXiv:1803.05457.\nClark, S. and Curran, J. R. (2004). Parsing the WSJ using\nCCG and log-linear models. In ACL-04, pp. 104–111.\nClark, S., Curran, J. R., and Osborne, M. (2003). Bootstrap-\nping pos taggers using unlabelled data. In CoNLL-03, pp.\n49–55.\nCoccaro, N. and Jurafsky, D. (1998). Towards better inte-\ngration of semantic predictors in statistical language mod-\neling. In ICSLP-98, Sydney, Vol. 6, pp. 2403–2406.\nCohen, K. B. and Demner-Fushman, D. (2014). Biomedical\nnatural language processing. Benjamins.\nCohen, M. H., Giangola, J. P., and Balogh, J. (2004). Voice\nUser Interface Design. Addison-Wesley.\nCohen, P. R. and Perrault, C. R. (1979). Elements of a plan-\nbased theory of speech acts. Cognitive Science, 3(3), 177–\n212.\nColby, K. M., Hilf, F. D., Weber, S., and Kraemer, H. C.\n(1972). Turing-like indistinguishability tests for the valida-\ntion of a computer simulation of paranoid processes. Arti-\nﬁcial Intelligence, 3, 199–221.\nColby, K. M., Weber, S., and Hilf, F. D. (1971). Artiﬁcial\nparanoia. Artiﬁcial Intelligence, 2(1), 1–25.\nCole, R. A., Novick, D. G., Vermeulen, P. J. E., Sutton, S.,\nFanty, M., Wessels, L. F. A., de Villiers, J. H., Schalkwyk,\nJ., Hansen, B., and Burnett, D. (1997). Experiments with a\nspoken dialogue system for taking the US census. Speech\nCommunication, 23, 243–260.\nCollins, M. (1996). A new statistical parser based on bi-\ngram lexical dependencies. In ACL-96, Santa Cruz, CA,\npp. 184–191.\nCollins, M. (1999). Head-Driven Statistical Models for Nat-\nural Language Parsing. Ph.D. thesis, University of Penn-\nsylvania, Philadelphia.\nCollins, M. (2000). Discriminative reranking for natural lan-\nguage parsing. In ICML 2000, Stanford, CA, pp. 175–182.\nCollins, M. (2003a). Head-driven statistical models for nat-\nural language parsing. Computational Linguistics, 29(4),\n589–637.\nCollins, M. (2003b). Head-driven statistical models for nat-\nural language parsing. Computational Linguistics, 29(4),\n589–637.\nCollins, M., Hajiˇc, J., Ramshaw, L. A., and Tillmann, C.\n(1999). A statistical parser for Czech. In ACL-99, College\nPark, MA, pp. 505–512.\nCollins, M. and Koo, T. (2005). Discriminative reranking\nfor natural language parsing. Computational Linguistics,\n31(1), 25–69.",
  "539": "Collins, M., Hajiˇc, J., Ramshaw, L. A., and Tillmann, C.\n(1999). A statistical parser for Czech. In ACL-99, College\nPark, MA, pp. 505–512.\nCollins, M. and Koo, T. (2005). Discriminative reranking\nfor natural language parsing. Computational Linguistics,\n31(1), 25–69.\nCollobert, R. and Weston, J. (2007). Fast semantic extrac-\ntion using a novel neural network architecture. In ACL-07,\npp. 560–567.\nCollobert, R. and Weston, J. (2008).\nA uniﬁed architec-\nture for natural language processing: Deep neural networks\nwith multitask learning. In ICML, pp. 160–167.\nCollobert,\nR.,\nWeston,\nJ.,\nBottou,\nL.,\nKarlen,\nM.,\nKavukcuoglu, K., and Kuksa, P. (2011). Natural language\nprocessing (almost) from scratch. The Journal of Machine\nLearning Research, 12, 2493–2537.\nCopestake, A. and Briscoe, T. (1995). Semi-productive pol-\nysemy and sense extension. Journal of Semantics, 12(1),\n15–68.\nCottrell, G. W. (1985).\nA Connectionist Approach to\nWord Sense Disambiguation. Ph.D. thesis, University of\nRochester, Rochester, NY. Revised version published by\nPitman, 1989.\nCover, T. M. and Thomas, J. A. (1991). Elements of Infor-\nmation Theory. Wiley.\nCovington, M. (2001). A fundamental algorithm for depen-\ndency parsing. In Proceedings of the 39th Annual ACM\nSoutheast Conference, pp. 95–102.\nCox, D. (1969). Analysis of Binary Data. Chapman and\nHall, London.\nCraven, M. and Kumlien, J. (1999). Constructing biolog-\nical knowledge bases by extracting information from text\nsources. In ISMB-99, pp. 77–86.\nCruse, D. A. (2004). Meaning in Language: an Introduction\nto Semantics and Pragmatics.\nOxford University Press.\nSecond edition.\nCucerzan, S. and Brill, E. (2004). Spelling correction as an\niterative process that exploits the collective knowledge of\nweb users. In EMNLP 2004, Vol. 4, pp. 293–300.\nCulicover, P. W. and Jackendoff, R. (2005). Simpler Syntax.\nOxford University Press.\nDagan, I., Marcus, S., and Markovitch, S. (1993). Contex-\ntual word similarity and estimation from sparse data. In\nACL-93, Columbus, Ohio, pp. 164–171.\nDamerau, F. J. (1964). A technique for computer detection\nand correction of spelling errors. Communications of the\nACM, 7(3), 171–176.\nDamerau, F. J. and Mays, E. (1989). An examination of un-\ndetected typing errors. Information Processing and Man-\nagement, 25(6), 659–664.\nDanieli, M. and Gerbino, E. (1995). Metrics for evaluating\ndialogue strategies in a spoken language system. In Pro-\nceedings of the 1995 AAAI Spring Symposium on Empir-\nical Methods in Discourse Interpretation and Generation,\nStanford, CA, pp. 34–39. AAAI Press.\nDas, S. R. and Chen, M. Y. (2001).\nYahoo!\nfor\nAmazon:\nSentiment parsing from small talk on the\nweb. EFA 2001 Barcelona Meetings. Available at SSRN:\nhttp://ssrn.com/abstract=276189.\nDavidson, D. (1967). The logical form of action sentences.\nIn Rescher, N. (Ed.), The Logic of Decision and Action.\nUniversity of Pittsburgh Press.\nDavidson, T., Warmsley, D., Macy, M., and Weber, I. (2017).\nAutomated hate speech detection and the problem of offen-\nsive language. In ICWSM 2017.",
  "540": "Davidson, D. (1967). The logical form of action sentences.\nIn Rescher, N. (Ed.), The Logic of Decision and Action.\nUniversity of Pittsburgh Press.\nDavidson, T., Warmsley, D., Macy, M., and Weber, I. (2017).\nAutomated hate speech detection and the problem of offen-\nsive language. In ICWSM 2017.\nDavies, M. (2012). Expanding horizons in historical linguis-\ntics with the 400-million word Corpus of Historical Amer-\nican English. Corpora, 7(2), 121–157.\nDavis, E. (1990). Representations of Commonsense Knowl-\nedge. Morgan Kaufmann.\nde Marneffe, M.-C., Dozat, T., Silveira, N., Haverinen, K.,\nGinter, F., Nivre, J., and Manning, C. D. (2014). Univer-\nsal stanford dependencies: A cross-linguistic typology.. In\nLREC, Vol. 14, pp. 4585–92.\nde Marneffe, M.-C., MacCartney, B., and Manning, C. D.\n(2006). Generating typed dependency parses from phrase\nstructure parses. In LREC-06.",
  "541": "Bibliography\n523\nde Marneffe, M.-C. and Manning, C. D. (2008). The stan-\nford typed dependencies representation. In Coling 2008:\nProceedings of the workshop on Cross-Framework and\nCross-Domain Parser Evaluation, pp. 1–8.\nDeerwester, S. C., Dumais, S. T., Furnas, G. W., Harshman,\nR. A., Landauer, T. K., Lochbaum, K. E., and Streeter, L.\n(1988). Computer information retrieval using latent seman-\ntic structure: Us patent 4,839,853..\nDeerwester, S. C., Dumais, S. T., Landauer, T. K., Furnas,\nG. W., and Harshman, R. A. (1990). Indexing by latent\nsemantics analysis. JASIS, 41(6), 391–407.\nDeJong, G. F. (1982). An overview of the FRUMP system.\nIn Lehnert, W. G. and Ringle, M. H. (Eds.), Strategies for\nNatural Language Processing, pp. 149–176. Lawrence Erl-\nbaum.\nDempster, A. P., Laird, N. M., and Rubin, D. B. (1977).\nMaximum likelihood from incomplete data via the EM al-\ngorithm. Journal of the Royal Statistical Society, 39(1),\n1–21.\nDeRose, S. J. (1988). Grammatical category disambiguation\nby statistical optimization. Computational Linguistics, 14,\n31–39.\nDi Marco, A. and Navigli, R. (2013). Clustering and di-\nversifying web search results with graph-based word sense\ninduction. Computational Linguistics, 39(3), 709–754.\nDiab, M. and Resnik, P. (2002). An unsupervised method\nfor word sense tagging using parallel corpora. In ACL-02,\npp. 255–262.\nDigman, J. M. (1990). Personality structure: Emergence of\nthe ﬁve-factor model. Annual Review of Psychology, 41(1),\n417–440.\nDo, Q. N. T., Bethard, S., and Moens, M.-F. (2017). Improv-\ning implicit semantic role labeling by predicting semantic\nframe arguments. In IJCNLP-17.\nDolan, W. B. (1994). Word sense ambiguation: Clustering\nrelated senses. In COLING-94, Kyoto, Japan, pp. 712–716.\ndos Santos, C., Xiang, B., and Zhou, B. (2015). Classifying\nrelations by ranking with convolutional neural networks. In\nACL 2015.\nDowty, D. R. (1979). Word Meaning and Montague Gram-\nmar. D. Reidel.\nDowty, D. R., Wall, R. E., and Peters, S. (1981). Introduc-\ntion to Montague Semantics. D. Reidel.\nDozat, T., Qi, P., and Manning, C. D. (2017). Stanford’s\ngraph-based neural dependency parser at the conll 2017\nshared task. In Proceedings of the CoNLL 2017 Shared\nTask, pp. 20–30.\nDror, R., Baumer, G., Bogomolov, M., and Reichart, R.\n(2017). Replicability analysis for natural language process-\ning: Testing signiﬁcance with multiple datasets. TACL, 5,\n471––486.\nDuda, R. O. and Hart, P. E. (1973). Pattern Classiﬁcation\nand Scene Analysis. John Wiley and Sons.\nEarley, J. (1968).\nAn Efﬁcient Context-Free Parsing Al-\ngorithm. Ph.D. thesis, Carnegie Mellon University, Pitts-\nburgh, PA.\nEarley, J. (1970).\nAn efﬁcient context-free parsing al-\ngorithm.\nCommunications of the ACM, 6(8), 451–455.\nReprinted in Grosz et al. (1986).\nEdmonds, J. (1967). Optimum branchings. Journal of Re-\nsearch of the National Bureau of Standards B, 71(4), 233–\n240.",
  "542": "burgh, PA.\nEarley, J. (1970).\nAn efﬁcient context-free parsing al-\ngorithm.\nCommunications of the ACM, 6(8), 451–455.\nReprinted in Grosz et al. (1986).\nEdmonds, J. (1967). Optimum branchings. Journal of Re-\nsearch of the National Bureau of Standards B, 71(4), 233–\n240.\nEfron, B. and Tibshirani, R. J. (1993). An introduction to\nthe bootstrap. CRC press.\nEgghe, L. (2007). Untangling Herdan’s law and Heaps’ law:\nMathematical and informetric arguments. JASIST, 58(5),\n702–709.\nEisner, J. (1996). Three new probabilistic models for de-\npendency parsing: An exploration. In COLING-96, Copen-\nhagen, pp. 340–345.\nEisner, J. (2002). An interactive spreadsheet for teaching\nthe forward-backward algorithm.\nIn Proceedings of the\nACL Workshop on Effective Tools and Methodologies for\nTeaching NLP and CL, pp. 10–18.\nEjerhed, E. I. (1988). Finding clauses in unrestricted text by\nﬁnitary and stochastic methods. In ANLP 1988, pp. 219–\n227.\nEkman, P. (1999). Basic emotions. In Dalgleish, T. and\nPower, M. J. (Eds.), Handbook of Cognition and Emotion,\npp. 45–60. Wiley.\nElman, J. L. (1990). Finding structure in time. Cognitive\nscience, 14(2), 179–211.\nErk, K. (2007). A simple, similarity-based model for selec-\ntional preferences. In ACL-07, pp. 216–223.\nEtzioni, O., Cafarella, M., Downey, D., Popescu, A.-M.,\nShaked, T., Soderland, S., Weld, D. S., and Yates, A.\n(2005).\nUnsupervised named-entity extraction from the\nweb: An experimental study. Artiﬁcial Intelligence, 165(1),\n91–134.\nEvans, N. (2000). Word classes in the world’s languages. In\nBooij, G., Lehmann, C., and Mugdan, J. (Eds.), Morphol-\nogy: A Handbook on Inﬂection and Word Formation, pp.\n708–732. Mouton.\nFader, A., Soderland, S., and Etzioni, O. (2011). Identifying\nrelations for open information extraction. In EMNLP-11,\npp. 1535–1545.\nFader,\nA.,\nZettlemoyer,\nL.,\nand Etzioni,\nO. (2013).\nParaphrase-driven learning for open question answering. In\nACL 2013, Soﬁa, Bulgaria, pp. 1608–1618.\nFano, R. M. (1961). Transmission of Information: A Statis-\ntical Theory of Communications. MIT Press.\nFast, E., Chen, B., and Bernstein, M. S. (2016). Empath:\nUnderstanding Topic Signals in Large-Scale Text. In CHI.\nFeldman, J. A. and Ballard, D. H. (1982). Connectionist\nmodels and their properties. Cognitive Science, 6, 205–\n254.\nFellbaum, C. (Ed.). (1998). WordNet: An Electronic Lexical\nDatabase. MIT Press.\nFensel, D., Hendler, J. A., Lieberman, H., and Wahlster,\nW. (Eds.). (2003). Spinning the Semantic Web: Bring the\nWorld Wide Web to its Full Potential. MIT Press, Cam-\nbridge, MA.\nFerro, L., Gerber, L., Mani, I., Sundheim, B., and Wilson,\nG. (2005). Tides 2005 standard for the annotation of tem-\nporal expressions. Tech. rep., MITRE.\nFerrucci, D. A. (2012). Introduction to “This is Watson”.\nIBM Journal of Research and Development, 56(3/4), 1:1–\n1:15.\nFessler, L. (2017). We tested bots like Siri and Alexa to see",
  "543": "G. (2005). Tides 2005 standard for the annotation of tem-\nporal expressions. Tech. rep., MITRE.\nFerrucci, D. A. (2012). Introduction to “This is Watson”.\nIBM Journal of Research and Development, 56(3/4), 1:1–\n1:15.\nFessler, L. (2017). We tested bots like Siri and Alexa to see\nwho would stand up to sexual harassment. In Quartz. Feb\n22, 2017. https://qz.com/911681/.\nFikes, R. E. and Nilsson, N. J. (1971). STRIPS: A new ap-\nproach to the application of theorem proving to problem\nsolving. Artiﬁcial Intelligence, 2, 189–208.\nFillmore, C. J. (1966). A proposal concerning english prepo-\nsitions. In Dinneen, F. P. (Ed.), 17th annual Round Table.,\nVol. 17 of Monograph Series on Language and Linguistics,\npp. 19–34. Georgetown University Press, Washington D.C.",
  "544": "524\nBibliography\nFillmore, C. J. (1968). The case for case. In Bach, E. W.\nand Harms, R. T. (Eds.), Universals in Linguistic Theory,\npp. 1–88. Holt, Rinehart & Winston.\nFillmore, C. J. (1985). Frames and the semantics of under-\nstanding. Quaderni di Semantica, VI(2), 222–254.\nFillmore, C. J. (2003). Valency and semantic roles: the con-\ncept of deep structure case. In ´Agel, V., Eichinger, L. M.,\nEroms, H. W., Hellwig, P., Heringer, H. J., and Lobin, H.\n(Eds.), Dependenz und Valenz: Ein internationales Hand-\nbuch der zeitgen¨ossischen Forschung, chap. 36, pp. 457–\n475. Walter de Gruyter.\nFillmore, C. J. (2012). Encounters with language. Compu-\ntational Linguistics, 38(4), 701–718.\nFillmore, C. J. and Baker, C. F. (2009). A frames approach\nto semantic analysis. In Heine, B. and Narrog, H. (Eds.),\nThe Oxford Handbook of Linguistic Analysis, pp. 313–340.\nOxford University Press.\nFillmore, C. J., Johnson, C. R., and Petruck, M. R. L. (2003).\nBackground to FrameNet. International journal of lexicog-\nraphy, 16(3), 235–250.\nFinkelstein, L., Gabrilovich, E., Matias, Y., Rivlin, E.,\nSolan, Z., Wolfman, G., and Ruppin, E. (2002). Placing\nsearch in context: The concept revisited. ACM Transac-\ntions on Information Systems, 20(1), 116––131.\nFirth, J. R. (1935). The technique of semantics. Transac-\ntions of the philological society, 34(1), 36–73.\nFirth, J. R. (1957). A synopsis of linguistic theory 1930–\n1955. In Studies in Linguistic Analysis. Philological Soci-\nety. Reprinted in Palmer, F. (ed.) 1968. Selected Papers of\nJ. R. Firth. Longman, Harlow.\nFoland, Jr., W. R. and Martin, J. H. (2015). Dependency-\nbased semantic role labeling using convolutional neural\nnetworks. In *SEM 2015), pp. 279–289.\nForbes-Riley, K. and Litman, D. J. (2011). Beneﬁts and\nchallenges of real-time uncertainty detection and adapta-\ntion in a spoken dialogue computer tutor. Speech Commu-\nnication, 53(9), 1115–1136.\nForchini, P. (2013). Using movie corpora to explore spoken\nAmerican English: Evidence from multi-dimensional anal-\nysis. In Bamford, J., Cavalieri, S., and Diani, G. (Eds.),\nVariation and Change in Spoken and Written Discourse:\nPerspectives from corpus linguistics, pp. 123–136. Ben-\njamins.\nForney, Jr., G. D. (1973). The Viterbi algorithm. Proceed-\nings of the IEEE, 61(3), 268–278.\nFrancis, H. S., Gregory, M. L., and Michaelis, L. A. (1999).\nAre lexical subjects deviant?.\nIn CLS-99. University of\nChicago.\nFrancis, W. N. and Kuˇcera, H. (1982). Frequency Analysis\nof English Usage. Houghton Mifﬂin, Boston.\nFranz, A. (1997).\nIndependence assumptions considered\nharmful. In ACL/EACL-97, Madrid, Spain, pp. 182–189.\nFranz, A. and Brants, T. (2006). All our n-gram are be-\nlong to you. http://googleresearch.blogspot.com/\n2006/08/all-our-n-gram-are-belong-to-you.\nhtml.\nFraser, N. M. and Gilbert, G. N. (1991). Simulating speech",
  "545": "Franz, A. and Brants, T. (2006). All our n-gram are be-\nlong to you. http://googleresearch.blogspot.com/\n2006/08/all-our-n-gram-are-belong-to-you.\nhtml.\nFraser, N. M. and Gilbert, G. N. (1991). Simulating speech\nsystems. Computer Speech and Language, 5, 81–99.\nFyshe, A., Wehbe, L., Talukdar, P. P., Murphy, B., and\nMitchell, T. M. (2015). A compositional and interpretable\nsemantic space. In NAACL HLT 2015.\nGabow, H. N., Galil, Z., Spencer, T., and Tarjan, R. E.\n(1986). Efﬁcient algorithms for ﬁnding minimum spanning\ntrees in undirected and directed graphs. Combinatorica,\n6(2), 109–122.\nGage, P. (1994). A new algorithm for data compression. The\nC Users Journal, 12(2), 23–38.\nGale, W. A. and Church, K. W. (1994).\nWhat is wrong\nwith adding one?.\nIn Oostdijk, N. and de Haan, P.\n(Eds.), Corpus-Based Research into Language, pp. 189–\n198. Rodopi.\nGale, W. A., Church, K. W., and Yarowsky, D. (1992a). Es-\ntimating upper and lower bounds on the performance of\nword-sense disambiguation programs. In ACL-92, Newark,\nDE, pp. 249–256.\nGale, W. A., Church, K. W., and Yarowsky, D. (1992b). One\nsense per discourse. In Proceedings DARPA Speech and\nNatural Language Workshop, pp. 233–237.\nGale, W. A., Church, K. W., and Yarowsky, D. (1992c).\nWork on statistical methods for word sense disambigua-\ntion. In Goldman, R. (Ed.), Proceedings of the 1992 AAAI\nFall Symposium on Probabilistic Approaches to Natural\nLanguage.\nGarg, N., Schiebinger, L., Jurafsky, D., and Zou, J. (2018).\nWord embeddings quantify 100 years of gender and eth-\nnic stereotypes. Proceedings of the National Academy of\nSciences, 115(16), E3635–E3644.\nGarside, R. (1987). The CLAWS word-tagging system. In\nGarside, R., Leech, G., and Sampson, G. (Eds.), The Com-\nputational Analysis of English, pp. 30–41. Longman.\nGarside, R., Leech, G., and McEnery, A. (1997). Corpus\nAnnotation. Longman.\nGazdar, G., Klein, E., Pullum, G. K., and Sag, I. A. (1985).\nGeneralized Phrase Structure Grammar. Blackwell.\nGerber, M. and Chai, J. Y. (2010). Beyond nombank: A\nstudy of implicit arguments for nominal predicates. In ACL\n2010, pp. 1583–1592.\nGil, D. (2000). Syntactic categories, cross-linguistic varia-\ntion and universal grammar. In Vogel, P. M. and Comrie,\nB. (Eds.), Approaches to the Typology of Word Classes, pp.\n173–216. Mouton.\nGildea, D. and Jurafsky, D. (2000). Automatic labeling of\nsemantic roles. In ACL-00, Hong Kong, pp. 512–520.\nGildea, D. and Jurafsky, D. (2002). Automatic labeling of\nsemantic roles.\nComputational Linguistics, 28(3), 245–\n288.\nGildea, D. and Palmer, M. (2002). The necessity of syntac-\ntic parsing for predicate argument recognition. In ACL-02,\nPhiladelphia, PA.\nGillick, L. and Cox, S. J. (1989). Some statistical issues\nin the comparison of speech recognition algorithms.\nIn\nICASSP-89, pp. 532–535.\nGinzburg, J. and Sag, I. A. (2000). Interrogative Investiga-",
  "546": "tic parsing for predicate argument recognition. In ACL-02,\nPhiladelphia, PA.\nGillick, L. and Cox, S. J. (1989). Some statistical issues\nin the comparison of speech recognition algorithms.\nIn\nICASSP-89, pp. 532–535.\nGinzburg, J. and Sag, I. A. (2000). Interrogative Investiga-\ntions: the Form, Meaning and Use of English Interroga-\ntives. CSLI.\nGiuliano, V. E. (1965).\nThe interpretation of word as-\nsociations.\nIn Stevens, M. E., Giuliano, V. E., and\nHeilprin, L. B. (Eds.), Statistical Association Methods\nFor Mechanized Documentation. Symposium Proceed-\nings. Washington, D.C., USA, March 17, 1964, pp. 25–\n32. https://nvlpubs.nist.gov/nistpubs/Legacy/\nMP/nbsmiscellaneouspub269.pdf.\nGiv´on, T. (1990). Syntax: A Functional Typological Intro-\nduction. John Benjamins.\nGlennie, A. (1960). On the syntax machine and the construc-\ntion of a universal compiler. Tech. rep. No. 2, Contr. NR\n049-141, Carnegie Mellon University (at the time Carnegie\nInstitute of Technology), Pittsburgh, PA.\nGodfrey, J., Holliman, E., and McDaniel, J. (1992).\nSWITCHBOARD: Telephone speech corpus for research\nand development. In ICASSP-92, San Francisco, pp. 517–\n520.",
  "547": "Bibliography\n525\nGoffman, E. (1974). Frame analysis: An essay on the orga-\nnization of experience. Harvard University Press.\nGoldberg, J., Ostendorf, M., and Kirchhoff, K. (2003). The\nimpact of response wording in error correction subdialogs.\nIn ISCA Tutorial and Research Workshop on Error Han-\ndling in Spoken Dialogue Systems.\nGoldberg, Y. (2017). Neural Network Methods for Natu-\nral Language Processing, Vol. 10 of Synthesis Lectures on\nHuman Language Technologies. Morgan & Claypool.\nGolding, A. R. and Roth, D. (1999). A Winnow based ap-\nproach to context-sensitive spelling correction. Machine\nLearning, 34(1-3), 107–130.\nGondek, D., Lally, A., Kalyanpur, A., Murdock, J. W.,\nDubou´e, P. A., Zhang, L., Pan, Y., Qiu, Z., and Welty, C.\n(2012). A framework for merging and ranking of answers\nin deepqa.\nIBM Journal of Research and Development,\n56(3/4), 14:1–14:12.\nGood, M. D., Whiteside, J. A., Wixon, D. R., and Jones, S. J.\n(1984). Building a user-derived interface. Communications\nof the ACM, 27(10), 1032–1043.\nGoodfellow, I., Bengio, Y., and Courville, A. (2016). Deep\nLearning. MIT Press.\nGoodman, J. (1997). Probabilistic feature grammars. In\nIWPT-97.\nGoodman, J. (2006). A bit of progress in language mod-\neling: Extended version.\nTech. rep. MSR-TR-2001-72,\nMachine Learning and Applied Statistics Group, Microsoft\nResearch, Redmond, WA.\nGoodwin, C. (1996). Transparent vision. In Ochs, E., Sche-\ngloff, E. A., and Thompson, S. A. (Eds.), Interaction and\nGrammar, pp. 370–404. Cambridge University Press.\nGould, J. D., Conti, J., and Hovanyecz, T. (1983). Compos-\ning letters with a simulated listening typewriter. Communi-\ncations of the ACM, 26(4), 295–308.\nGould, J. D. and Lewis, C. (1985). Designing for usability:\nKey principles and what designers think. Communications\nof the ACM, 28(3), 300–311.\nGould, S. J. (1980). The Panda’s Thumb. Penguin Group.\nGravano, A., Hirschberg, J., and Beˇnuˇs, ˇS. (2012). Afﬁrma-\ntive cue words in task-oriented dialogue. Computational\nLinguistics, 38(1), 1–39.\nGreen, B. F., Wolf, A. K., Chomsky, C., and Laughery, K.\n(1961). Baseball: An automatic question answerer. In Pro-\nceedings of the Western Joint Computer Conference 19, pp.\n219–224. Reprinted in Grosz et al. (1986).\nGreene, B. B. and Rubin, G. M. (1971). Automatic gram-\nmatical tagging of English.\nDepartment of Linguistics,\nBrown University, Providence, Rhode Island.\nGreenwald, A. G., McGhee, D. E., and Schwartz, J. L. K.\n(1998). Measuring individual differences in implicit cog-\nnition: the implicit association test.. Journal of personality\nand social psychology, 74(6), 1464–1480.\nGrenager, T. and Manning, C. D. (2006). Unsupervised Dis-\ncovery of a Statistical Verb Lexicon. In EMNLP 2006.\nGrishman, R. and Sundheim, B. (1995).\nDesign of the\nMUC-6 evaluation. In MUC-6, San Francisco, pp. 1–11.\nGrosz, B. J. (1977). The Representation and Use of Focus in\nDialogue Understanding. Ph.D. thesis, University of Cali-\nfornia, Berkeley.\nGrosz, B. J. and Sidner, C. L. (1980). Plans for discourse.",
  "548": "Design of the\nMUC-6 evaluation. In MUC-6, San Francisco, pp. 1–11.\nGrosz, B. J. (1977). The Representation and Use of Focus in\nDialogue Understanding. Ph.D. thesis, University of Cali-\nfornia, Berkeley.\nGrosz, B. J. and Sidner, C. L. (1980). Plans for discourse.\nIn Cohen, P. R., Morgan, J., and Pollack, M. E. (Eds.), In-\ntentions in Communication, pp. 417–444. MIT Press.\nGruber, J. S. (1965). Studies in Lexical Relations. Ph.D.\nthesis, MIT.\nGuindon, R. (1988). A multidisciplinary perspective on di-\nalogue structure in user-advisor dialogues. In Guindon, R.\n(Ed.), Cognitive Science and Its Applications for Human-\nComputer Interaction, pp. 163–200. Lawrence Erlbaum.\nGusﬁeld, D. (1997). Algorithms on Strings, Trees, and Se-\nquences: Computer Science and Computational Biology.\nCambridge University Press.\nGuyon, I. and Elisseeff, A. (2003). An introduction to vari-\nable and feature selection. The Journal of Machine Learn-\ning Research, 3, 1157–1182.\nHaghighi, A. and Klein, D. (2006). Prototype-driven gram-\nmar induction. In COLING/ACL 2006, pp. 881–888.\nHajiˇc, J. (1998). Building a Syntactically Annotated Cor-\npus:\nThe Prague Dependency Treebank, pp. 106–132.\nKarolinum.\nHajiˇc, J. (2000). Morphological tagging: Data vs. dictionar-\nies. In NAACL 2000. Seattle.\nHajiˇc, J., Ciaramita, M., Johansson, R., Kawahara, D.,\nMart´ı, M. A., M`arquez, L., Meyers, A., Nivre, J., Pad´o,\nS., ˇStˇep´anek, J., Stranˇa´k, P., Surdeanu, M., Xue, N., and\nZhang, Y. (2009).\nThe conll-2009 shared task: Syntac-\ntic and semantic dependencies in multiple languages. In\nCoNLL-09, pp. 1–18.\nHakkani-T¨ur, D., Oﬂazer, K., and T¨ur, G. (2002). Statis-\ntical morphological disambiguation for agglutinative lan-\nguages. Journal of Computers and Humanities, 36(4), 381–\n410.\nHakkani-T¨ur, D., T¨ur, G., Celikyilmaz, A., Chen, Y.-N.,\nGao, J., Deng, L., and Wang, Y.-Y. (2016). Multi-domain\njoint semantic frame parsing using bi-directional rnn-lstm..\nIn INTERSPEECH, pp. 715–719.\nHale, J. (2001). A probabilistic earley parser as a psycholin-\nguistic model. In NAACL 2001, pp. 159–166.\nHamilton, W. L., Clark, K., Leskovec, J., and Jurafsky, D.\n(2016a). Inducing domain-speciﬁc sentiment lexicons from\nunlabeled corpora. In EMNLP 2016.\nHamilton, W. L., Leskovec, J., and Jurafsky, D. (2016b).\nDiachronic word embeddings reveal statistical laws of se-\nmantic change. In ACL 2016.\nHarabagiu, S., Pasca, M., and Maiorano, S. (2000). Exper-\niments with open-domain textual question answering. In\nCOLING-00, Saarbr¨ucken, Germany.\nHarris, R. A. (2005). Voice Interaction Design: Crafting the\nNew Conversational Speech Systems. Morgan Kaufmann.\nHarris, Z. S. (1946). From morpheme to utterance. Lan-\nguage, 22(3), 161–183.\nHarris, Z. S. (1954). Distributional structure. Word, 10,",
  "549": "COLING-00, Saarbr¨ucken, Germany.\nHarris, R. A. (2005). Voice Interaction Design: Crafting the\nNew Conversational Speech Systems. Morgan Kaufmann.\nHarris, Z. S. (1946). From morpheme to utterance. Lan-\nguage, 22(3), 161–183.\nHarris, Z. S. (1954). Distributional structure. Word, 10,\n146–162. Reprinted in J. Fodor and J. Katz, The Structure\nof Language, Prentice Hall, 1964 and in Z. S. Harris, Pa-\npers in Structural and Transformational Linguistics, Rei-\ndel, 1970, 775–794.\nHarris, Z. S. (1962). String Analysis of Sentence Structure.\nMouton, The Hague.\nHastie, T., Tibshirani, R. J., and Friedman, J. H. (2001). The\nElements of Statistical Learning. Springer.\nHatzivassiloglou, V. and McKeown, K. R. (1997). Predict-\ning the semantic orientation of adjectives. In ACL/EACL-\n97, pp. 174–181.\nHatzivassiloglou, V. and Wiebe, J. (2000). Effects of adjec-\ntive orientation and gradability on sentence subjectivity. In\nCOLING-00, pp. 299–305.\nHe, L., Lee, K., Lewis, M., and Zettlemoyer, L. (2017). Deep\nsemantic role labeling: What works and what’s next. In\nACL 2017, pp. 473–483.",
  "550": "526\nBibliography\nHeaﬁeld, K. (2011). KenLM: Faster and smaller language\nmodel queries. In Workshop on Statistical Machine Trans-\nlation, pp. 187–197.\nHeaﬁeld, K., Pouzyrevsky, I., Clark, J. H., and Koehn, P.\n(2013). Scalable modiﬁed Kneser-Ney language model es-\ntimation.. In ACL 2013, pp. 690–696.\nHeaps, H. S. (1978). Information retrieval. Computational\nand theoretical aspects. Academic Press.\nHearst, M. A. (1991). Noun homograph disambiguation.\nIn Proceedings of the 7th Conference of the University of\nWaterloo Centre for the New OED and Text Research, pp.\n1–19.\nHearst, M. A. (1992a). Automatic acquisition of hyponyms\nfrom large text corpora. In COLING-92, Nantes, France.\nHearst, M. A. (1992b). Automatic acquisition of hyponyms\nfrom large text corpora. In COLING-92, Nantes, France.\nCOLING.\nHearst, M. A. (1998). Automatic discovery of WordNet re-\nlations.\nIn Fellbaum, C. (Ed.), WordNet: An Electronic\nLexical Database. MIT Press.\nHeckerman, D., Horvitz, E., Sahami, M., and Dumais, S. T.\n(1998). A bayesian approach to ﬁltering junk e-mail. In\nProceeding of AAAI-98 Workshop on Learning for Text\nCategorization, pp. 55–62.\nHeim, I. and Kratzer, A. (1998). Semantics in a Generative\nGrammar. Blackwell Publishers, Malden, MA.\nHemphill, C. T., Godfrey, J., and Doddington, G. (1990).\nThe ATIS spoken language systems pilot corpus. In Pro-\nceedings DARPA Speech and Natural Language Workshop,\nHidden Valley, PA, pp. 96–101.\nHenderson, P., Sinha, K., Angelard-Gontier, N., Ke, N. R.,\nFried, G., Lowe, R., and Pineau, J. (2017). Ethical chal-\nlenges in data-driven dialogue systems. In AAAI/ACM AI\nEthics and Society Conference.\nHendrickx, I., Kim, S. N., Kozareva, Z., Nakov, P.,\n´O S´eaghdha, D., Pad´o, S., Pennacchiotti, M., Romano, L.,\nand Szpakowicz, S. (2009). Semeval-2010 task 8: Multi-\nway classiﬁcation of semantic relations between pairs of\nnominals. In Proceedings of the Workshop on Semantic\nEvaluations: Recent Achievements and Future Directions,\npp. 94–99.\nHendrix, G. G., Thompson, C. W., and Slocum, J. (1973).\nLanguage processing via canonical verbs and semantic\nmodels. In Proceedings of IJCAI-73.\nHerdan, G. (1960). Type-token mathematics. The Hague,\nMouton.\nHermann, K. M., Kocisky, T., Grefenstette, E., Espeholt, L.,\nKay, W., Suleyman, M., and Blunsom, P. (2015). Teaching\nmachines to read and comprehend. In Advances in Neural\nInformation Processing Systems, pp. 1693–1701.\nHill, F., Reichart, R., and Korhonen, A. (2015). Simlex-999:\nEvaluating semantic models with (genuine) similarity esti-\nmation. Computational Linguistics, 41(4), 665–695.\nHindle, D. and Rooth, M. (1990). Structural ambiguity and\nlexical relations. In Proceedings DARPA Speech and Natu-\nral Language Workshop, Hidden Valley, PA, pp. 257–262.\nHindle, D. and Rooth, M. (1991). Structural ambiguity and\nlexical relations. In ACL-91, Berkeley, CA, pp. 229–236.\nHinkelman, E. A. and Allen, J. (1989). Two constraints on\nspeech act ambiguity. In ACL-89, Vancouver, Canada, pp.",
  "551": "ral Language Workshop, Hidden Valley, PA, pp. 257–262.\nHindle, D. and Rooth, M. (1991). Structural ambiguity and\nlexical relations. In ACL-91, Berkeley, CA, pp. 229–236.\nHinkelman, E. A. and Allen, J. (1989). Two constraints on\nspeech act ambiguity. In ACL-89, Vancouver, Canada, pp.\n212–219.\nHinton, G. E. (1986). Learning distributed representations\nof concepts. In COGSCI-86, pp. 1–12.\nHinton, G. E., Osindero, S., and Teh, Y.-W. (2006). A fast\nlearning algorithm for deep belief nets. Neural computa-\ntion, 18(7), 1527–1554.\nHinton, G. E., Srivastava, N., Krizhevsky, A., Sutskever,\nI., and Salakhutdinov, R. R. (2012).\nImproving neural\nnetworks by preventing co-adaptation of feature detectors.\narXiv preprint arXiv:1207.0580.\nHirschberg, J., Litman, D. J., and Swerts, M. (2001). Iden-\ntifying user corrections automatically in spoken dialogue\nsystems. In NAACL 2001.\nHirschman, L., Light, M., Breck, E., and Burger, J. D.\n(1999). Deep Read: A reading comprehension system. In\nACL-99, pp. 325–332.\nHirschman, L. and Pao, C. (1993). The cost of errors in a\nspoken language system. In EUROSPEECH-93, pp. 1419–\n1422.\nHirst, G. (1987). Semantic Interpretation and the Resolution\nof Ambiguity. Cambridge University Press.\nHirst, G. (1988).\nResolving lexical ambiguity computa-\ntionally with spreading activation and polaroid words. In\nSmall, S. L., Cottrell, G. W., and Tanenhaus, M. K. (Eds.),\nLexical Ambiguity Resolution, pp. 73–108. Morgan Kauf-\nmann.\nHirst, G. and Budanitsky, A. (2005). Correcting real-word\nspelling errors by restoring lexical cohesion. Natural Lan-\nguage Engineering, 11, 87–111.\nHirst, G. and Charniak, E. (1982). Word sense and case slot\ndisambiguation. In AAAI-82, pp. 95–98.\nHjelmslev, L. (1969).\nPrologomena to a Theory of Lan-\nguage. University of Wisconsin Press. Translated by Fran-\ncis J. Whitﬁeld; original Danish edition 1943.\nHobbs, J. R., Appelt, D. E., Bear, J., Israel, D., Kameyama,\nM., Stickel, M. E., and Tyson, M. (1997). FASTUS: A\ncascaded ﬁnite-state transducer for extracting information\nfrom natural-language text.\nIn Roche, E. and Schabes,\nY. (Eds.), Finite-State Language Processing, pp. 383–406.\nMIT Press.\nHockenmaier, J. and Steedman, M. (2007). Ccgbank: a cor-\npus of ccg derivations and dependency structures extracted\nfrom the penn treebank. Computational Linguistics, 33(3),\n355–396.\nHofmann, T. (1999). Probabilistic latent semantic indexing.\nIn SIGIR-99, Berkeley, CA.\nHopcroft, J. E. and Ullman, J. D. (1979). Introduction to\nAutomata Theory, Languages, and Computation. Addison-\nWesley.\nHorning, J. J. (1969). A Study of Grammatical Inference.\nPh.D. thesis, Stanford University.\nHouseholder, F. W. (1995). Dionysius Thrax, the technai,\nand Sextus Empiricus. In Koerner, E. F. K. and Asher, R. E.\n(Eds.), Concise History of the Language Sciences, pp. 99–\n103. Elsevier Science.\nHovy, E. H., Hermjakob, U., and Ravichandran, D. (2002).",
  "552": "Householder, F. W. (1995). Dionysius Thrax, the technai,\nand Sextus Empiricus. In Koerner, E. F. K. and Asher, R. E.\n(Eds.), Concise History of the Language Sciences, pp. 99–\n103. Elsevier Science.\nHovy, E. H., Hermjakob, U., and Ravichandran, D. (2002).\nA question/answer typology with surface text patterns. In\nHLT-01.\nHovy, E. H., Marcus, M. P., Palmer, M., Ramshaw, L. A.,\nand Weischedel, R. (2006). Ontonotes: The 90% solution.\nIn HLT-NAACL-06.\nHsu, B.-J. (2007). Generalized linear interpolation of lan-\nguage models. In IEEE ASRU-07, pp. 136–140.\nHu, M. and Liu, B. (2004a). Mining and summarizing cus-\ntomer reviews. In KDD, pp. 168–177.\nHu, M. and Liu, B. (2004b). Mining and summarizing cus-\ntomer reviews. In SIGKDD-04.",
  "553": "Bibliography\n527\nHuang, E. H., Socher, R., Manning, C. D., and Ng, A. Y.\n(2012). Improving word representations via global context\nand multiple word prototypes. In ACL 2012, pp. 873–882.\nHuang, L. and Chiang, D. (2005). Better k-best parsing. In\nIWPT-05, pp. 53–64.\nHuang, L. and Sagae, K. (2010). Dynamic programming for\nlinear-time incremental parsing. In ACL 2010, pp. 1077–\n1086.\nHuang, Z., Xu, W., and Yu, K. (2015). Bidirectional LSTM-\nCRF models for sequence tagging.\nIn arXiv preprint\narXiv:1508.01991.\nHuddleston, R. and Pullum, G. K. (2002). The Cambridge\nGrammar of the English Language. Cambridge University\nPress.\nHudson, R. A. (1984). Word Grammar. Blackwell.\nHuffman, S. (1996). Learning information extraction pat-\nterns from examples.\nIn Wertmer, S., Riloff, E., and\nScheller, G. (Eds.), Connectionist, Statistical, and Sym-\nbolic Approaches to Learning Natural Language Process-\ning, pp. 246–260. Springer.\nHutto, C. J., Folds, D., and Appling, S. (2015). Compu-\ntationally detecting and quantifying the degree of bias in\nsentence-level text of news stories. In HUSO 2015: The\nFirst International Conference on Human and Social Ana-\nlytics.\nHymes, D. (1974).\nWays of speaking.\nIn Bauman, R.\nand Sherzer, J. (Eds.), Explorations in the ethnography of\nspeaking, pp. 433–451. Cambridge University Press.\nIacobacci, I., Pilehvar, M. T., and Navigli, R. (2016). Em-\nbeddings for word sense disambiguation: An evaluation\nstudy. In ACL 2016, pp. 897–907.\nIrons, E. T. (1961). A syntax directed compiler for ALGOL\n60. Communications of the ACM, 4, 51–55.\nIsbell, C. L., Kearns, M., Kormann, D., Singh, S., and Stone,\nP. (2000). Cobot in LambdaMOO: A social statistics agent.\nIn AAAI/IAAI, pp. 36–41.\nISO8601 (2004). Data elements and interchange formats—\ninformation interchange—representation of dates and\ntimes. Tech. rep., International Organization for Standards\n(ISO).\nJackendoff, R. (1983).\nSemantics and Cognition.\nMIT\nPress.\nJacobs, P. S. and Rau, L. F. (1990).\nSCISOR: A system\nfor extracting information from on-line news. Communi-\ncations of the ACM, 33(11), 88–97.\nJaech, A., Mulcaire, G., Hathi, S., Ostendorf, M., and Smith,\nN. A. (2016). Hierarchical character-word models for lan-\nguage identiﬁcation. In ACL Workshop on NLP for Social\nMedia, pp. 84––93.\nJafarpour, S., Burges, C. J. C., and Ritter, A. (2009). Fil-\nter, rank, and transfer the knowledge: Learning to chat.\nIn NIPS Workshop on Advances in Ranking, Vancouver,\nCanada.\nJauhiainen, T., Lui, M., Zampieri, M., Baldwin, T., and\nLind´en, K. (2018). Automatic language identiﬁcation in\ntexts: A survey. arXiv preprint arXiv:1804.08186.\nJefferson, G. (1972). Side sequences. In Sudnow, D. (Ed.),\nStudies in social interaction, pp. 294–333. Free Press, New\nYork.\nJefferson, G. (1984). Notes on a systematic deployment of\nthe acknowledgement tokens ‘yeah’ and ‘mm hm’. Papers\nin Linguistics, 17(2), 197–216.\nJeffreys, H. (1948). Theory of Probability (2nd Ed.). Claren-\ndon Press. Section 3.23.",
  "554": "Studies in social interaction, pp. 294–333. Free Press, New\nYork.\nJefferson, G. (1984). Notes on a systematic deployment of\nthe acknowledgement tokens ‘yeah’ and ‘mm hm’. Papers\nin Linguistics, 17(2), 197–216.\nJeffreys, H. (1948). Theory of Probability (2nd Ed.). Claren-\ndon Press. Section 3.23.\nJekat, S., Klein, A., Maier, E., Maleck, I., Mast, M., and\nQuantz, J. (1995). Dialogue acts in verbmobil. Verbmobil–\nReport–65–95.\nJelinek, F. (1976). Continuous speech recognition by statis-\ntical methods. Proceedings of the IEEE, 64(4), 532–557.\nJelinek, F. (1990). Self-organized language modeling for\nspeech recognition. In Waibel, A. and Lee, K.-F. (Eds.),\nReadings in Speech Recognition, pp. 450–506. Morgan\nKaufmann. Originally distributed as IBM technical report\nin 1985.\nJelinek, F. (1997). Statistical Methods for Speech Recogni-\ntion. MIT Press.\nJelinek, F. and Lafferty, J. D. (1991).\nComputation of\nthe probability of initial substring generation by stochastic\ncontext-free grammars. Computational Linguistics, 17(3),\n315–323.\nJelinek, F., Lafferty, J. D., Magerman, D. M., Mercer, R. L.,\nRatnaparkhi, A., and Roukos, S. (1994). Decision tree pars-\ning using a hidden derivation model. In ARPA Human Lan-\nguage Technologies Workshop, Plainsboro, N.J., pp. 272–\n277.\nJelinek, F. and Mercer, R. L. (1980). Interpolated estimation\nof Markov source parameters from sparse data. In Gelsema,\nE. S. and Kanal, L. N. (Eds.), Proceedings, Workshop on\nPattern Recognition in Practice, pp. 381–397. North Hol-\nland.\nJi, H., Grishman, R., and Dang, H. T. (2010). Overview of\nthe tac 2011 knowledge base population track. In TAC-11.\nJiang, J. J. and Conrath, D. W. (1997). Semantic similarity\nbased on corpus statistics and lexical taxonomy. In RO-\nCLING X, Taiwan.\nJim´enez, V. M. and Marzal, A. (2000). Computation of the\nn best parse trees for weighted and stochastic context-free\ngrammars. In Advances in Pattern Recognition: Proceed-\nings of the Joint IAPR International Workshops, SSPR 2000\nand SPR 2000, Alicante, Spain, pp. 183–192. Springer.\nJohnson, M. (1998). PCFG models of linguistic tree repre-\nsentations. Computational Linguistics, 24(4), 613–632.\nJohnson, M. (2001). Joint and conditional estimation of tag-\nging and parsing models. In ACL-01, pp. 314–321.\nJohnson, M., Geman, S., Canon, S., Chi, Z., and Riezler, S.\n(1999). Estimators for stochastic “uniﬁcation-based” gram-\nmars. In ACL-99, pp. 535–541.\nJohnson, W. E. (1932). Probability: deductive and inductive\nproblems (appendix to). Mind, 41(164), 421–423.\nJohnson-Laird, P. N. (1983). Mental Models. Harvard Uni-\nversity Press, Cambridge, MA.\nJones, M. P. and Martin, J. H. (1997). Contextual spelling\ncorrection using latent semantic analysis. In ANLP 1997,\nWashington, D.C., pp. 166–173.\nJones, R., McCallum, A., Nigam, K., and Riloff, E. (1999).\nBootstrapping for text learning tasks. In IJCAI-99 Work-\nshop on Text Mining: Foundations, Techniques and Appli-\ncations.\nJones, T. (2015). Toward a description of African American\nVernacular English dialect regions using “Black Twitter”.\nAmerican Speech, 90(4), 403–440.",
  "555": "Bootstrapping for text learning tasks. In IJCAI-99 Work-\nshop on Text Mining: Foundations, Techniques and Appli-\ncations.\nJones, T. (2015). Toward a description of African American\nVernacular English dialect regions using “Black Twitter”.\nAmerican Speech, 90(4), 403–440.\nJoos, M. (1950). Description of language design. JASA, 22,\n701–708.\nJoshi, A. K. (1985). Tree adjoining grammars: How much\ncontext-sensitivity is required to provide reasonable struc-\ntural descriptions?. In Dowty, D. R., Karttunen, L., and\nZwicky, A. (Eds.), Natural Language Parsing, pp. 206–\n250. Cambridge University Press.\nJoshi, A. K. and Hopely, P. (1999). A parser from antiq-\nuity. In Kornai, A. (Ed.), Extended Finite State Models of\nLanguage, pp. 6–15. Cambridge University Press.",
  "556": "528\nBibliography\nJoshi, A. K. and Srinivas, B. (1994).\nDisambiguation of\nsuper parts of speech (or supertags): Almost parsing. In\nCOLING-94, Kyoto, pp. 154–160.\nJoshi, M., Choi, E., Weld, D. S., and Zettlemoyer, L.\n(2017). Triviaqa: A large scale distantly supervised chal-\nlenge dataset for reading comprehension. In ACL 2017.\nJurafsky, D. (2014). The Language of Food. W. W. Norton,\nNew York.\nJurafsky, D., Chahuneau, V., Routledge, B. R., and Smith,\nN. A. (2014). Narrative framing of consumer sentiment in\nonline restaurant reviews. First Monday, 19(4).\nJurafsky, D., Wooters, C., Tajchman, G., Segal, J., Stol-\ncke, A., Fosler, E., and Morgan, N. (1994). The Berke-\nley restaurant project. In ICSLP-94, Yokohama, Japan, pp.\n2139–2142.\nJurgens, D. and Klapaftis, I. P. (2013).\nSemeval-2013\ntask 13: Word sense induction for graded and non-graded\nsenses. In *SEM, pp. 290–299.\nJurgens, D., Tsvetkov, Y., and Jurafsky, D. (2017). Incorpo-\nrating dialectal variability for socially equitable language\nidentiﬁcation. In ACL 2017, pp. 51–57.\nJusteson, J. S. and Katz, S. M. (1991). Co-occurrences of\nantonymous adjectives and their contexts. Computational\nlinguistics, 17(1), 1–19.\nKalyanpur, A., Boguraev, B. K., Patwardhan, S., Murdock,\nJ. W., Lally, A., Welty, C., Prager, J. M., Coppola, B.,\nFokoue-Nkoutche, A., Zhang, L., Pan, Y., and Qiu, Z. M.\n(2012). Structured data and inference in deepqa. IBM Jour-\nnal of Research and Development, 56(3/4), 10:1–10:14.\nKang, J. S., Feng, S., Akoglu, L., and Choi, Y. (2014).\nConnotationwordnet: Learning connotation over the word+\nsense network. In ACL 2014.\nKannan, A. and Vinyals, O. (2016). Adversarial evaluation\nof dialogue models. In NIPS 2016 Workshop on Adversar-\nial Training.\nKaplan, R. M. (1973). A general syntactic processor. In\nRustin, R. (Ed.), Natural Language Processing, pp. 193–\n241. Algorithmics Press.\nKaplan, R. M., Riezler, S., King, T. H., Maxwell III, J. T.,\nVasserman, A., and Crouch, R. (2004). Speed and accuracy\nin shallow and deep stochastic parsing. In HLT-NAACL-04.\nKarlsson, F., Voutilainen, A., Heikkil¨a, J., and Anttila,\nA. (Eds.). (1995).\nConstraint Grammar: A Language-\nIndependent System for Parsing Unrestricted Text. Mouton\nde Gruyter.\nKarttunen, L. (1999). Comments on Joshi. In Kornai, A.\n(Ed.), Extended Finite State Models of Language, pp. 16–\n18. Cambridge University Press.\nKasami, T. (1965).\nAn efﬁcient recognition and syntax\nanalysis algorithm for context-free languages. Tech. rep.\nAFCRL-65-758, Air Force Cambridge Research Labora-\ntory, Bedford, MA.\nKashyap, R. L. and Oommen, B. J. (1983). Spelling cor-\nrection using probabilistic methods. Pattern Recognition\nLetters, 2, 147–154.\nKatz, J. J. and Fodor, J. A. (1963). The structure of a seman-\ntic theory. Language, 39, 170–210.",
  "557": "tory, Bedford, MA.\nKashyap, R. L. and Oommen, B. J. (1983). Spelling cor-\nrection using probabilistic methods. Pattern Recognition\nLetters, 2, 147–154.\nKatz, J. J. and Fodor, J. A. (1963). The structure of a seman-\ntic theory. Language, 39, 170–210.\nKawamoto, A. H. (1988).\nDistributed representations of\nambiguous words and their resolution in connectionist net-\nworks. In Small, S. L., Cottrell, G. W., and Tanenhaus, M.\n(Eds.), Lexical Ambiguity Resolution, pp. 195–228. Mor-\ngan Kaufman.\nKay, M. (1967). Experiments with a powerful parser. In\nProc. 2eme Conference Internationale sur le Traitement\nAutomatique des Langues, Grenoble.\nKay, M. (1973). The MIND system. In Rustin, R. (Ed.),\nNatural Language Processing, pp. 155–188. Algorithmics\nPress.\nKay, M. (1982). Algorithm schemata and data structures in\nsyntactic processing. In All´en, S. (Ed.), Text Processing:\nText Analysis and Generation, Text Typology and Attribu-\ntion, pp. 327–358. Almqvist and Wiksell, Stockholm.\nKay, P. and Fillmore, C. J. (1999). Grammatical construc-\ntions and linguistic generalizations: The What’s X Doing\nY? construction. Language, 75(1), 1–33.\nKeller, F. and Lapata, M. (2003). Using the web to obtain\nfrequencies for unseen bigrams. Computational Linguis-\ntics, 29, 459–484.\nKelly, E. F. and Stone, P. J. (1975). Computer Recognition\nof English Word Senses. North-Holland.\nKernighan, M. D., Church, K. W., and Gale, W. A. (1990).\nA spelling correction program base on a noisy channel\nmodel. In COLING-90, Helsinki, Vol. II, pp. 205–211.\nKiela, D. and Clark, S. (2014). A systematic study of seman-\ntic vector space model parameters. In Proceedings of the\nEACL 2nd Workshop on Continuous Vector Space Models\nand their Compositionality (CVSC), pp. 21–30.\nKilgarriff, A. (2001). English lexical sample task descrip-\ntion. In Proceedings of Senseval-2: Second International\nWorkshop on Evaluating Word Sense Disambiguation Sys-\ntems, Toulouse, France, pp. 17–20.\nKilgarriff, A. and Palmer, M. (Eds.). (2000). Computing\nand the Humanities: Special Issue on SENSEVAL, Vol. 34.\nKluwer.\nKilgarriff, A. and Rosenzweig, J. (2000). Framework and\nresults for English SENSEVAL. Computers and the Hu-\nmanities, 34, 15–48.\nKim, S. M. and Hovy, E. H. (2004). Determining the senti-\nment of opinions. In COLING-04.\nKingma, D. and Ba, J. (2015). Adam: A method for stochas-\ntic optimization. In ICLR 2015.\nKintsch, W. (1974).\nThe Representation of Meaning in\nMemory. Wiley, New York.\nKipper, K., Dang, H. T., and Palmer, M. (2000). Class-based\nconstruction of a verb lexicon. In AAAI-00, Austin, TX, pp.\n691–696.\nKleene, S. C. (1951). Representation of events in nerve nets\nand ﬁnite automata. Tech. rep. RM-704, RAND Corpora-\ntion. RAND Research Memorandum.\nKleene, S. C. (1956).\nRepresentation of events in nerve\nnets and ﬁnite automata. In Shannon, C. and McCarthy,\nJ. (Eds.), Automata Studies, pp. 3–41. Princeton University\nPress.\nKlein, D. (2005). The Unsupervised Learning of Natural\nLanguage Structure. Ph.D. thesis, Stanford University.",
  "558": "tion. RAND Research Memorandum.\nKleene, S. C. (1956).\nRepresentation of events in nerve\nnets and ﬁnite automata. In Shannon, C. and McCarthy,\nJ. (Eds.), Automata Studies, pp. 3–41. Princeton University\nPress.\nKlein, D. (2005). The Unsupervised Learning of Natural\nLanguage Structure. Ph.D. thesis, Stanford University.\nKlein, D. and Manning, C. D. (2001). Parsing and hyper-\ngraphs. In IWPT-01, pp. 123–134.\nKlein, D. and Manning, C. D. (2002).\nA generative\nconstituent-context model for improved grammar induc-\ntion. In ACL-02.\nKlein, D. and Manning, C. D. (2003a). A* parsing: Fast\nexact Viterbi parse selection. In HLT-NAACL-03.\nKlein, D. and Manning, C. D. (2003b). Accurate unlexical-\nized parsing. In HLT-NAACL-03.\nKlein, D. and Manning, C. D. (2004). Corpus-based induc-\ntion of syntactic structure: Models of dependency and con-\nstituency. In ACL-04, pp. 479–486.\nKlein, S. and Simmons, R. F. (1963). A computational ap-\nproach to grammatical coding of English words. Journal of\nthe Association for Computing Machinery, 10(3), 334–347.",
  "559": "Bibliography\n529\nKneser, R. and Ney, H. (1995). Improved backing-off for\nM-gram language modeling.\nIn ICASSP-95, Vol. 1, pp.\n181–184.\nKnuth, D. E. (1973). Sorting and Searching: The Art of\nComputer Programming Volume 3. Addison-Wesley.\nKoˇcisk`y, T., Schwarz, J., Blunsom, P., Dyer, C., Hermann,\nK. M., Melis, G., and Grefenstette, E. (2018). The Narra-\ntiveQA reading comprehension challenge. TACL, 6, 317–\n328.\nKrovetz, R. (1993). Viewing morphology as an inference\nprocess. In SIGIR-93, pp. 191–202.\nKrovetz, R. (1998). More than one sense per discourse. In\nProceedings of the ACL-SIGLEX SENSEVAL Workshop.\nKruskal, J. B. (1983). An overview of sequence compari-\nson. In Sankoff, D. and Kruskal, J. B. (Eds.), Time Warps,\nString Edits, and Macromolecules: The Theory and Prac-\ntice of Sequence Comparison, pp. 1–44. Addison-Wesley.\nKudo, T. and Matsumoto, Y. (2002). Japanese dependency\nanalysis using cascaded chunking. In CoNLL-02, pp. 63–\n69.\nKukich, K. (1992). Techniques for automatically correcting\nwords in text. ACM Computing Surveys, 24(4), 377–439.\nKullback, S. and Leibler, R. A. (1951). On information and\nsufﬁciency. Annals of Mathematical Statistics, 22, 79–86.\nKuno, S. (1965). The predictive analyzer and a path elimi-\nnation technique. Communications of the ACM, 8(7), 453–\n462.\nKuno, S. and Oettinger, A. G. (1963). Multiple-path syn-\ntactic analyzer. In Popplewell, C. M. (Ed.), Information\nProcessing 1962: Proceedings of the IFIP Congress 1962,\nMunich, pp. 306–312. North-Holland. Reprinted in Grosz\net al. (1986).\nKupiec, J. (1992). Robust part-of-speech tagging using a\nhidden Markov model. Computer Speech and Language,\n6, 225–242.\nKuˇcera, H. and Francis, W. N. (1967). Computational Anal-\nysis of Present-Day American English. Brown University\nPress, Providence, RI.\nLabov, W. and Fanshel, D. (1977). Therapeutic Discourse.\nAcademic Press.\nLafferty, J. D., McCallum, A., and Pereira, F. C. N. (2001).\nConditional random ﬁelds: Probabilistic models for seg-\nmenting and labeling sequence data. In ICML 2001, Stan-\nford, CA.\nLafferty, J. D., Sleator, D., and Temperley, D. (1992).\nGrammatical trigrams: A probabilistic model of link gram-\nmar. In Proceedings of the 1992 AAAI Fall Symposium on\nProbabilistic Approaches to Natural Language.\nLakoff, G. (1965). On the Nature of Syntactic Irregularity.\nPh.D. thesis, Indiana University. Published as Irregularity\nin Syntax. Holt, Rinehart, and Winston, New York, 1970.\nLakoff, G. (1972). Linguistics and natural logic. In David-\nson, D. and Harman, G. (Eds.), Semantics for Natural Lan-\nguage, pp. 545–665. D. Reidel.\nLakoff, G. and Johnson, M. (1980). Metaphors We Live By.\nUniversity of Chicago Press, Chicago, IL.\nLally, A., Prager, J. M., McCord, M. C., Boguraev, B. K.,\nPatwardhan, S., Fan, J., Fodor, P., and Chu-Carroll, J.\n(2012). Question analysis: How Watson reads a clue. IBM",
  "560": "Lakoff, G. and Johnson, M. (1980). Metaphors We Live By.\nUniversity of Chicago Press, Chicago, IL.\nLally, A., Prager, J. M., McCord, M. C., Boguraev, B. K.,\nPatwardhan, S., Fan, J., Fodor, P., and Chu-Carroll, J.\n(2012). Question analysis: How Watson reads a clue. IBM\nJournal of Research and Development, 56(3/4), 2:1–2:14.\nLample, G., Ballesteros, M., Subramanian, S., Kawakami,\nK., and Dyer, C. (2016). Neural architectures for named\nentity recognition. In NAACL HLT 2016.\nLandauer, T. K. (Ed.). (1995). The Trouble with Computers:\nUsefulness, Usability, and Productivity. MIT Press.\nLandauer, T. K. and Dumais, S. T. (1997). A solution to\nPlato’s problem: The Latent Semantic Analysis theory of\nacquisition, induction, and representation of knowledge.\nPsychological Review, 104, 211–240.\nLandes, S., Leacock, C., and Tengi, R. I. (1998). Building\nsemantic concordances. In Fellbaum, C. (Ed.), WordNet:\nAn Electronic Lexical Database, pp. 199–216. MIT Press.\nLang, J. and Lapata, M. (2014). Similarity-driven semantic\nrole induction via graph partitioning. Computational Lin-\nguistics, 40(3), 633–669.\nLapata, M. and Keller, F. (2004).\nThe web as a base-\nline: Evaluating the performance of unsupervised web-\nbased models for a range of NLP tasks. In HLT-NAACL-04.\nLapesa, G. and Evert, S. (2014). A large scale evaluation\nof distributional semantic models: Parameters, interactions\nand model selection. TACL, 2, 531–545.\nLari, K. and Young, S. J. (1990). The estimation of stochas-\ntic context-free grammars using the Inside-Outside algo-\nrithm. Computer Speech and Language, 4, 35–56.\nLau, J. H., Cook, P., McCarthy, D., Newman, D., and Bald-\nwin, T. (2012). Word sense induction for novel sense de-\ntection. In EACL-12, pp. 591–601.\nLeacock, C. and Chodorow, M. S. (1998). Combining lo-\ncal context and WordNet similarity for word sense identi-\nﬁcation. In Fellbaum, C. (Ed.), WordNet: An Electronic\nLexical Database, pp. 265–283. MIT Press.\nLeacock, C., Towell, G., and Voorhees, E. M. (1993).\nCorpus-based statistical sense resolution. In HLT-93, pp.\n260–265.\nLeCun, Y., Boser, B., Denker, J. S., Henderson, D., Howard,\nR. E., Hubbard, W., and Jackel, L. D. (1989). Backpropa-\ngation applied to handwritten zip code recognition. Neural\ncomputation, 1(4), 541–551.\nLeCun, Y., Boser, B. E., Denker, J. S., Henderson, D.,\nHoward, R. E., Hubbard, W. E., and Jackel, L. D. (1990).\nHandwritten digit recognition with a back-propagation net-\nwork. In NIPS 1990, pp. 396–404.\nLee, D. D. and Seung, H. S. (1999).\nLearning the parts\nof objects by non-negative matrix factorization. Nature,\n401(6755), 788–791.\nLee, K., Salant, S., Kwiatkowski, T., Parikh, A., Das, D., and\nBerant, J. (2017). Learning recurrent span representations\nfor extractive question answering. In arXiv 1611.01436.\nLehnert, W. G., Cardie, C., Fisher, D., Riloff, E., and\nWilliams, R. (1991). Description of the CIRCUS system",
  "561": "Lee, K., Salant, S., Kwiatkowski, T., Parikh, A., Das, D., and\nBerant, J. (2017). Learning recurrent span representations\nfor extractive question answering. In arXiv 1611.01436.\nLehnert, W. G., Cardie, C., Fisher, D., Riloff, E., and\nWilliams, R. (1991). Description of the CIRCUS system\nas used for MUC-3. In Sundheim, B. (Ed.), MUC-3, pp.\n223–233.\nLemon, O., Georgila, K., Henderson, J., and Stuttle, M.\n(2006). An ISU dialogue system exhibiting reinforcement\nlearning of dialogue policies: Generic slot-ﬁlling in the\nTALK in-car system. In EACL-06.\nLesk, M. E. (1986). Automatic sense disambiguation us-\ning machine readable dictionaries: How to tell a pine cone\nfrom an ice cream cone. In Proceedings of the 5th Inter-\nnational Conference on Systems Documentation, Toronto,\nCA, pp. 24–26.\nLeuski, A. and Traum, D. (2011).\nNPCEditor: Creating\nvirtual human dialogue using information retrieval tech-\nniques. AI Magazine, 32(2), 42–56.\nLevenshtein, V. I. (1966).\nBinary codes capable of cor-\nrecting deletions, insertions, and reversals.\nCybernetics\nand Control Theory, 10(8), 707–710. Original in Doklady\nAkademii Nauk SSSR 163(4): 845–848 (1965).",
  "562": "530\nBibliography\nLevesque, H. J., Cohen, P. R., and Nunes, J. H. T. (1990).\nOn acting together. In AAAI-90, Boston, MA, pp. 94–99.\nMorgan Kaufmann.\nLevin, B. (1977). Mapping sentences to case frames. Tech.\nrep. 167, MIT AI Laboratory. AI Working Paper 143.\nLevin, B. (1993). English Verb Classes and Alternations: A\nPreliminary Investigation. University of Chicago Press.\nLevin, B. and Rappaport Hovav, M. (2005). Argument Real-\nization. Cambridge University Press.\nLevin, E., Pieraccini, R., and Eckert, W. (2000). A stochas-\ntic model of human-machine interaction for learning dialog\nstrategies. IEEE Transactions on Speech and Audio Pro-\ncessing, 8, 11–23.\nLevinson, S. C. (1983). Conversational Analysis, chap. 6.\nCambridge University Press.\nLevow, G.-A. (1998). Characterizing and recognizing spo-\nken corrections in human-computer dialogue. In COLING-\nACL, pp. 736–742.\nLevy, O. and Goldberg, Y. (2014a). Dependency-based word\nembeddings. In ACL 2014.\nLevy, O. and Goldberg, Y. (2014b). Linguistic regularities\nin sparse and explicit word representations. In CoNLL-14.\nLevy, O. and Goldberg, Y. (2014c). Neural word embedding\nas implicit matrix factorization.\nIn NIPS 14, pp. 2177–\n2185.\nLevy, O., Goldberg, Y., and Dagan, I. (2015). Improving dis-\ntributional similarity with lessons learned from word em-\nbeddings. TACL, 3, 211–225.\nLevy, R. (2008). Expectation-based syntactic comprehen-\nsion. Cognition, 106(3), 1126–1177.\nLewis, M. and Steedman, M. (2014). A* ccg parsing with a\nsupertag-factored model.. In EMNLP, pp. 990–1000.\nLi, J., Chen, X., Hovy, E. H., and Jurafsky, D. (2015). Vi-\nsualizing and understanding neural models in NLP.\nIn\nNAACL HLT 2015.\nLi, J., Galley, M., Brockett, C., Gao, J., and Dolan, B.\n(2016a). A diversity-promoting objective function for neu-\nral conversation models. In NAACL HLT 2016.\nLi, J., Monroe, W., Ritter, A., Galley, M., Gao, J., and Juraf-\nsky, D. (2016b). Deep reinforcement learning for dialogue\ngeneration. In EMNLP 2016.\nLi, J., Monroe, W., Shi, T., Ritter, A., and Jurafsky, D.\n(2017). Adversarial learning for neural dialogue genera-\ntion. In EMNLP 2017.\nLi, X. and Roth, D. (2002). Learning question classiﬁers. In\nCOLING-02, pp. 556–562.\nLi, X. and Roth, D. (2005). Learning question classiﬁers:\nThe role of semantic information. Journal of Natural Lan-\nguage Engineering, 11(4).\nLin, D. (1995). A dependency-based method for evaluating\nbroad-coverage parsers. In IJCAI-95, Montreal, pp. 1420–\n1425.\nLin, D. (1998). An information-theoretic deﬁnition of simi-\nlarity. In ICML 1998, San Francisco, pp. 296–304.\nLin, D. (2003). Dependency-based evaluation of minipar. In\nWorkshop on the Evaluation of Parsing Systems.\nLin, J. (2007). An exploration of the principles underlying\nredundancy-based factoid question answering. ACM Trans-\nactions on Information Systems, 25(2).\nLin, Y., Michel, J.-B., Lieberman Aiden, E., Orwant, J.,\nBrockman, W., and Petrov, S. (2012). Syntactic annota-",
  "563": "Workshop on the Evaluation of Parsing Systems.\nLin, J. (2007). An exploration of the principles underlying\nredundancy-based factoid question answering. ACM Trans-\nactions on Information Systems, 25(2).\nLin, Y., Michel, J.-B., Lieberman Aiden, E., Orwant, J.,\nBrockman, W., and Petrov, S. (2012). Syntactic annota-\ntions for the google books ngram corpus. In ACL 2012, pp.\n169–174.\nLindsey, R. (1963). Inferential memory as the basis of ma-\nchines which understand natural language. In Feigenbaum,\nE. and Feldman, J. (Eds.), Computers and Thought, pp.\n217–233. McGraw Hill.\nLitman, D. J., Swerts, M., and Hirschberg, J. (2000). Pre-\ndicting automatic speech recognition performance using\nprosodic cues. In NAACL 2000.\nLitman, D. J., Walker, M. A., and Kearns, M. (1999). Auto-\nmatic detection of poor speech recognition at the dialogue\nlevel. In ACL-99, College Park, MA, pp. 309–316.\nLiu, B. and Zhang, L. (2012). A survey of opinion mining\nand sentiment analysis. In Aggarwal, C. C. and Zhai, C.\n(Eds.), Mining text data, pp. 415–464. Springer.\nLiu, C.-W., Lowe, R. T., Serban, I. V., Noseworthy, M.,\nCharlin, L., and Pineau, J. (2016). How NOT to evalu-\nate your dialogue system: An empirical study of unsuper-\nvised evaluation metrics for dialogue response generation.\nIn EMNLP 2016.\nLiu, X., Gales, M. J. F., and Woodland, P. C. (2013). Use of\ncontexts in language model interpolation and adaptation.\nComputer Speech & Language, 27(1), 301–321.\nLochbaum, K. E., Grosz, B. J., and Sidner, C. L. (2000).\nDiscourse structure and intention recognition. In Dale, R.,\nMoisl, H., and Somers, H. L. (Eds.), Handbook of Natural\nLanguage Processing. Marcel Dekker.\nLovins, J. B. (1968).\nDevelopment of a stemming algo-\nrithm.\nMechanical Translation and Computational Lin-\nguistics, 11(1–2), 9–13.\nLowe, R. T., Noseworthy, M., Serban, I. V., Angelard-\nGontier, N., Bengio, Y., and Pineau, J. (2017a). Towards\nan automatic Turing test: Learning to evaluate dialogue re-\nsponses. In ACL 2017.\nLowe, R. T., Pow, N., Serban, I. V., Charlin, L., Liu, C.-\nW., and Pineau, J. (2017b). Training end-to-end dialogue\nsystems with the ubuntu dialogue corpus. Dialogue & Dis-\ncourse, 8(1), 31–65.\nLuhn, H. P. (1957). A statistical approach to the mecha-\nnized encoding and searching of literary information. IBM\nJournal of Research and Development, 1(4), 309–317.\nLui, M. and Baldwin, T. (2011). Cross-domain feature se-\nlection for language identiﬁcation. In IJCNLP-11, pp. 553–\n561.\nLui, M. and Baldwin, T. (2012). langid.py: An off-the-\nshelf language identiﬁcation tool. In ACL 2012, pp. 25–30.\nLyons, J. (1977). Semantics. Cambridge University Press.\nMa, X. and Hovy, E. H. (2016). End-to-end sequence label-\ning via bi-directional LSTM-CNNs-CRF. In ACL 2016.\nMadhu, S. and Lytel, D. (1965). A ﬁgure of merit technique\nfor the resolution of non-grammatical ambiguity. Mechan-\nical Translation, 8(2), 9–13.\nMagerman, D. M. (1994). Natural Language Parsing as\nStatistical Pattern Recognition. Ph.D. thesis, University of\nPennsylvania.",
  "564": "Madhu, S. and Lytel, D. (1965). A ﬁgure of merit technique\nfor the resolution of non-grammatical ambiguity. Mechan-\nical Translation, 8(2), 9–13.\nMagerman, D. M. (1994). Natural Language Parsing as\nStatistical Pattern Recognition. Ph.D. thesis, University of\nPennsylvania.\nMagerman, D. M. (1995). Statistical decision-tree models\nfor parsing. In ACL-95, pp. 276–283.\nMagerman, D. M. and Marcus, M. P. (1991). Pearl: A prob-\nabilistic chart parser. In EACL-91, Berlin.\nMairesse, F. and Walker, M. A. (2008). Trainable generation\nof big-ﬁve personality styles through data-driven parameter\nestimation. In ACL-08, Columbus.\nManandhar, S., Klapaftis, I. P., Dligach, D., and Pradhan,\nS. (2010). Semeval-2010 task 14: Word sense induction &\ndisambiguation. In SemEval-2010, pp. 63–68.\nManning, C. D. (2011). Part-of-speech tagging from 97%\nto 100%: Is it time for some linguistics?. In CICLing 2011,\npp. 171–189.",
  "565": "Bibliography\n531\nManning, C. D., Raghavan, P., and Sch¨utze, H. (2008). In-\ntroduction to Information Retrieval. Cambridge.\nManning, C. D. and Sch¨utze, H. (1999). Foundations of\nStatistical Natural Language Processing. MIT Press.\nMarcus, M. P. (1980). A Theory of Syntactic Recognition\nfor Natural Language. MIT Press.\nMarcus, M. P. (1990). Summary of session 9: Automatic\nacquisition of linguistic structure. In Proceedings DARPA\nSpeech and Natural Language Workshop, Hidden Valley,\nPA, pp. 249–250.\nMarcus, M. P., Kim, G., Marcinkiewicz, M. A., MacIntyre,\nR., Bies, A., Ferguson, M., Katz, K., and Schasberger, B.\n(1994). The Penn Treebank: Annotating predicate argu-\nment structure.\nIn ARPA Human Language Technology\nWorkshop, Plainsboro, NJ, pp. 114–119. Morgan Kauf-\nmann.\nMarcus, M. P., Santorini, B., and Marcinkiewicz, M. A.\n(1993). Building a large annotated corpus of English: The\nPenn treebank.\nComputational Linguistics, 19(2), 313–\n330.\nMarkov, A. A. (1913). Essai d’une recherche statistique sur\nle texte du roman “Eugene Onegin” illustrant la liaison des\nepreuve en chain (‘Example of a statistical investigation of\nthe text of “Eugene Onegin” illustrating the dependence be-\ntween samples in chain’). Izvistia Imperatorskoi Akademii\nNauk (Bulletin de l’Acad´emie Imp´eriale des Sciences de\nSt.-P´etersbourg), 7, 153–162.\nMarkov, A. A. (2006). Classical text in translation: A. A.\nMarkov, an example of statistical investigation of the text\nEugene Onegin concerning the connection of samples in\nchains. Science in Context, 19(4), 591–600. Translated by\nDavid Link.\nMaron, M. E. (1961). Automatic indexing: an experimental\ninquiry. Journal of the ACM (JACM), 8(3), 404–417.\nM`arquez, L., Carreras, X., Litkowski, K. C., and Stevenson,\nS. (2008). Semantic role labeling: An introduction to the\nspecial issue. Computational linguistics, 34(2), 145–159.\nMarshall, I. (1983).\nChoice of grammatical word-class\nwithout GLobal syntactic analysis: Tagging words in the\nLOB corpus. Computers and the Humanities, 17, 139–150.\nMarshall, I. (1987). Tag selection using probabilistic meth-\nods. In Garside, R., Leech, G., and Sampson, G. (Eds.), The\nComputational Analysis of English, pp. 42–56. Longman.\nMartin, J. H. (1986). The acquisition of polysemy. In ICML\n1986, Irvine, CA, pp. 198–204.\nMasterman, M. (1957). The thesaurus in syntax and seman-\ntics. Mechanical Translation, 4(1), 1–2.\nMays, E., Damerau, F. J., and Mercer, R. L. (1991). Con-\ntext based spelling correction. Information Processing and\nManagement, 27(5), 517–522.\nMcCallum, A., Freitag, D., and Pereira, F. C. N. (2000).\nMaximum entropy Markov models for information extrac-\ntion and segmentation. In ICML 2000, pp. 591–598.\nMcCallum, A. and Nigam, K. (1998).\nA comparison\nof event models for naive bayes text classiﬁcation.\nIn\nAAAI/ICML-98 Workshop on Learning for Text Categoriza-\ntion, pp. 41–48.\nMcCawley, J. D. (1968). The role of semantics in a gram-\nmar. In Bach, E. W. and Harms, R. T. (Eds.), Universals in\nLinguistic Theory, pp. 124–169. Holt, Rinehart & Winston.",
  "566": "In\nAAAI/ICML-98 Workshop on Learning for Text Categoriza-\ntion, pp. 41–48.\nMcCawley, J. D. (1968). The role of semantics in a gram-\nmar. In Bach, E. W. and Harms, R. T. (Eds.), Universals in\nLinguistic Theory, pp. 124–169. Holt, Rinehart & Winston.\nMcCawley, J. D. (1993). Everything that Linguists Have Al-\nways Wanted to Know about Logic (2nd Ed.). University of\nChicago Press, Chicago, IL.\nMcCawley, J. D. (1998). The Syntactic Phenomena of En-\nglish. University of Chicago Press.\nMcClelland, J. L. and Elman, J. L. (1986). The TRACE\nmodel of speech perception. Cognitive Psychology, 18, 1–\n86.\nMcCulloch, W. S. and Pitts, W. (1943). A logical calculus of\nideas immanent in nervous activity. Bulletin of Mathemat-\nical Biophysics, 5, 115–133. Reprinted in Neurocomput-\ning: Foundations of Research, ed. by J. A. Anderson and E\nRosenfeld. MIT Press 1988.\nMcDonald, R., Crammer, K., and Pereira, F. C. N. (2005).\nOnline large-margin training of dependency parsers.\nIn\nACL-05, Ann Arbor, pp. 91–98.\nMcDonald, R. and Nivre, J. (2011). Analyzing and integrat-\ning dependency parsers. Computational Linguistics, 37(1),\n197–230.\nMcDonald, R., Pereira, F. C. N., Ribarov, K., and Hajiˇc, J.\n(2005). Non-projective dependency parsing using spanning\ntree algorithms. In HLT-EMNLP-05.\nMcGuiness, D. L. and van Harmelen, F. (2004). OWL web\nontology overview. Tech. rep. 20040210, World Wide Web\nConsortium.\nMehl, M. R., Gosling, S. D., and Pennebaker, J. W. (2006).\nPersonality in its natural habitat: manifestations and im-\nplicit folk theories of personality in daily life.. Journal of\nPersonality and Social Psychology, 90(5).\nMel’˘cuk, I. A. (1988).\nDependency Syntax: Theory and\nPractice. State University of New York Press.\nMerialdo, B. (1994). Tagging English text with a probabilis-\ntic model. Computational Linguistics, 20(2), 155–172.\nMesnil, G., Dauphin, Y., Yao, K., Bengio, Y., Deng, L.,\nHakkani-T¨ur, D., He, X., Heck, L., T¨ur, G., Yu, D., and\nZweig, G. (2015).\nUsing recurrent neural networks for\nslot ﬁlling in spoken language understanding. IEEE/ACM\nTransactions on Audio, Speech and Language Processing\n(TASLP), 23(3), 530–539.\nMetsis, V., Androutsopoulos, I., and Paliouras, G. (2006).\nSpam ﬁltering with naive bayes-which naive bayes?. In\nCEAS, pp. 27–28.\nMeyers, A., Reeves, R., Macleod, C., Szekely, R., Zielinska,\nV., Young, B., and Grishman, R. (2004). The nombank\nproject: An interim report. In Proceedings of the NAA-\nCL/HLT Workshop: Frontiers in Corpus Annotation.\nMicrosoft (2014). http://www.msxiaoice.com..\nMihalcea, R. (2007). Using wikipedia for automatic word\nsense disambiguation. In NAACL-HLT 07, pp. 196–203.\nMihalcea, R. and Moldovan, D. (2001). Automatic genera-\ntion of a coarse grained WordNet. In NAACL Workshop on\nWordNet and Other Lexical Resources.\nMikheev, A., Moens, M., and Grover, C. (1999). Named en-\ntity recognition without gazetteers. In EACL-99, Bergen,\nNorway, pp. 1–8.",
  "567": "Mihalcea, R. and Moldovan, D. (2001). Automatic genera-\ntion of a coarse grained WordNet. In NAACL Workshop on\nWordNet and Other Lexical Resources.\nMikheev, A., Moens, M., and Grover, C. (1999). Named en-\ntity recognition without gazetteers. In EACL-99, Bergen,\nNorway, pp. 1–8.\nMikolov, T. (2012). Statistical language models based on\nneural networks. Ph.D. thesis, Ph. D. thesis, Brno Univer-\nsity of Technology.\nMikolov, T., Chen, K., Corrado, G. S., and Dean, J.\n(2013). Efﬁcient estimation of word representations in vec-\ntor space. In ICLR 2013.\nMikolov, T., Kombrink, S., Burget, L., ˇCernock`y, J. H., and\nKhudanpur, S. (2011). Extensions of recurrent neural net-\nwork language model. In ICASSP-11, pp. 5528–5531.\nMikolov, T., Sutskever, I., Chen, K., Corrado, G. S., and\nDean, J. (2013a). Distributed representations of words and\nphrases and their compositionality. In NIPS 13, pp. 3111–\n3119.\nMikolov, T., Yih, W.-t., and Zweig, G. (2013b). Linguistic\nregularities in continuous space word representations. In\nNAACL HLT 2013, pp. 746–751.",
  "568": "532\nBibliography\nMiller, G. A. and Charles, W. G. (1991). Contextual cor-\nrelates of semantics similarity. Language and Cognitive\nProcesses, 6(1), 1–28.\nMiller, G. A. and Chomsky, N. (1963). Finitary models of\nlanguage users. In Luce, R. D., Bush, R. R., and Galanter,\nE. (Eds.), Handbook of Mathematical Psychology, Vol. II,\npp. 419–491. John Wiley.\nMiller, G. A., Leacock, C., Tengi, R. I., and Bunker, R. T.\n(1993). A semantic concordance. In Proceedings ARPA\nWorkshop on Human Language Technology, pp. 303–308.\nMiller, G. A. and Selfridge, J. A. (1950). Verbal context\nand the recall of meaningful material. American Journal of\nPsychology, 63, 176–185.\nMiller, S., Bobrow, R. J., Ingria, R., and Schwartz, R.\n(1994). Hidden understanding models of natural language.\nIn ACL-94, Las Cruces, NM, pp. 25–32.\nMinsky, M. (1961). Steps toward artiﬁcial intelligence. Pro-\nceedings of the IRE, 49(1), 8–30.\nMinsky, M. (1974). A framework for representing knowl-\nedge. Tech. rep. 306, MIT AI Laboratory. Memo 306.\nMinsky, M. and Papert, S. (1969). Perceptrons. MIT Press.\nMintz, M., Bills, S., Snow, R., and Jurafsky, D. (2009).\nDistant supervision for relation extraction without labeled\ndata. In ACL IJCNLP 2009.\nMitton, R. (1987). Spelling checkers, spelling correctors and\nthe misspellings of poor spellers. Information processing\n& management, 23(5), 495–505.\nMiwa, M. and Bansal, M. (2016). End-to-end relation ex-\ntraction using lstms on sequences and tree structures. In\nACL 2016, pp. 1105–1116.\nMohammad, S. M. and Turney, P. D. (2013). Crowdsourc-\ning a word-emotion association lexicon. Computational In-\ntelligence, 29(3), 436–465.\nMonroe, B. L., Colaresi, M. P., and Quinn, K. M. (2008).\nFightin’words: Lexical feature selection and evaluation for\nidentifying the content of political conﬂict. Political Anal-\nysis, 16(4), 372–403.\nMontague, R. (1973). The proper treatment of quantiﬁcation\nin ordinary English. In Thomason, R. (Ed.), Formal Philos-\nophy: Selected Papers of Richard Montague, pp. 247–270.\nYale University Press, New Haven, CT.\nMonz, C. (2004). Minimal span weighting retrieval for ques-\ntion answering. In SIGIR Workshop on Information Re-\ntrieval for Question Answering, pp. 23–30.\nMorgan, A. A., Hirschman, L., Colosimo, M., Yeh, A. S.,\nand Colombe, J. B. (2004). Gene name identiﬁcation and\nnormalization using a model organism database. Journal\nof Biomedical Informatics, 37(6), 396–410.\nMorgan, N. and Bourlard, H. (1989). Generalization and pa-\nrameter estimation in feedforward nets: Some experiments.\nIn Advances in neural information processing systems, pp.\n630–637.\nMorgan, N. and Bourlard, H. (1990). Continuous speech\nrecognition using multilayer perceptrons with hidden\nmarkov models. In ICASSP-90, pp. 413–416.\nMorris, W. (Ed.). (1985).\nAmerican Heritage Dictionary\n(2nd College Edition Ed.). Houghton Mifﬂin.\nMosteller, F. and Wallace, D. L. (1963). Inference in an au-\nthorship problem: A comparative study of discrimination\nmethods applied to the authorship of the disputed federal-\nist papers. Journal of the American Statistical Association,",
  "569": "Morris, W. (Ed.). (1985).\nAmerican Heritage Dictionary\n(2nd College Edition Ed.). Houghton Mifﬂin.\nMosteller, F. and Wallace, D. L. (1963). Inference in an au-\nthorship problem: A comparative study of discrimination\nmethods applied to the authorship of the disputed federal-\nist papers. Journal of the American Statistical Association,\n58(302), 275–309.\nMosteller, F. and Wallace, D. L. (1964). Inference and Dis-\nputed Authorship: The Federalist.\nSpringer-Verlag.\nA\nsecond edition appeared in 1984 as Applied Bayesian and\nClassical Inference.\nMrkˇsi´c, N., O’S´eaghdha, D., Wen, T.-H., Thomson, B., and\nYoung, S. J. (2017). Neural belief tracker: Data-driven di-\nalogue state tracking. In ACL 2017.\nMurdock, J. W., Fan, J., Lally, A., Shima, H., and Bogu-\nraev, B. K. (2012a). Textual evidence gathering and anal-\nysis. IBM Journal of Research and Development, 56(3/4),\n8:1–8:14.\nMurdock, J. W., Kalyanpur, A., Welty, C., Fan, J., Fer-\nrucci, D. A., Gondek, D. C., Zhang, L., and Kanayama,\nH. (2012b). Typing candidate answers using type coercion.\nIBM Journal of Research and Development, 56(3/4), 7:1–\n7:13.\nMurphy, K. P. (2012). Machine learning: A probabilistic\nperspective. MIT press.\nN´adas, A. (1984).\nEstimation of probabilities in the lan-\nguage model of the IBM speech recognition system. IEEE\nTransactions on Acoustics, Speech, Signal Processing,\n32(4), 859–861.\nNagata, M. and Morimoto, T. (1994). First steps toward sta-\ntistical modeling of dialogue to predict the speech act type\nof the next utterance. Speech Communication, 15, 193–\n203.\nNash-Webber, B. L. (1975). The role of semantics in auto-\nmatic speech understanding. In Bobrow, D. G. and Collins,\nA. (Eds.), Representation and Understanding, pp. 351–\n382. Academic Press.\nNaur, P., Backus, J. W., Bauer, F. L., Green, J., Katz, C.,\nMcCarthy, J., Perlis, A. J., Rutishauser, H., Samelson, K.,\nVauquois, B., Wegstein, J. H., van Wijnagaarden, A., and\nWoodger, M. (1960). Report on the algorithmic language\nALGOL 60. Communications of the ACM, 3(5), 299–314.\nRevised in CACM 6:1, 1-17, 1963.\nNavigli, R. (2006). Meaningful clustering of senses helps\nboost word sense disambiguation performance. In COL-\nING/ACL 2006, pp. 105–112.\nNavigli, R. (2009). Word sense disambiguation: A survey.\nACM Computing Surveys, 41(2).\nNavigli, R. and Lapata, M. (2010). An experimental study\nof graph connectivity for unsupervised word sense disam-\nbiguation. IEEE Transactions on Pattern Analysis and Ma-\nchine Intelligence, 32(4), 678–692.\nNavigli, R. and Vannella, D. (2013). Semeval-2013 task 11:\nWord sense induction & disambiguation within an end-user\napplication. In *SEM, pp. 193–201.\nNeedleman, S. B. and Wunsch, C. D. (1970).\nA gen-\neral method applicable to the search for similarities in the\namino-acid sequence of two proteins. Journal of Molecular\nBiology, 48, 443–453.\nNeff, G. and Nagy, P. (2016). Talking to bots: Symbiotic\nagency and the case of Tay. International Journal of Com-\nmunication, 10, 4915–4931.",
  "570": "A gen-\neral method applicable to the search for similarities in the\namino-acid sequence of two proteins. Journal of Molecular\nBiology, 48, 443–453.\nNeff, G. and Nagy, P. (2016). Talking to bots: Symbiotic\nagency and the case of Tay. International Journal of Com-\nmunication, 10, 4915–4931.\nNewell, A., Langer, S., and Hickey, M. (1998). The rˆole of\nnatural language processing in alternative and augmenta-\ntive communication. Natural Language Engineering, 4(1),\n1–16.\nNey, H. (1991). Dynamic programming parsing for context-\nfree grammars in continuous speech recognition.\nIEEE\nTransactions on Signal Processing, 39(2), 336–340.\nNg, A. Y. and Jordan, M. I. (2002). On discriminative vs.\ngenerative classiﬁers: A comparison of logistic regression\nand naive bayes. In NIPS 14, pp. 841–848.\nNg, H. T., Teo, L. H., and Kwan, J. L. P. (2000). A machine\nlearning approach to answering questions for reading com-\nprehension tests. In EMNLP 2000, pp. 124–132.",
  "571": "Bibliography\n533\nNielsen, J. (1992). The usability engineering life cycle. IEEE\nComputer, 25(3), 12–22.\nNielsen, M. A. (2015). Neural networks and Deep learning.\nDetermination Press USA.\nNigam, K., Lafferty, J. D., and McCallum, A. (1999). Using\nmaximum entropy for text classiﬁcation. In IJCAI-99 work-\nshop on machine learning for information ﬁltering, pp. 61–\n67.\nNilsson, J., Riedel, S., and Yuret, D. (2007). The conll 2007\nshared task on dependency parsing. In Proceedings of the\nCoNLL shared task session of EMNLP-CoNLL, pp. 915–\n932. sn.\nNIST (2005). Speech recognition scoring toolkit (sctk) ver-\nsion 2.1. http://www.nist.gov/speech/tools/.\nNivre, J. (2007).\nIncremental non-projective dependency\nparsing. In NAACL-HLT 07.\nNivre, J. (2003). An efﬁcient algorithm for projective de-\npendency parsing. In Proceedings of the 8th International\nWorkshop on Parsing Technologies (IWPT.\nNivre, J. (2006). Inductive Dependency Parsing. Springer.\nNivre, J. (2009). Non-projective dependency parsing in ex-\npected linear time. In ACL IJCNLP 2009, pp. 351–359.\nNivre, J., de Marneffe, M.-C., Ginter, F., Goldberg, Y.,\nHajiˇc, J., Manning, C. D., McDonald, R. T., Petrov, S.,\nPyysalo, S., Silveira, N., Tsarfaty, R., and Zeman, D.\n(2016a). Universal Dependencies v1: A multilingual tree-\nbank collection. In LREC.\nNivre, J., de Marneffe, M.-C., Ginter, F., Goldberg, Y.,\nHajiˇc, J., Manning, C. D., McDonald, R. T., Petrov, S.,\nPyysalo, S., Silveira, N., Tsarfaty, R., and Zeman, D.\n(2016b). Universal Dependencies v1: A multilingual tree-\nbank collection. In LREC-16.\nNivre, J., Hall, J., Nilsson, J., Chanev, A., Eryigit, G.,\nK¨ubler, S., Marinov, S., and Marsi, E. (2007).\nMalt-\nparser: A language-independent system for data-driven de-\npendency parsing. Natural Language Engineering, 13(02),\n95–135.\nNivre, J. and Nilsson, J. (2005). Pseudo-projective depen-\ndency parsing. In ACL-05, pp. 99–106.\nNivre, J. and Scholz, M. (2004). Deterministic dependency\nparsing of english text. In COLING-04, p. 64.\nNiwa, Y. and Nitta, Y. (1994). Co-occurrence vectors from\ncorpora vs. distance vectors from dictionaries. In ACL-94,\npp. 304–309.\nNoreen, E. W. (1989). Computer Intensive Methods for Test-\ning Hypothesis. Wiley.\nNorman, D. A. (1988).\nThe Design of Everyday Things.\nBasic Books.\nNorman, D. A. and Rumelhart, D. E. (1975). Explorations\nin Cognition. Freeman.\nNorvig, P. (1991). Techniques for automatic memoization\nwith applications to context-free parsing. Computational\nLinguistics, 17(1), 91–98.\nNorvig, P. (2007). How to write a spelling corrector. http:\n//www.norvig.com/spell-correct.html.\nNorvig, P. (2009). Natural language corpus data. In Segaran,\nT. and Hammerbacher, J. (Eds.), Beautiful data: the stories\nbehind elegant data solutions. O’Reilly.\nNosek, B. A., Banaji, M. R., and Greenwald, A. G. (2002a).\nHarvesting implicit group attitudes and beliefs from a\ndemonstration web site.",
  "572": "Norvig, P. (2009). Natural language corpus data. In Segaran,\nT. and Hammerbacher, J. (Eds.), Beautiful data: the stories\nbehind elegant data solutions. O’Reilly.\nNosek, B. A., Banaji, M. R., and Greenwald, A. G. (2002a).\nHarvesting implicit group attitudes and beliefs from a\ndemonstration web site.\nGroup Dynamics: Theory, Re-\nsearch, and Practice, 6(1), 101.\nNosek, B. A., Banaji, M. R., and Greenwald, A. G. (2002b).\nMath=male, me=female, therefore math̸= me. Journal of\npersonality and social psychology, 83(1), 44.\nO’Connor, B., Krieger, M., and Ahn, D. (2010). Tweetmo-\ntif: Exploratory search and topic summarization for twitter.\nIn ICWSM.\nOdell, M. K. and Russell, R. C. (1918/1922). U.S. Patents\n1261167 (1918), 1435663 (1922). Cited in Knuth (1973).\nOh, A. H. and Rudnicky, A. I. (2000). Stochastic language\ngeneration for spoken dialogue systems. In Proceedings\nof the 2000 ANLP/NAACL Workshop on Conversational\nsystems-Volume 3, pp. 27–32.\nOravecz, C. and Dienes, P. (2002). Efﬁcient stochastic part-\nof-speech tagging for Hungarian. In LREC-02, Las Palmas,\nCanary Islands, Spain, pp. 710–717.\nOsgood, C. E., Suci, G. J., and Tannenbaum, P. H. (1957).\nThe Measurement of Meaning. University of Illinois Press.\nPackard, D. W. (1973). Computer-assisted morphological\nanalysis of ancient Greek. In Zampolli, A. and Calzolari,\nN. (Eds.), Computational and Mathematical Linguistics:\nProceedings of the International Conference on Computa-\ntional Linguistics, Pisa, pp. 343–355. Leo S. Olschki.\nPalmer, D. (2012). Text preprocessing. In Indurkhya, N.\nand Damerau, F. J. (Eds.), Handbook of Natural Language\nProcessing, pp. 9–30. CRC Press.\nPalmer, M., Babko-Malaya, O., and Dang, H. T. (2004).\nDifferent sense granularities for different applications. In\nHLT-NAACL Workshop on Scalable Natural Language Un-\nderstanding, Boston, MA, pp. 49–56.\nPalmer, M., Dang, H. T., and Fellbaum, C. (2006). Mak-\ning ﬁne-grained and coarse-grained sense distinctions, both\nmanually and automatically. Natural Language Engineer-\ning, 13(2), 137–163.\nPalmer, M., Fellbaum, C., Cotton, S., Delfs, L., and Dang,\nH. T. (2001). English tasks: All-words and verb lexical\nsample. In Proceedings of Senseval-2: 2nd International\nWorkshop on Evaluating Word Sense Disambiguation Sys-\ntems, Toulouse, France, pp. 21–24.\nPalmer, M., Gildea, D., and Xue, N. (2010). Semantic role\nlabeling.\nSynthesis Lectures on Human Language Tech-\nnologies, 3(1), 1–103.\nPalmer, M., Kingsbury, P., and Gildea, D. (2005).\nThe\nproposition bank: An annotated corpus of semantic roles.\nComputational Linguistics, 31(1), 71–106.\nPalmer, M., Ng, H. T., and Dang, H. T. (2006). Evalua-\ntion of wsd systems. In Agirre, E. and Edmonds, P. (Eds.),\nWord Sense Disambiguation: Algorithms and Applications.\nKluwer.\nPang, B. and Lee, L. (2008). Opinion mining and sentiment\nanalysis. Foundations and trends in information retrieval,\n2(1-2), 1–135.\nPang, B., Lee, L., and Vaithyanathan, S. (2002). Thumbs",
  "573": "Word Sense Disambiguation: Algorithms and Applications.\nKluwer.\nPang, B. and Lee, L. (2008). Opinion mining and sentiment\nanalysis. Foundations and trends in information retrieval,\n2(1-2), 1–135.\nPang, B., Lee, L., and Vaithyanathan, S. (2002). Thumbs\nup? Sentiment classiﬁcation using machine learning tech-\nniques. In EMNLP 2002, pp. 79–86.\nPaolino,\nJ.\n(2017).\nGoogle\nHome\nvs\nAlexa:\nTwo\nsimple\nuser\nexperience\ndesign\ngestures\nthat\ndelighted\na\nfemale\nuser.\nIn\nMedium.\nJan\n4,\n2017.\nhttps://medium.com/startup-grind/\ngoogle-home-vs-alexa-56e26f69ac77.\nParsons, T. (1990). Events in the Semantics of English. MIT\nPress.\nPartee, B. H. (Ed.). (1976). Montague Grammar. Academic\nPress.\nPasca, M. (2003). Open-Domain Question Answering from\nLarge Text Collections. CSLI.\nPaszke, A., Gross, S., Chintala, S., Chanan, G., Yang, E.,\nDeVito, Z., Lin, Z., Desmaison, A., Antiga, L., and Lerer,\nA. (2017). Automatic differentiation in pytorch. In NIPS-\nW.",
  "574": "534\nBibliography\nPearl, C. (2017). Designing Voice User Interfaces: Princi-\nples of Conversational Experiences. O’Reilly.\nPedersen, T. and Bruce, R. (1997).\nDistinguishing word\nsenses in untagged text. In EMNLP 1997, Providence, RI.\nPeng, N., Poon, H., Quirk, C., Toutanova, K., and Yih, W.-t.\n(2017). Cross-sentence n-ary relation extraction with graph\nLSTMs. TACL, 5, 101–115.\nPenn, G. and Kiparsky, P. (2012). On P¯an.ini and the gen-\nerative capacity of contextualized replacement systems. In\nCOLING-12, pp. 943–950.\nPennebaker, J. W., Booth, R. J., and Francis, M. E. (2007).\nLinguistic Inquiry and Word Count: LIWC 2007. Austin,\nTX.\nPennebaker, J. W. and King, L. A. (1999). Linguistic styles:\nlanguage use as an individual difference. Journal of Per-\nsonality and Social Psychology, 77(6).\nPennington, J., Socher, R., and Manning, C. D. (2014).\nGlove: Global vectors for word representation. In EMNLP\n2014, pp. 1532–1543.\nPercival, W. K. (1976). On the historical source of immedi-\nate constituent analysis. In McCawley, J. D. (Ed.), Syntax\nand Semantics Volume 7, Notes from the Linguistic Under-\nground, pp. 229–242. Academic Press.\nPerrault, C. R. and Allen, J. (1980). A plan-based analy-\nsis of indirect speech acts. American Journal of Computa-\ntional Linguistics, 6(3-4), 167–182.\nPeterson, J. L. (1986). A note on undetected typing errors.\nCommunications of the ACM, 29(7), 633–637.\nPetrov, S., Barrett, L., Thibaux, R., and Klein, D. (2006).\nLearning accurate, compact, and interpretable tree annota-\ntion. In COLING/ACL 2006, Sydney, Australia, pp. 433–\n440.\nPetrov, S., Das, D., and McDonald, R. (2012). A universal\npart-of-speech tagset. In LREC-12.\nPetrov, S. and McDonald, R. (2012). Overview of the 2012\nshared task on parsing the web. In Notes of the First Work-\nshop on Syntactic Analysis of Non-Canonical Language\n(SANCL), Vol. 59.\nPhilips, L. (1990). Hanging on the metaphone. Computer\nLanguage, 7(12).\nPhillips, A. V. (1960). A question-answering routine. Tech.\nrep. 16, MIT AI Lab.\nPicard, R. W. (1995). Affective computing. Tech. rep. 321,\nMIT Media Lab Perceputal Computing Technical Report.\nRevised November 26, 1995.\nPieraccini, R., Levin, E., and Lee, C.-H. (1991). Stochastic\nrepresentation of conceptual structure in the ATIS task. In\nProceedings DARPA Speech and Natural Language Work-\nshop, Paciﬁc Grove, CA, pp. 121–124.\nPlutchik, R. (1962). The emotions: Facts, theories, and a\nnew model. Random House.\nPlutchik, R. (1980). A general psychoevolutionary theory\nof emotion. In Plutchik, R. and Kellerman, H. (Eds.), Emo-\ntion: Theory, Research, and Experience, Volume 1, pp. 3–\n33. Academic Press.\nPolifroni, J., Hirschman, L., Seneff, S., and Zue, V. W.\n(1992). Experiments in evaluating interactive spoken lan-\nguage systems. In Proceedings DARPA Speech and Natural\nLanguage Workshop, Harriman, NY, pp. 28–33.\nPollard, C. and Sag, I. A. (1994).\nHead-Driven Phrase\nStructure Grammar. University of Chicago Press.",
  "575": "Polifroni, J., Hirschman, L., Seneff, S., and Zue, V. W.\n(1992). Experiments in evaluating interactive spoken lan-\nguage systems. In Proceedings DARPA Speech and Natural\nLanguage Workshop, Harriman, NY, pp. 28–33.\nPollard, C. and Sag, I. A. (1994).\nHead-Driven Phrase\nStructure Grammar. University of Chicago Press.\nPonzetto, S. P. and Navigli, R. (2010). Knowledge-rich word\nsense disambiguation rivaling supervised systems. In ACL\n2010, pp. 1522–1531.\nPorter, M. F. (1980). An algorithm for sufﬁx stripping. Pro-\ngram, 14(3), 130–127.\nPotts, C. (2011). On the negativity of negation. In Li, N. and\nLutz, D. (Eds.), Proceedings of Semantics and Linguistic\nTheory 20, pp. 636–659. CLC Publications, Ithaca, NY.\nPradhan, S., Moschitti, A., Xue, N., Ng, H. T., Bj¨orkelund,\nA., Uryupina, O., Zhang, Y., and Zhong, Z. (2013). To-\nwards robust linguistic analysis using OntoNotes.\nIn\nCoNLL-13, pp. 143–152.\nPradhan, S., Ward, W., Hacioglu, K., Martin, J. H., and Ju-\nrafsky, D. (2005). Semantic role labeling using different\nsyntactic views. In ACL-05, Ann Arbor, MI.\nPurver, M. (2004). The theory and use of clariﬁcation re-\nquests in dialogue. Ph.D. thesis, University of London.\nPustejovsky, J. (1995). The Generative Lexicon. MIT Press.\nPustejovsky, J. and Boguraev, B. (Eds.). (1996). Lexical\nSemantics: The Problem of Polysemy. Oxford University\nPress.\nPustejovsky,\nJ.,\nCasta˜no,\nJ.,\nIngria,\nR.,\nSaur´ı,\nR.,\nGaizauskas, R., Setzer, A., and Katz, G. (2003a). TimeML:\nrobust speciﬁcation of event and temporal expressions in\ntext. In Proceedings of the 5th International Workshop on\nComputational Semantics (IWCS-5).\nPustejovsky, J., Hanks, P., Saur´ı, R., See, A., Gaizauskas,\nR., Setzer, A., Radev, D., Sundheim, B., Day, D. S., Ferro,\nL., and Lazo, M. (2003b). The TIMEBANK corpus. In\nProceedings of Corpus Linguistics 2003 Conference, pp.\n647–656. UCREL Technical Paper number 16.\nPustejovsky, J., Ingria, R., Saur´ı, R., Casta˜no, J., Littman, J.,\nGaizauskas, R., Setzer, A., Katz, G., and Mani, I. (2005).\nThe Speciﬁcation Language TimeML, chap. 27. Oxford.\nQiu, G., Liu, B., Bu, J., and Chen, C. (2009). Expanding\ndomain sentiment lexicon through double propagation.. In\nIJCAI-09, pp. 1199–1204.\nQuillian, M. R. (1968). Semantic memory. In Minsky, M.\n(Ed.), Semantic Information Processing, pp. 227–270. MIT\nPress.\nQuillian, M. R. (1969). The teachable language comprehen-\nder: A simulation program and theory of language. Com-\nmunications of the ACM, 12(8), 459–476.\nQuirk, R., Greenbaum, S., Leech, G., and Svartvik, J.\n(1985). A Comprehensive Grammar of the English Lan-\nguage. Longman.\nRabiner, L. R. (1989). A tutorial on hidden Markov models\nand selected applications in speech recognition. Proceed-\nings of the IEEE, 77(2), 257–286.",
  "576": "Quirk, R., Greenbaum, S., Leech, G., and Svartvik, J.\n(1985). A Comprehensive Grammar of the English Lan-\nguage. Longman.\nRabiner, L. R. (1989). A tutorial on hidden Markov models\nand selected applications in speech recognition. Proceed-\nings of the IEEE, 77(2), 257–286.\nRabiner, L. R. and Juang, B. H. (1993). Fundamentals of\nSpeech Recognition. Prentice Hall.\nRadford, A. (1988). Transformational Grammar: A First\nCourse. Cambridge University Press.\nRadford, A. (1997). Syntactic Theory and the Structure of\nEnglish: A Minimalist Approach. Cambridge University\nPress.\nRajpurkar, P., Jia, R., and Liang, P. (2018). Know what you\ndon’t know: Unanswerable questions for SQuAD. In ACL\n2018.\nRajpurkar, P., Zhang, J., Lopyrev, K., and Liang, P. (2016).\nSQuAD: 100,000+ questions for machine comprehension\nof text. In EMNLP 2016.\nRamshaw, L. A. and Marcus, M. P. (1995). Text chunking\nusing transformation-based learning. In Proceedings of the\n3rd Annual Workshop on Very Large Corpora, pp. 82–94.\nRanganath, R., Jurafsky, D., and McFarland, D. A. (2013).\nDetecting friendly, ﬂirtatious, awkward, and assertive\nspeech in speed-dates. Computer Speech and Language,\n27(1), 89–115.",
  "577": "Bibliography\n535\nRaphael, B. (1968). SIR: A computer program for seman-\ntic information retrieval. In Minsky, M. (Ed.), Semantic\nInformation Processing, pp. 33–145. MIT Press.\nRashkin, H., Bell, E., Choi, Y., and Volkova, S. (2017). Mul-\ntilingual connotation frames: A case study on social media\nfor targeted sentiment analysis and forecast. In ACL 2017,\npp. 459–464.\nRashkin, H., Singh, S., and Choi, Y. (2016). Connotation\nframes: A data-driven investigation. In ACL 2016, pp. 311–\n321.\nRatnaparkhi, A. (1996).\nA maximum entropy part-of-\nspeech tagger. In EMNLP 1996, Philadelphia, PA, pp. 133–\n142.\nRatnaparkhi, A. (1997). A linear observed time statisti-\ncal parser based on maximum entropy models. In EMNLP\n1997, Providence, RI, pp. 1–10.\nRatnaparkhi, A., Reynar, J. C., and Roukos, S. (1994). A\nmaximum entropy model for prepositional phrase attach-\nment. In ARPA Human Language Technologies Workshop,\nPlainsboro, N.J., pp. 250–255.\nRaviv, J. (1967). Decision making in Markov chains applied\nto the problem of pattern recognition. IEEE Transactions\non Information Theory, 13(4), 536–551.\nRaymond, C. and Riccardi, G. (2007). Generative and dis-\ncriminative algorithms for spoken language understanding.\nIn INTERSPEECH-07, pp. 1605–1608.\nRehder, B., Schreiner, M. E., Wolfe, M. B. W., Laham, D.,\nLandauer, T. K., and Kintsch, W. (1998).\nUsing Latent\nSemantic Analysis to assess knowledge: Some technical\nconsiderations. Discourse Processes, 25(2-3), 337–354.\nReichenbach, H. (1947).\nElements of Symbolic Logic.\nMacmillan, New York.\nReichert, T. A., Cohen, D. N., and Wong, A. K. C. (1973).\nAn application of information theory to genetic mutations\nand the matching of polypeptide sequences.\nJournal of\nTheoretical Biology, 42, 245–261.\nResnik, P. (1992). Probabilistic tree-adjoining grammar as\na framework for statistical natural language processing. In\nCOLING-92, Nantes, France, pp. 418–424.\nResnik, P. (1993). Semantic classes and syntactic ambigu-\nity. In Proceedings of the workshop on Human Language\nTechnology, pp. 278–283.\nResnik, P. (1995).\nUsing information content to evaluate\nsemantic similarity in a taxanomy. In International Joint\nConference for Artiﬁcial Intelligence (IJCAI-95), pp. 448–\n453.\nResnik, P. (1996). Selectional constraints: An information-\ntheoretic model and its computational realization. Cogni-\ntion, 61, 127–159.\nRichardson, M., Burges, C. J. C., and Renshaw, E. (2013).\nMCTest: A challenge dataset for the open-domain machine\ncomprehension of text. In EMNLP 2013, pp. 193–203.\nRiedel, S., Yao, L., and McCallum, A. (2010). Modeling\nrelations and their mentions without labeled text. In Ma-\nchine Learning and Knowledge Discovery in Databases,\npp. 148–163. Springer.\nRiedel, S., Yao, L., McCallum, A., and Marlin, B. M. (2013).\nRelation extraction with matrix factorization and universal\nschemas. In NAACL HLT 2013.\nRiesbeck, C. K. (1975). Conceptual analysis. In Schank,\nR. C. (Ed.), Conceptual Information Processing, pp. 83–\n156. American Elsevier, New York.\nRiezler, S., King, T. H., Kaplan, R. M., Crouch, R.,\nMaxwell III, J. T., and Johnson, M. (2002).\nParsing",
  "578": "Riesbeck, C. K. (1975). Conceptual analysis. In Schank,\nR. C. (Ed.), Conceptual Information Processing, pp. 83–\n156. American Elsevier, New York.\nRiezler, S., King, T. H., Kaplan, R. M., Crouch, R.,\nMaxwell III, J. T., and Johnson, M. (2002).\nParsing\nthe Wall Street Journal using a Lexical-Functional Gram-\nmar and discriminative estimation techniques. In ACL-02,\nPhiladelphia, PA.\nRiloff, E. (1993). Automatically constructing a dictionary\nfor information extraction tasks. In AAAI-93, Washington,\nD.C., pp. 811–816.\nRiloff, E. (1996). Automatically generating extraction pat-\nterns from untagged text. In AAAI-96, pp. 117–124.\nRiloff, E. and Jones, R. (1999). Learning dictionaries for\ninformation extraction by multi-level bootstrapping.\nIn\nAAAI-99, pp. 474–479.\nRiloff, E. and Schmelzenbach, M. (1998). An empirical ap-\nproach to conceptual case frame acquisition. In Proceed-\nings of the Sixth Workshop on Very Large Corpora, Mon-\ntreal, Canada, pp. 49–56.\nRiloff, E. and Shepherd, J. (1997). A corpus-based approach\nfor building semantic lexicons. In EMNLP 1997.\nRiloff, E. and Thelen, M. (2000). A rule-based question an-\nswering system for reading comprehension tests. In Pro-\nceedings of ANLP/NAACL workshop on reading compre-\nhension tests, pp. 13–19.\nRiloff, E. and Wiebe, J. (2003). Learning extraction pat-\nterns for subjective expressions. In EMNLP 2003, Sapporo,\nJapan.\nRitter, A., Cherry, C., and Dolan, B. (2011). Data-driven\nresponse generation in social media. In EMNLP-11, pp.\n583–593.\nRitter, A., Etzioni, O., and Mausam (2010). A latent dirich-\nlet allocation method for selectional preferences. In ACL\n2010, pp. 424–434.\nRitter, A., Zettlemoyer, L., Mausam, and Etzioni, O. (2013).\nModeling missing data in distant supervision for informa-\ntion extraction.. TACL, 1, 367–378.\nRoark, B. (2001). Probabilistic top-down parsing and lan-\nguage modeling. Computational Linguistics, 27(2), 249–\n276.\nRoark, B., Saraclar, M., and Collins, M. (2007). Discrim-\ninative n-gram language modeling. Computer Speech &\nLanguage, 21(2), 373–392.\nRohde, D. L. T., Gonnerman, L. M., and Plaut, D. C. (2006).\nAn improved model of semantic similarity based on lexical\nco-occurrence. Communications of the ACM, 8, 627–633.\nRooth, M., Riezler, S., Prescher, D., Carroll, G., and Beil,\nF. (1999). Inducing a semantically annotated lexicon via\nEM-based clustering. In ACL-99, College Park, MA, pp.\n104–111.\nRosenblatt, F. (1958).\nThe perceptron:\nA probabilis-\ntic model for information storage and organization in the\nbrain.. Psychological review, 65(6), 386–408.\nRosenfeld, R. (1996).\nA maximum entropy approach to\nadaptive statistical language modeling. Computer Speech\nand Language, 10, 187–228.\nRothe, S., Ebert, S., and Sch¨utze, H. (2016). Ultradense\nWord Embeddings by Orthogonal Transformation.\nIn\nNAACL HLT 2016.\nRoy, N., Pineau, J., and Thrun, S. (2000). Spoken dialog\nmanagement for robots. In ACL-00, Hong Kong.\nRubenstein, H. and Goodenough, J. B. (1965).\nContex-\ntual correlates of synonymy. Communications of the ACM,\n8(10), 627–633.",
  "579": "In\nNAACL HLT 2016.\nRoy, N., Pineau, J., and Thrun, S. (2000). Spoken dialog\nmanagement for robots. In ACL-00, Hong Kong.\nRubenstein, H. and Goodenough, J. B. (1965).\nContex-\ntual correlates of synonymy. Communications of the ACM,\n8(10), 627–633.\nRumelhart, D. E., Hinton, G. E., and Williams, R. J. (1986).\nLearning internal representations by error propagation. In\nRumelhart, D. E. and McClelland, J. L. (Eds.), Parallel\nDistributed Processing, Vol. 2, pp. 318–362. MIT Press.",
  "580": "536\nBibliography\nRumelhart, D. E. and McClelland, J. L. (1986a). On learn-\ning the past tense of English verbs. In Rumelhart, D. E. and\nMcClelland, J. L. (Eds.), Parallel Distributed Processing,\nVol. 2, pp. 216–271. MIT Press.\nRumelhart, D. E. and McClelland, J. L. (Eds.). (1986b).\nParallel Distributed Processing. MIT Press.\nRuppenhofer, J., Ellsworth, M., Petruck, M. R. L., Johnson,\nC. R., Baker, C. F., and Scheffczyk, J. (2016). FrameNet\nII: Extended theory and practice..\nRuppenhofer, J., Sporleder, C., Morante, R., Baker, C., and\nPalmer, M. (2010). Semeval-2010 task 10: Linking events\nand their participants in discourse. In Proceedings of the\n5th International Workshop on Semantic Evaluation, pp.\n45–50.\nRussell, J. A. (1980). A circumplex model of affect. Journal\nof personality and social psychology, 39(6), 1161–1178.\nRussell, S. and Norvig, P. (2002). Artiﬁcial Intelligence: A\nModern Approach (2nd Ed.). Prentice Hall.\nSacks, H., Schegloff, E. A., and Jefferson, G. (1974). A\nsimplest systematics for the organization of turn-taking for\nconversation. Language, 50(4), 696–735.\nSag, I. A. and Liberman, M. Y. (1975). The intonational dis-\nambiguation of indirect speech acts. In CLS-75, pp. 487–\n498. University of Chicago.\nSag, I. A., Wasow, T., and Bender, E. M. (Eds.). (2003). Syn-\ntactic Theory: A Formal Introduction. CSLI Publications,\nStanford, CA.\nSahami, M., Dumais, S. T., Heckerman, D., and Horvitz, E.\n(1998). A Bayesian approach to ﬁltering junk e-mail. In\nAAAI Workshop on Learning for Text Categorization, pp.\n98–105.\nSakoe, H. and Chiba, S. (1971). A dynamic programming\napproach to continuous speech recognition. In Proceed-\nings of the Seventh International Congress on Acoustics,\nBudapest, Vol. 3, pp. 65–69. Akad´emiai Kiad´o.\nSalomaa, A. (1969). Probabilistic and weighted grammars.\nInformation and Control, 15, 529–544.\nSalton, G. (1971). The SMART Retrieval System: Experi-\nments in Automatic Document Processing. Prentice Hall.\nSampson, G. (1987). Alternative grammatical coding sys-\ntems.\nIn Garside, R., Leech, G., and Sampson, G.\n(Eds.), The Computational Analysis of English, pp. 165–\n183. Longman.\nSamuelsson, C. (1993). Morphological tagging based en-\ntirely on Bayesian inference. In 9th Nordic Conference on\nComputational Linguistics NODALIDA-93. Stockholm.\nSankoff, D. (1972).\nMatching sequences under deletion-\ninsertion constraints. Proceedings of the Natural Academy\nof Sciences of the U.S.A., 69, 4–6.\nSankoff, D. and Labov, W. (1979). On the uses of variable\nrules. Language in society, 8(2-3), 189–222.\nSap, M., Prasettio, M. C., Holtzman, A., Rashkin, H., and\nChoi, Y. (2017). Connotation frames of power and agency\nin modern ﬁlms. In EMNLP 2017, pp. 2329–2334.\nSchabes, Y. (1990). Mathematical and Computational As-\npects of Lexicalized Grammars. Ph.D. thesis, University of\nPennsylvania, Philadelphia, PA.\nSchabes, Y. (1992).\nStochastic lexicalized tree-adjoining\ngrammars. In COLING-92, Nantes, France, pp. 426–433.",
  "581": "Schabes, Y. (1990). Mathematical and Computational As-\npects of Lexicalized Grammars. Ph.D. thesis, University of\nPennsylvania, Philadelphia, PA.\nSchabes, Y. (1992).\nStochastic lexicalized tree-adjoining\ngrammars. In COLING-92, Nantes, France, pp. 426–433.\nSchabes, Y., Abeill´e, A., and Joshi, A. K. (1988).\nPars-\ning strategies with ‘lexicalized’ grammars: Applications to\nTree Adjoining Grammars. In COLING-88, Budapest, pp.\n578–583.\nSchank, R. C. (1972). Conceptual dependency: A theory\nof natural language processing. Cognitive Psychology, 3,\n552–631.\nSchank, R. C. and Abelson, R. P. (1975). Scripts, plans, and\nknowledge. In Proceedings of IJCAI-75, pp. 151–157.\nSchank, R. C. and Abelson, R. P. (1977). Scripts, Plans,\nGoals and Understanding. Lawrence Erlbaum.\nSchegloff, E. A. (1968). Sequencing in conversational open-\nings. American Anthropologist, 70, 1075–1095.\nSchegloff, E. A. (1972). Notes on a conversational practice:\nFormulating place. In Sudnow, D. (Ed.), Studies in social\ninteraction, New York. Free Press.\nSchegloff, E. A. (1982).\nDiscourse as an interactional\nachievement: Some uses of ‘uh huh’ and other things that\ncome between sentences. In Tannen, D. (Ed.), Analyzing\nDiscourse: Text and Talk, pp. 71–93. Georgetown Univer-\nsity Press, Washington, D.C.\nScherer, K. R. (2000). Psychological models of emotion.\nIn Borod, J. C. (Ed.), The neuropsychology of emotion, pp.\n137–162. Oxford.\nSchone, P. and Jurafsky, D. (2000). Knowlege-free induction\nof morphology using latent semantic analysis. In CoNLL-\n00.\nSchone, P. and Jurafsky, D. (2001). Knowledge-free induc-\ntion of inﬂectional morphologies. In NAACL 2001.\nSch¨onkﬁnkel, M. (1924).\n¨Uber die Bausteine der mathe-\nmatischen Logik. Mathematische Annalen, 92, 305–316.\nEnglish translation appears in From Frege to G¨odel: A\nSource Book in Mathematical Logic, Harvard University\nPress, 1967.\nSch¨utze, H. (1992a). Context space. In Goldman, R. (Ed.),\nProceedings of the 1992 AAAI Fall Symposium on Proba-\nbilistic Approaches to Natural Language.\nSch¨utze, H. (1992b). Dimensions of meaning. In Proceed-\nings of Supercomputing ’92, pp. 787–796. IEEE Press.\nSch¨utze, H. (1997a).\nAmbiguity Resolution in Language\nLearning – Computational and Cognitive Models. CSLI,\nStanford, CA.\nSch¨utze, H. (1997b).\nAmbiguity Resolution in Language\nLearning: Computational and Cognitive Models.\nCSLI\nPublications, Stanford, CA.\nSch¨utze, H. (1998). Automatic word sense discrimination.\nComputational Linguistics, 24(1), 97–124.\nSch¨utze, H., Hull, D. A., and Pedersen, J. (1995). A com-\nparison of classiﬁers and document representations for the\nrouting problem. In SIGIR-95, pp. 229–237.\nSch¨utze, H. and Pedersen, J. (1993). A vector model for\nsyntagmatic and paradigmatic relatedness. In Proceedings\nof the 9th Annual Conference of the UW Centre for the New\nOED and Text Research, pp. 104–113.\nSch¨utze, H. and Singer, Y. (1994). Part-of-speech tagging\nusing a variable memory Markov model. In ACL-94, Las\nCruces, NM, pp. 181–187.\nSchwartz, H. A., Eichstaedt, J. C., Kern, M. L., Dziurzyn-",
  "582": "OED and Text Research, pp. 104–113.\nSch¨utze, H. and Singer, Y. (1994). Part-of-speech tagging\nusing a variable memory Markov model. In ACL-94, Las\nCruces, NM, pp. 181–187.\nSchwartz, H. A., Eichstaedt, J. C., Kern, M. L., Dziurzyn-\nski, L., Ramones, S. M., Agrawal, M., Shah, A., Kosin-\nski, M., Stillwell, D., Seligman, M. E. P., and Ungar, L. H.\n(2013). Personality, gender, and age in the language of so-\ncial media: The open-vocabulary approach. PloS one, 8(9),\ne73791.\nSchwartz, R. and Chow, Y.-L. (1990).\nThe N-best algo-\nrithm: An efﬁcient and exact procedure for ﬁnding the N\nmost likely sentence hypotheses. In ICASSP-90, Vol. 1, pp.\n81–84.\nSchwenk, H. (2007). Continuous space language models.\nComputer Speech & Language, 21(3), 492–518.",
  "583": "Bibliography\n537\nScott, M. and Shillcock, R. (2003).\nEye movements re-\nveal the on-line computation of lexical probabilities during\nreading. Psychological Science, 14(6), 648–652.\nS´eaghdha, D. O. (2010). Latent variable models of selec-\ntional preference. In ACL 2010, pp. 435–444.\nSeddah, D., Tsarfaty, R., K¨ubler, S., Candito, M., Choi,\nJ. D., Farkas, R., Foster, J., Goenaga, I., Gojenola, K.,\nGoldberg, Y., Green, S., Habash, N., Kuhlmann, M., Maier,\nW., Nivre, J., Przepi´orkowski, A., Roth, R., Seeker, W.,\nVersley, Y., Vincze, V., Woli´nski, M., Wr´oblewska, A.,\nand Villemonte de la Cl´ergerie, E. (2013). Overview of\nthe SPMRL 2013 shared task: cross-framework evalua-\ntion of parsing morphologically rich languages. In Pro-\nceedings of the 4th Workshop on Statistical Parsing of\nMorphologically-Rich Languages.\nSekine, S. and Collins, M. (1997).\nThe evalb software.\nhttp://cs.nyu.edu/cs/projects/proteus/evalb.\nSennrich, R., Haddow, B., and Birch, A. (2016). Neural ma-\nchine translation of rare words with subword units. In ACL\n2016.\nSeo, M., Kembhavi, A., Farhadi, A., and Hajishirzi, H.\n(2017). Bidirectional attention ﬂow for machine compre-\nhension. In ICLR 2017.\nSerban, I. V., Lowe, R. T., Charlin, L., and Pineau, J. (2017).\nA survey of available corpora for building data-driven dia-\nlogue systems.. arXiv preprint arXiv:1512.05742.\nSgall, P., Hajiˇcov´a, E., and Panevova, J. (1986). The Mean-\ning of the Sentence in its Pragmatic Aspects. Reidel.\nShang, L., Lu, Z., and Li, H. (2015). Neural responding ma-\nchine for short-text conversation. In ACL 2015, pp. 1577–\n1586.\nShannon, C. E. (1948). A mathematical theory of commu-\nnication. Bell System Technical Journal, 27(3), 379–423.\nContinued in the following volume.\nShannon, C. E. (1951). Prediction and entropy of printed\nEnglish. Bell System Technical Journal, 30, 50–64.\nSheil, B. A. (1976). Observations on context free parsing.\nSMIL: Statistical Methods in Linguistics, 1, 71–109.\nShriberg, E., Bates, R., Taylor, P., Stolcke, A., Jurafsky, D.,\nRies, K., Coccaro, N., Martin, R., Meteer, M., and Van Ess-\nDykema, C. (1998). Can prosody aid the automatic classi-\nﬁcation of dialog acts in conversational speech?. Language\nand Speech (Special Issue on Prosody and Conversation),\n41(3-4), 439–487.\nSimmons, R. F. (1965). Answering English questions by\ncomputer: A survey. Communications of the ACM, 8(1),\n53–70.\nSimmons, R. F. (1973).\nSemantic networks: Their com-\nputation and use for understanding English sentences. In\nSchank, R. C. and Colby, K. M. (Eds.), Computer Models\nof Thought and Language, pp. 61–113. W.H. Freeman and\nCo.\nSimmons, R. F., Klein, S., and McConlogue, K. (1964). In-\ndexing and dependency logic for answering english ques-\ntions. American Documentation, 15(3), 196–204.\nSimons, G. F. and Fennig, C. D. (2018). Ethnologue: Lan-\nguages of the world, twenty-ﬁrst edition.. Dallas, Texas.",
  "584": "Simmons, R. F., Klein, S., and McConlogue, K. (1964). In-\ndexing and dependency logic for answering english ques-\ntions. American Documentation, 15(3), 196–204.\nSimons, G. F. and Fennig, C. D. (2018). Ethnologue: Lan-\nguages of the world, twenty-ﬁrst edition.. Dallas, Texas.\nSIL International.\nSingh, S. P., Litman, D. J., Kearns, M., and Walker, M. A.\n(2002). Optimizing dialogue management with reinforce-\nment learning: Experiments with the NJFun system. Jour-\nnal of Artiﬁcial Intelligence Research (JAIR), 16, 105–133.\nSleator, D. and Temperley, D. (1993). Parsing English with\na link grammar. In IWPT-93.\nSmall, S. L., Cottrell, G. W., and Tanenhaus, M. (Eds.).\n(1988). Lexical Ambiguity Resolution. Morgan Kaufman.\nSmall, S. L. and Rieger, C. (1982). Parsing and compre-\nhending with Word Experts. In Lehnert, W. G. and Ringle,\nM. H. (Eds.), Strategies for Natural Language Processing,\npp. 89–147. Lawrence Erlbaum.\nSmith, D. A. and Eisner, J. (2007). Bootstrapping feature-\nrich dependency parsers with entropic priors. In EMNLP/-\nCoNLL 2007, Prague, pp. 667–677.\nSmith, N. A. and Eisner, J. (2005). Guiding unsupervised\ngrammar induction using contrastive estimation. In IJCAI\nWorkshop on Grammatical Inference Applications, Edin-\nburgh, pp. 73–82.\nSmith, V. L. and Clark, H. H. (1993). On the course of an-\nswering questions. Journal of Memory and Language, 32,\n25–38.\nSmolensky, P. (1988). On the proper treatment of connec-\ntionism. Behavioral and brain sciences, 11(1), 1–23.\nSmolensky, P. (1990). Tensor product variable binding and\nthe representation of symbolic structures in connectionist\nsystems. Artiﬁcial intelligence, 46(1-2), 159–216.\nSnow, R., Jurafsky, D., and Ng, A. Y. (2005). Learning syn-\ntactic patterns for automatic hypernym discovery. In Saul,\nL. K., Weiss, Y., and Bottou, L. (Eds.), NIPS 17, pp. 1297–\n1304. MIT Press.\nSnow, R., Prakash, S., Jurafsky, D., and Ng, A. Y. (2007).\nLearning to merge word senses. In EMNLP/CoNLL 2007,\npp. 1005–1014.\nSocher, R., Huval, B., Manning, C. D., and Ng, A. Y. (2012).\nSemantic compositionality through recursive matrix-vector\nspaces. In EMNLP 2012, pp. 1201–1211.\nSoderland, S., Fisher, D., Aseltine, J., and Lehnert, W. G.\n(1995). CRYSTAL: Inducing a conceptual dictionary. In\nIJCAI-95, Montreal, pp. 1134–1142.\nSøgaard, A. (2010).\nSimple semi-supervised training of\npart-of-speech taggers. In ACL 2010, pp. 205–208.\nSøgaard, A., Johannsen, A., Plank, B., Hovy, D., and\nAlonso, H. M. (2014). What’s in a p-value in NLP?. In\nCoNLL-14.\nSolorio, T., Blair, E., Maharjan, S., Bethard, S., Diab, M.,\nGhoneim, M., Hawwari, A., AlGhamdi, F., Hirschberg, J.,\nChang, A., and Fung, P. (2014).\nOverview for the ﬁrst\nshared task on language identiﬁcation in code-switched\ndata. In Proceedings of the First Workshop on Computa-\ntional Approaches to Code Switching, pp. 62–72.",
  "585": "Ghoneim, M., Hawwari, A., AlGhamdi, F., Hirschberg, J.,\nChang, A., and Fung, P. (2014).\nOverview for the ﬁrst\nshared task on language identiﬁcation in code-switched\ndata. In Proceedings of the First Workshop on Computa-\ntional Approaches to Code Switching, pp. 62–72.\nSordoni, A., Galley, M., Auli, M., Brockett, C., Ji, Y.,\nMitchell, M., Nie, J.-Y., Gao, J., and Dolan, B. (2015). A\nneural network approach to context-sensitive generation of\nconversational responses. In NAACL HLT 2015, pp. 196–\n205.\nSparck Jones, K. (1972). A statistical interpretation of term\nspeciﬁcity and its application in retrieval. Journal of Doc-\numentation, 28(1), 11–21.\nSparck Jones, K. (1986). Synonymy and Semantic Classiﬁ-\ncation. Edinburgh University Press, Edinburgh. Republi-\ncation of 1964 PhD Thesis.\nSpitkovsky, V. I. and Chang, A. X. (2012). A cross-lingual\ndictionary for English Wikipedia concepts. In LREC-12,\nIstanbul, Turkey.\nSrivastava, N., Hinton, G. E., Krizhevsky, A., Sutskever, I.,\nand Salakhutdinov, R. R. (2014). Dropout: a simple way\nto prevent neural networks from overﬁtting.. Journal of\nMachine Learning Research, 15(1), 1929–1958.\nStalnaker, R. C. (1978). Assertion. In Cole, P. (Ed.), Prag-\nmatics: Syntax and Semantics Volume 9, pp. 315–332. Aca-\ndemic Press.",
  "586": "538\nBibliography\nStamatatos, E. (2009). A survey of modern authorship at-\ntribution methods. JASIST, 60(3), 538–556.\nSteedman, M. (1989). Constituency and coordination in a\ncombinatory grammar. In Baltin, M. R. and Kroch, A. S.\n(Eds.), Alternative Conceptions of Phrase Structure, pp.\n201–231. University of Chicago.\nSteedman, M. (1996). Surface Structure and Interpretation.\nMIT Press. Linguistic Inquiry Monograph, 30.\nSteedman, M. (2000).\nThe Syntactic Process.\nThe MIT\nPress.\nStetina, J. and Nagao, M. (1997). Corpus based PP attach-\nment ambiguity resolution with a semantic dictionary. In\nZhou, J. and Church, K. W. (Eds.), Proceedings of the Fifth\nWorkshop on Very Large Corpora, Beijing, China, pp. 66–\n80.\nStifelman, L. J., Arons, B., Schmandt, C., and Hulteen, E. A.\n(1993). VoiceNotes: A speech interface for a hand-held\nvoice notetaker. In Human Factors in Computing Systems:\nINTERCHI ’93 Conference Proceedings, pp. 179–186.\nStolcke, A. (1995). An efﬁcient probabilistic context-free\nparsing algorithm that computes preﬁx probabilities. Com-\nputational Linguistics, 21(2), 165–202.\nStolcke, A. (1998). Entropy-based pruning of backoff lan-\nguage models. In Proc. DARPA Broadcast News Transcrip-\ntion and Understanding Workshop, Lansdowne, VA, pp.\n270–274.\nStolcke, A. (2002). SRILM – an extensible language mod-\neling toolkit. In ICSLP-02, Denver, CO.\nStolcke, A., Ries, K., Coccaro, N., Shriberg, E., Bates, R.,\nJurafsky, D., Taylor, P., Martin, R., Meteer, M., and Van\nEss-Dykema, C. (2000).\nDialogue act modeling for au-\ntomatic tagging and recognition of conversational speech.\nComputational Linguistics, 26(3), 339–371.\nStolz, W. S., Tannenbaum, P. H., and Carstensen, F. V.\n(1965). A stochastic approach to the grammatical coding\nof English. Communications of the ACM, 8(6), 399–405.\nStone, P., Dunphry, D., Smith, M., and Ogilvie, D. (1966).\nThe General Inquirer: A Computer Approach to Content\nAnalysis. Cambridge, MA: MIT Press.\nStoyanchev, S. and Johnston, M. (2015). Localized error\ndetection for targeted clariﬁcation in a virtual assistant. In\nICASSP-15, pp. 5241–5245.\nStoyanchev, S., Liu, A., and Hirschberg, J. (2013). Mod-\nelling human clariﬁcation strategies.\nIn SIGDIAL 2013,\npp. 137–141.\nStoyanchev, S., Liu, A., and Hirschberg, J. (2014). Towards\nnatural clariﬁcation questions in dialogue systems. In AISB\nsymposium on questions, discourse and dialogue.\nStr¨otgen, J. and Gertz, M. (2013). Multilingual and cross-\ndomain temporal tagging. Language Resources and Eval-\nuation, 47(2), 269–298.\nSuendermann, D., Evanini, K., Liscombe, J., Hunter, P.,\nDayanidhi, K., and Pieraccini, R. (2009). From rule-based\nto statistical grammars: Continuous improvement of large-\nscale spoken dialog systems.\nIn ICASSP-09, pp. 4713–\n4716.\nSundheim, B. (Ed.). (1991). Proceedings of MUC-3.\nSundheim, B. (Ed.). (1992). Proceedings of MUC-4.\nSundheim, B. (Ed.). (1993). Proceedings of MUC-5, Balti-\nmore, MD.\nSundheim, B. (Ed.). (1995). Proceedings of MUC-6.",
  "587": "4716.\nSundheim, B. (Ed.). (1991). Proceedings of MUC-3.\nSundheim, B. (Ed.). (1992). Proceedings of MUC-4.\nSundheim, B. (Ed.). (1993). Proceedings of MUC-5, Balti-\nmore, MD.\nSundheim, B. (Ed.). (1995). Proceedings of MUC-6.\nSurdeanu, M. (2013). Overview of the TAC2013 Knowl-\nedge Base Population evaluation: English slot ﬁlling and\ntemporal slot ﬁlling. In TAC-13.\nSurdeanu, M., Harabagiu, S., Williams, J., and Aarseth, P.\n(2003). Using predicate-argument structures for informa-\ntion extraction. In ACL-03, pp. 8–15.\nSurdeanu, M., Johansson, R., Meyers, A., M`arquez, L., and\nNivre, J. (2008a). The conll-2008 shared task on joint pars-\ning of syntactic and semantic dependencies. In CoNLL-08,\npp. 159–177.\nSurdeanu, M., Johansson, R., Meyers, A., M`arquez, L., and\nNivre, J. (2008b). The conll-2008 shared task on joint pars-\ning of syntactic and semantic dependencies. In CoNLL-08,\npp. 159–177.\nSwerts, M., Litman, D. J., and Hirschberg, J. (2000). Cor-\nrections in spoken dialogue systems. In ICSLP-00, Beijing,\nChina.\nSwier, R. and Stevenson, S. (2004). Unsupervised semantic\nrole labelling. In EMNLP 2004, pp. 95–102.\nSwitzer, P. (1965).\nVector images in document re-\ntrieval.\nIn Stevens,\nM. E.,\nGiuliano,\nV. E.,\nand\nHeilprin,\nL. B. (Eds.),\nStatistical Association Meth-\nods For Mechanized Documentation. Symposium Pro-\nceedings. Washington, D.C., USA, March 17, 1964,\npp. 163–171. https://nvlpubs.nist.gov/nistpubs/\nLegacy/MP/nbsmiscellaneouspub269.pdf.\nTalbot, D. and Osborne, M. (2007). Smoothed Bloom Fil-\nter Language Models: Tera-Scale LMs on the Cheap. In\nEMNLP/CoNLL 2007, pp. 468–476.\nTalmor, A. and Berant, J. (2018). The web as a knowledge-\nbase for answering complex questions.\nIn NAACL HLT\n2018.\nTannen, D. (1979). What’s in a frame? Surface evidence for\nunderlying expectations. In Freedle, R. (Ed.), New Direc-\ntions in Discourse Processing, pp. 137–181. Ablex.\nTaskar, B., Klein, D., Collins, M., Koller, D., and Manning,\nC. D. (2004). Max-margin parsing. In EMNLP 2004, pp.\n1–8.\nter Meulen, A. (1995). Representing Time in Natural Lan-\nguage. MIT Press.\nTesni`ere, L. (1959). ´El´ements de Syntaxe Structurale. Li-\nbrairie C. Klincksieck, Paris.\nThede, S. M. and Harper, M. P. (1999). A second-order hid-\nden Markov model for part-of-speech tagging. In ACL-99,\nCollege Park, MA, pp. 175–182.\nThompson, K. (1968). Regular expression search algorithm.\nCommunications of the ACM, 11(6), 419–422.\nTibshirani, R. J. (1996). Regression shrinkage and selec-\ntion via the lasso. Journal of the Royal Statistical Society.\nSeries B (Methodological), 58(1), 267–288.\nTitov, I. and Henderson, J. (2006). Loss minimization in\nparse reranking. In EMNLP 2006.\nTitov, I. and Khoddam, E. (2014). Unsupervised induction",
  "588": "tion via the lasso. Journal of the Royal Statistical Society.\nSeries B (Methodological), 58(1), 267–288.\nTitov, I. and Henderson, J. (2006). Loss minimization in\nparse reranking. In EMNLP 2006.\nTitov, I. and Khoddam, E. (2014). Unsupervised induction\nof semantic roles within a reconstruction-error minimiza-\ntion framework. In NAACL HLT 2015.\nTitov, I. and Klementiev, A. (2012). A Bayesian approach\nto unsupervised semantic role induction. In EACL-12, pp.\n12–22.\nTomkins, S. S. (1962). Affect, imagery, consciousness: Vol.\nI. The positive affects. Springer.\nToutanova, K., Klein, D., Manning, C. D., and Singer, Y.\n(2003). Feature-rich part-of-speech tagging with a cyclic\ndependency network. In HLT-NAACL-03.\nToutanova, K., Manning, C. D., Flickinger, D., and Oepen,\nS. (2005). Stochastic HPSG Parse Disambiguation using\nthe Redwoods Corpus. Research on Language & Compu-\ntation, 3(1), 83–105.",
  "589": "Bibliography\n539\nToutanova, K. and Moore, R. C. (2002).\nPronunciation\nmodeling for improved spelling correction.\nIn ACL-02,\nPhiladelphia, PA, pp. 144–151.\nTseng, H., Chang, P.-C., Andrew, G., Jurafsky, D., and Man-\nning, C. D. (2005a). Conditional random ﬁeld word seg-\nmenter. In Proceedings of the Fourth SIGHAN Workshop\non Chinese Language Processing.\nTseng, H., Jurafsky, D., and Manning, C. D. (2005b). Mor-\nphological features help POS tagging of unknown words\nacross language varieties.\nIn Proceedings of the 4th\nSIGHAN Workshop on Chinese Language Processing.\nTurian, J., Ratinov, L., and Bengio, Y. (2010).\nWord\nrepresentations: a simple and general method for semi-\nsupervised learning. In ACL 2010, pp. 384–394.\nTurney, P. D. (2002).\nThumbs up or thumbs down? se-\nmantic orientation applied to unsupervised classiﬁcation of\nreviews. In ACL-02.\nTurney, P. D. and Littman, M. (2003). Measuring praise and\ncriticism: Inference of semantic orientation from associa-\ntion. ACM Transactions on Information Systems (TOIS),\n21, 315–346.\nUzZaman, N., Llorens, H., Derczynski, L., Allen, J., Ver-\nhagen, M., and Pustejovsky, J. (2013). Semeval-2013 task\n1: Tempeval-3: Evaluating time expressions, events, and\ntemporal relations. In SemEval-13, pp. 1–9.\nvan Benthem, J. and ter Meulen, A. (Eds.). (1997). Hand-\nbook of Logic and Language. MIT Press.\nvan der Maaten, L. and Hinton, G. E. (2008). Visualiz-\ning high-dimensional data using t-sne. Journal of Machine\nLearning Research, 9, 2579–2605.\nvan Rijsbergen, C. J. (1975). Information Retrieval. But-\nterworths.\nVan Valin, Jr., R. D. and La Polla, R. (1997). Syntax: Struc-\nture, Meaning, and Function. Cambridge University Press.\nVanLehn, K., Jordan, P. W., Ros´e, C., Bhembe, D., B¨ottner,\nM., Gaydos, A., Makatchev, M., Pappuswamy, U., Ringen-\nberg, M., Roque, A., Siler, S., Srivastava, R., and Wilson,\nR. (2002). The architecture of Why2-Atlas: A coach for\nqualitative physics essay writing. In Proc. Intelligent Tu-\ntoring Systems.\nVasilescu, F., Langlais, P., and Lapalme, G. (2004). Evaluat-\ning variants of the lesk approach for disambiguating words.\nIn LREC-04, Lisbon, Portugal, pp. 633–636. ELRA.\nVeblen, T. (1899). Theory of the Leisure Class. Macmillan\nCompany, New York.\nVelikovich, L., Blair-Goldensohn, S., Hannan, K., and Mc-\nDonald, R. (2010). The viability of web-derived polarity\nlexicons. In NAACL HLT 2010, pp. 777–785.\nVendler, Z. (1967). Linguistics in Philosophy. Cornell Uni-\nversity Press, Ithaca, NY.\nVerhagen, M., Gaizauskas, R., Schilder, F., Hepple, M.,\nMoszkowicz, J., and Pustejovsky, J. (2009). The tempeval\nchallenge: identifying temporal relations in text. Language\nResources and Evaluation, 43(2), 161–179.\nVerhagen, M., Mani, I., Sauri, R., Knippen, R., Jang, S. B.,\nLittman, J., Rumshisky, A., Phillips, J., and Pustejovsky,\nJ. (2005). Automating temporal annotation with tarsqi. In\nACL-05, pp. 81–84.",
  "590": "Resources and Evaluation, 43(2), 161–179.\nVerhagen, M., Mani, I., Sauri, R., Knippen, R., Jang, S. B.,\nLittman, J., Rumshisky, A., Phillips, J., and Pustejovsky,\nJ. (2005). Automating temporal annotation with tarsqi. In\nACL-05, pp. 81–84.\nVintsyuk, T. K. (1968). Speech discrimination by dynamic\nprogramming. Cybernetics, 4(1), 52–57. Russian Kiber-\nnetika 4(1):81-88. 1968.\nVinyals, O. and Le, Q. (2015).\nA neural conversational\nmodel. In Proceedings of ICML Deep Learning Workshop,\nLille, France.\nViterbi, A. J. (1967). Error bounds for convolutional codes\nand an asymptotically optimum decoding algorithm. IEEE\nTransactions on Information Theory, IT-13(2), 260–269.\nVoutilainen, A. (1995). Morphological disambiguation. In\nKarlsson, F., Voutilainen, A., Heikkil¨a, J., and Anttila,\nA. (Eds.), Constraint Grammar: A Language-Independent\nSystem for Parsing Unrestricted Text, pp. 165–284. Mouton\nde Gruyter.\nVoutilainen, A. (1999).\nHandcrafted rules.\nIn van Hal-\nteren, H. (Ed.), Syntactic Wordclass Tagging, pp. 217–246.\nKluwer.\nWade, E., Shriberg, E., and Price, P. J. (1992). User behav-\niors affecting speech recognition. In ICSLP-92, pp. 995–\n998.\nWagner, R. A. and Fischer, M. J. (1974).\nThe string-to-\nstring correction problem. Journal of the Association for\nComputing Machinery, 21, 168–173.\nWalker, M. A. (2000).\nAn application of reinforcement\nlearning to dialogue strategy selection in a spoken dialogue\nsystem for email.\nJournal of Artiﬁcial Intelligence Re-\nsearch, 12, 387–416.\nWalker, M. A., Fromer, J. C., and Narayanan, S. S. (1998).\nLearning optimal dialogue strategies: A case study of a\nspoken dialogue agent for email.\nIn COLING/ACL-98,\nMontreal, Canada, pp. 1345–1351.\nWalker, M. A., Kamm, C. A., and Litman, D. J. (2001).\nTowards developing general models of usability with PAR-\nADISE. Natural Language Engineering: Special Issue on\nBest Practice in Spoken Dialogue Systems, 6(3), 363–377.\nWalker, M. A. and Whittaker, S. (1990). Mixed initiative in\ndialogue: An investigation into discourse segmentation. In\nACL-90, Pittsburgh, PA, pp. 70–78.\nWang, H., Lu, Z., Li, H., and Chen, E. (2013). A dataset for\nresearch on short-text conversations.. In EMNLP 2013, pp.\n935–945.\nWang, S. and Manning, C. D. (2012).\nBaselines and bi-\ngrams: Simple, good sentiment and topic classiﬁcation. In\nACL 2012, pp. 90–94.\nWard, N. and Tsukahara, W. (2000).\nProsodic features\nwhich cue back-channel feedback in English and Japanese.\nJournal of Pragmatics, 32, 1177–1207.\nWard, W. and Issar, S. (1994).\nRecent improvements\nin the CMU spoken language understanding system.\nIn\nARPA Human Language Technologies Workshop, Plains-\nboro, N.J.\nWarriner, A. B., Kuperman, V., and Brysbaert, M. (2013).\nNorms of valence, arousal, and dominance for 13,915 En-\nglish lemmas. Behavior Research Methods, 45(4), 1191–\n1207.\nWeaver, W. (1949/1955). Translation. In Locke, W. N. and\nBoothe, A. D. (Eds.), Machine Translation of Languages,\npp. 15–23. MIT Press.",
  "591": "Norms of valence, arousal, and dominance for 13,915 En-\nglish lemmas. Behavior Research Methods, 45(4), 1191–\n1207.\nWeaver, W. (1949/1955). Translation. In Locke, W. N. and\nBoothe, A. D. (Eds.), Machine Translation of Languages,\npp. 15–23. MIT Press.\nReprinted from a memorandum\nwritten by Weaver in 1949.\nWeinschenk, S. and Barker, D. T. (2000). Designing Effec-\ntive Speech Interfaces. Wiley.\nWeischedel, R., Hovy, E. H., Marcus, M. P., Palmer, M.,\nBelvin, R., Pradhan, S., Ramshaw, L. A., and Xue, N.\n(2011). Ontonotes: A large training corpus for enhanced\nprocessing. In Joseph Olive, Caitlin Christianson, J. M.\n(Ed.), Handbook of Natural Language Processing and Ma-\nchine Translation: DARPA Global Automatic Language\nExploitation, pp. 54–63. Springer.\nWeischedel, R., Meteer, M., Schwartz, R., Ramshaw, L. A.,\nand Palmucci, J. (1993). Coping with ambiguity and un-\nknown words through probabilistic models. Computational\nLinguistics, 19(2), 359–382.",
  "592": "540\nBibliography\nWeizenbaum, J. (1966). ELIZA – A computer program for\nthe study of natural language communication between man\nand machine. Communications of the ACM, 9(1), 36–45.\nWeizenbaum, J. (1976). Computer Power and Human Rea-\nson: From Judgement to Calculation. W.H. Freeman and\nCompany.\nWen, T.-H., Gaˇsi´c, M., Kim, D., Mrkˇsi´c, N., Su, P.-H.,\nVandyke, D., and Young, S. J. (2015a).\nStochastic lan-\nguage generation in dialogue using recurrent neural net-\nworks with convolutional sentence reranking. In SIGDIAL\n2015, pp. 275––284.\nWen, T.-H., Gaˇsi´c, M., Mrkˇsi´c, N., Su, P.-H., Vandyke, D.,\nand Young, S. J. (2015b). Semantically conditioned lstm-\nbased natural language generation for spoken dialogue sys-\ntems. In EMNLP 2015.\nWhitelaw, C., Hutchinson, B., Chung, G. Y., and El-\nlis, G. (2009). Using the web for language independent\nspellchecking and autocorrection. In EMNLP-09, pp. 890–\n899.\nWidrow, B. and Hoff, M. E. (1960). Adaptive switching\ncircuits. In IRE WESCON Convention Record, Vol. 4, pp.\n96–104.\nWiebe, J. (1994). Tracking point of view in narrative. Com-\nputational Linguistics, 20(2), 233–287.\nWiebe, J. (2000). Learning subjective adjectives from cor-\npora. In AAAI-00, Austin, TX, pp. 735–740.\nWiebe, J., Bruce, R. F., and O’Hara, T. P. (1999). Devel-\nopment and use of a gold-standard data set for subjectivity\nclassiﬁcations. In ACL-99, pp. 246–253.\nWiebe, J., Wilson, T., and Cardie, C. (2005). Annotating ex-\npressions of opinions and emotions in language. Language\nresources and evaluation, 39(2-3), 165–210.\nWierzbicka, A. (1992). Semantics, Culture, and Cognition:\nUniversity Human Concepts in Culture-Speciﬁc Conﬁgura-\ntions. Oxford University Press.\nWierzbicka, A. (1996). Semantics: Primes and Universals.\nOxford University Press.\nWilcox-O’Hearn, L. A. (2014).\nDetection is the central\nproblem in real-word spelling correction. http://arxiv.\norg/abs/1408.3153.\nWilcox-O’Hearn, L. A., Hirst, G., and Budanitsky, A.\n(2008).\nReal-word spelling correction with trigrams: A\nreconsideration of the Mays, Damerau, and Mercer model.\nIn CICLing-2008, pp. 605–616.\nWilensky, R. (1983).\nPlanning and Understanding: A\nComputational Approach to Human Reasoning. Addison-\nWesley.\nWilks, Y. (1973). An artiﬁcial intelligence approach to ma-\nchine translation. In Schank, R. C. and Colby, K. M. (Eds.),\nComputer Models of Thought and Language, pp. 114–151.\nW.H. Freeman.\nWilks, Y. (1975a). An intelligent analyzer and understander\nof English. Communications of the ACM, 18(5), 264–274.\nWilks, Y. (1975b). Preference semantics. In Keenan, E. L.\n(Ed.), The Formal Semantics of Natural Language, pp.\n329–350. Cambridge Univ. Press.\nWilks, Y. (1975c). A preferential, pattern-seeking, seman-\ntics for natural language inference. Artiﬁcial Intelligence,\n6(1), 53–74.\nWilliams, J. D., Raux, A., and Henderson, M. (2016). The\ndialog state tracking challenge series: A review. Dialogue\n& Discourse, 7(3), 4–33.",
  "593": "Wilks, Y. (1975c). A preferential, pattern-seeking, seman-\ntics for natural language inference. Artiﬁcial Intelligence,\n6(1), 53–74.\nWilliams, J. D., Raux, A., and Henderson, M. (2016). The\ndialog state tracking challenge series: A review. Dialogue\n& Discourse, 7(3), 4–33.\nWilliams, J. D. and Young, S. J. (2007). Partially observ-\nable markov decision processes for spoken dialog systems.\nComputer Speech and Language, 21(1), 393–422.\nWilson, T., Wiebe, J., and Hoffmann, P. (2005). Recogniz-\ning contextual polarity in phrase-level sentiment analysis.\nIn HLT-EMNLP-05, pp. 347–354.\nWinkler, W. E. (2006). Overview of record linkage and cur-\nrent research directions.\nTech. rep., Statistical Research\nDivision, U.S. Census Bureau.\nWinograd, T. (1972).\nUnderstanding Natural Language.\nAcademic Press.\nWinston, P. H. (1977). Artiﬁcial Intelligence. Addison Wes-\nley.\nWitten, I. H. and Bell, T. C. (1991). The zero-frequency\nproblem: Estimating the probabilities of novel events in\nadaptive text compression. IEEE Transactions on Informa-\ntion Theory, 37(4), 1085–1094.\nWitten, I. H. and Frank, E. (2005). Data Mining: Practical\nMachine Learning Tools and Techniques (2nd Ed.). Mor-\ngan Kaufmann.\nWittgenstein, L. (1953).\nPhilosophical Investigations.\n(Translated by Anscombe, G.E.M.). Blackwell.\nWoods, W. A. (1967). Semantics for a Question-Answering\nSystem. Ph.D. thesis, Harvard University.\nWoods, W. A. (1973). Progress in natural language under-\nstanding. In Proceedings of AFIPS National Conference,\npp. 441–450.\nWoods, W. A. (1975). What’s in a link: Foundations for\nsemantic networks. In Bobrow, D. G. and Collins, A. M.\n(Eds.), Representation and Understanding: Studies in Cog-\nnitive Science, pp. 35–82. Academic Press.\nWoods, W. A. (1978). Semantics and quantiﬁcation in natu-\nral language question answering. In Yovits, M. (Ed.), Ad-\nvances in Computers, pp. 2–64. Academic.\nWoods, W. A., Kaplan, R. M., and Nash-Webber, B. L.\n(1972). The lunar sciences natural language information\nsystem: Final report. Tech. rep. 2378, BBN.\nWoodsend, K. and Lapata, M. (2015). Distributed represen-\ntations for unsupervised semantic role labeling. In EMNLP\n2015, pp. 2482–2491.\nWu, F. and Weld, D. S. (2007). Autonomously semantifying\nWikipedia. In CIKM-07, pp. 41–50.\nWu, F. and Weld, D. S. (2010). Open information extraction\nusing Wikipedia. In ACL 2010, pp. 118–127.\nWu, Z. and Palmer, M. (1994). Verb semantics and lexical\nselection. In ACL-94, Las Cruces, NM, pp. 133–138.\nWundt, W. (1900). V¨olkerpsychologie: eine Untersuchung\nder Entwicklungsgesetze von Sprache, Mythus, und Sitte.\nW. Engelmann, Leipzig. Band II: Die Sprache, Zweiter\nTeil.\nXia, F. and Palmer, M. (2001). Converting dependency struc-\ntures to phrase structures. In HLT-01, San Diego, pp. 1–5.\nXue, N. and Palmer, M. (2004). Calibrating features for se-\nmantic role labeling. In EMNLP 2004.\nYamada, H. and Matsumoto, Y. (2003). Statistical depen-\ndency analysis with support vector machines. In Noord,\nG. V. (Ed.), IWPT-03, pp. 195–206.",
  "594": "Xue, N. and Palmer, M. (2004). Calibrating features for se-\nmantic role labeling. In EMNLP 2004.\nYamada, H. and Matsumoto, Y. (2003). Statistical depen-\ndency analysis with support vector machines. In Noord,\nG. V. (Ed.), IWPT-03, pp. 195–206.\nYan, Z., Duan, N., Bao, J.-W., Chen, P., Zhou, M., Li, Z.,\nand Zhou, J. (2016). DocChat: An information retrieval ap-\nproach for chatbot engines using unstructured documents.\nIn ACL 2016.\nYang, Y., Yih, W.-t., and Meek, C. (2015).\nWikiqa: A\nchallenge dataset for open-domain question answering. In\nEMNLP 2015.\nYang, Y. and Pedersen, J. (1997). A comparative study on\nfeature selection in text categorization. In ICML, pp. 412–\n420.",
  "595": "Bibliography\n541\nYankelovich, N., Levow, G.-A., and Marx, M. (1995). De-\nsigning SpeechActs: Issues in speech user interfaces. In\nHuman Factors in Computing Systems: CHI ’95 Confer-\nence Proceedings, Denver, CO, pp. 369–376.\nYarowsky, D. (1995).\nUnsupervised word sense disam-\nbiguation rivaling supervised methods. In ACL-95, Cam-\nbridge, MA, pp. 189–196.\nYasseri, T., Kornai, A., and Kert´esz, J. (2012). A practical\napproach to language complexity: a Wikipedia case study.\nPloS one, 7(11).\nYih, W.-t., Richardson, M., Meek, C., Chang, M.-W., and\nSuh, J. (2016). The value of semantic parse labeling for\nknowledge base question answering.\nIn ACL 2016, pp.\n201–206.\nYngve, V. H. (1955). Syntax and the problem of multiple\nmeaning. In Locke, W. N. and Booth, A. D. (Eds.), Ma-\nchine Translation of Languages, pp. 208–226. MIT Press.\nYngve, V. H. (1970). On getting a word in edgewise. In\nCLS-70, pp. 567–577. University of Chicago.\nYoung, S. J., Gaˇsi´c, M., Keizer, S., Mairesse, F., Schatz-\nmann, J., Thomson, B., and Yu, K. (2010).\nThe Hid-\nden Information State model: A practical framework for\nPOMDP-based spoken dialogue management. Computer\nSpeech & Language, 24(2), 150–174.\nYounger, D. H. (1967). Recognition and parsing of context-\nfree languages in time n3. Information and Control, 10,\n189–208.\nYuret, D. (1998). Discovery of Linguistic Relations Using\nLexical Attraction. Ph.D. thesis, MIT.\nYuret, D. (2004). Some experiments with a Naive Bayes\nWSD system. In Senseval-3: 3rd International Workshop\non the Evaluation of Systems for the Semantic Analysis of\nText.\nZapirain, B., Agirre, E., M`arquez, L., and Surdeanu, M.\n(2013). Selectional preferences for semantic role classiﬁ-\ncation. Computational Linguistics, 39(3), 631–663.\nZavrel, J. and Daelemans, W. (1997). Memory-based learn-\ning: Using similarity for smoothing.\nIn ACL/EACL-97,\nMadrid, Spain, pp. 436–443.\nZelle, J. M. and Mooney, R. J. (1996). Learning to parse\ndatabase queries using inductive logic programming.\nIn\nAAAI-96, pp. 1050–1055.\nZeman, D. (2008). Reusable tagset conversion using tagset\ndrivers.. In LREC-08.\nZeman, D., Popel, M., Straka, M., Hajiˇc, J., Nivre, J., Gin-\nter, F., Luotolahti, J., Pyysalo, S., Petrov, S., Potthast, M.,\nTyers, F. M., Badmaeva, E., Gokirmak, M., Nedoluzhko,\nA., Cinkov´a, S., Hajic, Jr., J., Hlav´acov´a, J., Kettnerov´a,\nV., Uresov´a, Z., Kanerva, J., Ojala, S., Missil¨a, A., Man-\nning, C. D., Schuster, S., Reddy, S., Taji, D., Habash,\nN., Leung, H., de Marneffe, M.-C., Sanguinetti, M., Simi,\nM., Kanayama, H., de Paiva, V., Droganova, K., Alonso,\nH. M., C¸ ¨oltekin, C¸ ., Sulubacak, U., Uszkoreit, H., Macke-\ntanz, V., Burchardt, A., Harris, K., Marheinecke, K., Rehm,",
  "596": "M., Kanayama, H., de Paiva, V., Droganova, K., Alonso,\nH. M., C¸ ¨oltekin, C¸ ., Sulubacak, U., Uszkoreit, H., Macke-\ntanz, V., Burchardt, A., Harris, K., Marheinecke, K., Rehm,\nG., Kayadelen, T., Attia, M., El-Kahky, A., Yu, Z., Pitler,\nE., Lertpradit, S., Mandl, M., Kirchner, J., Alcalde, H. F.,\nStrnadov´a, J., Banerjee, E., Manurung, R., Stella, A., Shi-\nmada, A., Kwak, S., Mendonc¸a, G., Lando, T., Nitisaroj,\nR., and Li, J. (2017). Conll 2017 shared task: Multilin-\ngual parsing from raw text to universal dependencies. In\nProceedings of the CoNLL 2017 Shared Task: Multilingual\nParsing from Raw Text to Universal Dependencies, Van-\ncouver, Canada, August 3-4, 2017, pp. 1–19.\nZettlemoyer, L. and Collins, M. (2005). Learning to map\nsentences to logical form: Structured classiﬁcation with\nprobabilistic categorial grammars. In Uncertainty in Ar-\ntiﬁcial Intelligence, UAI’05, pp. 658–666.\nZhang, Y. and Clark, S. (2008). A tale of two parsers: inves-\ntigating and combining graph-based and transition-based\ndependency parsing using beam-search. In EMNLP-08, pp.\n562–571.\nZhang, Y. and Nivre, J. (2011).\nTransition-based depen-\ndency parsing with rich non-local features. In ACL 2011,\npp. 188–193.\nZhao, H., Chen, W., Kit, C., and Zhou, G. (2009). Mul-\ntilingual dependency learning: A huge feature engineering\nmethod to semantic dependency parsing. In CoNLL-09, pp.\n55–60.\nZhao, J., Wang, T., Yatskar, M., Ordonez, V., and Chang, K.-\nW. (2017). Men also like shopping: Reducing gender bias\nampliﬁcation using corpus-level constraints.\nIn EMNLP\n2017.\nZhong, Z. and Ng, H. T. (2010). It makes sense: A wide-\ncoverage word sense disambiguation system for free text.\nIn ACL 2010, pp. 78–83.\nZhou, D., Bousquet, O., Lal, T. N., Weston, J., and\nSch¨olkopf, B. (2004). Learning with local and global con-\nsistency. In NIPS 2004.\nZhou, G., Su, J., Zhang, J., and Zhang, M. (2005). Exploring\nvarious knowledge in relation extraction. In ACL-05, Ann\nArbor, MI, pp. 427–434.\nZhou, J. and Xu, W. (2015). End-to-end learning of seman-\ntic role labeling using recurrent neural networks. In ACL\n2015, pp. 1127–1137.\nZhu, X. and Ghahramani, Z. (2002).\nLearning from la-\nbeled and unlabeled data with label propagation. Tech. rep.\nCMU-CALD-02, CMU.\nZhu, X., Ghahramani, Z., and Lafferty, J. (2003).\nSemi-\nsupervised learning using gaussian ﬁelds and harmonic\nfunctions. In ICML 2003, pp. 912–919.\nZue, V. W., Glass, J., Goodine, D., Leung, H., Phillips, M.,\nPolifroni, J., and Seneff, S. (1989). Preliminary evalua-\ntion of the VOYAGER spoken language system.\nIn Pro-\nceedings DARPA Speech and Natural Language Workshop,\nCape Cod, MA, pp. 160–167.\nZwicky, A. and Sadock, J. M. (1975). Ambiguity tests and\nhow to fail them. In Kimball, J. (Ed.), Syntax and Seman-",
  "597": "tion of the VOYAGER spoken language system.\nIn Pro-\nceedings DARPA Speech and Natural Language Workshop,\nCape Cod, MA, pp. 160–167.\nZwicky, A. and Sadock, J. M. (1975). Ambiguity tests and\nhow to fail them. In Kimball, J. (Ed.), Syntax and Seman-\ntics 4, pp. 1–36. Academic Press.",
  "598": "Author Index\nˇSevˇc´ıkov´a, M., 274, 293\nˇStˇep´anek, J., 274, 293\nAarseth, P., 375\nAbadi, M., 145\nAbeill´e, A., 267, 268\nAbelson, R. P., 351, 362,\n375\nAbney, S. P., 174, 235, 263,\n264, 267, 268, 514\nAdriaans, P., 268\nAgarwal, A., 145\nAggarwal, C. C., 80\nAgichtein, E., 340, 354\nAgirre, E., 126, 376, 504,\n510, 514\nAgrawal, M., 393–395\nAhmad, F., 488\nAhn, D., 35\nAhn, Y.-Y., 384\nAho, A. V., 235, 275\nAjdukiewicz, K., 214\nAkoglu, L., 514\nAlcalde, H. F., 291\nAlGhamdi, F., 21\nAlgoet, P. H., 58\nAllen, J., 323, 349, 350,\n355, 459, 460\nAlonso, H. M., 81, 291\nAmsler, R. A., 514\nAn, J., 384\nAnderson, A. H., 452\nAndrew, G., 35\nAndroutsopoulos, I., 80\nAngelard-Gontier, N., 430,\n443\nAntiga, L., 145\nAnttila, A., 175, 293\nAppelt, D. E., 352–354\nAppling, S., 443\nArons, B., 443\nArtstein, R., 430\nAseltine, J., 354\nAtkins, S., 505\nAtkinson, K., 490\nAuer, S., 336, 412\nAuli, M., 429\nAustin, J. L., 447, 459\nAwadallah, A. H., 454\nBa, J., 145\nBaayen, R. H., 35\nBabko-Malaya, O., 514\nBacchiani, M., 61\nBaccianella, S., 387, 514\nBach, K., 447\nBackus, J. W., 195, 220\nBadmaeva, E., 291\nBahl, L. R., 60, 174\nBaker, C., 376\nBaker, C. F., 362\nBaker, J. K., 60, 245, 267\nBaldwin, T., 72, 80, 514\nBallard, D. H., 150\nBallesteros, M., 333, 354\nBalogh, J., 443, 449, 455\nBanaji, M. R., 125\nBanea, C., 126, 504\nBanerjee, E., 291\nBanerjee, S., 502\nBangalore, S., 267\nBanko, M., 355, 405, 408\nBansal, M., 338, 354\nBao, J.-W., 429\nBar-Hillel, Y., 214\nBarham, P., 145\nBarker, D. T., 455\nBarrett, L., 250, 267\nBates, R., 459\nBauer, F. L., 220\nBaum, L. E., 474, 479\nBaum, L. F., 442\nBaumer, G., 81\nBayes, T., 65\nBazell, C. E., 220\nBear, J., 352–354\nBecker, C., 336, 412\nBeil, F., 376\nBejˇcek, E., 274, 293\nBell, E., 376, 395, 396\nBell, T. C., 61, 255\nBellegarda, J. R., 61, 128,\n444\nBellman, R., 31, 35\nBelvin, R., 274\nBender, E. M., 221\nBengio, S., 430\nBengio, Y., 61, 101, 119,\n129, 135–137, 145,",
  "599": "Bell, T. C., 61, 255\nBellegarda, J. R., 61, 128,\n444\nBellman, R., 31, 35\nBelvin, R., 274\nBender, E. M., 221\nBengio, S., 430\nBengio, Y., 61, 101, 119,\n129, 135–137, 145,\n150, 430, 436\nBeˇnuˇs, ˇS., 459\nBerant, J., 414, 415, 419,\n421\nBerg-Kirkpatrick, T.,\n77–79, 81\nBerger, A., 100\nBergsma, S., 376, 489\nBernstein, M. S., 398\nBethard, S., 21, 355, 376\nBever, T. G., 265\nBhat, I., 285\nBhat, R. A., 285\nBhembe, D., 424\nBiber, D., 221\nBies, A., 208, 210\nBikel, D. M., 255, 268, 354\nBills, S., 341, 354\nBirch, A., 27, 29\nBird, S., 35\nBisani, M., 81\nBishop, C. M., 81, 100\nBizer, C., 336, 412\nBj¨orkelund, A., 368\nBlack, E., 263, 264, 267,\n514\nBlair, C. R., 491\nBlair, E., 21\nBlair-Goldensohn, S., 397,\n514\nBlei, D. M., 129, 514\nBlodgett, S. L., 21, 47, 72\nBloomﬁeld, L., 211, 220,\n356\nBlunsom, P., 419, 421\nBobrow, D. G., 323, 374,\n375, 423, 431, 432,\n444\nBobrow, R. J., 444\nBod, R., 267\nBogomolov, M., 81\nBoguraev, B., 514\nBoguraev, B. K., 421\nBohus, D., 455\nBojanowski, P., 129\nBollacker, K., 336, 412\nBolukbasi, T., 125, 126\nBooth, R. J., 71, 382, 383\nBooth, T. L., 238, 267\nBordes, A., 404, 410, 421\nBorges, J. L., 63\nBoser, B., 150\nBoser, B. E., 150\nB¨ottner, M., 424\nBottou, L., 119, 129, 354,\n375\nBourlard, H., 150\nBousquet, O., 397\nBowman, S. R., 430\nBoyd-Graber, J., 421, 514\nBrachman, R. J., 322, 323\nBrants, T., 56, 166, 167,\n174, 175\nBr´eal, M., 103\nBreck, E., 409, 421\nBresnan, J., 214, 220, 237\nBrevdo, E., 145\nBrill, E., 267, 405, 408,\n489, 490\nBrin, S., 354\nBriscoe, T., 264, 267, 514\nBroadhead, M., 355\nBrockett, C., 429\nBrockman, W., 124\nBrockmann, C., 372\nBrody, S., 514\nBroschart, J., 152\nBruce, B. C., 460\nBruce, R., 514\nBruce, R. F., 378, 505\nBrysbaert, M., 381–383\nBryson, J. J., 125\nBu, J., 514\nBuchholz, S., 294\nBuck, C., 48\nBudanitsky, A., 104, 492\nBullinaria, J. A., 129\nBulyko, I., 61, 454\nBunker, R. T., 505\nBurchardt, A., 291",
  "600": "Bryson, J. J., 125\nBu, J., 514\nBuchholz, S., 294\nBuck, C., 48\nBudanitsky, A., 104, 492\nBullinaria, J. A., 129\nBulyko, I., 61, 454\nBunker, R. T., 505\nBurchardt, A., 291\nBurger, J. D., 409, 421\nBurges, C. J. C., 419, 428\nBurget, L., 129\nBurkett, D., 77–79, 81\nBurnett, D., 442\nCafarella, M., 355\nCaliskan, A., 125\nCandito, M., 294\nCanon, S., 268\nCardie, C., 126, 354, 504,\n514\nCarletta, J., 452\nCarmel, D., 421\nCarpenter, B., 458\nCarpenter, R., 424, 429\nCarreras, X., 368, 375\nCarroll, G., 268, 376\nCarroll, J., 264, 267\nCarstensen, F. V., 174\nCasta˜no, J., 345, 347, 349\nCelikyilmaz, A., 436\nCer, D., 126, 504\nˇCernock`y, J. H., 129\nChahuneau, V., 391, 392\nChai, J. Y., 376\nChambers, N., 352, 355,\n372\nChanan, G., 145\nChanev, A., 293\nChang, A., 21\nChang, A. X., 347, 355,\n414, 418\nChang, J. S., 514\nChang, K.-W., 125, 126\nChang, M.-W., 419\nChang, P.-C., 35\nCharles, W. G., 397, 504\nCharlin, L., 428, 430\nCharniak, E., 174, 211, 250,\n267, 268, 513\nChe, W., 375\nChelba, C., 242\nChen, B., 398\nChen, C., 514\nChen, D., 283, 404, 410,\n421\nChen, E., 429\nChen, J. N., 514\nChen, K., 119, 127, 129\nChen, M. Y., 397\nChen, P., 429\nChen, S. F., 53, 55, 61\nChen, W., 375\nChen, X., 107\nChen, Y.-N., 436\nChen, Z., 145\nCherry, C., 429\nChi, Z., 268\nChiang, D., 268\nChiba, S., 479\nChierchia, G., 323\nChinchor, N., 81, 354\nChintala, S., 145\nChiticariu, L., 333\nChklovski, T., 514\n543",
  "601": "544\nAuthor Index\nChodorow, M. S., 500\nChoi, E., 419\nChoi, J. D., 285, 294\nChoi, Y., 376, 395, 396,\n419, 514\nChomsky, C., 322, 412, 420\nChomsky, N., 60, 195, 213,\n220\nChou, A., 414, 419\nChow, Y.-L., 268\nChristodoulopoulos, C.,\n175\nChu, Y.-J., 288\nChu-Carroll, J., 421, 458,\n459\nChung, G. Y., 488, 489\nChurch, A., 308\nChurch, K. W., 22, 51, 53,\n56, 61, 116, 174,\n235, 372, 481,\n483–485, 492, 507,\n511\nCiaramita, M., 294\nCinkov´a, S., 291\nCitro, C., 145\nClark, A., 268\nClark, C., 421\nClark, E., 103\nClark, H. H., 20, 445, 448,\n459\nClark, J. H., 55, 56, 61\nClark, K., 384–386\nClark, P., 419\nClark, S., 129, 175, 268,\n283\nCoccaro, N., 128, 459\nCohen, D. N., 479\nCohen, K. B., 327\nCohen, M. H., 443, 449,\n455\nCohen, P. R., 459, 460\nColaresi, M. P., 390–392\nColby, K. M., 427, 428, 444\nCole, R. A., 442\nCollins, M., 61, 211–213,\n248, 250, 251, 255,\n264, 267, 268, 275,\n413\nCollobert, R., 119, 129,\n354, 375\nColombe, J. B., 354\nColosimo, M., 354\nC¸ ¨oltekin, C¸ ., 291\nConrad, S., 221\nConrath, D. W., 502\nConti, J., 442\nCook, P., 514\nCopestake, A., 514\nCoppola, B., 421\nCorrado, G. S., 119, 127,\n129\nCotton, S., 505\nCottrell, G. W., 513\nCourville, A., 101,\n135–137, 150\nCover, T. M., 57, 58\nCovington, M., 277, 293\nCowhey, I., 419\nCox, D., 99\nCox, S. J., 81\nCrammer, K., 293\nCraven, M., 354\nCrouch, R., 268\nCruse, D. A., 129\nCucerzan, S., 489\nCulicover, P. W., 221\nCurran, J. R., 175, 268\nCyganiak, R., 336, 412\nDaelemans, W., 267\nDagan, I., 116, 118, 129\nDai, A. M., 430\nDamerau, F. J., 481, 486,\n487, 491, 492\nDang, H. T., 354, 359, 375,\n505, 506, 514\nDanieli, M., 441, 455\nDanilevsky, M., 333\nDas, D., 293, 421\nDas, S. R., 397\nDauphin, Y., 436\nDavidson, D., 312, 323\nDavidson, T., 443\nDavies, M., 124\nDavis, A., 145\nDavis, E., 323\nDay, D. S., 349\nDayanidhi, K., 437\nDean, J., 56, 119, 127, 129,\n145\nDeerwester, S. C., 128\nDeJong, G. F., 354, 375\nDelfs, L., 505\nDella Pietra, S. A., 100",
  "602": "Davis, A., 145\nDavis, E., 323\nDay, D. S., 349\nDayanidhi, K., 437\nDean, J., 56, 119, 127, 129,\n145\nDeerwester, S. C., 128\nDeJong, G. F., 354, 375\nDelfs, L., 505\nDella Pietra, S. A., 100\nDella Pietra, V. J., 100\nDemner-Fushman, D., 327\nDempster, A. P., 474, 484\nDeng, L., 436\nDenker, J. S., 150\nDerczynski, L., 355\nDeRose, S. J., 174\nDesmaison, A., 145\nDevin, M., 145\nDeVito, Z., 145\nde Lacalle, O. L., 514\nde Marneffe, M.-C., 155,\n173, 208, 272, 291,\n293\nde Paiva, V., 291\nde Villiers, J. H., 442\nDiab, M., 21, 126, 504, 514\nDienes, P., 172\nDigman, J. M., 393\nDi Marco, A., 514\nDligach, D., 512\nDo, Q. N. T., 376\nDoddington, G., 194\nDoherty-Sneddon, G., 452\nDolan, B., 429\nDolan, W. B., 514\ndos Santos, C., 338, 354\nDowney, D., 355\nDowty, D. R., 323, 373\nDozat, T., 272, 291, 293\nDroganova, K., 291\nDror, R., 81\nDuan, N., 429\nDubou´e, P. A., 421\nDucharme, R., 119, 129,\n145\nDuda, R. O., 512\nDumais, S. T., 71, 80, 126,\n128, 405, 408, 504\nDunphry, D., 71, 381, 397\nDyer, C., 333, 354, 419\nDziurzynski, L., 393–395\nEagon, J. A., 479\nEarley, J., 226, 235\nEbert, S., 398\nEckert, W., 460\nEdmonds, J., 288\nEdmonds, P., 514\nEfron, B., 78\nEgghe, L., 35\nEichstaedt, J. C., 393–395\nEisner, J., 268, 293, 466\nEjerhed, E. I., 235\nEkman, P., 380\nEl-Kahky, A., 291\nElisseeff, A., 81\nEllis, G., 488, 489\nEllsworth, M., 362, 363\nElman, J. L., 150, 178\nErk, K., 376\nEryigit, G., 293\nEspeholt, L., 421\nEsuli, A., 387, 514\nEtzioni, O., 342, 343, 354,\n355, 376, 414, 415,\n419\nEvanini, K., 437\nEvans, C., 336, 412\nEvans, N., 152\nEvert, S., 129\nFader, A., 342, 343, 355,\n414, 415\nFan, J., 421\nFano, R. M., 116\nFanshel, D., 452\nFanty, M., 442\nFarhadi, A., 421\nFarkas, R., 294\nFast, E., 398\nFeldman, J. A., 150\nFellbaum, C., 497, 505, 514\nFeng, S., 421, 514\nFennig, C. D., 21\nFensel, D., 321\nFerguson, J., 466\nFerguson, M., 208, 210\nFerro, L., 345, 347, 349\nFerrucci, D. A., 421\nFessler, L., 443\nFikes, R. E., 460",
  "603": "Feng, S., 421, 514\nFennig, C. D., 21\nFensel, D., 321\nFerguson, J., 466\nFerguson, M., 208, 210\nFerro, L., 345, 347, 349\nFerrucci, D. A., 421\nFessler, L., 443\nFikes, R. E., 460\nFillmore, C. J., 220, 221,\n322, 357, 362, 374,\n375\nFinegan, E., 221\nFinkelstein, L., 126, 504\nFirth, J. R., 101, 106, 459\nFisch, A., 404, 410, 421\nFischer, M. J., 31, 479, 492\nFisher, D., 354\nFlickinger, D., 263, 264,\n268\nFodor, J. A., 128, 323, 370\nFodor, P., 421\nFokoue-Nkoutche, A., 421\nFoland, Jr., W. R., 375\nFolds, D., 443\nForbes-Riley, K., 423, 424\nForchini, P., 428\nForney, Jr., G. D., 479\nFosler, E., 41\nFoster, J., 294\nFox Tree, J. E., 20\nFrancis, H. S., 246\nFrancis, M. E., 71, 382, 383\nFrancis, W. N., 19, 174\nFrank, E., 81, 100\nFranz, A., 56, 267\nFraser, N. M., 442\nFreitag, D., 354\nFried, G., 443\nFriedman, J. H., 80, 100\nFromer, J. C., 455\nFrostig, R., 414, 419\nFung, P., 21\nFurnas, G. W., 128\nFyshe, A., 129\nGabow, H. N., 289\nGabrilovich, E., 126, 504\nGage, P., 27\nGaizauskas, R., 345, 347,\n349\nGale, W. A., 51, 53, 61,\n372, 481, 483–485,\n492, 507, 511\nGales, M. J. F., 61\nGalil, Z., 289\nGalley, M., 35, 429, 430\nGandhe, S., 430\nGao, J., 56, 429, 430, 436\nGardner, M., 421\nGarg, N., 126\nGarside, R., 174, 175\nGaˇsi´c, M., 450, 451, 458\nGauvain, J.-L., 61, 129\nGazdar, G., 207\nGdaniec, C., 263, 264\nGeman, S., 268\nGeorgila, K., 460\nGerber, L., 345, 347\nGerber, M., 376\nGerbino, E., 441, 455\nGerten, J., 430\nGertz, M., 347, 355\nGhahramani, Z., 397\nGhemawat, S., 145\nGhoneim, M., 21\nGiangola, J. P., 443, 449,\n455\nGil, D., 152\nGilbert, G. N., 442\nGildea, D., 366, 375\nGillick, L., 81\nGinter, F., 155, 173, 208,\n272, 291, 293\nGinzburg, J., 458",
  "604": "Author Index\n545\nGiuliano, V. E., 128\nGiv´on, T., 246\nGlass, J., 442\nGlennie, A., 235\nGodfrey, J., 19, 194\nGoebel, R., 376, 489\nGoenaga, I., 294\nGoffman, E., 375\nGojenola, K., 294\nGokirmak, M., 291\nGoldberg, J., 454\nGoldberg, Y., 118, 124,\n129, 150, 155, 173,\n208, 272, 293, 294\nGolding, A. R., 489\nGoldwater, S., 175\nGondek, D., 421\nGonnerman, L. M., 123\nGonzalez-Agirre, A., 126,\n504\nGood, M. D., 442\nGoodenough, J. B., 504\nGoodfellow, I., 135–137,\n145, 150\nGoodine, D., 442\nGoodman, J., 53, 55, 61,\n268\nGoodwin, C., 459\nGosling, S. D., 393\nGould, J. D., 442\nGould, S. J., 101\nGravano, A., 459\nGravano, L., 340, 354\nGrave, E., 129\nGreen, B. F., 322, 412, 420\nGreen, J., 220\nGreen, L., 21, 72\nGreen, S., 294\nGreenbaum, S., 221\nGreene, B. B., 174\nGreenwald, A. G., 125\nGrefenstette, E., 419, 421\nGregory, M. L., 246\nGrenager, T., 376\nGrishman, R., 263, 264,\n352, 354, 361\nGross, S., 145\nGrosz, B. J., 459, 460\nGrover, C., 331\nGruber, J. S., 357, 374\nGuo, Y., 375\nGusﬁeld, D., 34, 35\nGuyon, I., 81\nHabash, N., 291, 294\nHacioglu, K., 375\nHaddow, B., 27, 29\nHaghighi, A., 268\nHajiˇc, J., 155, 172, 173,\n208, 264, 272, 274,\n291, 293, 294\nHajiˇcov´a, E., 274, 293\nHajishirzi, H., 421\nHakkani-T¨ur, D., 172, 436\nHale, J., 264\nHall, J., 293\nHamilton, W. L., 125,\n384–386\nHanks, P., 116, 349\nHannan, K., 397, 514\nHansen, B., 442\nHarabagiu, S., 375, 404\nHarnish, R., 447\nHarper, M. P., 175\nHarris, K., 291\nHarris, R. A., 443\nHarris, Z. S., 101, 106, 174,\n235\nHarshman, R. A., 128\nHart, P. E., 512\nHart, T., 56\nHastie, T., 80, 100\nHathi, S., 80\nHatzivassiloglou, V., 385,\n386, 397\nHaverinen, K., 272, 293\nHawwari, A., 21\nHe, H., 419\nHe, L., 367, 375\nHe, X., 436\nHeaﬁeld, K., 48, 55, 56, 61\nHeaps, H. S., 20, 35\nHearst, M. A., 336, 337,\n342, 354, 511, 514\nHeck, L., 436\nHeckerman, D., 71, 80\nHeikkil¨a, J., 175, 293\nHeim, I., 308, 323\nHellmann, S., 336, 412\nHemphill, C. T., 194",
  "605": "Hearst, M. A., 336, 337,\n342, 354, 511, 514\nHeck, L., 436\nHeckerman, D., 71, 80\nHeikkil¨a, J., 175, 293\nHeim, I., 308, 323\nHellmann, S., 336, 412\nHemphill, C. T., 194\nHenderson, D., 150\nHenderson, J., 268, 460\nHenderson, M., 447\nHenderson, P., 443\nHendler, J. A., 321\nHendrickson, C., 174\nHendrickx, I., 354\nHendrix, G. G., 374\nHepple, M., 349\nHerdan, G., 20, 35\nHermann, K. M., 419, 421\nHermjakob, U., 404\nHickey, M., 38\nHilf, F. D., 427, 428, 444\nHill, F., 103, 126, 504\nHindle, D., 263, 264, 267\nHinkelman, E. A., 459\nHinton, G. E., 123, 142,\n145, 150\nHirschberg, J., 21, 453,\n454, 456, 458, 459\nHirschman, L., 81, 354,\n409, 421, 441, 442\nHirst, G., 104, 370, 492,\n513\nHjelmslev, L., 128\nHlav´acov´a, J., 291\nHobbs, J. R., 352–354\nHockenmaier, J., 219\nHoff, M. E., 149\nHoffmann, P., 71, 381\nHofmann, T., 129\nHolliman, E., 19\nHoltzman, A., 376, 396\nHopcroft, J. E., 199\nHopely, P., 174, 420\nHorning, J. J., 268\nHorvitz, E., 71, 80\nHouseholder, F. W., 175\nHovanyecz, T., 442\nHovy, D., 81\nHovy, E. H., 107, 274, 354,\n386, 404, 514\nHoward, R. E., 150\nHsu, B.-J., 61\nHu, M., 71, 381, 386\nHuang, E. H., 126, 504\nHuang, L., 268, 283\nHuang, Z., 354\nHubbard, W., 150\nHuddleston, R., 200, 221\nHudson, R. A., 293\nHuffman, S., 354\nHull, D. A., 99, 128\nHulteen, E. A., 443\nHunter, P., 437\nHutchinson, B., 488, 489\nHutto, C. J., 443\nHuval, B., 354\nHymes, D., 375\nIacobacci, I., 506\nIngria, R., 263, 264, 345,\n347, 349, 444\nIrons, E. T., 235\nIrving, G., 145\nIsard, A., 452\nIsard, M., 145\nIsard, S., 452\nIsbell, C. L., 429\nISO8601, 346, 347\nIsrael, D., 352–354\nIssar, S., 434\nIyyer, M., 419\nJackel, L. D., 150\nJackendoff, R., 221, 315\nJacobs, P. S., 354\nJacobson, N., 174\nJaech, A., 80\nJafarpour, S., 428\nJang, S. B., 346\nJauhiainen, T., 80\nJauvin, C., 119, 129, 145\nJefferson, G., 449, 451\nJeffreys, H., 60\nJekat, S., 450\nJelinek, F., 52, 60, 165, 242,\n263, 264, 267, 479\nJi, H., 354\nJi, Y., 429",
  "606": "Jauhiainen, T., 80\nJauvin, C., 119, 129, 145\nJefferson, G., 449, 451\nJeffreys, H., 60\nJekat, S., 450\nJelinek, F., 52, 60, 165, 242,\n263, 264, 267, 479\nJi, H., 354\nJi, Y., 429\nJia, R., 409\nJia, Y., 145\nJiang, J. J., 502\nJim´enez, V. M., 268\nJ´ınov´a, P., 274, 293\nJohannsen, A., 81\nJohansson, R., 294, 375\nJohansson, S., 221\nJohnson, C. R., 362, 363\nJohnson, M., 249, 268, 315\nJohnson, W. E., 61\nJohnson-Laird, P. N., 362\nJohnston, M., 458\nJones, M. P., 128, 492\nJones, R., 340, 354, 454\nJones, S. J., 442\nJones, T., 21\nJoos, M., 101, 106, 127\nJordan, M. I., 87, 129\nJordan, P. W., 424\nJoshi, A. K., 174, 214, 221,\n237, 267, 268, 420\nJoshi, M., 419\nJoulin, A., 129\nJozefowicz, R., 145, 430\nJuang, B. H., 479\nJurafsky, D., 21, 35, 41, 47,\n72, 107, 114, 125,\n126, 128, 173, 341,\n342, 352, 354, 366,\n372, 375, 384–386,\n391–393, 430, 459,\n514\nJurgens, D., 21, 47, 72, 512\nJusteson, J. S., 397\nKaiser, L., 145\nKalai, A. T., 125, 126\nKalyanpur, A., 421\nKameyama, M., 352–354\nKamm, C. A., 441\nKanayama, H., 291, 421\nKanerva, J., 291\nKang, J. S., 514\nKannan, A., 430\nKaplan, R. M., 226, 268,\n374, 421, 423, 431,\n432, 444\nKarlen, M., 119, 129, 354,\n375\nKarlsson, F., 175, 293\nKarttunen, L., 174, 420\nKasami, T., 223, 235\nKashyap, R. L., 492\nKatz, C., 220\nKatz, G., 345, 347, 349\nKatz, J. J., 128, 323, 370\nKatz, K., 208, 210\nKatz, S. M., 397\nKavukcuoglu, K., 119, 129,\n354, 375\nKawahara, D., 294\nKawakami, K., 333, 354\nKawamoto, A. H., 513\nKay, M., 226, 235, 374,\n423, 431, 432, 444\nKay, P., 220, 221\nKay, W., 421\nKayadelen, T., 291\nKe, N. R., 443\nKearns, M., 429, 454, 460\nKeizer, S., 450, 451\nKeller, F., 372, 489\nKelly, E. F., 513\nKembhavi, A., 421\nKern, M. L., 393–395\nKernighan, M. D., 481,\n483–485, 492\nKert´esz, J., 35\nKettnerov´a, V., 274, 291,\n293",
  "607": "546\nAuthor Index\nKhoddam, E., 376\nKhot, T., 419\nKhudanpur, S., 129\nKiela, D., 129\nKilgarriff, A., 505–508\nKim, D., 458\nKim, G., 208, 210\nKim, S. M., 386\nKim, S. N., 354\nKing, L. A., 393\nKing, T. H., 268\nKingma, D., 145\nKingsbury, P., 375\nKintsch, W., 128, 322\nKiparsky, P., 356\nKipper, K., 359, 375\nKirchhoff, K., 454\nKit, C., 375\nKlapaftis, I. P., 512\nKlavans, J. L., 263, 264\nKleene, S. C., 34\nKlein, A., 450\nKlein, D., 77–79, 81, 171,\n172, 175, 249, 250,\n259, 267, 268\nKlein, E., 35, 207\nKlein, S., 174, 175, 420\nKlementiev, A., 376\nKneser, R., 53, 54\nKnippen, R., 346\nKnuth, D. E., 491\nKobilarov, G., 336, 412\nKocisky, T., 421\nKoˇcisk`y, T., 419\nKoehn, P., 55, 56, 61\nKol´aˇrov´a, V., 274, 293\nKoller, D., 268\nKombrink, S., 129\nKondrak, G., 488\nKoo, T., 268\nKorhonen, A., 103, 126,\n504\nKormann, D., 429\nKornai, A., 35\nKosinski, M., 393–395\nKowtko, J. C., 452\nKozareva, Z., 354\nKraemer, H. C., 428\nKratzer, A., 308, 323\nKrieger, M., 35\nKrizhevsky, A., 145\nKrovetz, R., 27, 511\nKruskal, J. B., 35, 479\nK¨ubler, S., 293, 294\nKuˇcera, H., 19, 174\nKudlur, M., 145\nKudo, T., 293\nKuhlmann, M., 294\nKukich, K., 486, 492\nKuksa, P., 119, 129, 354,\n375\nKulkarni, R. G., 454\nKullback, S., 371\nKumlien, J., 354\nKuno, S., 235\nKuperman, V., 381–383\nKupiec, J., 174\nKwak, H., 384\nKwak, S., 291\nKwan, J. L. P., 421\nKwiatkowski, T., 421\nLabov, W., 99, 452\nLafferty, J., 397\nLafferty, J. D., 100, 171,\n267, 354\nLaham, D., 128\nLaird, N. M., 474, 484\nLakoff, G., 315, 323, 373\nLal, T. N., 397\nLally, A., 421\nLamblin, P., 150\nLample, G., 333, 354\nLandauer, T. K., 126, 128,\n442, 504\nLandes, S., 505\nLando, T., 291\nLang, J., 376\nLanger, S., 38\nLanglais, P., 508\nLapalme, G., 508\nLapata, M., 372, 376, 489,\n509, 510, 514\nLapesa, G., 129\nLari, K., 245, 268\nLarochelle, H., 150\nLau, J. H., 514\nLaughery, K., 322, 412, 420\nLazo, M., 349",
  "608": "Lapalme, G., 508\nLapata, M., 372, 376, 489,\n509, 510, 514\nLapesa, G., 129\nLari, K., 245, 268\nLarochelle, H., 150\nLau, J. H., 514\nLaughery, K., 322, 412, 420\nLazo, M., 349\nLa Polla, R., 221\nLe, Q., 429\nLeacock, C., 500, 505\nLeCun, Y., 150\nLee, C.-H., 436, 444\nLee, D. D., 129\nLee, K., 367, 375, 421\nLee, L., 80, 397\nLeech, G., 175, 221\nLehmann, J., 336, 412\nLehnert, W. G., 354\nLeibler, R. A., 371\nLemon, O., 460\nLerer, A., 145\nLertpradit, S., 291\nLesk, M. E., 508, 513\nLeskovec, J., 125, 384–386\nLeung, H., 291, 442\nLeuski, A., 428, 430\nLevenberg, J., 145\nLevenshtein, V. I., 30\nLevesque, H. J., 323, 459\nLevin, B., 359, 375\nLevin, E., 436, 444, 460\nLevinson, S. C., 459\nLevow, G.-A., 442,\n453–455\nLevy, J. P., 129\nLevy, O., 118, 124, 129\nLevy, R., 264\nLewis, C., 442\nLewis, D. L., 81, 354\nLewis, M., 259, 367, 375\nLi, H., 429\nLi, J., 107, 291, 429, 430\nLi, X., 404, 406\nLi, Y., 333, 375\nLi, Z., 375, 429\nLiang, P., 409, 414, 415,\n419\nLiberman, M. Y., 263, 264,\n459\nLieberman, H., 321\nLieberman Aiden, E., 124\nLight, M., 409, 421\nLin, D., 264, 293, 376, 489,\n501, 502\nLin, J., 404, 408\nLin, Y., 124\nLin, Z., 145\nLind´en, K., 80\nLindsey, R., 322\nLiscombe, J., 437\nLitkowski, K. C., 375\nLitman, D. J., 423, 424,\n441, 453, 454, 456,\n460\nLittman, J., 345–347\nLittman, M., 384, 386\nLiu, A., 458\nLiu, B., 71, 80, 381, 386,\n514\nLiu, C.-W., 430\nLiu, T., 375\nLiu, T.-H., 288\nLiu, X., 61\nLlorens, H., 355\nLochbaum, K. E., 128, 460\nLoper, E., 35\nLopez-Gazpio, I., 126, 504\nL´opez de Lacalle, O., 510\nLopyrev, K., 409, 419\nLovins, J. B., 34\nLowe, J. B., 362\nLowe, R., 443\nLowe, R. T., 428, 430\nLu, Z., 429\nLuhn, H. P., 113\nLui, M., 72, 80\nLuotolahti, J., 291\nLyons, J., 323\nLytel, D., 513\nMa, X., 354\nMacCartney, B., 293\nMacIntyre, R., 208, 210\nMacketanz, V., 291\nMacleod, C., 361\nMacy, M., 443\nMadhu, S., 513\nMagerman, D. M., 212,\n267, 275\nMaharjan, S., 21",
  "609": "Lytel, D., 513\nMa, X., 354\nMacCartney, B., 293\nMacIntyre, R., 208, 210\nMacketanz, V., 291\nMacleod, C., 361\nMacy, M., 443\nMadhu, S., 513\nMagerman, D. M., 212,\n267, 275\nMaharjan, S., 21\nMaier, E., 450\nMaier, W., 294\nMaiorano, S., 404\nMairesse, F., 379, 395, 450,\n451\nMakatchev, M., 424\nMaleck, I., 450\nManandhar, S., 512\nMandl, M., 291\nMan´e, D., 145\nMani, I., 345–347\nManning, C. D., 35, 80, 87,\n119, 123, 124, 126,\n129, 155, 171–173,\n175, 208, 245, 249,\n250, 259, 267, 268,\n272, 283, 291, 293,\n347, 354, 355, 376,\n397, 411, 414, 504\nManurung, R., 291\nMarcinkiewicz, M. A., 154,\n208, 210, 243, 274,\n293\nMarcus, M. P., 154, 185,\n208, 210, 243, 263,\n264, 267, 274, 293,\n375, 514\nMarcus, S., 116\nMarheinecke, K., 291\nMarinov, S., 293\nMaritxalar, M., 126, 504\nMarkov, A. A., 60, 479\nMarkovitch, S., 116\nMarlin, B. M., 355\nMaron, M. E., 80\nM`arquez, L., 294, 368, 375,\n376\nMarshall, C., 459\nMarshall, I., 174\nMarsi, E., 293, 294\nMart´ı, M. A., 294\nMartin, J. H., 128, 375,\n492, 514\nMartin, R., 459\nMartinez, D., 376\nMarx, M., 442, 455\nMarzal, A., 268\nMast, M., 450\nMasterman, M., 322, 513\nMatias, Y., 126, 504\nMatsumoto, Y., 293\nMausam, 376\nMausam., 354\nMaxwell III, J. T., 268\nMays, E., 481, 486, 487,\n491, 492\nMcCallum, A., 80, 100,\n171, 354, 355\nMcCarthy, D., 514\nMcCarthy, J., 220\nMcCawley, J. D., 221, 323\nMcClelland, J. L., 150\nMcConlogue, K., 420\nMcConnell-Ginet, S., 323\nMcCord, M. C., 421\nMcCulloch, W. S., 131, 149\nMcDaniel, J., 19\nMcDonald, R., 287, 293,\n294, 397, 514\nMcDonald, R. T., 155, 173,\n208, 272, 293\nMcEnery, A., 175\nMcFarland, D. A., 393\nMcGhee, D. E., 125\nMcGuiness, D. L., 321\nMcKeown, K. R., 385, 386,\n397\nMeek, C., 410, 419\nMehl, M. R., 393",
  "610": "Author Index\n547\nMel’˘cuk, I. A., 293\nMelis, G., 419\nMendonc¸a, G., 291\nMercer, R. L., 52, 60, 165,\n174, 267, 481, 486,\n487, 492\nMerialdo, B., 174\nMesnil, G., 436\nMeteer, M., 174, 459\nMetsis, V., 80\nMeyers, A., 294, 361, 375\nMichaelis, L. A., 246\nMichel, J.-B., 124\nMicrosoft., 423, 425, 429\nMihalcea, R., 126, 504,\n506, 514\nMikheev, A., 331\nMikolov, T., 61, 119, 124,\n127, 129\nMikulov´a, M., 274, 293\nMiller, G. A., 46, 60, 397,\n504, 505\nMiller, S., 255, 354, 444\nMinsky, M., 80, 134, 150,\n375\nMintz, M., 341, 354\nM´ırovsk´y, J., 274, 293\nMissil¨a, A., 291\nMitchell, M., 429\nMitchell, T. M., 129\nMitton, R., 492\nMiwa, M., 338, 354\nMoens, M., 331\nMoens, M.-F., 376\nMohammad, S. M., 381,\n382\nMoldovan, D., 514\nMonga, R., 145\nMonroe, B. L., 390–392\nMonroe, W., 430\nMontague, R., 323\nMonz, C., 405\nMooney, R. J., 413\nMoore, R. C., 489, 490\nMoore, S., 145\nMorante, R., 376\nMorgan, A. A., 354\nMorgan, N., 41, 150\nMorimoto, T., 459\nMorin, F., 61, 129\nMorris, W., 495\nMoschitti, A., 368\nMosteller, F., 65, 80\nMoszkowicz, J., 349\nMrkˇsi´c, N., 452, 458\nMulcaire, G., 80\nMurdock, J. W., 421\nMurphy, B., 129\nMurphy, K. P., 81, 100\nMurray, D., 145\nN´adas, A., 61\nNagao, M., 267\nNagata, M., 459\nNagy, P., 443\nNakov, P., 354\nNarayanan, A., 125\nNarayanan, S. S., 455\nNash-Webber, B. L., 374,\n421\nNaur, P., 220\nNavigli, R., 506, 509, 510,\n512, 514\nNedoluzhko, A., 274, 291,\n293\nNeedleman, S. B., 479\nNeff, G., 443\nNewell, A., 38\nNewman, D., 514\nNey, H., 53, 54, 81, 242\nNg, A. Y., 87, 126, 129,\n342, 354, 504, 514\nNg, H. T., 368, 421, 506\nNielsen, J., 442\nNielsen, M. A., 150\nNigam, K., 80, 100, 354\nNilsson, J., 293, 294\nNilsson, N. J., 460\nNIST, 35\nNitisaroj, R., 291\nNitta, Y., 116\nNivre, J., 155, 173, 208,\n272, 277, 283, 285,\n287, 291, 293, 294,\n375\nNiwa, Y., 116\nNoreen, E. W., 78\nNorman, D. A., 322, 374,\n375, 423, 431, 432,\n444, 448\nNorvig, P., 37, 136, 235,",
  "611": "272, 277, 283, 285,\n287, 291, 293, 294,\n375\nNiwa, Y., 116\nNoreen, E. W., 78\nNorman, D. A., 322, 374,\n375, 423, 431, 432,\n444, 448\nNorvig, P., 37, 136, 235,\n304, 323, 484, 487,\n488, 492\nNosek, B. A., 125\nNoseworthy, M., 430\nNovick, D. G., 442\nNunes, J. H. T., 459\nO’Connor, B., 21, 35, 47,\n72\nO’Hara, T. P., 378\nOch, F. J., 56\nOdell, M. K., 491\nOepen, S., 268\nOettinger, A. G., 235\nOﬂazer, K., 172\nOgilvie, D., 71, 381, 397\nOh, A. H., 456, 457\nOjala, S., 291\nOlah, C., 145\nOommen, B. J., 492\nOravecz, C., 172\nOrdonez, V., 126\nOrwant, J., 124\nOsborne, M., 56, 175\nO’S´eaghdha, D., 452\nOsgood, C. E., 105, 106,\n128, 381, 397\nOsindero, S., 150\nOstendorf, M., 61, 80, 454\nOzertem, U., 454\n´O S´eaghdha, D., 354\nPackard, D. W., 34\nPad´o, S., 294, 354\nPaliouras, G., 80\nPalmer, D., 35\nPalmer, M., 274, 275, 285,\n293, 359, 375, 376,\n500, 505, 506, 514\nPalmucci, J., 174\nPan, Y., 421\nPanevova, J., 293\nPanevov´a, J., 274, 293\nPang, B., 80, 397\nPao, C., 441\nPaolino, J., 443\nPapert, S., 134, 150\nPappuswamy, U., 424\nParikh, A., 421\nParitosh, P., 336, 412\nParsons, T., 312, 323\nPartee, B. H., 323\nPasca, M., 404, 405, 408\nPaszke, A., 145\nPatwardhan, S., 421\nPearl, C., 443\nPedersen, J., 81, 99, 124,\n128\nPedersen, T., 502, 514\nPeng, N., 338, 354\nPenn, G., 356\nPennacchiotti, M., 354\nPennebaker, J. W., 71, 382,\n383, 393\nPennington, J., 119, 123,\n124, 129, 411\nPercival, W. K., 220\nPereira, F. C. N., 171, 293,\n354\nPerkowitz, M., 174\nPerlis, A. J., 220\nPerrault, C. R., 460\nPeters, S., 323\nPeterson, J. L., 491\nPetrie, T., 479\nPetrov, S., 124, 155, 173,\n208, 250, 267, 272,\n291, 293, 294\nPetruck, M. R. L., 362, 363\nPhilips, L., 490\nPhillips, A. V., 420\nPhillips, J., 346\nPhillips, M., 442\nPicard, R. W., 378\nPieraccini, R., 436, 437,\n444, 460\nPilehvar, M. T., 506\nPineau, J., 428, 430, 443,\n460\nPitler, E., 291, 489\nPitts, W., 131, 149\nPlank, B., 81",
  "612": "Phillips, M., 442\nPicard, R. W., 378\nPieraccini, R., 436, 437,\n444, 460\nPilehvar, M. T., 506\nPineau, J., 428, 430, 443,\n460\nPitler, E., 291, 489\nPitts, W., 131, 149\nPlank, B., 81\nPlaut, D. C., 123\nPlutchik, R., 380, 381\nPol´akov´a, L., 274, 293\nPolifroni, J., 441, 442\nPollard, C., 211, 212, 214,\n220, 237\nPonzetto, S. P., 506\nPoon, H., 338, 354\nPopat, A. C., 56\nPopel, M., 291\nPopescu, A.-M., 355\nPopovici, D., 150\nPorter, M. F., 26, 27\nPotthast, M., 291\nPotts, C., 388, 389\nPouzyrevsky, I., 55, 56, 61\nPow, N., 430\nPradhan, S., 368, 375, 512\nPrager, J. M., 421\nPrakash, S., 514\nPrasettio, M. C., 376, 396\nPrescher, D., 376\nPrice, P. J., 453\nPrzepi´orkowski, A., 294\nPullum, G. K., 200, 207,\n221\nPurver, M., 458\nPustejovsky, J., 345–347,\n349, 355, 514\nPyysalo, S., 155, 173, 208,\n272, 291, 293\nQi, P., 291\nQin, B., 375\nQiu, G., 514\nQiu, Z., 421\nQiu, Z. M., 421\nQuantz, J., 450\nQuillian, M. R., 322, 499,\n513\nQuinn, K. M., 390–392\nQuirk, C., 338, 354\nQuirk, R., 221\nRabiner, L. R., 466, 476,\n477, 479\nRadev, D., 349\nRadford, A., 195, 221\nRaghavan, P., 80, 129\nRajpurkar, P., 409, 419\nRamshaw, L. A., 174, 185,\n264, 274, 514\nRanganath, R., 393\nRaphael, B., 322\nRappaport Hovav, M., 359\nRashkin, H., 376, 395, 396\nRatinov, L., 129\nRatnaparkhi, A., 100, 175,\n255, 267\nRau, L. F., 354\nRaux, A., 447\nRavichandran, D., 404\nRaviv, J., 492\nRaymond, C., 436\nReddy, S., 291\nReeves, R., 361\nRehder, B., 128\nRehm, G., 291\nReichart, R., 81, 103, 126,\n504\nReichenbach, H., 314\nReichert, T. A., 479\nReiss, F., 333\nReiss, F. R., 333\nRenshaw, E., 419\nResnik, P., 267, 370, 371,\n376, 500, 501, 514\nReynar, J. C., 267\nRibarov, K., 293",
  "613": "548\nAuthor Index\nRiccardi, G., 436\nRichardson, M., 419\nRiedel, S., 294, 354, 355\nRieger, C., 513\nRies, K., 459\nRiesbeck, C. K., 513\nRiezler, S., 268, 376\nRigau, G., 126, 504\nRiley, M., 61\nRiloff, E., 340, 354, 376,\n381, 397, 398, 421\nRingenberg, M., 424\nRitter, A., 354, 376,\n428–430\nRivlin, E., 126, 504\nRoark, B., 61, 268\nRodriguez, P., 421\nRohde, D. L. T., 123\nRomano, L., 354\nRooth, M., 267, 376\nRoque, A., 424\nRos´e, C., 424\nRosenblatt, F., 149\nRosenfeld, R., 61, 100\nRosenzweig, J., 505, 507,\n508\nRoth, D., 404, 406, 489\nRoth, R., 294\nRothe, S., 398\nRoukos, S., 263, 264, 267\nRoutledge, B. R., 391, 392\nRoy, N., 460\nRubenstein, H., 504\nRubin, D. B., 474, 484\nRubin, G. M., 174\nRudnicky, A. I., 455–457\nRumelhart, D. E., 142, 150,\n322\nRumshisky, A., 346\nRuppenhofer, J., 362, 363,\n376\nRuppin, E., 126, 504\nRussell, J. A., 380\nRussell, R. C., 491\nRussell, S., 37, 136, 304,\n323\nRutishauser, H., 220\nSabharwal, A., 419\nSacks, H., 451\nSadock, J. M., 298\nSag, I. A., 207, 211, 212,\n214, 220, 221, 237,\n458, 459\nSagae, K., 283\nSahami, M., 71, 80\nSakoe, H., 479\nSalakhutdinov, R. R., 145\nSalant, S., 421\nSaligrama, V., 125, 126\nSalomaa, A., 267\nSalton, G., 108, 128\nSamelson, K., 220\nSampson, G., 175\nSamuelsson, C., 167, 175\nSanﬁlippo, A., 264\nSankoff, D., 99, 479\nSantorini, B., 154, 208,\n243, 263, 264, 274,\n293\nSap, M., 376, 396\nSaraclar, M., 61\nSaur´ı, R., 345, 347, 349\nSauri, R., 346\nSchabes, Y., 267, 268\nSchaefer, E. F., 448\nSchalkwyk, J., 442\nSchank, R. C., 322, 351,\n362, 375\nSchapire, R. E., 174, 267\nSchasberger, B., 208, 210\nSchatzmann, J., 450, 451\nScheffczyk, J., 362, 363\nSchegloff, E. A., 449, 451\nScherer, K. R., 378, 379,\n393\nSchiebinger, L., 126\nSchilder, F., 349\nSchmandt, C., 443\nSchmelzenbach, M., 376\nSchmolze, J. G., 323\nSchoenick, C., 419\nSch¨olkopf, B., 397\nScholz, M., 293\nSchone, P., 128\nSch¨onkﬁnkel, M., 308\nSchreiner, M. E., 128\nSchuster, M., 145\nSchuster, S., 291",
  "614": "Schmelzenbach, M., 376\nSchmolze, J. G., 323\nSchoenick, C., 419\nSch¨olkopf, B., 397\nScholz, M., 293\nSchone, P., 128\nSch¨onkﬁnkel, M., 308\nSchreiner, M. E., 128\nSchuster, M., 145\nSchuster, S., 291\nSch¨utze, H., 80, 99, 124,\n128, 129, 174, 245,\n268, 372, 398, 511,\n514\nSchwartz, H. A., 393–395\nSchwartz, J. L. K., 125\nSchwartz, R., 174, 255,\n268, 354, 444\nSchwarz, J., 419\nSchwenk, H., 61, 129\nScott, M., 264\nS´eaghdha, D. O., 376\nSebastiani, F., 387, 514\nSeddah, D., 294\nSee, A., 349\nSegal, J., 41\nSekine, S., 264\nSelfridge, J. A., 46\nSen´ecal, J.-S., 61, 129\nSeneff, S., 441, 442\nSennrich, R., 27, 29\nSeo, M., 421\nSerban, I. V., 428, 430\nSethi, R., 235\nSetzer, A., 345, 347, 349\nSeung, H. S., 129\nSøgaard, A., 81, 175\nSgall, P., 293\nShah, A., 393–395\nShaked, T., 355\nShang, L., 429\nShannon, C. E., 46, 60, 492\nSharma, D., 285\nSheil, B. A., 235\nSheinwald, D., 421\nShepherd, J., 397, 398\nShi, T., 430\nShillcock, R., 264\nShima, H., 421\nShimada, A., 291\nShlens, J., 145\nShriberg, E., 453, 459\nShrivastava, M., 285\nSidner, C. L., 459, 460\nSiler, S., 424\nSilveira, N., 155, 173, 208,\n272, 293\nSimi, M., 291\nSimmons, R. F., 174, 175,\n322, 364, 374, 420,\n421, 513\nSimons, G. F., 21\nSinger, Y., 171, 172, 174,\n175, 267\nSingh, S., 376, 395, 396,\n429\nSingh, S. P., 460\nSinha, K., 443\nSleator, D., 267, 293\nSlocum, J., 374\nSmall, S. L., 513\nSmith, D. A., 268\nSmith, M., 71, 381, 397\nSmith, N. A., 80, 268, 391,\n392\nSmith, V. L., 445\nSmolensky, P., 150\nSnow, R., 341, 342, 354,\n514\nSocher, R., 119, 123, 124,\n126, 129, 354, 411,\n504\nSoderland, S., 342, 343,\n354, 355, 414\nSolan, Z., 126, 504\nSolorio, T., 21\nSordoni, A., 429\nSoroa, A., 510\nSparck Jones, K., 114, 128,\n513, 514\nSpencer, T., 289\nSpitkovsky, V. I., 418\nSporleder, C., 376\nSproat, R., 61\nSrinivas, B., 268\nSrivastava, N., 145\nSrivastava, R., 424\nStalnaker, R. C., 448\nStamatatos, E., 80\nSteedman, M., 175, 214,\n219, 259\nSteiner, B., 145",
  "615": "Sporleder, C., 376\nSproat, R., 61\nSrinivas, B., 268\nSrivastava, N., 145\nSrivastava, R., 424\nStalnaker, R. C., 448\nStamatatos, E., 80\nSteedman, M., 175, 214,\n219, 259\nSteiner, B., 145\nStent, A., 294\nˇStˇep´anek, J., 294\nStetina, J., 267\nStevenson, S., 375, 376\nStifelman, L. J., 443\nStillwell, D., 393–395\nStoics, 151\nStolcke, A., 41, 56, 61, 267,\n459\nStolz, W. S., 174\nStone, P., 71, 381, 397, 429\nStone, P. J., 513\nStoyanchev, S., 458\nStraka, M., 291\nStranˇa´k, P., 294\nStreeter, L., 128\nStrnadov´a, J., 291\nStr¨otgen, J., 347, 355\nStrzalkowski, T., 263, 264\nSturge, T., 336, 412\nStuttle, M., 460\nSu, J., 354\nSu, P.-H., 458\nSubramanian, S., 333, 354\nSuci, G. J., 105, 106, 128,\n381, 397\nSuendermann, D., 437\nSuh, J., 419\nSulubacak, U., 291\nSundheim, B., 345, 347,\n349, 352, 354\nSurdeanu, M., 294, 354,\n375, 376\nSutskever, I., 119, 129, 145\nSutton, S., 442\nSvartvik, J., 221\nSwerts, M., 453, 454, 456\nSwier, R., 376\nSwitzer, P., 128\nSzekely, R., 361\nSzpakowicz, S., 354\nTafjord, O., 419\nTajchman, G., 41\nTaji, D., 291\nTalbot, D., 56\nTalmor, A., 419\nTalukdar, P. P., 129\nTalwar, K., 145\nTanenhaus, M., 513\nTannen, D., 375\nTannenbaum, P. H., 105,\n106, 128, 174, 381,\n397\nTarjan, R. E., 289\nTaskar, B., 268\nTaylor, J., 336, 412\nTaylor, P., 459\nTeh, Y.-W., 150\nTemperley, D., 267, 293\nTengi, R. I., 505\nTeo, L. H., 421\nter Meulen, A., 323\nTesni`ere, L., 293, 374\nTetreault, J., 294\nThede, S. M., 175\nThelen, M., 421\nThibaux, R., 250, 267\nThomas, J. A., 57, 58\nThompson, C. W., 374\nThompson, H., 374, 423,\n431, 432, 444\nThompson, K., 34\nThompson, R. A., 238\nThomson, B., 450–452\nThrax, D., 151\nThrun, S., 460\nTibshirani, R. J., 78, 80, 94,\n100\nTillmann, C., 264",
  "616": "Author Index\n549\nTitov, I., 268, 376\nTomkins, S. S., 380\nToutanova, K., 171, 172,\n175, 268, 338, 354,\n490\nTowell, G., 505\nTraum, D., 428, 430\nTsarfaty, R., 155, 173, 208,\n272, 293, 294\nTseng, H., 35, 173\nTsukahara, W., 449\nTsvetkov, Y., 21, 47, 72\nTucker, P., 145\nT¨ur, G., 172, 436\nTurian, J., 129\nTurney, P. D., 381, 382,\n384, 386, 397\nTyers, F. M., 291\nTyson, M., 352–354\nUllman, J. D., 199, 235,\n275\nUngar, L. H., 393–395\nUresov´a, Z., 291\nUria, L., 126, 504\nUszkoreit, H., 291\nUzZaman, N., 355\nVaithyanathan, S., 80, 397\nvan Benthem, J., 323\nvan der Maaten, L., 123\nVan Ess-Dykema, C., 459\nvan Harmelen, F., 321\nvan Rijsbergen, C. J., 74,\n234, 263\nVan Valin, Jr., R. D., 221\nvan Wijnagaarden, A., 220\nvan Zaanen, M., 268\nVandyke, D., 458\nVanhoucke, V., 145\nVanLehn, K., 424\nVannella, D., 512\nVan Ooyen, B., 48\nVasilescu, F., 508\nVasserman, A., 268\nVasudevan, V., 145\nVauquois, B., 220\nVelikovich, L., 397, 514\nVendler, Z., 315\nVerhagen, M., 346, 349,\n355\nVermeulen, P. J. E., 442\nVersley, Y., 294\nVi´egas, F., 145\nVillemonte de la Cl´ergerie,\nE., 294\nVilnis, L., 430\nVincent, P., 101, 119, 129,\n145\nVincze, V., 294\nVintsyuk, T. K., 479\nVinyals, O., 145, 429, 430\nViterbi, A. J., 479\nVolkova, S., 376, 395, 396\nVoorhees, E. M., 505\nVoutilainen, A., 175, 293\nWade, E., 453\nWagner, R. A., 31, 479, 492\nWahlster, W., 321\nWalker, M. A., 379, 395,\n432, 441, 454, 455,\n460\nWall, R. E., 323\nWallace, D. L., 65, 80\nWang, H., 429\nWang, S., 80, 87, 397\nWang, T., 126\nWang, Y.-Y., 436\nWard, N., 449\nWard, W., 375, 434\nWarden, P., 145\nWarmsley, D., 443\nWarriner, A. B., 381–383\nWasow, T., 221\nWattenberg, M., 145\nWeaver, W., 513\nWeber, I., 443\nWeber, S., 427, 428, 444\nWegstein, J. H., 220\nWehbe, L., 129\nWeinschenk, S., 455\nWeischedel, R., 174, 255,\n274, 354, 514\nWeizenbaum, J., 10, 18,\n423, 425, 444\nWeld, D. S., 354, 355, 419\nWelty, C., 421\nWen, T.-H., 452, 458\nWessels, L. F. A., 442\nWeston, J., 119, 129, 354,",
  "617": "274, 354, 514\nWeizenbaum, J., 10, 18,\n423, 425, 444\nWeld, D. S., 354, 355, 419\nWelty, C., 421\nWen, T.-H., 452, 458\nWessels, L. F. A., 442\nWeston, J., 119, 129, 354,\n375, 397, 404, 410,\n421\nWhitelaw, C., 488, 489\nWhiteside, J. A., 442\nWhittaker, S., 432\nWicke, M., 145\nWidrow, B., 149\nWiebe, J., 71, 126, 378,\n381, 397, 504, 505,\n514\nWierzbicka, A., 128\nWilcox-O’Hearn, L. A.,\n488, 492\nWilde, O., 480\nWilensky, R., 460\nWilkes-Gibbs, D., 459\nWilks, Y., 322, 370, 374,\n513\nWilliams, J., 375\nWilliams, J. D., 447, 460\nWilliams, R., 354\nWilliams, R. J., 142, 150\nWilson, G., 345, 347\nWilson, R., 424\nWilson, T., 71, 381, 514\nWinkler, W. E., 491\nWinograd, T., 322, 323,\n374, 423, 431, 432,\n444\nWinston, P. H., 375\nWitten, I. H., 61, 81, 100,\n255\nWittgenstein, L., 106, 447,\n459\nWixon, D. R., 442\nWolf, A. K., 322, 412, 420\nWolfe, M. B. W., 128\nWoli´nski, M., 294\nWong, A. K. C., 479\nWoodger, M., 220\nWoodland, P. C., 61\nWoods, W. A., 322, 421\nWoodsend, K., 376\nWooters, C., 41\nWr´oblewska, A., 294\nWu, F., 354\nWu, Z., 500\nWundt, W., 195, 220\nWunsch, C. D., 479\nXia, F., 275, 293\nXiang, B., 338, 354\nXu, P., 56\nXu, W., 354, 375\nXue, N., 274, 294, 368, 375\nYamada, H., 293\nYan, Z., 429\nYang, E., 145\nYang, Y., 81, 410, 419\nYankelovich, N., 442, 455\nYao, K., 436\nYao, L., 354, 355\nYarowsky, D., 372, 507,\n510, 511, 514\nYasseri, T., 35\nYates, A., 355\nYatskar, M., 126, 419\nYeh, A. S., 354\nYih, W.-t., 124, 127, 338,\n354, 410, 419\nYngve, V. H., 235, 449\nYoung, B., 361\nYoung, S. J., 245, 268, 450,\n451, 458, 460\nYounger, D. H., 223, 235\nYu, D., 436\nYu, K., 354, 450, 451\nYu, Y., 145\nYu, Z., 291\nYuret, D., 268, 294, 509\nZampieri, M., 80\nZapirain, B., 376\nZavrel, J., 267\nZelle, J. M., 413\nZeman, D., 155, 173, 208,\n272, 291, 293\nZettlemoyer, L., 354, 367,\n375, 413, 415, 419\nZhai, C., 80\nZhang, J., 354, 409, 419\nZhang, L., 80, 421\nZhang, M., 354",
  "618": "Zelle, J. M., 413\nZeman, D., 155, 173, 208,\n272, 291, 293\nZettlemoyer, L., 354, 367,\n375, 413, 415, 419\nZhai, C., 80\nZhang, J., 354, 409, 419\nZhang, L., 80, 421\nZhang, M., 354\nZhang, Y., 283, 294, 368\nZhao, H., 375\nZhao, J., 126\nZheng, X., 145\nZhong, Z., 368, 506\nZhou, B., 338, 354\nZhou, D., 397\nZhou, G., 354, 375\nZhou, J., 375, 429\nZhou, M., 429\nZhu, H., 333\nZhu, X., 397, 514\nZielinska, V., 361\nZik´anov´a, ˇS., 274, 293\nZou, J., 126\nZou, J. Y., 125, 126\nZue, V. W., 441, 442\nZweig, G., 124, 127, 436\nZwicky, A., 298",
  "619": "Subject Index\nλ-reduction, 308\n*?, 15\n+?, 15\nF-measure, 263\n10-fold cross-validation, 76\n→(derives), 196\nˆ, 65, 482\n* (RE Kleene *), 13\n+ (RE Kleene +), 13\n. (RE any character), 13\n$ (RE end-of-line), 13\n( (RE precedence symbol),\n14\n[ (RE character\ndisjunction), 12\n\\B (RE non\nword-boundary), 14\n\\b (RE word-boundary),\n14\n] (RE character\ndisjunction), 12\nˆ (RE start-of-line), 13\n[ˆ] (single-char negation),\n12\n∃(there exists), 306\n∀(for all), 306\n=⇒(implies), 309\nλ-expressions, 308\nλ-reduction, 308\n∧(and), 306\n¬ (not), 306\n∨(or), 309\n4-gram, 42\n4-tuple, 198\n5-gram, 42\nAAVE, 21\nabduction, 311\nABox, 316\nABSITY, 513\nabsolute discounting, 53\nabsolute temporal\nexpression, 345\nabstract word, 382\naccomplishment\nexpressions, 315\naccuracy\nin WSD, 506\nachievement expressions,\n315, 316\nacknowledgment speech\nact, 448\nactivation, 132\nactivity expressions, 315,\n315\nadaptation\nlanguage model, 61\nadd-k, 51\nadd-one smoothing, 49\nadjacency pairs, 451\nadjective, 152, 203\nadjective phrase, 203\nadjunction in TAG, 221\nadverb, 152\ndays of the week coded\nas noun instead of,\n153\ndegree, 153\ndirectional, 153\nlocative, 153\nmanner, 153\nsyntactic position of, 203\ntemporal, 153\nadversarial evaluation, 430\naffective, 378\nafﬁx, 26\nagent, as thematic role, 357\nagglomerative clustering,\n512\nALGOL, 220\nalgorithm\nCKY, 225\nCorpus Lesk, 507, 508\nextended gloss overlap,\n502\nextended Lesk, 502\nforward, 470\nforward-backward, 478\ninside-outside, 245\nJiang-Conrath word\nsimilarity, 502\nKneser-Ney discounting,\n53\nLesk, 507\nLin word similarity, 502\nminimum edit distance,\n33\nn-gram tiling for question\nanswering, 408\nnaive Bayes classiﬁer, 65\npath-length based\nsimilarity, 500\npointwise mutual\ninformation, 116\nprobabilistic CKY, 243\nResnik word similarity,\n501\nsemantic role labeling,\n365\nSimpliﬁed Lesk, 507\nSoundex, 491\nunsupervised word sense\ndisambiguation, 512\nViterbi, 161, 471\nYarowsky, 510\nalignment, 30\nminimum cost, 32\nstring, 30\nvia minimum edit\ndistance, 32\nall-words task in WSD, 505\nAllen relations, 349\nambiguity\namount of part-of-speech\nin Brown corpus,\n156\nattachment, 224\ncoordination, 224, 225,\n248\nin meaning\nrepresentations, 298\npart-of-speech, 156\nPCFG in, 240\nprepositional phrase\nattachment, 246\nresolution of tag, 156\ntests distinguishing from\nvagueness, 298\nword sense, 504\nAmerican Structuralism,\n220\nanchor texts, 417\nanchors in regular\nexpressions, 13, 34\nanswer type, 404\nanswer type taxonomy, 404\nantonym, 103\nany-of, 75\nAP, 203\napproximate\nrandomization, 78\nARC, 419\narc eager, 284\narc standard, 277\nargmax, 482\nAristotle, 151, 315\narity, 312\narticle (part-of-speech), 153\naspect, 315\naspell, 490\nASR\nconﬁdence, 455\nassociation, 104\nATIS, 194\ncorpus, 197, 200\nATN, 375",
  "620": "ARC, 419\narc eager, 284\narc standard, 277\nargmax, 482\nAristotle, 151, 315\narity, 312\narticle (part-of-speech), 153\naspect, 315\naspell, 490\nASR\nconﬁdence, 455\nassociation, 104\nATIS, 194\ncorpus, 197, 200\nATN, 375\nATRANS, 373\nattachment ambiguity, 224\naugmentative\ncommunication, 38\nauthorship attribution, 63\nautocorrect, 488\nauxiliary verb, 154\nbackchannel, 449\nbackoff\nin smoothing, 51\nbackprop, 142\nbacktrace, 473\nin minimum edit\ndistance, 32\nBackus-Naur Form, 195\nbackward chaining, 310\nbackward composition, 216\nbackward probability, 474\nbag of word, 505\nbag of words, 65, 66\nbag-of-words, 65\nbarge-in, 441\nbaseline\nmost frequent sense, 507\ntake the ﬁrst sense, 507\nbasic emotions, 380\nBayes’ rule, 65, 482\ndropping denominator,\n66, 160, 482\nBayesian inference, 65, 482\nBDI, 460\nBeam search, 285\nbeam search, 165\nbeam width, 166, 285\nBerkeley Restaurant\nProject, 41\nBernoulli naive Bayes, 80\nbi-LSTM, 329\nbias term, 84, 132\nbidirectional RNN, 187\nbigram, 39\nbinary branching, 213\nbinary NB, 70\nbinary tree, 213\nbits for measuring entropy,\n57\nBloom ﬁlters, 56\nBNF (Backus-Naur Form),\n195\nbootstrap, 79\nbootstrap algorithm, 79\nbootstrap test, 78\nbootstrapping, 78, 510\nfor WSD, 510\ngenerating seeds, 511\nin IE, 339\nBPE, 27\nbracketed notation, 197\nBritish National Corpus\n(BNC)\nPOS tags for phrases,\n155\nBrown, 155\nBrown corpus, 19\noriginal tagging of, 174\nbyte-pair encoding, 27\ncandidates, 481\ncanonical form, 299\ncaptalization\nfor unknown words, 167\ncapture group, 18\ncardinal number, 203\ncascade, 27\nregular expression in\nEliza, 18\ncase\nsensitivity in regular\nexpression search,\n11\ncase folding, 24\ncase frame, 359, 374\ncategorial grammar, 214,\n214\nCD (conceptual\ndependency), 373\ncentroid, 115\n551",
  "621": "552\nSubject Index\nCFG, see context-free\ngrammar\nchain rule, 98, 142\nchannel model, 482, 483\ncharacter embeddings, 332\nCharniak parser, 250\nChatbots, 425\nchatbots, 10\nChinese\nword segmentation, 25\nChomsky normal form,\n213, 242\nChomsky-adjunction, 214\nchunking, 231, 232\nCIRCUS, 354\ncitation form, 102\nCKY algorithm, 223\nprobabilistic, 243\nclariﬁcation questions, 458\nclass-based n-gram, 61\nclause, 201\nclitic, 24\norigin of term, 151\nclosed class, 152\nclosed vocabulary, 48\nclustering\nin word sense\ndisambiguation, 514\nCNF, see Chomsky normal\nform\ncoarse senses, 514\nCOCA, 483\nCocke-Kasami-Younger\nalgorithm, see CKY\ncode switching, 21\ncollaborative completion,\n448\nCollins parser, 250\ncollocation, 505\ncombinatory categorial\ngrammar, 214\ncommissive speech act, 448\ncommon ground, 448, 459\ncommon nouns, 152\ncomplement, 206, 206\ncomplementizer, 153\ncompleteness in FOL, 311\ncomponential analysis, 372\nComputational Grammar\nCoder (CGC), 174\ncomputational semantics,\n296\nconcatenation, 34\nconceptual dependency, 373\nconcordance, semantic, 505\nconcrete word, 382\nconditional independence,\n266\nconﬁdence\nASR, 455\nin relation extraction, 340\nconﬁdence values, 340\nconﬁguration, 275\nconfusion matrix\nin spelling correction,\n484\nconfusion sets, 489\nconjoined phrase, 207\nconjunction, 153\nconjunctions, 207\nas closed class, 153\nconnectionist, 150\nconnotation frame, 395\nconnotation frames, 376\nconnotations, 105, 379\nconsistent, 238\nconstants in FOL, 305\nconstative speech act, 448\nconstituency, 194\nevidence for, 195\nconstituent, 194\nConstraint Grammar, 293\nConstruction Grammar, 220\ncontent planning, 456\ncontext embedding, 122\ncontext-free grammar, 194,\n195, 198, 219\nChomsky normal form,\n213\ninvention of, 220\nmultiplying probabilities,\n240, 266\nnon-terminal symbol,\n196\nproductions, 195\nrules, 195\nterminal symbol, 196\nweak and strong\nequivalence, 213\ncontingency table, 73\ncontinuer, 449\nconversation, 422\nconversational agents, 422\nconversational analysis, 451\nconvex, 90\ncoordinate noun phrase,\n207\ncoordination ambiguity,\n225, 248\ncopula, 154\ncorpora, 19\nCOCA, 483\ncorpus, 19\nATIS, 197\nBNC, 155\nBrown, 19, 174\nLOB, 174\nregular expression\nsearching inside, 11\nSwitchboard, 19\nTimeBank, 349\nCorpus Lesk, 508\nCorpus of Contemporary\nEnglish, 483\ncorrection act detection,\n453\ncosine\nas a similarity metric,\n112\ncost function, 87\ncount nouns, 152\ncounters, 34\ncounts\ntreating low as zero, 170\nCRF, 172\ncross entropy loss, 88, 141\ncross-brackets, 263\ncross-entropy, 58\ncross-validation, 76\n10-fold, 76\ncrowdsourcing, 382\ncurrying, 308\nDamerau-Levenshtein, 483\ndate\nfully qualiﬁed, 347\nnormalization, 435\ndative alternation, 359\ndecision boundary, 85, 135\ndecision tree\nuse in WSD, 514\ndeclarative sentence\nstructure, 200\ndecoder, 471, 471\ndecoding, 160, 471\nViterbi, 160, 471\ndeduction\nin FOL, 310\ndeduplication, 491\ndeep, 131\ndeep learning, 131\ndeep role, 357\ndegree, 510\ndegree adverb, 153\ndeleted interpolation, 165",
  "622": "decision tree\nuse in WSD, 514\ndeclarative sentence\nstructure, 200\ndecoder, 471, 471\ndecoding, 160, 471\nViterbi, 160, 471\ndeduction\nin FOL, 310\ndeduplication, 491\ndeep, 131\ndeep learning, 131\ndeep role, 357\ndegree, 510\ndegree adverb, 153\ndeleted interpolation, 165\ndelexicalized, 457\ndenotation, 301\ndependency\ngrammar, 270\nlexical, 248\ndependency tree, 273\ndependent, 271\nderivation\ndirect (in a formal\nlanguage), 199\nsyntactic, 196, 196, 199,\n199\ndescription logics, 316\nDet, 196\ndeterminer, 153, 196, 202\ndevelopment test set, 76\ndevelopment test set\n(dev-test), 44\ndevset, see development\ntest set (dev-test), 76\ndialog, 422\ndialog act, 447, 450\nacknowledgment, 449\nbackchannel, 449\ncontinuer, 449\ncorrection, 453\ndialog manager\ndesign, 442\ndialog policy, 454\ndialog systems, 422\ndesign, 442\nevaluation, 441\ndiathesis alternation, 359\ndiff program, 35\ndimension, 108\ndiphthong\norigin of term, 151\ndirect derivation (in a\nformal language),\n199\ndirectional adverb, 153\ndirective speech act, 448\ndisambiguation\nPCFGs for, 239\nrole of probabilistic\nparsing, 237\nsyntactic, 225\nvia PCFG, 240\ndiscount, 49, 51, 52\ndiscounting, 49\ndiscovery procedure, 220\ndiscriminative model, 83\ndisﬂuency, 20\ndisjunction, 34\npipe in regular\nexpressions as, 14\nsquare braces in regular\nexpression as, 12\ndispreferred response, 445\ndistance, 254\ncosine, 112\ndistant supervision, 341\ndistributional hypothesis,\n101\ndistributional similarity,\n220\ndocument frequency, 113\ndocument vector, 115\ndomain, 301\ndomain classiﬁcation, 434\ndomain ontology, 430\ndomination in syntax, 196\ndot product, 84, 111\ndropout, 145\nduration\ntemporal expression, 345\ndynamic programming, 31\nand parsing, 225\nforward algorithm as,\n468\nhistory, 479\nViterbi as, 161, 471\nE-step (expectation step) in\nEM, 478\nEarnest, The Importance of\nBeing, 480\nearnesty, importance, 480\nedge-factored, 286\nedit distance\nminimum, 31\nELIZA, 10\nimplementation, 18\nsample conversation, 18\nElman Networks, 178\nEM\nBaum-Welch as, 474\nE-step, 478\nfor deleted interpolation,\n52\nfor spelling correction,\n484\ninside-outside in parsing,\n245\nM-step, 478",
  "623": "Subject Index\n553\nembedded verb, 204\nembeddings, 107\ncharacter, 332\ncosine for similarity, 111\nGloVe, 119\nskip-gram, learning, 121\nsparse, 110\ntf-idf, 113\nword2vec, 119\nemission probabilities, 158,\n465\nEmoLex, 381\nemotion, 379\nempty category, 201\nEnglish\nsimpliﬁed grammar\nrules, 197\nentity linking, 414\nentropy, 57\nand perplexity, 57\ncross-entropy, 58\nper-word, 58\nrate, 58\nrelative, 371\nerror backpropagation, 142\nerror model, 484\nEuclidean distance\nin L2 regularization, 94\nEugene Onegin, 60, 479\nevalb, 264\nevaluating parsers, 263\nevaluation\n10-fold cross-validation,\n76\ncomparing models, 45\ncross-validation, 76\ndevelopment test set, 44,\n76\ndevset, 76\ndevset or development\ntest set, 44\ndialog systems, 441\nextrinsic, 43, 506\nmost frequent class\nbaseline, 156\nnamed entity recognition,\n333\nof n-gram, 43\nof n-grams via\nperplexity, 44\npseudoword, 372\nrelation extraction, 344\ntest set, 43\ntraining on the test set, 43\ntraining set, 43\nunsupervised WSD, 512\nword similarity, 503\nWSD systems, 506\nEvent extraction, 327\nevent extraction, 348\nevent variable, 312\nevents\nrepresentation of, 311\nexistential there, 154\nexpansion, 197, 200\nexpectation step, 245\nexpectation step in EM, 478\nExpectation-Maximization,\nsee EM\nexplicit conﬁrmation, 454\nexpressiveness, of a\nmeaning\nrepresentation, 300\nextended gloss overlap, 502\nExtended Lesk, 502\nextended Lesk, 502\nextrinsic, 506\nextrinsic evaluation, 43\nF (for F-measure), 74, 234,\n263\nF-measure, 74, 234\nin NER, 333\nfactoid question, 402\nfalse negatives, 15\nfalse positives, 15\nfasttext, 129\nFASTUS, 352\nfeature cutoff, 170\nfeature interactions, 87\nfeature selection, 79\ninformation gain, 79\nfeature template, 282\nfeature templates, 87\npart-of-speech tagging,\n169\nFederalist papers, 80\nfeed-forward network, 137\nﬁlled pause, 20\nﬁller, 20\nﬁnal lowering, 453\nFirst Order Logic, see FOL\nﬁrst-order co-occurrence,\n124\nfocus, 416\nFOL, 296, 304\n∃(there exists), 306\n∀(for all), 306\n=⇒(implies), 309\n∧(and), 306, 309\n¬ (not), 306, 309\n∨(or), 309\nand veriﬁability, 304\nconstants, 305\nexpressiveness of, 300,\n304\nfunctions, 305\ninference in, 304\nterms, 304\nvariables, 305\nfold (in cross-validation),\n76\nfood in NLP\nice cream, 466\nformal language, 198\nforward algorithm, 468, 469\nFORWARD ALGORITHM,\n470\nforward chaining, 310\nforward composition, 216\nforward trellis, 468\nforward-backward\nalgorithm, 474, 479\nbackward probability in,\n474\nrelation to inside-outside,\n245\nFORWARD-BACKWARD\nALGORITHM, 478\nFosler, E., see\nFosler-Lussier, E.\nfragment of word, 20\nframe\nsemantic, 362\nframe elements, 362\nFrameNet, 362\nframes, 431\nfree word order, 270\nFreebase, 336\nFrump, 354\nfully qualiﬁed date\nexpressions, 347\nfully-connected, 137\nfunction word, 152, 173\nfunctional grammar, 221\nfunctions in FOL, 305\ngarden-path sentences, 265,\n267\ngaussian\nprior on weights, 95\ngazetteer, 331\nGeneral Inquirer, 71, 381",
  "624": "free word order, 270\nFreebase, 336\nFrump, 354\nfully qualiﬁed date\nexpressions, 347\nfully-connected, 137\nfunction word, 152, 173\nfunctional grammar, 221\nfunctions in FOL, 305\ngarden-path sentences, 265,\n267\ngaussian\nprior on weights, 95\ngazetteer, 331\nGeneral Inquirer, 71, 381\ngeneralize, 94\ngeneralized semantic role,\n360\ngeneration\nof sentences to test a\nCFG grammar, 197\ntemplate-based, 438\ngenerative grammar, 198\ngenerative lexicon, 514\ngenerative model, 83\ngenerative syntax, 221\ngenerator, 196\ngenitive NP, 222\ngerundive postmodiﬁer, 203\nGilbert and Sullivan, 327,\n480\ngloss, 497\nGodzilla, speaker as, 368\ngold labels, 73\nThe Gondoliers, 480\nGood-Turing, 53\ngovernment and binding,\n220\ngradient, 90\nGrammar\nConstraint, 293\nConstruction, 220\nGovernment and\nBinding, 220\nHead-Driven Phrase\nStructure (HPSG),\n211, 220\nLexical-Functional\n(LFG), 220\nLink, 293\nProbabilistic Tree\nAdjoining, 267\nTree Adjoining, 221\ngrammar\nbinary branching, 213\ncategorial, 214, 214\nCCG, 214\nchecking, 223\ncombinatory categorial,\n214\nequivalence, 213\ngenerative, 198\nstrong equivalence, 213\nweak equivalence, 213\nGrammar Rock, 151\ngrammatical function, 271\ngrammatical relation, 271\ngrammatical sentences, 198\ngreedy, 170\ngreedy RE patterns, 15\ngreeting, 154\ngrep, 11, 11, 34\nground, 448\ngrounding\nﬁve kinds of, 448\nHamilton, Alexander, 80\nhanzi, 25\nharmonic mean, 75, 234,\n263\nHays, D., 293\nhead, 211, 271\nﬁnding, 211\nin lexicalized grammar,\n250\ntag, 250\nhead tag, 250\nHead-Driven Phrase\nStructure Grammar\n(HPSG), 211, 220\nHeaps’ Law, 20\nHECTOR corpus, 505\nheld out, 43\nheld-out, 52\nHerdan’s Law, 20\nhidden, 158, 465\nhidden layer, 137\nas representation of\ninput, 138\nhidden units, 137\nHMM, 158, 465\ndeleted interpolation, 165\nformal deﬁnition of, 158,\n465\ninitial distribution, 158,\n465\nobservation likelihood,\n158, 465\nobservations, 158, 465\nsimplifying assumptions\nfor POS tagging,\n160\nstates, 158, 465\ntransition probabilities,\n158, 465\ntrigram POS tagging, 163\nholonym, 496\nhomographs, 493\nhomonym, 493\nhomonymy, 493\nhomophones, 494\nhuman parsing, 264",
  "625": "554\nSubject Index\nhuman sentence processing,\n264, 264\nHungarian\npart-of-speech tagging,\n172\nhyperarticulation, 453\nhypernym, 104, 336, 496\nand information content,\n501\nin Extended Lesk, 503\nlexico-syntactic patterns\nfor, 336\nHyperparameter, 145\nhyponym, 104, 496\nIBM, 60\nIBM Thomas J. Watson\nResearch Center, 60\nice cream, 466\nIDF, 508\nidf, 114\nidf term weighting, 114\nif then reasoning in FOL,\n310\nimmediately dominates,\n196\nimperative sentence\nstructure, 200\nimplicit argument, 376\nimplicit conﬁrmation, 454\nimplied hierarchy\nin description logics, 320\nindeﬁnite article, 202\nindirect speech acts, 452\ninference, 300\nin FOL, 310\ninference-based learning,\n291\ninﬁnitives, 206\ninfoboxes, 335\ninformation extraction (IE),\n327\nbootstrapping, 339\npartial parsing for, 231\ninformation gain, 79\nfor feature selection, 79\nInformation retrieval, 109\ninformation-content word\nsimilarity, 500\ninitiative, 432\nmixed, 433\nsingle, 432\nsystem, 432\ninner product, 111\ninside-outside algorithm,\n245, 266\ninstance checking, 319\nintent determination, 434\nintercept, 84\ninterjection, 154\nintermediate semantic\nrepresentations, 298\ninternal rule in a CFG\nparse, 251\ninterpersonal stance, 393\nInterpolated Kneser-Ney\ndiscounting, 53, 55\ninterpolation\nin smoothing, 51\ninterpretable, 97\ninterpretation, 302\nintonation, 453\nintransitive verbs, 206\nintrinsic evaluation, 43\ninverse document\nfrequency, 508\nIOB, 232, 330, 436\nIOB tagging\nfor NER, 330\nfor temporal expressions,\n345\nslot ﬁlling, 436\nIolanthe, 480\nIR\nidf term weighting, 114\nvector space model, 108\nIS-A, 105\nis-a, 336\nISO 8601, 346\niSRL, 376\nJaro-Winkler, 491\nJay, John, 80\nJiang-Conrath distance, 502\njoint intention, 459\njoint probability, 239\nKatz backoff, 52\nKBP, 354\nKenLM, 56, 61\nKL divergence, 371\nKL-ONE, 323\nKleene *, 13\nsneakiness of matching\nzero things, 13\nKleene +, 13\nKneser-Ney discounting, 53\nknowledge base, 297, 299\nKRL, 323\nKullback-Leibler\ndivergence, 371\nL1 regularization, 94\nL2 regularization, 94\nlabel bias, 171\nlabeled precision, 263\nlabeled recall, 263\nlambda notation, 308\nlanguage generation, 438\nlanguage ID, 72\nlanguage id, 63\nlanguage model, 38\nadaptation, 61\nPCFG, 241\nLaplace smoothing, 49\nLaplace smoothing:for\nPMI, 118\nlasso regression, 94\nlatent semantic analysis,\n128\nLCS, 501\nLDC, 24, 243\nlearning rate, 90\nlemma, 20, 102\nversus wordform, 20\nlemmatization, 11\nLesk algorithm, 507\nCorpus, 508\nExtended, 502\nSimpliﬁed, 507\nletter-to-sound\nfor spell checking, 491\nLevenshtein distance, 30\nlexical\nambiguity resolution,\n513\ncategory, 196\ndatabase, 497\ndependency, 237, 248\nhead, 267\nsemantics, 102\ntrigger, in IE, 345\nlexical answer type, 416\nlexical dependency, 248\nlexical rule\nin a CFG parse, 251\nlexical sample task in\nWSD, 504\nLexical-Functional\nGrammar (LFG),\n220\nlexicalized grammar, 250\nlexico-syntactic pattern,\n336\nlexicon, 196\nlikelihood, 66, 482\nLin similarity, 502",
  "626": "semantics, 102\ntrigger, in IE, 345\nlexical answer type, 416\nlexical dependency, 248\nlexical rule\nin a CFG parse, 251\nlexical sample task in\nWSD, 504\nLexical-Functional\nGrammar (LFG),\n220\nlexicalized grammar, 250\nlexico-syntactic pattern,\n336\nlexicon, 196\nlikelihood, 66, 482\nLin similarity, 502\nlinear classiﬁers, 67\nlinear interpolation for\nn-grams, 52\nlinearly separable, 135\nLinguistic Data\nConsortium, 24, 243\nLink Grammar, 293\nliteral meaning, 296\nLIWC, 71, 382\nLM, 38\nLOB corpus, 174\nlocative, 153\nlocative adverb, 153\nlog\nwhy used for\nprobabilities, 43\nlog likelihood ratio, 390\nlog odds ratio, 390\nlog probabilities, 43, 43\nlogical connectives, 305\nlogical vocabulary, 301\nlogistic function, 84\nlogistic regression, 82\nconditional maximum\nlikelihood\nestimation, 88\nlearning in, 87\nrelation to neural\nnetworks, 139\nlong-distance dependency,\n208\ntraces in the Penn\nTreebank, 208\nwh-questions, 201\nlookahead in RE, 19\nloss, 87\nlowest common subsumer,\n501\nLSI, see latent semantic\nanalysis\nLSTM, 354\nfor NER, 332\nfor slot ﬁlling, 436\nfor SRL, 366\nLUNAR, 421\nLunar, 322\nM-step (maximization step)\nin EM, 478\nmachine learning\nfor NER, 333\nfor WSD, 505\ntextbooks, 81, 100\nmacroaveraging, 75\nMadison, James, 80\nManhattan distance\nin L1 regularization, 94\nmanner adverb, 153\nmarker passing for WSD,\n513\nMarkov, 40\nassumption, 40\nMarkov assumption, 157,\n464\nMarkov chain, 60, 157, 464\nformal deﬁnition of, 158,\n465\ninitial distribution, 158,\n465\nN-gram as, 158, 465\nstates, 158, 465\ntransition probabilities,\n158, 465\nMarkov model, 40\nformal deﬁnition of, 158,\n465\nhistory, 60\nMarx, G., 223\nmass nouns, 152\nMaxEnt\nGaussian priors, 95\nregularization, 95\nmaxent, 100\nmaximization step, 245\nmaximization step in EM,\n478\nmaximum entropy, 99\nmaximum matching, 25\nmaximum spanning tree,\n287\nMaxMatch, 25\nMCTest, 419\nmean reciprocal rank, 418\nmeaning representation,\n295\nas set of symbols, 296\nearly uses, 322\nlanguages, 297\nmeaning representation\nlanguages, 295\nmechanical indexing, 128\nMEMM, 168\ncompared to HMM, 168",
  "627": "Subject Index\n555\ninference (decoding),\n171\nlearning, 171\nViterbi decoding, 171\nmeronym, 496\nmeronymy, 496\nMeSH (Medical Subject\nHeadings), 64, 504\nMessage Understanding\nConference, 352\nmetarule, 207\nmetonymy, 494\nMicro-Planner, 322\nmicroaveraging, 75\nminibatch, 92\nminimum edit distance, 30,\n30, 31, 161, 471\nexample of, 33\nMINIMUM EDIT DISTANCE,\n33\nmixed initiative, 433\nMLE\nfor n-grams, 40\nfor n-grams, intuition, 41\nMLP, 137\nmodal verb, 154\nmodel, 301\nmodiﬁed Kneser-Ney, 55\nmodus ponens, 310\nMontague semantics, 323\nmorpheme, 26\nMoses, Michelangelo statue\nof, 422\nmost frequent sense, 507\nMRR, 418\nMUC, 352, 354\nmulti-label classiﬁcation,\n75\nmulti-layer perceptrons,\n137\nmultinomial classiﬁcation,\n75\nmultinomial naive Bayes,\n65\nmultinomial naive Bayes\nclassiﬁer, 65\nmultinominal logistic\nregression, 95\nN-best list, 435\nN-gram\nas Markov chain, 158,\n465\nn-gram, 38, 40\nabsolute discounting, 53\nadaptation, 61\nadd-one smoothing, 49\nas approximation, 40\nas generators, 46\nequation for, 40\nexample of, 42\nfor Shakespeare, 46\nhistory of, 60\ninterpolation, 51\nKatz backoff, 52\nKenLM, 56, 61\nKneser-Ney discounting,\n53\nlogprobs in, 43\nnormalizing, 41\nparameter estimation, 41\nsensitivity to corpus, 45\nsmoothing, 49\nSRILM, 61\ntest set, 43\ntraining set, 43\nunknown words, 48\nn-gram\ntiling, 408\nnaive Bayes\nmultinomial, 65\nsimplifying assumptions,\n66\nnaive Bayes assumption, 66\nnaive Bayes classiﬁer\nuse in text categorization,\n65\nnamed entity, 328\nlist of types, 329\nrecognition, 327, 329\nnamed entity recognition,\n184\nnames\nand gazetteers, 331\ncensus lists, 331\nNarrativeQA, 419\nnegative log likelihood loss,\n141\nnegative part-of-speech,\n154\nneo-Davidsonian, 312\nNER, 327\nneural nets, 61\nneural networks\nrelation to logistic\nregression, 139\nnewline character, 17\nnoisy channel model\nfor spelling, 481\ninvention of, 492\nnoisy-or, 340\nNomBank, 361\nNominal, 196\nnon-capturing group, 18\nnon-ﬁnite postmodiﬁer, 203\nnon-greedy, 15\nnon-logical vocabulary, 301\nnon-terminal symbols, 196,\n197\nnormal form, 213, 213\nnormalization\ndates, 435\ntemporal, 346\nword, 23\nnormalization of\nprobabilities, 40\nnormalized, 328\nnormalizing, 139\nnoun, 152\nabstract, 152, 202\ncommon, 152\ncount, 152\ndays of the week coded\nas, 153\nmass, 152, 202\nproper, 152\nnoun phrase, 194\nconstituents, 196\nNP, 196, 197\nNP attachment, 246\nnull hypothesis, 77\nnumerals\nas closed class, 153\nobject, syntactic\nfrequency of pronouns\nas, 245\nobservation bias, 171\nobservation likelihood\nrole in forward, 469\nrole in Viterbi, 162, 472\nOCR, 492\nold information, and word\norder, 246\non-line sentence-processing\nexperiments, 267\none sense per collocation,\n511\none-hot vector, 146\none-of, 75\nontology, 316\nOntoNotes, 514\nOOV (out of vocabulary)\nwords, 48\nOOV rate, 48\nopen class, 152\nopen information",
  "628": "OCR, 492\nold information, and word\norder, 246\non-line sentence-processing\nexperiments, 267\none sense per collocation,\n511\none-hot vector, 146\none-of, 75\nontology, 316\nOntoNotes, 514\nOOV (out of vocabulary)\nwords, 48\nOOV rate, 48\nopen class, 152\nopen information\nextraction, 342\nopen vocabulary system\nunknown words in, 48\noperation list, 30\noperator precedence, 14, 14\noptical character\nrecognition, 492\noptionality\nof determiners, 202\nuse of ? in regular\nexpressions for, 12\nordinal number, 203\noverﬁtting, 94\nparallel distributed\nprocessing, 150\nparent annotation, 249\nparse tree, 196, 199\nparsed corpus, 266\nparsing\nambiguity, 223\nchunking, 231\nCKY, 226, 243\nCYK, see CKY\nevaluation, 263\nhistory, 235\npartial, 231\nprobabilistic CKY, 243\nrelation to grammars,\n200\nshallow, 231\nsyntactic, 223\nwell-formed substring\ntable, 235\npart-of-speech\nadjective, 152\nadverb, 152\nas used in CFG, 196\nclosed class, 152, 153\ngreeting, 154\ninterjection, 154\nnegative, 154\nnoun, 152\nopen class, 152\nparticle, 153\nsubtle distinction\nbetween verb and\nnoun, 152\nusefulness of, 151\nverb, 152\npart-of-speech tagger\nPARTS, 174\nTAGGIT, 174\nPart-of-speech tagging, 156\npart-of-speech tagging\nambiguity and, 156\namount of ambiguity in\nBrown corpus, 156\nand morphological\nanalysis, 172\ncapitalization, 167\nfeature templates, 169\nfor phrases, 155\nhistory of, 174\nHungarian, 172\nStanford tagger, 172\nstate of the art, 157\nTurkish, 172\nunknown words, 167\npart-whole, 496\npartial parsing, 231\nparticle, 153\nPARTS tagger, 174\nparts-of-speech, 151\npassage retrieval, 405\npassages, 405\npath-length based\nsimilarity, 500\npattern, regular expression,\n11\nPCFG, 238\nfor disambiguation, 239\nlack of lexical sensitivity,\n246\nlexicalized, 267\nparse probability, 239\npoor independence\nassumption, 245\nrule probabilities, 238\nuse in language\nmodeling, 241\nPDP, 150\nPenn Treebank, 208\nfor statistical parsing,\n243\nPOS tags for phrases,\n155\ntagging accuracy, 157\ntagset, 154, 154\nPenn Treebank\ntokenization, 24\nper-word entropy, 58\nperceptron, 134\nperplexity, 44, 59",
  "629": "556\nSubject Index\nas weighted average\nbranching factor, 44\ndeﬁned via\ncross-entropy, 59\npersonal pronoun, 153\npersonality, 392\npersonalized page rank, 510\nphones\nin spell checking, 491\nphrasal verb, 153\nphrase-structure grammar,\n195, 220\npipe, 14\nThe Pirates of Penzance,\n327\nplanning\nand speech acts, 460\nshared plans, 459\nplural, 202\nPointwise mutual\ninformation, 116\npoliteness marker, 154\npolysemy, 494\nPorter stemmer, 26\nPOS, 151\npossessive NP, 222\npossessive pronoun, 153\npostdeterminer, 203\npostmodiﬁer, 203\npostposed constructions,\n195\nPotts diagram, 388\nPP, 197\nPPMI, 116\npre-sequence, 451\nprecedence, 14\nprecedence, operator, 14\nPrecision, 74\nprecision, 234\nin NER, 333\npredeterminer, 204\npredicate, 206\npredicate-argument\nrelations, 206\npreference semantics, 513\npreposed constructions, 195\nprepositional phrase, 203\nattachment, 246\nconstituency, 197\npreposing, 195\nprepositions, 153\nas closed class, 153\npretraining, 146\nprimitive decomposition,\n372\nprinciple of contrast, 103\nprior probability, 66, 482\nprobabilistic CKY\nalgorithm, 242, 243\nprobabilistic parsing, 242\nby humans, 264\nproductions, 195\nprogressive prompting, 455\nprojection layer, 147\nProlog, 311\nprompt, 439\nprompts, 438\npronoun, 153\nand old information, 246\nas closed class, 153\npersonal, 153\npossessive, 153\nwh-, 153\nPropBank, 360\nproper noun, 152\npropositional meaning, 103\nprosody, 453\nPROTO-AGENT, 360\nPROTO-PATIENT, 360\npseudoword, 372\nPTAG, 267\nPTRANS, 373\npunctuation\nfor numbers\ncross-linguistically,\n24\nfor sentence\nsegmentation, 29\npart-of-speech tags, 154\nstripping before\npart-of-speech\ntagging, 156\ntokenization, 24\ntreated as words, 19\ntreated as words in LM,\n47\nQuAC, 419\nqualia structure, 514\nquantiﬁer\nas part of speech, 203\nsemantics, 306\nquery\nreformulation in QA, 404\nquestion\nclassiﬁcation, 404\nfactoid, 402\nquestion answering\nevaluation, 418\nfactoid questions, 402\nquery reformulation in,\n404\nrange, regular expression,\n12\nrapid reprompting, 455\nRDF, 336\nRDF triple, 336\nRE\nregular expression, 11\nreading comprehension,\n409\nreading time, 264\nreal-word spelling errors,\n480\nRecall, 74\nrecall, 234\nin NER, 333\nrecipe\nmeaning of, 295\nreference point, 314\nreformulation, 448\nregister in RE, 18\nregression\nlasso, 94\nridge, 95\nregular expression, 11, 34\nsubstitutions, 17\nregularization, 94\nrejection\nconversation act, 455\nrelatedness, 104\nrelation extraction, 327\nrelative\ntemporal expression, 345\nrelative entropy, 371\nrelative frequency, 41\nrelative pronoun, 204\nrelexicalize, 457\nReLU, 133\nreporting events, 349\nrepresentation learning, 101\nResnik similarity, 501\nresolution for inference,\n311\nresolve, 156\nresponse generation, 428\nrestrictive grammar, 438\nrestrictive relative clause,\n204\nReVerb, 342\nreversives, 103\nrewrite, 196\nRiau Indonesian, 152\nridge regression, 95\nrole-ﬁller extraction, 352\nrow vector, 109\nrules\ncontext-free, 195\ncontext-free, expansion,\n196, 200\ncontext-free, sample, 197\nS as start symbol in CFG,\n196\nSAE, 21\nsampling\nused in clustering, 512",
  "630": "ReVerb, 342\nreversives, 103\nrewrite, 196\nRiau Indonesian, 152\nridge regression, 95\nrole-ﬁller extraction, 352\nrow vector, 109\nrules\ncontext-free, 195\ncontext-free, expansion,\n196, 200\ncontext-free, sample, 197\nS as start symbol in CFG,\n196\nSAE, 21\nsampling\nused in clustering, 512\nsaturated, 134\nSch¨onkﬁnkelization, 308\n“Schoolhouse Rock”, 151\nSCISOR, 354\nsclite package, 35\nscript\nSchankian, 362\nscripts, 351\nsecond-order\nco-occurrence, 124\nseed pattern in IE, 339\nseed tuples, 339\nsegmentation\nChinese word, 25\nmaximum matching, 25\nsentence, 29\nword, 23\nselectional association, 371\nselectional preference\nstrength, 370\nselectional preferences\npseudowords for\nevaluation, 372\nselectional restriction, 368\nrepresenting with events,\n369\nviolations in WSD, 370\nsemantic analysis, 296\nsemantic concordance, 505\nsemantic drift in IE, 340\nsemantic feature, 128\nsemantic ﬁeld, 104\nsemantic frame, 104\nsemantic grammars, 434\nsemantic network\nfor word sense\ndisambiguation, 513\nsemantic networks\norigins, 322\nsemantic processing, 295\nsemantic relations in IE,\n334\ntable, 335\nsemantic role, 357, 357,\n360\nSemantic role labeling, 364\nsemantics, 295\nlexical, 102\nsense\naccuracy in WSD, 506\nword, 493\nSENSEVAL\nand WSD evaluation, 506\nSENSEVAL corpus, 505\nsentence\nsegmentation, 29\nsentence realization, 456\nsentence segmentation, 11\nsentence selection, 410\nsentential complements,\n205\nsentiment, 105\norigin of term, 397\nsentiment analysis, 63\nsentiment lexicons, 71\nSentiWordNet, 387\nsequence model, 157\nSGNS, 119\nShakespeare\nn-gram approximations\nto, 46\nshallow parse, 231\nshared plans, 459\nshift-reduce parsing, 275\nSHRDLU, 322\nside sequence, 451\nsigmoid, 84, 132\nsimilarity, 103\nSimple Recurrent\nNetworks, 178\nSimpliﬁed Lesk, 507\nskip-gram, 119\nslot ﬁlling, 354, 434\nslots, 431\nsmoothing, 49, 49\nabsolute discounting, 53\nadd-one, 49\ndiscounting, 49\nfor HMM POS tagging,\n165\ninterpolation, 51\nKatz backoff, 52\nKneser-Ney discounting,\n53",
  "631": "Subject Index\n557\nLaplace, 49\nlinear interpolation, 52\nsnippets, 407\nsoftmax, 95, 139\nSoundex, 491\nspam detection, 63, 71\nspan, 407\nspeech acts, 447\nspell checking\npronunciation, 491\nspelling correction\nuse of n-grams in, 37\nSPELLING CORRECTION\nALGORITHM, 483,\n491\nspelling errors\ncontext-dependent, 480\ncorrection, EM, 484\ndetection, real words,\n486\nnoisy channel model for\ncorrection, 483\nnon-word, 480\nreal word, 480\nsplit, 248\nsplit and merge, 250\nSQuAD, 409, 419\nSRILM, 61\nSRL, 364\nStacked RNNs, 186\nStanford tagger, 172\nstart symbol, 196\nstate\nsemantic representation\nof, 311\nstationary stochastic\nprocess, 58\nstatistical parsing, 242\nstative expressions, 315\nstem, 26\nStemming, 11\nstemming, 26\nstop words, 68\nstrong equivalence of\ngrammars, 213\nstructural ambiguity, 223\nstupid backoff, 56\nsubcategorization\nand probabilistic\ngrammars, 237\ntagsets for, 206\nsubcategorization frame,\n206\nexamples, 206\nsubcategorize for, 206\nsubdialog, 451\nsubject, syntactic\nfrequency of pronouns\nas, 245\nin wh-questions, 201\nsubjectivity, 378, 397\nsubstitutability, 220\nsubstitution in TAG, 221\nsubstitution operator\n(regular\nexpressions), 17\nsubsumption, 317, 319\nsuperordinate, 104, 496\nSupertagging, 257\nsupertagging, 267\nsupervised machine\nlearning, 64\nSwitchboard, 155\nSwitchboard Corpus, 19\nsynonyms, 103, 496\nsynset, 497\nsyntactic categories, 151\nsyntactic disambiguation,\n225\nsyntactic movement, 208\nsyntax, 194\norigin of term, 151\nsystem-initiative, 432\nTAG, 221, 267\nTAGGIT, 174\ntagset\ndifference between Penn\nTreebank and\nBrown, 155\nhistory of Penn\nTreebank, 155\nPenn Treebank, 154, 154\ntable of Penn Treebank\ntags, 154\ntanh, 133\ntarget embedding, 122\nTay, 443\nTBox, 316\ntechnai, 151\ntelic eventualities, 316\ntemplate ﬁlling, 328, 351\ntemplate recognition, 351\ntemplate, in IE, 351\ntemplate-based generation,\n438\ntemporal adverb, 153\ntemporal anchor, 348\ntemporal expression\nabsolute, 345\nmetaphor for, 315\nrecognition, 328\nrelative, 345\ntemporal expressions, 328\ntemporal logic, 313\ntemporal normalization,\n346\ntemporal reasoning, 323\ntense logic, 313\nterm\nclustering, 513, 514\nin FOL, 304\nterm frequency, 113\nterm-document matrix, 108\nterm-term matrix, 110\nterminal symbol, 196\nterminology\nin description logics, 316\ntest set, 43\ndevelopment, 44\nhow to choose, 44\ntext categorization, 63\nbag of words assumption,\n65\nnaive Bayes approach, 65\nunknown words, 68\ntext normalization, 10\npart-of-speech tagging,\n155\ntf-idf, 114\nthematic grid, 359\nthematic role, 357\nand diathesis alternation,\n359\nexamples of, 358\nproblems, 359\ntheme, 357\ntheme, as thematic role, 357\nthere, existential in English,\n154\nthesaurus, 513\ntime, representation of, 312\nTimeBank, 349\ntokenization, 10\nsentence, 29\nword, 23\ntokens, word, 20\ntopic (information\nstructure), 246\ntopic models, 104\ntrace, 201, 208\ntraining oracle, 280\ntraining set, 43\ncross-validation, 76\nhow to choose, 44\nTransformations and\nDiscourse Analysis\nProject (TDAP),\n174\ntransition probability",
  "632": "TimeBank, 349\ntokenization, 10\nsentence, 29\nword, 23\ntokens, word, 20\ntopic (information\nstructure), 246\ntopic models, 104\ntrace, 201, 208\ntraining oracle, 280\ntraining set, 43\ncross-validation, 76\nhow to choose, 44\nTransformations and\nDiscourse Analysis\nProject (TDAP),\n174\ntransition probability\nrole in forward, 469\nrole in Viterbi, 162, 472\ntransitive verbs, 206\nTREC, 421\nTree Adjoining Grammar\n(TAG), 221\nadjunction in, 221\nprobabilistic, 267\nsubstitution in, 221\ntreebank, 208, 243\ntrigram, 42\ntruth-conditional semantics,\n303\nTurkish\npart-of-speech tagging,\n172\nturn correction ratio, 441\nturns, 423\ntype raising, 216\ntyped dependency structure,\n270\ntypes\nword, 20\nungrammatical sentences,\n198\nunique beginner, 498\nunit production, 226\nunit vector, 112\nuniversal, 433\nUniversal Dependencies,\n272\nUnix, 11\n<UNK>, 48\nunknown words, 27\nin n-grams, 48\nin part-of-speech\ntagging, 167\nin text categorization, 68\nuser-centered design, 442\nutterance, 19\nV (vocabulary), 482\nvagueness, 298\ntests distinguishing from\nambiguity, 298\nvariable, 305\nexistentially quantiﬁed,\n307\nuniversally quantiﬁed,\n307\nvariables in FOL, 305\nvector, 108, 132\nvector length, 111\nvector semantics, 101, 106\nvector space, 108\nvector space model, 108\nverb\ncopula, 154\nmodal, 154\nphrasal, 153\nverb alternations, 359\nverb phrase, 196, 205\nVerbs, 152\nveriﬁability, 297\nViterbi algorithm, 31, 161,\n471\nbacktrace in, 473\ndecoding in MEMM, 171\nhistory of, 479\nVITERBI ALGORITHM,\n161, 472\nvoice user interface, 442\nVoiceXML, 438\nVP attachment, 246\nweak equivalence of\ngrammars, 213\nWeb Ontology Language,\n321\nWebQuestions, 419\nwell-formed substring\ntable, 235\nWFST, 235\nwh-non-subject-question,\n201\nwh-phrase, 201, 201\nwh-pronoun, 153\nwh-subject-questions, 201\nwh-word, 201\nWikiQA, 419\nwildcard, regular\nexpression, 13\nWizard-of-Oz system, 442\nword\nboundary, regular\nexpression notation,\n14\nclosed class, 152\ndeﬁnition of, 19\nfragment, 20\nfunction, 152, 173",
  "633": "558\nSubject Index\nopen class, 152\npunctuation as, 19\ntokens, 20\ntypes, 20\nword error rate, 26\nword normalization, 23\nword segmentation, 23\nword sense, 493\nword sense disambiguation,\n504, see WSD\nword sense induction, 511\nword shape, 169, 331\nword tokenization, 23\nword-word matrix, 110\nword2vec, 119\nwordform, 20\nand lemma, 102\nversus lemma, 20\nWordNet, 497, 497\nworld knowledge, 295\nWSD, 504\nAI-oriented efforts, 513\nall-words task, 505\nbootstrapping, 510, 514\ndecision tree approach,\n514\nevaluation of, 506\nhistory, 513\nhistory of, 514\nlexical sample task, 504\nneural network\napproaches, 513\nrobust approach, 513\nsupervised machine\nlearning, 514\nunsupervised machine\nlearning, 511\nWSI, 511\nWSJ, 155\nX-bar schemata, 220\nYarowsky algorithm, 510\nyes-no questions, 200, 452\nyield, 240\nYonkers Racetrack, 57\nzero-width, 19\nzeros, 48\nzeugma, 495",
  "634": "Attention Is All You Need\nAshish Vaswani∗\nGoogle Brain\navaswani@google.com\nNoam Shazeer∗\nGoogle Brain\nnoam@google.com\nNiki Parmar∗\nGoogle Research\nnikip@google.com\nJakob Uszkoreit∗\nGoogle Research\nusz@google.com\nLlion Jones∗\nGoogle Research\nllion@google.com\nAidan N. Gomez∗†\nUniversity of Toronto\naidan@cs.toronto.edu\nŁukasz Kaiser∗\nGoogle Brain\nlukaszkaiser@google.com\nIllia Polosukhin∗‡\nillia.polosukhin@gmail.com\nAbstract\nThe dominant sequence transduction models are based on complex recurrent or\nconvolutional neural networks that include an encoder and a decoder. The best\nperforming models also connect the encoder and decoder through an attention\nmechanism. We propose a new simple network architecture, the Transformer,\nbased solely on attention mechanisms, dispensing with recurrence and convolutions\nentirely. Experiments on two machine translation tasks show these models to\nbe superior in quality while being more parallelizable and requiring signiﬁcantly\nless time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-\nto-German translation task, improving over the existing best results, including\nensembles, by over 2 BLEU. On the WMT 2014 English-to-French translation task,\nour model establishes a new single-model state-of-the-art BLEU score of 41.0 after\ntraining for 3.5 days on eight GPUs, a small fraction of the training costs of the\nbest models from the literature.\n1\nIntroduction\nRecurrent neural networks, long short-term memory [12] and gated recurrent [7] neural networks\nin particular, have been ﬁrmly established as state of the art approaches in sequence modeling and\ntransduction problems such as language modeling and machine translation [29, 2, 5]. Numerous\nefforts have since continued to push the boundaries of recurrent language models and encoder-decoder\narchitectures [31, 21, 13].\n∗Equal contribution. Listing order is random. Jakob proposed replacing RNNs with self-attention and started\nthe effort to evaluate this idea. Ashish, with Illia, designed and implemented the ﬁrst Transformer models and\nhas been crucially involved in every aspect of this work. Noam proposed scaled dot-product attention, multi-head\nattention and the parameter-free position representation and became the other person involved in nearly every\ndetail. Niki designed, implemented, tuned and evaluated countless model variants in our original codebase and\ntensor2tensor. Llion also experimented with novel model variants, was responsible for our initial codebase, and\nefﬁcient inference and visualizations. Lukasz and Aidan spent countless long days designing various parts of and\nimplementing tensor2tensor, replacing our earlier codebase, greatly improving results and massively accelerating\nour research.\n†Work performed while at Google Brain.\n‡Work performed while at Google Research.\n31st Conference on Neural Information Processing Systems (NIPS 2017), Long Beach, CA, USA.",
  "635": "Recurrent models typically factor computation along the symbol positions of the input and output\nsequences. Aligning the positions to steps in computation time, they generate a sequence of hidden\nstates ht, as a function of the previous hidden state ht−1 and the input for position t. This inherently\nsequential nature precludes parallelization within training examples, which becomes critical at longer\nsequence lengths, as memory constraints limit batching across examples. Recent work has achieved\nsigniﬁcant improvements in computational efﬁciency through factorization tricks [18] and conditional\ncomputation [26], while also improving model performance in case of the latter. The fundamental\nconstraint of sequential computation, however, remains.\nAttention mechanisms have become an integral part of compelling sequence modeling and transduc-\ntion models in various tasks, allowing modeling of dependencies without regard to their distance in\nthe input or output sequences [2, 16]. In all but a few cases [22], however, such attention mechanisms\nare used in conjunction with a recurrent network.\nIn this work we propose the Transformer, a model architecture eschewing recurrence and instead\nrelying entirely on an attention mechanism to draw global dependencies between input and output.\nThe Transformer allows for signiﬁcantly more parallelization and can reach a new state of the art in\ntranslation quality after being trained for as little as twelve hours on eight P100 GPUs.\n2\nBackground\nThe goal of reducing sequential computation also forms the foundation of the Extended Neural GPU\n[20], ByteNet [15] and ConvS2S [8], all of which use convolutional neural networks as basic building\nblock, computing hidden representations in parallel for all input and output positions. In these models,\nthe number of operations required to relate signals from two arbitrary input or output positions grows\nin the distance between positions, linearly for ConvS2S and logarithmically for ByteNet. This makes\nit more difﬁcult to learn dependencies between distant positions [11]. In the Transformer this is\nreduced to a constant number of operations, albeit at the cost of reduced effective resolution due\nto averaging attention-weighted positions, an effect we counteract with Multi-Head Attention as\ndescribed in section 3.2.\nSelf-attention, sometimes called intra-attention is an attention mechanism relating different positions\nof a single sequence in order to compute a representation of the sequence. Self-attention has been\nused successfully in a variety of tasks including reading comprehension, abstractive summarization,\ntextual entailment and learning task-independent sentence representations [4, 22, 23, 19].\nEnd-to-end memory networks are based on a recurrent attention mechanism instead of sequence-\naligned recurrence and have been shown to perform well on simple-language question answering and\nlanguage modeling tasks [28].\nTo the best of our knowledge, however, the Transformer is the ﬁrst transduction model relying\nentirely on self-attention to compute representations of its input and output without using sequence-\naligned RNNs or convolution. In the following sections, we will describe the Transformer, motivate\nself-attention and discuss its advantages over models such as [14, 15] and [8].\n3\nModel Architecture\nMost competitive neural sequence transduction models have an encoder-decoder structure [5, 2, 29].\nHere, the encoder maps an input sequence of symbol representations (x1, ..., xn) to a sequence\nof continuous representations z = (z1, ..., zn). Given z, the decoder then generates an output\nsequence (y1, ..., ym) of symbols one element at a time. At each step the model is auto-regressive\n[9], consuming the previously generated symbols as additional input when generating the next.\nThe Transformer follows this overall architecture using stacked self-attention and point-wise, fully\nconnected layers for both the encoder and decoder, shown in the left and right halves of Figure 1,\nrespectively.\n3.1\nEncoder and Decoder Stacks\nEncoder:\nThe encoder is composed of a stack of N = 6 identical layers. Each layer has two\nsub-layers. The ﬁrst is a multi-head self-attention mechanism, and the second is a simple, position-\n2",
  "636": "Figure 1: The Transformer - model architecture.\nwise fully connected feed-forward network. We employ a residual connection [10] around each of\nthe two sub-layers, followed by layer normalization [1]. That is, the output of each sub-layer is\nLayerNorm(x + Sublayer(x)), where Sublayer(x) is the function implemented by the sub-layer\nitself. To facilitate these residual connections, all sub-layers in the model, as well as the embedding\nlayers, produce outputs of dimension dmodel = 512.\nDecoder:\nThe decoder is also composed of a stack of N = 6 identical layers. In addition to the two\nsub-layers in each encoder layer, the decoder inserts a third sub-layer, which performs multi-head\nattention over the output of the encoder stack. Similar to the encoder, we employ residual connections\naround each of the sub-layers, followed by layer normalization. We also modify the self-attention\nsub-layer in the decoder stack to prevent positions from attending to subsequent positions. This\nmasking, combined with fact that the output embeddings are offset by one position, ensures that the\npredictions for position i can depend only on the known outputs at positions less than i.\n3.2\nAttention\nAn attention function can be described as mapping a query and a set of key-value pairs to an output,\nwhere the query, keys, values, and output are all vectors. The output is computed as a weighted sum\nof the values, where the weight assigned to each value is computed by a compatibility function of the\nquery with the corresponding key.\n3.2.1\nScaled Dot-Product Attention\nWe call our particular attention \"Scaled Dot-Product Attention\" (Figure 2). The input consists of\nqueries and keys of dimension dk, and values of dimension dv. We compute the dot products of the\n3",
  "637": "Scaled Dot-Product Attention\nMulti-Head Attention\nFigure 2: (left) Scaled Dot-Product Attention. (right) Multi-Head Attention consists of several\nattention layers running in parallel.\nquery with all keys, divide each by √dk, and apply a softmax function to obtain the weights on the\nvalues.\nIn practice, we compute the attention function on a set of queries simultaneously, packed together\ninto a matrix Q. The keys and values are also packed together into matrices K and V . We compute\nthe matrix of outputs as:\nAttention(Q, K, V ) = softmax(QKT\n√dk\n)V\n(1)\nThe two most commonly used attention functions are additive attention [2], and dot-product (multi-\nplicative) attention. Dot-product attention is identical to our algorithm, except for the scaling factor\nof\n1\n√dk . Additive attention computes the compatibility function using a feed-forward network with\na single hidden layer. While the two are similar in theoretical complexity, dot-product attention is\nmuch faster and more space-efﬁcient in practice, since it can be implemented using highly optimized\nmatrix multiplication code.\nWhile for small values of dk the two mechanisms perform similarly, additive attention outperforms\ndot product attention without scaling for larger values of dk [3]. We suspect that for large values of\ndk, the dot products grow large in magnitude, pushing the softmax function into regions where it has\nextremely small gradients 4. To counteract this effect, we scale the dot products by\n1\n√dk .\n3.2.2\nMulti-Head Attention\nInstead of performing a single attention function with dmodel-dimensional keys, values and queries,\nwe found it beneﬁcial to linearly project the queries, keys and values h times with different, learned\nlinear projections to dk, dk and dv dimensions, respectively. On each of these projected versions of\nqueries, keys and values we then perform the attention function in parallel, yielding dv-dimensional\noutput values. These are concatenated and once again projected, resulting in the ﬁnal values, as\ndepicted in Figure 2.\nMulti-head attention allows the model to jointly attend to information from different representation\nsubspaces at different positions. With a single attention head, averaging inhibits this.\n4To illustrate why the dot products get large, assume that the components of q and k are independent random\nvariables with mean 0 and variance 1. Then their dot product, q · k = Pdk\ni=1 qiki, has mean 0 and variance dk.\n4",
  "638": "MultiHead(Q, K, V ) = Concat(head1, ..., headh)W O\nwhere headi = Attention(QW Q\ni , KW K\ni , V W V\ni )\nWhere the projections are parameter matrices W Q\ni\n∈Rdmodel×dk, W K\ni\n∈Rdmodel×dk, W V\ni\n∈Rdmodel×dv\nand W O ∈Rhdv×dmodel.\nIn this work we employ h = 8 parallel attention layers, or heads. For each of these we use\ndk = dv = dmodel/h = 64. Due to the reduced dimension of each head, the total computational cost\nis similar to that of single-head attention with full dimensionality.\n3.2.3\nApplications of Attention in our Model\nThe Transformer uses multi-head attention in three different ways:\n• In \"encoder-decoder attention\" layers, the queries come from the previous decoder layer,\nand the memory keys and values come from the output of the encoder. This allows every\nposition in the decoder to attend over all positions in the input sequence. This mimics the\ntypical encoder-decoder attention mechanisms in sequence-to-sequence models such as\n[31, 2, 8].\n• The encoder contains self-attention layers. In a self-attention layer all of the keys, values\nand queries come from the same place, in this case, the output of the previous layer in the\nencoder. Each position in the encoder can attend to all positions in the previous layer of the\nencoder.\n• Similarly, self-attention layers in the decoder allow each position in the decoder to attend to\nall positions in the decoder up to and including that position. We need to prevent leftward\ninformation ﬂow in the decoder to preserve the auto-regressive property. We implement this\ninside of scaled dot-product attention by masking out (setting to −∞) all values in the input\nof the softmax which correspond to illegal connections. See Figure 2.\n3.3\nPosition-wise Feed-Forward Networks\nIn addition to attention sub-layers, each of the layers in our encoder and decoder contains a fully\nconnected feed-forward network, which is applied to each position separately and identically. This\nconsists of two linear transformations with a ReLU activation in between.\nFFN(x) = max(0, xW1 + b1)W2 + b2\n(2)\nWhile the linear transformations are the same across different positions, they use different parameters\nfrom layer to layer. Another way of describing this is as two convolutions with kernel size 1.\nThe dimensionality of input and output is dmodel = 512, and the inner-layer has dimensionality\ndff = 2048.\n3.4\nEmbeddings and Softmax\nSimilarly to other sequence transduction models, we use learned embeddings to convert the input\ntokens and output tokens to vectors of dimension dmodel. We also use the usual learned linear transfor-\nmation and softmax function to convert the decoder output to predicted next-token probabilities. In\nour model, we share the same weight matrix between the two embedding layers and the pre-softmax\nlinear transformation, similar to [24]. In the embedding layers, we multiply those weights by √dmodel.\n3.5\nPositional Encoding\nSince our model contains no recurrence and no convolution, in order for the model to make use of the\norder of the sequence, we must inject some information about the relative or absolute position of the\ntokens in the sequence. To this end, we add \"positional encodings\" to the input embeddings at the\n5",
  "639": "Table 1: Maximum path lengths, per-layer complexity and minimum number of sequential operations\nfor different layer types. n is the sequence length, d is the representation dimension, k is the kernel\nsize of convolutions and r the size of the neighborhood in restricted self-attention.\nLayer Type\nComplexity per Layer\nSequential\nMaximum Path Length\nOperations\nSelf-Attention\nO(n2 · d)\nO(1)\nO(1)\nRecurrent\nO(n · d2)\nO(n)\nO(n)\nConvolutional\nO(k · n · d2)\nO(1)\nO(logk(n))\nSelf-Attention (restricted)\nO(r · n · d)\nO(1)\nO(n/r)\nbottoms of the encoder and decoder stacks. The positional encodings have the same dimension dmodel\nas the embeddings, so that the two can be summed. There are many choices of positional encodings,\nlearned and ﬁxed [8].\nIn this work, we use sine and cosine functions of different frequencies:\nPE(pos,2i) = sin(pos/100002i/dmodel)\nPE(pos,2i+1) = cos(pos/100002i/dmodel)\nwhere pos is the position and i is the dimension. That is, each dimension of the positional encoding\ncorresponds to a sinusoid. The wavelengths form a geometric progression from 2π to 10000 · 2π. We\nchose this function because we hypothesized it would allow the model to easily learn to attend by\nrelative positions, since for any ﬁxed offset k, PEpos+k can be represented as a linear function of\nPEpos.\nWe also experimented with using learned positional embeddings [8] instead, and found that the two\nversions produced nearly identical results (see Table 3 row (E)). We chose the sinusoidal version\nbecause it may allow the model to extrapolate to sequence lengths longer than the ones encountered\nduring training.\n4\nWhy Self-Attention\nIn this section we compare various aspects of self-attention layers to the recurrent and convolu-\ntional layers commonly used for mapping one variable-length sequence of symbol representations\n(x1, ..., xn) to another sequence of equal length (z1, ..., zn), with xi, zi ∈Rd, such as a hidden\nlayer in a typical sequence transduction encoder or decoder. Motivating our use of self-attention we\nconsider three desiderata.\nOne is the total computational complexity per layer. Another is the amount of computation that can\nbe parallelized, as measured by the minimum number of sequential operations required.\nThe third is the path length between long-range dependencies in the network. Learning long-range\ndependencies is a key challenge in many sequence transduction tasks. One key factor affecting the\nability to learn such dependencies is the length of the paths forward and backward signals have to\ntraverse in the network. The shorter these paths between any combination of positions in the input\nand output sequences, the easier it is to learn long-range dependencies [11]. Hence we also compare\nthe maximum path length between any two input and output positions in networks composed of the\ndifferent layer types.\nAs noted in Table 1, a self-attention layer connects all positions with a constant number of sequentially\nexecuted operations, whereas a recurrent layer requires O(n) sequential operations. In terms of\ncomputational complexity, self-attention layers are faster than recurrent layers when the sequence\nlength n is smaller than the representation dimensionality d, which is most often the case with\nsentence representations used by state-of-the-art models in machine translations, such as word-piece\n[31] and byte-pair [25] representations. To improve computational performance for tasks involving\nvery long sequences, self-attention could be restricted to considering only a neighborhood of size r in\n6",
  "640": "the input sequence centered around the respective output position. This would increase the maximum\npath length to O(n/r). We plan to investigate this approach further in future work.\nA single convolutional layer with kernel width k < n does not connect all pairs of input and output\npositions. Doing so requires a stack of O(n/k) convolutional layers in the case of contiguous kernels,\nor O(logk(n)) in the case of dilated convolutions [15], increasing the length of the longest paths\nbetween any two positions in the network. Convolutional layers are generally more expensive than\nrecurrent layers, by a factor of k. Separable convolutions [6], however, decrease the complexity\nconsiderably, to O(k · n · d + n · d2). Even with k = n, however, the complexity of a separable\nconvolution is equal to the combination of a self-attention layer and a point-wise feed-forward layer,\nthe approach we take in our model.\nAs side beneﬁt, self-attention could yield more interpretable models. We inspect attention distributions\nfrom our models and present and discuss examples in the appendix. Not only do individual attention\nheads clearly learn to perform different tasks, many appear to exhibit behavior related to the syntactic\nand semantic structure of the sentences.\n5\nTraining\nThis section describes the training regime for our models.\n5.1\nTraining Data and Batching\nWe trained on the standard WMT 2014 English-German dataset consisting of about 4.5 million\nsentence pairs. Sentences were encoded using byte-pair encoding [3], which has a shared source-\ntarget vocabulary of about 37000 tokens. For English-French, we used the signiﬁcantly larger WMT\n2014 English-French dataset consisting of 36M sentences and split tokens into a 32000 word-piece\nvocabulary [31]. Sentence pairs were batched together by approximate sequence length. Each training\nbatch contained a set of sentence pairs containing approximately 25000 source tokens and 25000\ntarget tokens.\n5.2\nHardware and Schedule\nWe trained our models on one machine with 8 NVIDIA P100 GPUs. For our base models using\nthe hyperparameters described throughout the paper, each training step took about 0.4 seconds. We\ntrained the base models for a total of 100,000 steps or 12 hours. For our big models,(described on the\nbottom line of table 3), step time was 1.0 seconds. The big models were trained for 300,000 steps\n(3.5 days).\n5.3\nOptimizer\nWe used the Adam optimizer [17] with β1 = 0.9, β2 = 0.98 and ϵ = 10−9. We varied the learning\nrate over the course of training, according to the formula:\nlrate = d−0.5\nmodel · min(step_num−0.5, step_num · warmup_steps−1.5)\n(3)\nThis corresponds to increasing the learning rate linearly for the ﬁrst warmup_steps training steps,\nand decreasing it thereafter proportionally to the inverse square root of the step number. We used\nwarmup_steps = 4000.\n5.4\nRegularization\nWe employ three types of regularization during training:\nResidual Dropout\nWe apply dropout [27] to the output of each sub-layer, before it is added to the\nsub-layer input and normalized. In addition, we apply dropout to the sums of the embeddings and the\npositional encodings in both the encoder and decoder stacks. For the base model, we use a rate of\nPdrop = 0.1.\n7",
  "641": "Table 2: The Transformer achieves better BLEU scores than previous state-of-the-art models on the\nEnglish-to-German and English-to-French newstest2014 tests at a fraction of the training cost.\nModel\nBLEU\nTraining Cost (FLOPs)\nEN-DE\nEN-FR\nEN-DE\nEN-FR\nByteNet [15]\n23.75\nDeep-Att + PosUnk [32]\n39.2\n1.0 · 1020\nGNMT + RL [31]\n24.6\n39.92\n2.3 · 1019\n1.4 · 1020\nConvS2S [8]\n25.16\n40.46\n9.6 · 1018\n1.5 · 1020\nMoE [26]\n26.03\n40.56\n2.0 · 1019\n1.2 · 1020\nDeep-Att + PosUnk Ensemble [32]\n40.4\n8.0 · 1020\nGNMT + RL Ensemble [31]\n26.30\n41.16\n1.8 · 1020\n1.1 · 1021\nConvS2S Ensemble [8]\n26.36\n41.29\n7.7 · 1019\n1.2 · 1021\nTransformer (base model)\n27.3\n38.1\n3.3 · 1018\nTransformer (big)\n28.4\n41.0\n2.3 · 1019\nLabel Smoothing\nDuring training, we employed label smoothing of value ϵls = 0.1 [30]. This\nhurts perplexity, as the model learns to be more unsure, but improves accuracy and BLEU score.\n6\nResults\n6.1\nMachine Translation\nOn the WMT 2014 English-to-German translation task, the big transformer model (Transformer (big)\nin Table 2) outperforms the best previously reported models (including ensembles) by more than 2.0\nBLEU, establishing a new state-of-the-art BLEU score of 28.4. The conﬁguration of this model is\nlisted in the bottom line of Table 3. Training took 3.5 days on 8 P100 GPUs. Even our base model\nsurpasses all previously published models and ensembles, at a fraction of the training cost of any of\nthe competitive models.\nOn the WMT 2014 English-to-French translation task, our big model achieves a BLEU score of 41.0,\noutperforming all of the previously published single models, at less than 1/4 the training cost of the\nprevious state-of-the-art model. The Transformer (big) model trained for English-to-French used\ndropout rate Pdrop = 0.1, instead of 0.3.\nFor the base models, we used a single model obtained by averaging the last 5 checkpoints, which\nwere written at 10-minute intervals. For the big models, we averaged the last 20 checkpoints. We\nused beam search with a beam size of 4 and length penalty α = 0.6 [31]. These hyperparameters\nwere chosen after experimentation on the development set. We set the maximum output length during\ninference to input length + 50, but terminate early when possible [31].\nTable 2 summarizes our results and compares our translation quality and training costs to other model\narchitectures from the literature. We estimate the number of ﬂoating point operations used to train a\nmodel by multiplying the training time, the number of GPUs used, and an estimate of the sustained\nsingle-precision ﬂoating-point capacity of each GPU 5.\n6.2\nModel Variations\nTo evaluate the importance of different components of the Transformer, we varied our base model\nin different ways, measuring the change in performance on English-to-German translation on the\ndevelopment set, newstest2013. We used beam search as described in the previous section, but no\ncheckpoint averaging. We present these results in Table 3.\nIn Table 3 rows (A), we vary the number of attention heads and the attention key and value dimensions,\nkeeping the amount of computation constant, as described in Section 3.2.2. While single-head\nattention is 0.9 BLEU worse than the best setting, quality also drops off with too many heads.\n5We used values of 2.8, 3.7, 6.0 and 9.5 TFLOPS for K80, K40, M40 and P100, respectively.\n8",
  "642": "Table 3: Variations on the Transformer architecture. Unlisted values are identical to those of the base\nmodel. All metrics are on the English-to-German translation development set, newstest2013. Listed\nperplexities are per-wordpiece, according to our byte-pair encoding, and should not be compared to\nper-word perplexities.\nN\ndmodel\ndff\nh\ndk\ndv\nPdrop\nϵls\ntrain\nPPL\nBLEU\nparams\nsteps\n(dev)\n(dev)\n×106\nbase\n6\n512\n2048\n8\n64\n64\n0.1\n0.1\n100K\n4.92\n25.8\n65\n(A)\n1\n512\n512\n5.29\n24.9\n4\n128\n128\n5.00\n25.5\n16\n32\n32\n4.91\n25.8\n32\n16\n16\n5.01\n25.4\n(B)\n16\n5.16\n25.1\n58\n32\n5.01\n25.4\n60\n(C)\n2\n6.11\n23.7\n36\n4\n5.19\n25.3\n50\n8\n4.88\n25.5\n80\n256\n32\n32\n5.75\n24.5\n28\n1024\n128\n128\n4.66\n26.0\n168\n1024\n5.12\n25.4\n53\n4096\n4.75\n26.2\n90\n(D)\n0.0\n5.77\n24.6\n0.2\n4.95\n25.5\n0.0\n4.67\n25.3\n0.2\n5.47\n25.7\n(E)\npositional embedding instead of sinusoids\n4.92\n25.7\nbig\n6\n1024\n4096\n16\n0.3\n300K\n4.33\n26.4\n213\nIn Table 3 rows (B), we observe that reducing the attention key size dk hurts model quality. This\nsuggests that determining compatibility is not easy and that a more sophisticated compatibility\nfunction than dot product may be beneﬁcial. We further observe in rows (C) and (D) that, as expected,\nbigger models are better, and dropout is very helpful in avoiding over-ﬁtting. In row (E) we replace our\nsinusoidal positional encoding with learned positional embeddings [8], and observe nearly identical\nresults to the base model.\n7\nConclusion\nIn this work, we presented the Transformer, the ﬁrst sequence transduction model based entirely on\nattention, replacing the recurrent layers most commonly used in encoder-decoder architectures with\nmulti-headed self-attention.\nFor translation tasks, the Transformer can be trained signiﬁcantly faster than architectures based\non recurrent or convolutional layers. On both WMT 2014 English-to-German and WMT 2014\nEnglish-to-French translation tasks, we achieve a new state of the art. In the former task our best\nmodel outperforms even all previously reported ensembles.\nWe are excited about the future of attention-based models and plan to apply them to other tasks. We\nplan to extend the Transformer to problems involving input and output modalities other than text and\nto investigate local, restricted attention mechanisms to efﬁciently handle large inputs and outputs\nsuch as images, audio and video. Making generation less sequential is another research goals of ours.\nThe code we used to train and evaluate our models is available at https://github.com/\ntensorflow/tensor2tensor.\nAcknowledgements\nWe are grateful to Nal Kalchbrenner and Stephan Gouws for their fruitful\ncomments, corrections and inspiration.\n9",
  "643": "References\n[1] Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hinton. Layer normalization. arXiv preprint\narXiv:1607.06450, 2016.\n[2] Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. Neural machine translation by jointly\nlearning to align and translate. CoRR, abs/1409.0473, 2014.\n[3] Denny Britz, Anna Goldie, Minh-Thang Luong, and Quoc V. Le. Massive exploration of neural\nmachine translation architectures. CoRR, abs/1703.03906, 2017.\n[4] Jianpeng Cheng, Li Dong, and Mirella Lapata. Long short-term memory-networks for machine\nreading. arXiv preprint arXiv:1601.06733, 2016.\n[5] Kyunghyun Cho, Bart van Merrienboer, Caglar Gulcehre, Fethi Bougares, Holger Schwenk,\nand Yoshua Bengio. Learning phrase representations using rnn encoder-decoder for statistical\nmachine translation. CoRR, abs/1406.1078, 2014.\n[6] Francois Chollet. Xception: Deep learning with depthwise separable convolutions. arXiv\npreprint arXiv:1610.02357, 2016.\n[7] Junyoung Chung, Çaglar Gülçehre, Kyunghyun Cho, and Yoshua Bengio. Empirical evaluation\nof gated recurrent neural networks on sequence modeling. CoRR, abs/1412.3555, 2014.\n[8] Jonas Gehring, Michael Auli, David Grangier, Denis Yarats, and Yann N. Dauphin. Convolu-\ntional sequence to sequence learning. arXiv preprint arXiv:1705.03122v2, 2017.\n[9] Alex Graves.\nGenerating sequences with recurrent neural networks.\narXiv preprint\narXiv:1308.0850, 2013.\n[10] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for im-\nage recognition. In Proceedings of the IEEE Conference on Computer Vision and Pattern\nRecognition, pages 770–778, 2016.\n[11] Sepp Hochreiter, Yoshua Bengio, Paolo Frasconi, and Jürgen Schmidhuber. Gradient ﬂow in\nrecurrent nets: the difﬁculty of learning long-term dependencies, 2001.\n[12] Sepp Hochreiter and Jürgen Schmidhuber. Long short-term memory. Neural computation,\n9(8):1735–1780, 1997.\n[13] Rafal Jozefowicz, Oriol Vinyals, Mike Schuster, Noam Shazeer, and Yonghui Wu. Exploring\nthe limits of language modeling. arXiv preprint arXiv:1602.02410, 2016.\n[14] Łukasz Kaiser and Ilya Sutskever. Neural GPUs learn algorithms. In International Conference\non Learning Representations (ICLR), 2016.\n[15] Nal Kalchbrenner, Lasse Espeholt, Karen Simonyan, Aaron van den Oord, Alex Graves, and Ko-\nray Kavukcuoglu. Neural machine translation in linear time. arXiv preprint arXiv:1610.10099v2,\n2017.\n[16] Yoon Kim, Carl Denton, Luong Hoang, and Alexander M. Rush. Structured attention networks.\nIn International Conference on Learning Representations, 2017.\n[17] Diederik Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In ICLR, 2015.\n[18] Oleksii Kuchaiev and Boris Ginsburg. Factorization tricks for LSTM networks. arXiv preprint\narXiv:1703.10722, 2017.\n[19] Zhouhan Lin, Minwei Feng, Cicero Nogueira dos Santos, Mo Yu, Bing Xiang, Bowen\nZhou, and Yoshua Bengio. A structured self-attentive sentence embedding. arXiv preprint\narXiv:1703.03130, 2017.\n[20] Samy Bengio Łukasz Kaiser. Can active memory replace attention? In Advances in Neural\nInformation Processing Systems, (NIPS), 2016.\n10",
  "644": "[21] Minh-Thang Luong, Hieu Pham, and Christopher D Manning. Effective approaches to attention-\nbased neural machine translation. arXiv preprint arXiv:1508.04025, 2015.\n[22] Ankur Parikh, Oscar Täckström, Dipanjan Das, and Jakob Uszkoreit. A decomposable attention\nmodel. In Empirical Methods in Natural Language Processing, 2016.\n[23] Romain Paulus, Caiming Xiong, and Richard Socher. A deep reinforced model for abstractive\nsummarization. arXiv preprint arXiv:1705.04304, 2017.\n[24] Oﬁr Press and Lior Wolf. Using the output embedding to improve language models. arXiv\npreprint arXiv:1608.05859, 2016.\n[25] Rico Sennrich, Barry Haddow, and Alexandra Birch. Neural machine translation of rare words\nwith subword units. arXiv preprint arXiv:1508.07909, 2015.\n[26] Noam Shazeer, Azalia Mirhoseini, Krzysztof Maziarz, Andy Davis, Quoc Le, Geoffrey Hinton,\nand Jeff Dean. Outrageously large neural networks: The sparsely-gated mixture-of-experts\nlayer. arXiv preprint arXiv:1701.06538, 2017.\n[27] Nitish Srivastava, Geoffrey E Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdi-\nnov. Dropout: a simple way to prevent neural networks from overﬁtting. Journal of Machine\nLearning Research, 15(1):1929–1958, 2014.\n[28] Sainbayar Sukhbaatar, arthur szlam, Jason Weston, and Rob Fergus. End-to-end memory\nnetworks. In C. Cortes, N. D. Lawrence, D. D. Lee, M. Sugiyama, and R. Garnett, editors,\nAdvances in Neural Information Processing Systems 28, pages 2440–2448. Curran Associates,\nInc., 2015.\n[29] Ilya Sutskever, Oriol Vinyals, and Quoc VV Le. Sequence to sequence learning with neural\nnetworks. In Advances in Neural Information Processing Systems, pages 3104–3112, 2014.\n[30] Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jonathon Shlens, and Zbigniew Wojna.\nRethinking the inception architecture for computer vision. CoRR, abs/1512.00567, 2015.\n[31] Yonghui Wu, Mike Schuster, Zhifeng Chen, Quoc V Le, Mohammad Norouzi, Wolfgang\nMacherey, Maxim Krikun, Yuan Cao, Qin Gao, Klaus Macherey, et al. Google’s neural machine\ntranslation system: Bridging the gap between human and machine translation. arXiv preprint\narXiv:1609.08144, 2016.\n[32] Jie Zhou, Ying Cao, Xuguang Wang, Peng Li, and Wei Xu. Deep recurrent models with\nfast-forward connections for neural machine translation. CoRR, abs/1606.04199, 2016.\n11",
  "645": "Attention Is All You Need\nAshish Vaswani∗\nGoogle Brain\navaswani@google.com\nNoam Shazeer∗\nGoogle Brain\nnoam@google.com\nNiki Parmar∗\nGoogle Research\nnikip@google.com\nJakob Uszkoreit∗\nGoogle Research\nusz@google.com\nLlion Jones∗\nGoogle Research\nllion@google.com\nAidan N. Gomez∗†\nUniversity of Toronto\naidan@cs.toronto.edu\nŁukasz Kaiser∗\nGoogle Brain\nlukaszkaiser@google.com\nIllia Polosukhin∗‡\nillia.polosukhin@gmail.com\nAbstract\nThe dominant sequence transduction models are based on complex recurrent or\nconvolutional neural networks that include an encoder and a decoder. The best\nperforming models also connect the encoder and decoder through an attention\nmechanism. We propose a new simple network architecture, the Transformer,\nbased solely on attention mechanisms, dispensing with recurrence and convolutions\nentirely. Experiments on two machine translation tasks show these models to\nbe superior in quality while being more parallelizable and requiring signiﬁcantly\nless time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-\nto-German translation task, improving over the existing best results, including\nensembles, by over 2 BLEU. On the WMT 2014 English-to-French translation task,\nour model establishes a new single-model state-of-the-art BLEU score of 41.0 after\ntraining for 3.5 days on eight GPUs, a small fraction of the training costs of the\nbest models from the literature.\n1\nIntroduction\nRecurrent neural networks, long short-term memory [12] and gated recurrent [7] neural networks\nin particular, have been ﬁrmly established as state of the art approaches in sequence modeling and\ntransduction problems such as language modeling and machine translation [29, 2, 5]. Numerous\nefforts have since continued to push the boundaries of recurrent language models and encoder-decoder\narchitectures [31, 21, 13].\n∗Equal contribution. Listing order is random. Jakob proposed replacing RNNs with self-attention and started\nthe effort to evaluate this idea. Ashish, with Illia, designed and implemented the ﬁrst Transformer models and\nhas been crucially involved in every aspect of this work. Noam proposed scaled dot-product attention, multi-head\nattention and the parameter-free position representation and became the other person involved in nearly every\ndetail. Niki designed, implemented, tuned and evaluated countless model variants in our original codebase and\ntensor2tensor. Llion also experimented with novel model variants, was responsible for our initial codebase, and\nefﬁcient inference and visualizations. Lukasz and Aidan spent countless long days designing various parts of and\nimplementing tensor2tensor, replacing our earlier codebase, greatly improving results and massively accelerating\nour research.\n†Work performed while at Google Brain.\n‡Work performed while at Google Research.\n31st Conference on Neural Information Processing Systems (NIPS 2017), Long Beach, CA, USA.",
  "646": "Recurrent models typically factor computation along the symbol positions of the input and output\nsequences. Aligning the positions to steps in computation time, they generate a sequence of hidden\nstates ht, as a function of the previous hidden state ht−1 and the input for position t. This inherently\nsequential nature precludes parallelization within training examples, which becomes critical at longer\nsequence lengths, as memory constraints limit batching across examples. Recent work has achieved\nsigniﬁcant improvements in computational efﬁciency through factorization tricks [18] and conditional\ncomputation [26], while also improving model performance in case of the latter. The fundamental\nconstraint of sequential computation, however, remains.\nAttention mechanisms have become an integral part of compelling sequence modeling and transduc-\ntion models in various tasks, allowing modeling of dependencies without regard to their distance in\nthe input or output sequences [2, 16]. In all but a few cases [22], however, such attention mechanisms\nare used in conjunction with a recurrent network.\nIn this work we propose the Transformer, a model architecture eschewing recurrence and instead\nrelying entirely on an attention mechanism to draw global dependencies between input and output.\nThe Transformer allows for signiﬁcantly more parallelization and can reach a new state of the art in\ntranslation quality after being trained for as little as twelve hours on eight P100 GPUs.\n2\nBackground\nThe goal of reducing sequential computation also forms the foundation of the Extended Neural GPU\n[20], ByteNet [15] and ConvS2S [8], all of which use convolutional neural networks as basic building\nblock, computing hidden representations in parallel for all input and output positions. In these models,\nthe number of operations required to relate signals from two arbitrary input or output positions grows\nin the distance between positions, linearly for ConvS2S and logarithmically for ByteNet. This makes\nit more difﬁcult to learn dependencies between distant positions [11]. In the Transformer this is\nreduced to a constant number of operations, albeit at the cost of reduced effective resolution due\nto averaging attention-weighted positions, an effect we counteract with Multi-Head Attention as\ndescribed in section 3.2.\nSelf-attention, sometimes called intra-attention is an attention mechanism relating different positions\nof a single sequence in order to compute a representation of the sequence. Self-attention has been\nused successfully in a variety of tasks including reading comprehension, abstractive summarization,\ntextual entailment and learning task-independent sentence representations [4, 22, 23, 19].\nEnd-to-end memory networks are based on a recurrent attention mechanism instead of sequence-\naligned recurrence and have been shown to perform well on simple-language question answering and\nlanguage modeling tasks [28].\nTo the best of our knowledge, however, the Transformer is the ﬁrst transduction model relying\nentirely on self-attention to compute representations of its input and output without using sequence-\naligned RNNs or convolution. In the following sections, we will describe the Transformer, motivate\nself-attention and discuss its advantages over models such as [14, 15] and [8].\n3\nModel Architecture\nMost competitive neural sequence transduction models have an encoder-decoder structure [5, 2, 29].\nHere, the encoder maps an input sequence of symbol representations (x1, ..., xn) to a sequence\nof continuous representations z = (z1, ..., zn). Given z, the decoder then generates an output\nsequence (y1, ..., ym) of symbols one element at a time. At each step the model is auto-regressive\n[9], consuming the previously generated symbols as additional input when generating the next.\nThe Transformer follows this overall architecture using stacked self-attention and point-wise, fully\nconnected layers for both the encoder and decoder, shown in the left and right halves of Figure 1,\nrespectively.\n3.1\nEncoder and Decoder Stacks\nEncoder:\nThe encoder is composed of a stack of N = 6 identical layers. Each layer has two\nsub-layers. The ﬁrst is a multi-head self-attention mechanism, and the second is a simple, position-\n2",
  "647": "Figure 1: The Transformer - model architecture.\nwise fully connected feed-forward network. We employ a residual connection [10] around each of\nthe two sub-layers, followed by layer normalization [1]. That is, the output of each sub-layer is\nLayerNorm(x + Sublayer(x)), where Sublayer(x) is the function implemented by the sub-layer\nitself. To facilitate these residual connections, all sub-layers in the model, as well as the embedding\nlayers, produce outputs of dimension dmodel = 512.\nDecoder:\nThe decoder is also composed of a stack of N = 6 identical layers. In addition to the two\nsub-layers in each encoder layer, the decoder inserts a third sub-layer, which performs multi-head\nattention over the output of the encoder stack. Similar to the encoder, we employ residual connections\naround each of the sub-layers, followed by layer normalization. We also modify the self-attention\nsub-layer in the decoder stack to prevent positions from attending to subsequent positions. This\nmasking, combined with fact that the output embeddings are offset by one position, ensures that the\npredictions for position i can depend only on the known outputs at positions less than i.\n3.2\nAttention\nAn attention function can be described as mapping a query and a set of key-value pairs to an output,\nwhere the query, keys, values, and output are all vectors. The output is computed as a weighted sum\nof the values, where the weight assigned to each value is computed by a compatibility function of the\nquery with the corresponding key.\n3.2.1\nScaled Dot-Product Attention\nWe call our particular attention \"Scaled Dot-Product Attention\" (Figure 2). The input consists of\nqueries and keys of dimension dk, and values of dimension dv. We compute the dot products of the\n3",
  "648": "Scaled Dot-Product Attention\nMulti-Head Attention\nFigure 2: (left) Scaled Dot-Product Attention. (right) Multi-Head Attention consists of several\nattention layers running in parallel.\nquery with all keys, divide each by √dk, and apply a softmax function to obtain the weights on the\nvalues.\nIn practice, we compute the attention function on a set of queries simultaneously, packed together\ninto a matrix Q. The keys and values are also packed together into matrices K and V . We compute\nthe matrix of outputs as:\nAttention(Q, K, V ) = softmax(QKT\n√dk\n)V\n(1)\nThe two most commonly used attention functions are additive attention [2], and dot-product (multi-\nplicative) attention. Dot-product attention is identical to our algorithm, except for the scaling factor\nof\n1\n√dk . Additive attention computes the compatibility function using a feed-forward network with\na single hidden layer. While the two are similar in theoretical complexity, dot-product attention is\nmuch faster and more space-efﬁcient in practice, since it can be implemented using highly optimized\nmatrix multiplication code.\nWhile for small values of dk the two mechanisms perform similarly, additive attention outperforms\ndot product attention without scaling for larger values of dk [3]. We suspect that for large values of\ndk, the dot products grow large in magnitude, pushing the softmax function into regions where it has\nextremely small gradients 4. To counteract this effect, we scale the dot products by\n1\n√dk .\n3.2.2\nMulti-Head Attention\nInstead of performing a single attention function with dmodel-dimensional keys, values and queries,\nwe found it beneﬁcial to linearly project the queries, keys and values h times with different, learned\nlinear projections to dk, dk and dv dimensions, respectively. On each of these projected versions of\nqueries, keys and values we then perform the attention function in parallel, yielding dv-dimensional\noutput values. These are concatenated and once again projected, resulting in the ﬁnal values, as\ndepicted in Figure 2.\nMulti-head attention allows the model to jointly attend to information from different representation\nsubspaces at different positions. With a single attention head, averaging inhibits this.\n4To illustrate why the dot products get large, assume that the components of q and k are independent random\nvariables with mean 0 and variance 1. Then their dot product, q · k = Pdk\ni=1 qiki, has mean 0 and variance dk.\n4",
  "649": "MultiHead(Q, K, V ) = Concat(head1, ..., headh)W O\nwhere headi = Attention(QW Q\ni , KW K\ni , V W V\ni )\nWhere the projections are parameter matrices W Q\ni\n∈Rdmodel×dk, W K\ni\n∈Rdmodel×dk, W V\ni\n∈Rdmodel×dv\nand W O ∈Rhdv×dmodel.\nIn this work we employ h = 8 parallel attention layers, or heads. For each of these we use\ndk = dv = dmodel/h = 64. Due to the reduced dimension of each head, the total computational cost\nis similar to that of single-head attention with full dimensionality.\n3.2.3\nApplications of Attention in our Model\nThe Transformer uses multi-head attention in three different ways:\n• In \"encoder-decoder attention\" layers, the queries come from the previous decoder layer,\nand the memory keys and values come from the output of the encoder. This allows every\nposition in the decoder to attend over all positions in the input sequence. This mimics the\ntypical encoder-decoder attention mechanisms in sequence-to-sequence models such as\n[31, 2, 8].\n• The encoder contains self-attention layers. In a self-attention layer all of the keys, values\nand queries come from the same place, in this case, the output of the previous layer in the\nencoder. Each position in the encoder can attend to all positions in the previous layer of the\nencoder.\n• Similarly, self-attention layers in the decoder allow each position in the decoder to attend to\nall positions in the decoder up to and including that position. We need to prevent leftward\ninformation ﬂow in the decoder to preserve the auto-regressive property. We implement this\ninside of scaled dot-product attention by masking out (setting to −∞) all values in the input\nof the softmax which correspond to illegal connections. See Figure 2.\n3.3\nPosition-wise Feed-Forward Networks\nIn addition to attention sub-layers, each of the layers in our encoder and decoder contains a fully\nconnected feed-forward network, which is applied to each position separately and identically. This\nconsists of two linear transformations with a ReLU activation in between.\nFFN(x) = max(0, xW1 + b1)W2 + b2\n(2)\nWhile the linear transformations are the same across different positions, they use different parameters\nfrom layer to layer. Another way of describing this is as two convolutions with kernel size 1.\nThe dimensionality of input and output is dmodel = 512, and the inner-layer has dimensionality\ndff = 2048.\n3.4\nEmbeddings and Softmax\nSimilarly to other sequence transduction models, we use learned embeddings to convert the input\ntokens and output tokens to vectors of dimension dmodel. We also use the usual learned linear transfor-\nmation and softmax function to convert the decoder output to predicted next-token probabilities. In\nour model, we share the same weight matrix between the two embedding layers and the pre-softmax\nlinear transformation, similar to [24]. In the embedding layers, we multiply those weights by √dmodel.\n3.5\nPositional Encoding\nSince our model contains no recurrence and no convolution, in order for the model to make use of the\norder of the sequence, we must inject some information about the relative or absolute position of the\ntokens in the sequence. To this end, we add \"positional encodings\" to the input embeddings at the\n5",
  "650": "Table 1: Maximum path lengths, per-layer complexity and minimum number of sequential operations\nfor different layer types. n is the sequence length, d is the representation dimension, k is the kernel\nsize of convolutions and r the size of the neighborhood in restricted self-attention.\nLayer Type\nComplexity per Layer\nSequential\nMaximum Path Length\nOperations\nSelf-Attention\nO(n2 · d)\nO(1)\nO(1)\nRecurrent\nO(n · d2)\nO(n)\nO(n)\nConvolutional\nO(k · n · d2)\nO(1)\nO(logk(n))\nSelf-Attention (restricted)\nO(r · n · d)\nO(1)\nO(n/r)\nbottoms of the encoder and decoder stacks. The positional encodings have the same dimension dmodel\nas the embeddings, so that the two can be summed. There are many choices of positional encodings,\nlearned and ﬁxed [8].\nIn this work, we use sine and cosine functions of different frequencies:\nPE(pos,2i) = sin(pos/100002i/dmodel)\nPE(pos,2i+1) = cos(pos/100002i/dmodel)\nwhere pos is the position and i is the dimension. That is, each dimension of the positional encoding\ncorresponds to a sinusoid. The wavelengths form a geometric progression from 2π to 10000 · 2π. We\nchose this function because we hypothesized it would allow the model to easily learn to attend by\nrelative positions, since for any ﬁxed offset k, PEpos+k can be represented as a linear function of\nPEpos.\nWe also experimented with using learned positional embeddings [8] instead, and found that the two\nversions produced nearly identical results (see Table 3 row (E)). We chose the sinusoidal version\nbecause it may allow the model to extrapolate to sequence lengths longer than the ones encountered\nduring training.\n4\nWhy Self-Attention\nIn this section we compare various aspects of self-attention layers to the recurrent and convolu-\ntional layers commonly used for mapping one variable-length sequence of symbol representations\n(x1, ..., xn) to another sequence of equal length (z1, ..., zn), with xi, zi ∈Rd, such as a hidden\nlayer in a typical sequence transduction encoder or decoder. Motivating our use of self-attention we\nconsider three desiderata.\nOne is the total computational complexity per layer. Another is the amount of computation that can\nbe parallelized, as measured by the minimum number of sequential operations required.\nThe third is the path length between long-range dependencies in the network. Learning long-range\ndependencies is a key challenge in many sequence transduction tasks. One key factor affecting the\nability to learn such dependencies is the length of the paths forward and backward signals have to\ntraverse in the network. The shorter these paths between any combination of positions in the input\nand output sequences, the easier it is to learn long-range dependencies [11]. Hence we also compare\nthe maximum path length between any two input and output positions in networks composed of the\ndifferent layer types.\nAs noted in Table 1, a self-attention layer connects all positions with a constant number of sequentially\nexecuted operations, whereas a recurrent layer requires O(n) sequential operations. In terms of\ncomputational complexity, self-attention layers are faster than recurrent layers when the sequence\nlength n is smaller than the representation dimensionality d, which is most often the case with\nsentence representations used by state-of-the-art models in machine translations, such as word-piece\n[31] and byte-pair [25] representations. To improve computational performance for tasks involving\nvery long sequences, self-attention could be restricted to considering only a neighborhood of size r in\n6",
  "651": "the input sequence centered around the respective output position. This would increase the maximum\npath length to O(n/r). We plan to investigate this approach further in future work.\nA single convolutional layer with kernel width k < n does not connect all pairs of input and output\npositions. Doing so requires a stack of O(n/k) convolutional layers in the case of contiguous kernels,\nor O(logk(n)) in the case of dilated convolutions [15], increasing the length of the longest paths\nbetween any two positions in the network. Convolutional layers are generally more expensive than\nrecurrent layers, by a factor of k. Separable convolutions [6], however, decrease the complexity\nconsiderably, to O(k · n · d + n · d2). Even with k = n, however, the complexity of a separable\nconvolution is equal to the combination of a self-attention layer and a point-wise feed-forward layer,\nthe approach we take in our model.\nAs side beneﬁt, self-attention could yield more interpretable models. We inspect attention distributions\nfrom our models and present and discuss examples in the appendix. Not only do individual attention\nheads clearly learn to perform different tasks, many appear to exhibit behavior related to the syntactic\nand semantic structure of the sentences.\n5\nTraining\nThis section describes the training regime for our models.\n5.1\nTraining Data and Batching\nWe trained on the standard WMT 2014 English-German dataset consisting of about 4.5 million\nsentence pairs. Sentences were encoded using byte-pair encoding [3], which has a shared source-\ntarget vocabulary of about 37000 tokens. For English-French, we used the signiﬁcantly larger WMT\n2014 English-French dataset consisting of 36M sentences and split tokens into a 32000 word-piece\nvocabulary [31]. Sentence pairs were batched together by approximate sequence length. Each training\nbatch contained a set of sentence pairs containing approximately 25000 source tokens and 25000\ntarget tokens.\n5.2\nHardware and Schedule\nWe trained our models on one machine with 8 NVIDIA P100 GPUs. For our base models using\nthe hyperparameters described throughout the paper, each training step took about 0.4 seconds. We\ntrained the base models for a total of 100,000 steps or 12 hours. For our big models,(described on the\nbottom line of table 3), step time was 1.0 seconds. The big models were trained for 300,000 steps\n(3.5 days).\n5.3\nOptimizer\nWe used the Adam optimizer [17] with β1 = 0.9, β2 = 0.98 and ϵ = 10−9. We varied the learning\nrate over the course of training, according to the formula:\nlrate = d−0.5\nmodel · min(step_num−0.5, step_num · warmup_steps−1.5)\n(3)\nThis corresponds to increasing the learning rate linearly for the ﬁrst warmup_steps training steps,\nand decreasing it thereafter proportionally to the inverse square root of the step number. We used\nwarmup_steps = 4000.\n5.4\nRegularization\nWe employ three types of regularization during training:\nResidual Dropout\nWe apply dropout [27] to the output of each sub-layer, before it is added to the\nsub-layer input and normalized. In addition, we apply dropout to the sums of the embeddings and the\npositional encodings in both the encoder and decoder stacks. For the base model, we use a rate of\nPdrop = 0.1.\n7",
  "652": "Table 2: The Transformer achieves better BLEU scores than previous state-of-the-art models on the\nEnglish-to-German and English-to-French newstest2014 tests at a fraction of the training cost.\nModel\nBLEU\nTraining Cost (FLOPs)\nEN-DE\nEN-FR\nEN-DE\nEN-FR\nByteNet [15]\n23.75\nDeep-Att + PosUnk [32]\n39.2\n1.0 · 1020\nGNMT + RL [31]\n24.6\n39.92\n2.3 · 1019\n1.4 · 1020\nConvS2S [8]\n25.16\n40.46\n9.6 · 1018\n1.5 · 1020\nMoE [26]\n26.03\n40.56\n2.0 · 1019\n1.2 · 1020\nDeep-Att + PosUnk Ensemble [32]\n40.4\n8.0 · 1020\nGNMT + RL Ensemble [31]\n26.30\n41.16\n1.8 · 1020\n1.1 · 1021\nConvS2S Ensemble [8]\n26.36\n41.29\n7.7 · 1019\n1.2 · 1021\nTransformer (base model)\n27.3\n38.1\n3.3 · 1018\nTransformer (big)\n28.4\n41.0\n2.3 · 1019\nLabel Smoothing\nDuring training, we employed label smoothing of value ϵls = 0.1 [30]. This\nhurts perplexity, as the model learns to be more unsure, but improves accuracy and BLEU score.\n6\nResults\n6.1\nMachine Translation\nOn the WMT 2014 English-to-German translation task, the big transformer model (Transformer (big)\nin Table 2) outperforms the best previously reported models (including ensembles) by more than 2.0\nBLEU, establishing a new state-of-the-art BLEU score of 28.4. The conﬁguration of this model is\nlisted in the bottom line of Table 3. Training took 3.5 days on 8 P100 GPUs. Even our base model\nsurpasses all previously published models and ensembles, at a fraction of the training cost of any of\nthe competitive models.\nOn the WMT 2014 English-to-French translation task, our big model achieves a BLEU score of 41.0,\noutperforming all of the previously published single models, at less than 1/4 the training cost of the\nprevious state-of-the-art model. The Transformer (big) model trained for English-to-French used\ndropout rate Pdrop = 0.1, instead of 0.3.\nFor the base models, we used a single model obtained by averaging the last 5 checkpoints, which\nwere written at 10-minute intervals. For the big models, we averaged the last 20 checkpoints. We\nused beam search with a beam size of 4 and length penalty α = 0.6 [31]. These hyperparameters\nwere chosen after experimentation on the development set. We set the maximum output length during\ninference to input length + 50, but terminate early when possible [31].\nTable 2 summarizes our results and compares our translation quality and training costs to other model\narchitectures from the literature. We estimate the number of ﬂoating point operations used to train a\nmodel by multiplying the training time, the number of GPUs used, and an estimate of the sustained\nsingle-precision ﬂoating-point capacity of each GPU 5.\n6.2\nModel Variations\nTo evaluate the importance of different components of the Transformer, we varied our base model\nin different ways, measuring the change in performance on English-to-German translation on the\ndevelopment set, newstest2013. We used beam search as described in the previous section, but no\ncheckpoint averaging. We present these results in Table 3.\nIn Table 3 rows (A), we vary the number of attention heads and the attention key and value dimensions,\nkeeping the amount of computation constant, as described in Section 3.2.2. While single-head\nattention is 0.9 BLEU worse than the best setting, quality also drops off with too many heads.\n5We used values of 2.8, 3.7, 6.0 and 9.5 TFLOPS for K80, K40, M40 and P100, respectively.\n8",
  "653": "Table 3: Variations on the Transformer architecture. Unlisted values are identical to those of the base\nmodel. All metrics are on the English-to-German translation development set, newstest2013. Listed\nperplexities are per-wordpiece, according to our byte-pair encoding, and should not be compared to\nper-word perplexities.\nN\ndmodel\ndff\nh\ndk\ndv\nPdrop\nϵls\ntrain\nPPL\nBLEU\nparams\nsteps\n(dev)\n(dev)\n×106\nbase\n6\n512\n2048\n8\n64\n64\n0.1\n0.1\n100K\n4.92\n25.8\n65\n(A)\n1\n512\n512\n5.29\n24.9\n4\n128\n128\n5.00\n25.5\n16\n32\n32\n4.91\n25.8\n32\n16\n16\n5.01\n25.4\n(B)\n16\n5.16\n25.1\n58\n32\n5.01\n25.4\n60\n(C)\n2\n6.11\n23.7\n36\n4\n5.19\n25.3\n50\n8\n4.88\n25.5\n80\n256\n32\n32\n5.75\n24.5\n28\n1024\n128\n128\n4.66\n26.0\n168\n1024\n5.12\n25.4\n53\n4096\n4.75\n26.2\n90\n(D)\n0.0\n5.77\n24.6\n0.2\n4.95\n25.5\n0.0\n4.67\n25.3\n0.2\n5.47\n25.7\n(E)\npositional embedding instead of sinusoids\n4.92\n25.7\nbig\n6\n1024\n4096\n16\n0.3\n300K\n4.33\n26.4\n213\nIn Table 3 rows (B), we observe that reducing the attention key size dk hurts model quality. This\nsuggests that determining compatibility is not easy and that a more sophisticated compatibility\nfunction than dot product may be beneﬁcial. We further observe in rows (C) and (D) that, as expected,\nbigger models are better, and dropout is very helpful in avoiding over-ﬁtting. In row (E) we replace our\nsinusoidal positional encoding with learned positional embeddings [8], and observe nearly identical\nresults to the base model.\n7\nConclusion\nIn this work, we presented the Transformer, the ﬁrst sequence transduction model based entirely on\nattention, replacing the recurrent layers most commonly used in encoder-decoder architectures with\nmulti-headed self-attention.\nFor translation tasks, the Transformer can be trained signiﬁcantly faster than architectures based\non recurrent or convolutional layers. On both WMT 2014 English-to-German and WMT 2014\nEnglish-to-French translation tasks, we achieve a new state of the art. In the former task our best\nmodel outperforms even all previously reported ensembles.\nWe are excited about the future of attention-based models and plan to apply them to other tasks. We\nplan to extend the Transformer to problems involving input and output modalities other than text and\nto investigate local, restricted attention mechanisms to efﬁciently handle large inputs and outputs\nsuch as images, audio and video. Making generation less sequential is another research goals of ours.\nThe code we used to train and evaluate our models is available at https://github.com/\ntensorflow/tensor2tensor.\nAcknowledgements\nWe are grateful to Nal Kalchbrenner and Stephan Gouws for their fruitful\ncomments, corrections and inspiration.\n9",
  "654": "References\n[1] Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hinton. Layer normalization. arXiv preprint\narXiv:1607.06450, 2016.\n[2] Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. Neural machine translation by jointly\nlearning to align and translate. CoRR, abs/1409.0473, 2014.\n[3] Denny Britz, Anna Goldie, Minh-Thang Luong, and Quoc V. Le. Massive exploration of neural\nmachine translation architectures. CoRR, abs/1703.03906, 2017.\n[4] Jianpeng Cheng, Li Dong, and Mirella Lapata. Long short-term memory-networks for machine\nreading. arXiv preprint arXiv:1601.06733, 2016.\n[5] Kyunghyun Cho, Bart van Merrienboer, Caglar Gulcehre, Fethi Bougares, Holger Schwenk,\nand Yoshua Bengio. Learning phrase representations using rnn encoder-decoder for statistical\nmachine translation. CoRR, abs/1406.1078, 2014.\n[6] Francois Chollet. Xception: Deep learning with depthwise separable convolutions. arXiv\npreprint arXiv:1610.02357, 2016.\n[7] Junyoung Chung, Çaglar Gülçehre, Kyunghyun Cho, and Yoshua Bengio. Empirical evaluation\nof gated recurrent neural networks on sequence modeling. CoRR, abs/1412.3555, 2014.\n[8] Jonas Gehring, Michael Auli, David Grangier, Denis Yarats, and Yann N. Dauphin. Convolu-\ntional sequence to sequence learning. arXiv preprint arXiv:1705.03122v2, 2017.\n[9] Alex Graves.\nGenerating sequences with recurrent neural networks.\narXiv preprint\narXiv:1308.0850, 2013.\n[10] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for im-\nage recognition. In Proceedings of the IEEE Conference on Computer Vision and Pattern\nRecognition, pages 770–778, 2016.\n[11] Sepp Hochreiter, Yoshua Bengio, Paolo Frasconi, and Jürgen Schmidhuber. Gradient ﬂow in\nrecurrent nets: the difﬁculty of learning long-term dependencies, 2001.\n[12] Sepp Hochreiter and Jürgen Schmidhuber. Long short-term memory. Neural computation,\n9(8):1735–1780, 1997.\n[13] Rafal Jozefowicz, Oriol Vinyals, Mike Schuster, Noam Shazeer, and Yonghui Wu. Exploring\nthe limits of language modeling. arXiv preprint arXiv:1602.02410, 2016.\n[14] Łukasz Kaiser and Ilya Sutskever. Neural GPUs learn algorithms. In International Conference\non Learning Representations (ICLR), 2016.\n[15] Nal Kalchbrenner, Lasse Espeholt, Karen Simonyan, Aaron van den Oord, Alex Graves, and Ko-\nray Kavukcuoglu. Neural machine translation in linear time. arXiv preprint arXiv:1610.10099v2,\n2017.\n[16] Yoon Kim, Carl Denton, Luong Hoang, and Alexander M. Rush. Structured attention networks.\nIn International Conference on Learning Representations, 2017.\n[17] Diederik Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In ICLR, 2015.\n[18] Oleksii Kuchaiev and Boris Ginsburg. Factorization tricks for LSTM networks. arXiv preprint\narXiv:1703.10722, 2017.\n[19] Zhouhan Lin, Minwei Feng, Cicero Nogueira dos Santos, Mo Yu, Bing Xiang, Bowen\nZhou, and Yoshua Bengio. A structured self-attentive sentence embedding. arXiv preprint\narXiv:1703.03130, 2017.\n[20] Samy Bengio Łukasz Kaiser. Can active memory replace attention? In Advances in Neural\nInformation Processing Systems, (NIPS), 2016.\n10",
  "655": "[21] Minh-Thang Luong, Hieu Pham, and Christopher D Manning. Effective approaches to attention-\nbased neural machine translation. arXiv preprint arXiv:1508.04025, 2015.\n[22] Ankur Parikh, Oscar Täckström, Dipanjan Das, and Jakob Uszkoreit. A decomposable attention\nmodel. In Empirical Methods in Natural Language Processing, 2016.\n[23] Romain Paulus, Caiming Xiong, and Richard Socher. A deep reinforced model for abstractive\nsummarization. arXiv preprint arXiv:1705.04304, 2017.\n[24] Oﬁr Press and Lior Wolf. Using the output embedding to improve language models. arXiv\npreprint arXiv:1608.05859, 2016.\n[25] Rico Sennrich, Barry Haddow, and Alexandra Birch. Neural machine translation of rare words\nwith subword units. arXiv preprint arXiv:1508.07909, 2015.\n[26] Noam Shazeer, Azalia Mirhoseini, Krzysztof Maziarz, Andy Davis, Quoc Le, Geoffrey Hinton,\nand Jeff Dean. Outrageously large neural networks: The sparsely-gated mixture-of-experts\nlayer. arXiv preprint arXiv:1701.06538, 2017.\n[27] Nitish Srivastava, Geoffrey E Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdi-\nnov. Dropout: a simple way to prevent neural networks from overﬁtting. Journal of Machine\nLearning Research, 15(1):1929–1958, 2014.\n[28] Sainbayar Sukhbaatar, arthur szlam, Jason Weston, and Rob Fergus. End-to-end memory\nnetworks. In C. Cortes, N. D. Lawrence, D. D. Lee, M. Sugiyama, and R. Garnett, editors,\nAdvances in Neural Information Processing Systems 28, pages 2440–2448. Curran Associates,\nInc., 2015.\n[29] Ilya Sutskever, Oriol Vinyals, and Quoc VV Le. Sequence to sequence learning with neural\nnetworks. In Advances in Neural Information Processing Systems, pages 3104–3112, 2014.\n[30] Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jonathon Shlens, and Zbigniew Wojna.\nRethinking the inception architecture for computer vision. CoRR, abs/1512.00567, 2015.\n[31] Yonghui Wu, Mike Schuster, Zhifeng Chen, Quoc V Le, Mohammad Norouzi, Wolfgang\nMacherey, Maxim Krikun, Yuan Cao, Qin Gao, Klaus Macherey, et al. Google’s neural machine\ntranslation system: Bridging the gap between human and machine translation. arXiv preprint\narXiv:1609.08144, 2016.\n[32] Jie Zhou, Ying Cao, Xuguang Wang, Peng Li, and Wei Xu. Deep recurrent models with\nfast-forward connections for neural machine translation. CoRR, abs/1606.04199, 2016.\n11",
  "656": "Attention Is All You Need\nAshish Vaswani∗\nGoogle Brain\navaswani@google.com\nNoam Shazeer∗\nGoogle Brain\nnoam@google.com\nNiki Parmar∗\nGoogle Research\nnikip@google.com\nJakob Uszkoreit∗\nGoogle Research\nusz@google.com\nLlion Jones∗\nGoogle Research\nllion@google.com\nAidan N. Gomez∗†\nUniversity of Toronto\naidan@cs.toronto.edu\nŁukasz Kaiser∗\nGoogle Brain\nlukaszkaiser@google.com\nIllia Polosukhin∗‡\nillia.polosukhin@gmail.com\nAbstract\nThe dominant sequence transduction models are based on complex recurrent or\nconvolutional neural networks that include an encoder and a decoder. The best\nperforming models also connect the encoder and decoder through an attention\nmechanism. We propose a new simple network architecture, the Transformer,\nbased solely on attention mechanisms, dispensing with recurrence and convolutions\nentirely. Experiments on two machine translation tasks show these models to\nbe superior in quality while being more parallelizable and requiring signiﬁcantly\nless time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-\nto-German translation task, improving over the existing best results, including\nensembles, by over 2 BLEU. On the WMT 2014 English-to-French translation task,\nour model establishes a new single-model state-of-the-art BLEU score of 41.0 after\ntraining for 3.5 days on eight GPUs, a small fraction of the training costs of the\nbest models from the literature.\n1\nIntroduction\nRecurrent neural networks, long short-term memory [12] and gated recurrent [7] neural networks\nin particular, have been ﬁrmly established as state of the art approaches in sequence modeling and\ntransduction problems such as language modeling and machine translation [29, 2, 5]. Numerous\nefforts have since continued to push the boundaries of recurrent language models and encoder-decoder\narchitectures [31, 21, 13].\n∗Equal contribution. Listing order is random. Jakob proposed replacing RNNs with self-attention and started\nthe effort to evaluate this idea. Ashish, with Illia, designed and implemented the ﬁrst Transformer models and\nhas been crucially involved in every aspect of this work. Noam proposed scaled dot-product attention, multi-head\nattention and the parameter-free position representation and became the other person involved in nearly every\ndetail. Niki designed, implemented, tuned and evaluated countless model variants in our original codebase and\ntensor2tensor. Llion also experimented with novel model variants, was responsible for our initial codebase, and\nefﬁcient inference and visualizations. Lukasz and Aidan spent countless long days designing various parts of and\nimplementing tensor2tensor, replacing our earlier codebase, greatly improving results and massively accelerating\nour research.\n†Work performed while at Google Brain.\n‡Work performed while at Google Research.\n31st Conference on Neural Information Processing Systems (NIPS 2017), Long Beach, CA, USA.",
  "657": "Recurrent models typically factor computation along the symbol positions of the input and output\nsequences. Aligning the positions to steps in computation time, they generate a sequence of hidden\nstates ht, as a function of the previous hidden state ht−1 and the input for position t. This inherently\nsequential nature precludes parallelization within training examples, which becomes critical at longer\nsequence lengths, as memory constraints limit batching across examples. Recent work has achieved\nsigniﬁcant improvements in computational efﬁciency through factorization tricks [18] and conditional\ncomputation [26], while also improving model performance in case of the latter. The fundamental\nconstraint of sequential computation, however, remains.\nAttention mechanisms have become an integral part of compelling sequence modeling and transduc-\ntion models in various tasks, allowing modeling of dependencies without regard to their distance in\nthe input or output sequences [2, 16]. In all but a few cases [22], however, such attention mechanisms\nare used in conjunction with a recurrent network.\nIn this work we propose the Transformer, a model architecture eschewing recurrence and instead\nrelying entirely on an attention mechanism to draw global dependencies between input and output.\nThe Transformer allows for signiﬁcantly more parallelization and can reach a new state of the art in\ntranslation quality after being trained for as little as twelve hours on eight P100 GPUs.\n2\nBackground\nThe goal of reducing sequential computation also forms the foundation of the Extended Neural GPU\n[20], ByteNet [15] and ConvS2S [8], all of which use convolutional neural networks as basic building\nblock, computing hidden representations in parallel for all input and output positions. In these models,\nthe number of operations required to relate signals from two arbitrary input or output positions grows\nin the distance between positions, linearly for ConvS2S and logarithmically for ByteNet. This makes\nit more difﬁcult to learn dependencies between distant positions [11]. In the Transformer this is\nreduced to a constant number of operations, albeit at the cost of reduced effective resolution due\nto averaging attention-weighted positions, an effect we counteract with Multi-Head Attention as\ndescribed in section 3.2.\nSelf-attention, sometimes called intra-attention is an attention mechanism relating different positions\nof a single sequence in order to compute a representation of the sequence. Self-attention has been\nused successfully in a variety of tasks including reading comprehension, abstractive summarization,\ntextual entailment and learning task-independent sentence representations [4, 22, 23, 19].\nEnd-to-end memory networks are based on a recurrent attention mechanism instead of sequence-\naligned recurrence and have been shown to perform well on simple-language question answering and\nlanguage modeling tasks [28].\nTo the best of our knowledge, however, the Transformer is the ﬁrst transduction model relying\nentirely on self-attention to compute representations of its input and output without using sequence-\naligned RNNs or convolution. In the following sections, we will describe the Transformer, motivate\nself-attention and discuss its advantages over models such as [14, 15] and [8].\n3\nModel Architecture\nMost competitive neural sequence transduction models have an encoder-decoder structure [5, 2, 29].\nHere, the encoder maps an input sequence of symbol representations (x1, ..., xn) to a sequence\nof continuous representations z = (z1, ..., zn). Given z, the decoder then generates an output\nsequence (y1, ..., ym) of symbols one element at a time. At each step the model is auto-regressive\n[9], consuming the previously generated symbols as additional input when generating the next.\nThe Transformer follows this overall architecture using stacked self-attention and point-wise, fully\nconnected layers for both the encoder and decoder, shown in the left and right halves of Figure 1,\nrespectively.\n3.1\nEncoder and Decoder Stacks\nEncoder:\nThe encoder is composed of a stack of N = 6 identical layers. Each layer has two\nsub-layers. The ﬁrst is a multi-head self-attention mechanism, and the second is a simple, position-\n2",
  "658": "Figure 1: The Transformer - model architecture.\nwise fully connected feed-forward network. We employ a residual connection [10] around each of\nthe two sub-layers, followed by layer normalization [1]. That is, the output of each sub-layer is\nLayerNorm(x + Sublayer(x)), where Sublayer(x) is the function implemented by the sub-layer\nitself. To facilitate these residual connections, all sub-layers in the model, as well as the embedding\nlayers, produce outputs of dimension dmodel = 512.\nDecoder:\nThe decoder is also composed of a stack of N = 6 identical layers. In addition to the two\nsub-layers in each encoder layer, the decoder inserts a third sub-layer, which performs multi-head\nattention over the output of the encoder stack. Similar to the encoder, we employ residual connections\naround each of the sub-layers, followed by layer normalization. We also modify the self-attention\nsub-layer in the decoder stack to prevent positions from attending to subsequent positions. This\nmasking, combined with fact that the output embeddings are offset by one position, ensures that the\npredictions for position i can depend only on the known outputs at positions less than i.\n3.2\nAttention\nAn attention function can be described as mapping a query and a set of key-value pairs to an output,\nwhere the query, keys, values, and output are all vectors. The output is computed as a weighted sum\nof the values, where the weight assigned to each value is computed by a compatibility function of the\nquery with the corresponding key.\n3.2.1\nScaled Dot-Product Attention\nWe call our particular attention \"Scaled Dot-Product Attention\" (Figure 2). The input consists of\nqueries and keys of dimension dk, and values of dimension dv. We compute the dot products of the\n3",
  "659": "Scaled Dot-Product Attention\nMulti-Head Attention\nFigure 2: (left) Scaled Dot-Product Attention. (right) Multi-Head Attention consists of several\nattention layers running in parallel.\nquery with all keys, divide each by √dk, and apply a softmax function to obtain the weights on the\nvalues.\nIn practice, we compute the attention function on a set of queries simultaneously, packed together\ninto a matrix Q. The keys and values are also packed together into matrices K and V . We compute\nthe matrix of outputs as:\nAttention(Q, K, V ) = softmax(QKT\n√dk\n)V\n(1)\nThe two most commonly used attention functions are additive attention [2], and dot-product (multi-\nplicative) attention. Dot-product attention is identical to our algorithm, except for the scaling factor\nof\n1\n√dk . Additive attention computes the compatibility function using a feed-forward network with\na single hidden layer. While the two are similar in theoretical complexity, dot-product attention is\nmuch faster and more space-efﬁcient in practice, since it can be implemented using highly optimized\nmatrix multiplication code.\nWhile for small values of dk the two mechanisms perform similarly, additive attention outperforms\ndot product attention without scaling for larger values of dk [3]. We suspect that for large values of\ndk, the dot products grow large in magnitude, pushing the softmax function into regions where it has\nextremely small gradients 4. To counteract this effect, we scale the dot products by\n1\n√dk .\n3.2.2\nMulti-Head Attention\nInstead of performing a single attention function with dmodel-dimensional keys, values and queries,\nwe found it beneﬁcial to linearly project the queries, keys and values h times with different, learned\nlinear projections to dk, dk and dv dimensions, respectively. On each of these projected versions of\nqueries, keys and values we then perform the attention function in parallel, yielding dv-dimensional\noutput values. These are concatenated and once again projected, resulting in the ﬁnal values, as\ndepicted in Figure 2.\nMulti-head attention allows the model to jointly attend to information from different representation\nsubspaces at different positions. With a single attention head, averaging inhibits this.\n4To illustrate why the dot products get large, assume that the components of q and k are independent random\nvariables with mean 0 and variance 1. Then their dot product, q · k = Pdk\ni=1 qiki, has mean 0 and variance dk.\n4",
  "660": "MultiHead(Q, K, V ) = Concat(head1, ..., headh)W O\nwhere headi = Attention(QW Q\ni , KW K\ni , V W V\ni )\nWhere the projections are parameter matrices W Q\ni\n∈Rdmodel×dk, W K\ni\n∈Rdmodel×dk, W V\ni\n∈Rdmodel×dv\nand W O ∈Rhdv×dmodel.\nIn this work we employ h = 8 parallel attention layers, or heads. For each of these we use\ndk = dv = dmodel/h = 64. Due to the reduced dimension of each head, the total computational cost\nis similar to that of single-head attention with full dimensionality.\n3.2.3\nApplications of Attention in our Model\nThe Transformer uses multi-head attention in three different ways:\n• In \"encoder-decoder attention\" layers, the queries come from the previous decoder layer,\nand the memory keys and values come from the output of the encoder. This allows every\nposition in the decoder to attend over all positions in the input sequence. This mimics the\ntypical encoder-decoder attention mechanisms in sequence-to-sequence models such as\n[31, 2, 8].\n• The encoder contains self-attention layers. In a self-attention layer all of the keys, values\nand queries come from the same place, in this case, the output of the previous layer in the\nencoder. Each position in the encoder can attend to all positions in the previous layer of the\nencoder.\n• Similarly, self-attention layers in the decoder allow each position in the decoder to attend to\nall positions in the decoder up to and including that position. We need to prevent leftward\ninformation ﬂow in the decoder to preserve the auto-regressive property. We implement this\ninside of scaled dot-product attention by masking out (setting to −∞) all values in the input\nof the softmax which correspond to illegal connections. See Figure 2.\n3.3\nPosition-wise Feed-Forward Networks\nIn addition to attention sub-layers, each of the layers in our encoder and decoder contains a fully\nconnected feed-forward network, which is applied to each position separately and identically. This\nconsists of two linear transformations with a ReLU activation in between.\nFFN(x) = max(0, xW1 + b1)W2 + b2\n(2)\nWhile the linear transformations are the same across different positions, they use different parameters\nfrom layer to layer. Another way of describing this is as two convolutions with kernel size 1.\nThe dimensionality of input and output is dmodel = 512, and the inner-layer has dimensionality\ndff = 2048.\n3.4\nEmbeddings and Softmax\nSimilarly to other sequence transduction models, we use learned embeddings to convert the input\ntokens and output tokens to vectors of dimension dmodel. We also use the usual learned linear transfor-\nmation and softmax function to convert the decoder output to predicted next-token probabilities. In\nour model, we share the same weight matrix between the two embedding layers and the pre-softmax\nlinear transformation, similar to [24]. In the embedding layers, we multiply those weights by √dmodel.\n3.5\nPositional Encoding\nSince our model contains no recurrence and no convolution, in order for the model to make use of the\norder of the sequence, we must inject some information about the relative or absolute position of the\ntokens in the sequence. To this end, we add \"positional encodings\" to the input embeddings at the\n5",
  "661": "Table 1: Maximum path lengths, per-layer complexity and minimum number of sequential operations\nfor different layer types. n is the sequence length, d is the representation dimension, k is the kernel\nsize of convolutions and r the size of the neighborhood in restricted self-attention.\nLayer Type\nComplexity per Layer\nSequential\nMaximum Path Length\nOperations\nSelf-Attention\nO(n2 · d)\nO(1)\nO(1)\nRecurrent\nO(n · d2)\nO(n)\nO(n)\nConvolutional\nO(k · n · d2)\nO(1)\nO(logk(n))\nSelf-Attention (restricted)\nO(r · n · d)\nO(1)\nO(n/r)\nbottoms of the encoder and decoder stacks. The positional encodings have the same dimension dmodel\nas the embeddings, so that the two can be summed. There are many choices of positional encodings,\nlearned and ﬁxed [8].\nIn this work, we use sine and cosine functions of different frequencies:\nPE(pos,2i) = sin(pos/100002i/dmodel)\nPE(pos,2i+1) = cos(pos/100002i/dmodel)\nwhere pos is the position and i is the dimension. That is, each dimension of the positional encoding\ncorresponds to a sinusoid. The wavelengths form a geometric progression from 2π to 10000 · 2π. We\nchose this function because we hypothesized it would allow the model to easily learn to attend by\nrelative positions, since for any ﬁxed offset k, PEpos+k can be represented as a linear function of\nPEpos.\nWe also experimented with using learned positional embeddings [8] instead, and found that the two\nversions produced nearly identical results (see Table 3 row (E)). We chose the sinusoidal version\nbecause it may allow the model to extrapolate to sequence lengths longer than the ones encountered\nduring training.\n4\nWhy Self-Attention\nIn this section we compare various aspects of self-attention layers to the recurrent and convolu-\ntional layers commonly used for mapping one variable-length sequence of symbol representations\n(x1, ..., xn) to another sequence of equal length (z1, ..., zn), with xi, zi ∈Rd, such as a hidden\nlayer in a typical sequence transduction encoder or decoder. Motivating our use of self-attention we\nconsider three desiderata.\nOne is the total computational complexity per layer. Another is the amount of computation that can\nbe parallelized, as measured by the minimum number of sequential operations required.\nThe third is the path length between long-range dependencies in the network. Learning long-range\ndependencies is a key challenge in many sequence transduction tasks. One key factor affecting the\nability to learn such dependencies is the length of the paths forward and backward signals have to\ntraverse in the network. The shorter these paths between any combination of positions in the input\nand output sequences, the easier it is to learn long-range dependencies [11]. Hence we also compare\nthe maximum path length between any two input and output positions in networks composed of the\ndifferent layer types.\nAs noted in Table 1, a self-attention layer connects all positions with a constant number of sequentially\nexecuted operations, whereas a recurrent layer requires O(n) sequential operations. In terms of\ncomputational complexity, self-attention layers are faster than recurrent layers when the sequence\nlength n is smaller than the representation dimensionality d, which is most often the case with\nsentence representations used by state-of-the-art models in machine translations, such as word-piece\n[31] and byte-pair [25] representations. To improve computational performance for tasks involving\nvery long sequences, self-attention could be restricted to considering only a neighborhood of size r in\n6",
  "662": "the input sequence centered around the respective output position. This would increase the maximum\npath length to O(n/r). We plan to investigate this approach further in future work.\nA single convolutional layer with kernel width k < n does not connect all pairs of input and output\npositions. Doing so requires a stack of O(n/k) convolutional layers in the case of contiguous kernels,\nor O(logk(n)) in the case of dilated convolutions [15], increasing the length of the longest paths\nbetween any two positions in the network. Convolutional layers are generally more expensive than\nrecurrent layers, by a factor of k. Separable convolutions [6], however, decrease the complexity\nconsiderably, to O(k · n · d + n · d2). Even with k = n, however, the complexity of a separable\nconvolution is equal to the combination of a self-attention layer and a point-wise feed-forward layer,\nthe approach we take in our model.\nAs side beneﬁt, self-attention could yield more interpretable models. We inspect attention distributions\nfrom our models and present and discuss examples in the appendix. Not only do individual attention\nheads clearly learn to perform different tasks, many appear to exhibit behavior related to the syntactic\nand semantic structure of the sentences.\n5\nTraining\nThis section describes the training regime for our models.\n5.1\nTraining Data and Batching\nWe trained on the standard WMT 2014 English-German dataset consisting of about 4.5 million\nsentence pairs. Sentences were encoded using byte-pair encoding [3], which has a shared source-\ntarget vocabulary of about 37000 tokens. For English-French, we used the signiﬁcantly larger WMT\n2014 English-French dataset consisting of 36M sentences and split tokens into a 32000 word-piece\nvocabulary [31]. Sentence pairs were batched together by approximate sequence length. Each training\nbatch contained a set of sentence pairs containing approximately 25000 source tokens and 25000\ntarget tokens.\n5.2\nHardware and Schedule\nWe trained our models on one machine with 8 NVIDIA P100 GPUs. For our base models using\nthe hyperparameters described throughout the paper, each training step took about 0.4 seconds. We\ntrained the base models for a total of 100,000 steps or 12 hours. For our big models,(described on the\nbottom line of table 3), step time was 1.0 seconds. The big models were trained for 300,000 steps\n(3.5 days).\n5.3\nOptimizer\nWe used the Adam optimizer [17] with β1 = 0.9, β2 = 0.98 and ϵ = 10−9. We varied the learning\nrate over the course of training, according to the formula:\nlrate = d−0.5\nmodel · min(step_num−0.5, step_num · warmup_steps−1.5)\n(3)\nThis corresponds to increasing the learning rate linearly for the ﬁrst warmup_steps training steps,\nand decreasing it thereafter proportionally to the inverse square root of the step number. We used\nwarmup_steps = 4000.\n5.4\nRegularization\nWe employ three types of regularization during training:\nResidual Dropout\nWe apply dropout [27] to the output of each sub-layer, before it is added to the\nsub-layer input and normalized. In addition, we apply dropout to the sums of the embeddings and the\npositional encodings in both the encoder and decoder stacks. For the base model, we use a rate of\nPdrop = 0.1.\n7",
  "663": "Table 2: The Transformer achieves better BLEU scores than previous state-of-the-art models on the\nEnglish-to-German and English-to-French newstest2014 tests at a fraction of the training cost.\nModel\nBLEU\nTraining Cost (FLOPs)\nEN-DE\nEN-FR\nEN-DE\nEN-FR\nByteNet [15]\n23.75\nDeep-Att + PosUnk [32]\n39.2\n1.0 · 1020\nGNMT + RL [31]\n24.6\n39.92\n2.3 · 1019\n1.4 · 1020\nConvS2S [8]\n25.16\n40.46\n9.6 · 1018\n1.5 · 1020\nMoE [26]\n26.03\n40.56\n2.0 · 1019\n1.2 · 1020\nDeep-Att + PosUnk Ensemble [32]\n40.4\n8.0 · 1020\nGNMT + RL Ensemble [31]\n26.30\n41.16\n1.8 · 1020\n1.1 · 1021\nConvS2S Ensemble [8]\n26.36\n41.29\n7.7 · 1019\n1.2 · 1021\nTransformer (base model)\n27.3\n38.1\n3.3 · 1018\nTransformer (big)\n28.4\n41.0\n2.3 · 1019\nLabel Smoothing\nDuring training, we employed label smoothing of value ϵls = 0.1 [30]. This\nhurts perplexity, as the model learns to be more unsure, but improves accuracy and BLEU score.\n6\nResults\n6.1\nMachine Translation\nOn the WMT 2014 English-to-German translation task, the big transformer model (Transformer (big)\nin Table 2) outperforms the best previously reported models (including ensembles) by more than 2.0\nBLEU, establishing a new state-of-the-art BLEU score of 28.4. The conﬁguration of this model is\nlisted in the bottom line of Table 3. Training took 3.5 days on 8 P100 GPUs. Even our base model\nsurpasses all previously published models and ensembles, at a fraction of the training cost of any of\nthe competitive models.\nOn the WMT 2014 English-to-French translation task, our big model achieves a BLEU score of 41.0,\noutperforming all of the previously published single models, at less than 1/4 the training cost of the\nprevious state-of-the-art model. The Transformer (big) model trained for English-to-French used\ndropout rate Pdrop = 0.1, instead of 0.3.\nFor the base models, we used a single model obtained by averaging the last 5 checkpoints, which\nwere written at 10-minute intervals. For the big models, we averaged the last 20 checkpoints. We\nused beam search with a beam size of 4 and length penalty α = 0.6 [31]. These hyperparameters\nwere chosen after experimentation on the development set. We set the maximum output length during\ninference to input length + 50, but terminate early when possible [31].\nTable 2 summarizes our results and compares our translation quality and training costs to other model\narchitectures from the literature. We estimate the number of ﬂoating point operations used to train a\nmodel by multiplying the training time, the number of GPUs used, and an estimate of the sustained\nsingle-precision ﬂoating-point capacity of each GPU 5.\n6.2\nModel Variations\nTo evaluate the importance of different components of the Transformer, we varied our base model\nin different ways, measuring the change in performance on English-to-German translation on the\ndevelopment set, newstest2013. We used beam search as described in the previous section, but no\ncheckpoint averaging. We present these results in Table 3.\nIn Table 3 rows (A), we vary the number of attention heads and the attention key and value dimensions,\nkeeping the amount of computation constant, as described in Section 3.2.2. While single-head\nattention is 0.9 BLEU worse than the best setting, quality also drops off with too many heads.\n5We used values of 2.8, 3.7, 6.0 and 9.5 TFLOPS for K80, K40, M40 and P100, respectively.\n8",
  "664": "Table 3: Variations on the Transformer architecture. Unlisted values are identical to those of the base\nmodel. All metrics are on the English-to-German translation development set, newstest2013. Listed\nperplexities are per-wordpiece, according to our byte-pair encoding, and should not be compared to\nper-word perplexities.\nN\ndmodel\ndff\nh\ndk\ndv\nPdrop\nϵls\ntrain\nPPL\nBLEU\nparams\nsteps\n(dev)\n(dev)\n×106\nbase\n6\n512\n2048\n8\n64\n64\n0.1\n0.1\n100K\n4.92\n25.8\n65\n(A)\n1\n512\n512\n5.29\n24.9\n4\n128\n128\n5.00\n25.5\n16\n32\n32\n4.91\n25.8\n32\n16\n16\n5.01\n25.4\n(B)\n16\n5.16\n25.1\n58\n32\n5.01\n25.4\n60\n(C)\n2\n6.11\n23.7\n36\n4\n5.19\n25.3\n50\n8\n4.88\n25.5\n80\n256\n32\n32\n5.75\n24.5\n28\n1024\n128\n128\n4.66\n26.0\n168\n1024\n5.12\n25.4\n53\n4096\n4.75\n26.2\n90\n(D)\n0.0\n5.77\n24.6\n0.2\n4.95\n25.5\n0.0\n4.67\n25.3\n0.2\n5.47\n25.7\n(E)\npositional embedding instead of sinusoids\n4.92\n25.7\nbig\n6\n1024\n4096\n16\n0.3\n300K\n4.33\n26.4\n213\nIn Table 3 rows (B), we observe that reducing the attention key size dk hurts model quality. This\nsuggests that determining compatibility is not easy and that a more sophisticated compatibility\nfunction than dot product may be beneﬁcial. We further observe in rows (C) and (D) that, as expected,\nbigger models are better, and dropout is very helpful in avoiding over-ﬁtting. In row (E) we replace our\nsinusoidal positional encoding with learned positional embeddings [8], and observe nearly identical\nresults to the base model.\n7\nConclusion\nIn this work, we presented the Transformer, the ﬁrst sequence transduction model based entirely on\nattention, replacing the recurrent layers most commonly used in encoder-decoder architectures with\nmulti-headed self-attention.\nFor translation tasks, the Transformer can be trained signiﬁcantly faster than architectures based\non recurrent or convolutional layers. On both WMT 2014 English-to-German and WMT 2014\nEnglish-to-French translation tasks, we achieve a new state of the art. In the former task our best\nmodel outperforms even all previously reported ensembles.\nWe are excited about the future of attention-based models and plan to apply them to other tasks. We\nplan to extend the Transformer to problems involving input and output modalities other than text and\nto investigate local, restricted attention mechanisms to efﬁciently handle large inputs and outputs\nsuch as images, audio and video. Making generation less sequential is another research goals of ours.\nThe code we used to train and evaluate our models is available at https://github.com/\ntensorflow/tensor2tensor.\nAcknowledgements\nWe are grateful to Nal Kalchbrenner and Stephan Gouws for their fruitful\ncomments, corrections and inspiration.\n9",
  "665": "References\n[1] Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hinton. Layer normalization. arXiv preprint\narXiv:1607.06450, 2016.\n[2] Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. Neural machine translation by jointly\nlearning to align and translate. CoRR, abs/1409.0473, 2014.\n[3] Denny Britz, Anna Goldie, Minh-Thang Luong, and Quoc V. Le. Massive exploration of neural\nmachine translation architectures. CoRR, abs/1703.03906, 2017.\n[4] Jianpeng Cheng, Li Dong, and Mirella Lapata. Long short-term memory-networks for machine\nreading. arXiv preprint arXiv:1601.06733, 2016.\n[5] Kyunghyun Cho, Bart van Merrienboer, Caglar Gulcehre, Fethi Bougares, Holger Schwenk,\nand Yoshua Bengio. Learning phrase representations using rnn encoder-decoder for statistical\nmachine translation. CoRR, abs/1406.1078, 2014.\n[6] Francois Chollet. Xception: Deep learning with depthwise separable convolutions. arXiv\npreprint arXiv:1610.02357, 2016.\n[7] Junyoung Chung, Çaglar Gülçehre, Kyunghyun Cho, and Yoshua Bengio. Empirical evaluation\nof gated recurrent neural networks on sequence modeling. CoRR, abs/1412.3555, 2014.\n[8] Jonas Gehring, Michael Auli, David Grangier, Denis Yarats, and Yann N. Dauphin. Convolu-\ntional sequence to sequence learning. arXiv preprint arXiv:1705.03122v2, 2017.\n[9] Alex Graves.\nGenerating sequences with recurrent neural networks.\narXiv preprint\narXiv:1308.0850, 2013.\n[10] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for im-\nage recognition. In Proceedings of the IEEE Conference on Computer Vision and Pattern\nRecognition, pages 770–778, 2016.\n[11] Sepp Hochreiter, Yoshua Bengio, Paolo Frasconi, and Jürgen Schmidhuber. Gradient ﬂow in\nrecurrent nets: the difﬁculty of learning long-term dependencies, 2001.\n[12] Sepp Hochreiter and Jürgen Schmidhuber. Long short-term memory. Neural computation,\n9(8):1735–1780, 1997.\n[13] Rafal Jozefowicz, Oriol Vinyals, Mike Schuster, Noam Shazeer, and Yonghui Wu. Exploring\nthe limits of language modeling. arXiv preprint arXiv:1602.02410, 2016.\n[14] Łukasz Kaiser and Ilya Sutskever. Neural GPUs learn algorithms. In International Conference\non Learning Representations (ICLR), 2016.\n[15] Nal Kalchbrenner, Lasse Espeholt, Karen Simonyan, Aaron van den Oord, Alex Graves, and Ko-\nray Kavukcuoglu. Neural machine translation in linear time. arXiv preprint arXiv:1610.10099v2,\n2017.\n[16] Yoon Kim, Carl Denton, Luong Hoang, and Alexander M. Rush. Structured attention networks.\nIn International Conference on Learning Representations, 2017.\n[17] Diederik Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In ICLR, 2015.\n[18] Oleksii Kuchaiev and Boris Ginsburg. Factorization tricks for LSTM networks. arXiv preprint\narXiv:1703.10722, 2017.\n[19] Zhouhan Lin, Minwei Feng, Cicero Nogueira dos Santos, Mo Yu, Bing Xiang, Bowen\nZhou, and Yoshua Bengio. A structured self-attentive sentence embedding. arXiv preprint\narXiv:1703.03130, 2017.\n[20] Samy Bengio Łukasz Kaiser. Can active memory replace attention? In Advances in Neural\nInformation Processing Systems, (NIPS), 2016.\n10",
  "666": "[21] Minh-Thang Luong, Hieu Pham, and Christopher D Manning. Effective approaches to attention-\nbased neural machine translation. arXiv preprint arXiv:1508.04025, 2015.\n[22] Ankur Parikh, Oscar Täckström, Dipanjan Das, and Jakob Uszkoreit. A decomposable attention\nmodel. In Empirical Methods in Natural Language Processing, 2016.\n[23] Romain Paulus, Caiming Xiong, and Richard Socher. A deep reinforced model for abstractive\nsummarization. arXiv preprint arXiv:1705.04304, 2017.\n[24] Oﬁr Press and Lior Wolf. Using the output embedding to improve language models. arXiv\npreprint arXiv:1608.05859, 2016.\n[25] Rico Sennrich, Barry Haddow, and Alexandra Birch. Neural machine translation of rare words\nwith subword units. arXiv preprint arXiv:1508.07909, 2015.\n[26] Noam Shazeer, Azalia Mirhoseini, Krzysztof Maziarz, Andy Davis, Quoc Le, Geoffrey Hinton,\nand Jeff Dean. Outrageously large neural networks: The sparsely-gated mixture-of-experts\nlayer. arXiv preprint arXiv:1701.06538, 2017.\n[27] Nitish Srivastava, Geoffrey E Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdi-\nnov. Dropout: a simple way to prevent neural networks from overﬁtting. Journal of Machine\nLearning Research, 15(1):1929–1958, 2014.\n[28] Sainbayar Sukhbaatar, arthur szlam, Jason Weston, and Rob Fergus. End-to-end memory\nnetworks. In C. Cortes, N. D. Lawrence, D. D. Lee, M. Sugiyama, and R. Garnett, editors,\nAdvances in Neural Information Processing Systems 28, pages 2440–2448. Curran Associates,\nInc., 2015.\n[29] Ilya Sutskever, Oriol Vinyals, and Quoc VV Le. Sequence to sequence learning with neural\nnetworks. In Advances in Neural Information Processing Systems, pages 3104–3112, 2014.\n[30] Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jonathon Shlens, and Zbigniew Wojna.\nRethinking the inception architecture for computer vision. CoRR, abs/1512.00567, 2015.\n[31] Yonghui Wu, Mike Schuster, Zhifeng Chen, Quoc V Le, Mohammad Norouzi, Wolfgang\nMacherey, Maxim Krikun, Yuan Cao, Qin Gao, Klaus Macherey, et al. Google’s neural machine\ntranslation system: Bridging the gap between human and machine translation. arXiv preprint\narXiv:1609.08144, 2016.\n[32] Jie Zhou, Ying Cao, Xuguang Wang, Peng Li, and Wei Xu. Deep recurrent models with\nfast-forward connections for neural machine translation. CoRR, abs/1606.04199, 2016.\n11"
}