"""
Streamlit RAG é—®ç­”åº”ç”¨
æ•´åˆæ–‡æ¡£ä¸Šä¼ ã€å¤„ç†ã€å‘é‡æ£€ç´¢å’Œç­”æ¡ˆç”ŸæˆåŠŸèƒ½
"""

import os
import tempfile
from typing import Dict, List, Optional, Tuple

import faiss
import numpy as np
import streamlit as st
from openai import OpenAI

from document_process import chunk_pdf_texts
from embedding import local_embedding
from faiss_demo import (
    EMBEDDING_DIM,
    DEFAULT_TOP_K,
    create_index,
    load_meta,
    save_meta,
    add_texts,
    search,
    _normalize,
)
from generation import generate_answer

# é¡µé¢é…ç½®
st.set_page_config(
    page_title="RAG é—®ç­”åŠ©æ‰‹",
    page_icon="ğŸ“š",
    layout="wide",
    initial_sidebar_state="expanded"
)

# åˆå§‹åŒ– OpenAI å®¢æˆ·ç«¯
@st.cache_resource
def get_openai_client():
    return OpenAI()

# é…ç½®è·¯å¾„
BASE_DIR = os.path.dirname(os.path.abspath(__file__))
UPLOAD_DIR = os.path.join(BASE_DIR, "uploads")
OUT_DIR = os.path.join(BASE_DIR, "faiss_out")
INDEX_FILE_PATH = os.path.join(OUT_DIR, "rag.index.faiss")
META_FILE_PATH = os.path.join(OUT_DIR, "rag.meta.json")

# ç¡®ä¿ç›®å½•å­˜åœ¨
os.makedirs(UPLOAD_DIR, exist_ok=True)
os.makedirs(OUT_DIR, exist_ok=True)

# åˆå§‹åŒ– session state
if "index" not in st.session_state:
    st.session_state.index = None
if "meta" not in st.session_state:
    st.session_state.meta = {}
if "processed_files" not in st.session_state:
    st.session_state.processed_files = []
if "processing_status" not in st.session_state:
    st.session_state.processing_status = "æœªå¼€å§‹"


def load_index_if_exists():
    """å¦‚æœç´¢å¼•æ–‡ä»¶å­˜åœ¨ï¼ŒåŠ è½½ç´¢å¼•å’Œå…ƒæ•°æ®"""
    if os.path.exists(INDEX_FILE_PATH) and os.path.exists(META_FILE_PATH):
        try:
            index = faiss.read_index(INDEX_FILE_PATH)
            meta = load_meta(META_FILE_PATH)
            return index, meta
        except Exception as e:
            st.error(f"åŠ è½½ç´¢å¼•å¤±è´¥: {e}")
            return None, {}
    return None, {}


def process_uploaded_file(uploaded_file) -> Tuple[Optional[faiss.Index], Dict[int, str], str]:
    """
    å¤„ç†ä¸Šä¼ çš„æ–‡ä»¶ï¼šè§£æã€åˆ†å—ã€å‘é‡åŒ–ã€æ„å»ºç´¢å¼•
    
    Returns:
        (index, meta, status_message)
    """
    try:
        # ä¿å­˜ä¸Šä¼ çš„æ–‡ä»¶
        file_path = os.path.join(UPLOAD_DIR, uploaded_file.name)
        with open(file_path, "wb") as f:
            f.write(uploaded_file.getbuffer())
        
        # 1. æ–‡æ¡£è§£æå’Œåˆ†å—
        with st.spinner("æ­£åœ¨è§£ææ–‡æ¡£..."):
            texts = chunk_pdf_texts(file_path)
            if not texts:
                return None, {}, "é”™è¯¯ï¼šæœªèƒ½ä»æ–‡æ¡£ä¸­æå–æ–‡æœ¬"
        
        st.success(f"âœ“ æ–‡æ¡£è§£æå®Œæˆï¼Œå…±æå– {len(texts)} ä¸ªæ–‡æœ¬å—")
        
        # 2. å‘é‡åŒ–
        progress_bar = st.progress(0)
        status_text = st.empty()
        
        # åŠ è½½æˆ–åˆ›å»ºç´¢å¼•
        if st.session_state.index is None:
            index = create_index(dim=EMBEDDING_DIM)
            meta = {}
        else:
            index = st.session_state.index
            meta = st.session_state.meta.copy()
        
        # æ‰¹é‡å¤„ç†å‘é‡åŒ–
        batch_size = 25
        total_batches = (len(texts) + batch_size - 1) // batch_size
        
        status_text.text(f"æ­£åœ¨å‘é‡åŒ–æ–‡æœ¬å— (0/{len(texts)})...")
        
        vectors = []
        for i, batch_start in enumerate(range(0, len(texts), batch_size)):
            batch = texts[batch_start:batch_start + batch_size]
            try:
                batch_vecs = local_embedding(batch)
                arr = np.asarray(batch_vecs, dtype="float32")
                if arr.ndim != 2 or arr.shape[1] != EMBEDDING_DIM:
                    return None, {}, f"é”™è¯¯ï¼šå‘é‡ç»´åº¦ä¸åŒ¹é…ï¼ŒæœŸæœ› {EMBEDDING_DIM}ï¼Œå¾—åˆ° {arr.shape[1]}"
                vectors.append(arr)
                
                progress = (i + 1) / total_batches
                progress_bar.progress(progress)
                status_text.text(f"æ­£åœ¨å‘é‡åŒ–æ–‡æœ¬å— ({min(batch_start + len(batch), len(texts))}/{len(texts)})...")
            except Exception as e:
                return None, {}, f"å‘é‡åŒ–å¤±è´¥: {e}"
        
        if not vectors:
            return None, {}, "é”™è¯¯ï¼šæœªèƒ½ç”Ÿæˆå‘é‡"
        
        vectors_array = np.vstack(vectors).astype("float32", copy=False)
        _normalize(vectors_array)
        
        st.success(f"âœ“ å‘é‡åŒ–å®Œæˆï¼Œå…±ç”Ÿæˆ {len(vectors_array)} ä¸ªå‘é‡")
        
        # 3. æ·»åŠ åˆ°ç´¢å¼•
        status_text.text("æ­£åœ¨æ„å»º FAISS ç´¢å¼•...")
        start_id = (max(meta.keys()) + 1) if meta else 0
        ids = np.arange(start_id, start_id + len(texts)).astype("int64")
        
        index.add_with_ids(vectors_array, ids)
        for i, t in zip(ids.tolist(), texts):
            meta[i] = t
        
        # 4. ä¿å­˜ç´¢å¼•
        status_text.text("æ­£åœ¨ä¿å­˜ç´¢å¼•...")
        faiss.write_index(index, INDEX_FILE_PATH)
        save_meta(META_FILE_PATH, meta)
        
        progress_bar.empty()
        status_text.empty()
        
        return index, meta, f"âœ“ å¤„ç†å®Œæˆï¼å·²æ·»åŠ  {len(texts)} ä¸ªæ–‡æœ¬å—åˆ°ç´¢å¼•"
        
    except Exception as e:
        return None, {}, f"å¤„ç†æ–‡ä»¶æ—¶å‡ºé”™: {e}"


def search_and_answer(question: str, top_k: int = DEFAULT_TOP_K):
    """
    æ£€ç´¢ç›¸å…³æ–‡æ¡£å¹¶ç”Ÿæˆç­”æ¡ˆ
    
    Returns:
        (answer, search_results)
    """
    if st.session_state.index is None or not st.session_state.meta:
        return None, [], "é”™è¯¯ï¼šè¯·å…ˆä¸Šä¼ å¹¶å¤„ç†æ–‡æ¡£"
    
    # 1. å‘é‡æ£€ç´¢
    with st.spinner("æ­£åœ¨æ£€ç´¢ç›¸å…³æ–‡æ¡£..."):
        search_results = search(
            index=st.session_state.index,
            meta=st.session_state.meta,
            query=question,
            top_k=top_k
        )
    
    if not search_results:
        return None, [], "æœªæ‰¾åˆ°ç›¸å…³æ–‡æ¡£"
    
    # 2. æ„å»ºä¸Šä¸‹æ–‡
    context_parts = []
    for i, r in enumerate(search_results, 1):
        context_parts.append(f"[ç‰‡æ®µ {i}] {r['text']}")
    context = "\n\n".join(context_parts)
    
    # 3. ç”Ÿæˆç­”æ¡ˆ
    with st.spinner("æ­£åœ¨ç”Ÿæˆç­”æ¡ˆ..."):
        try:
            client = get_openai_client()
            answer = generate_answer(context_text=context, user_question=question)
            return answer, search_results, None
        except Exception as e:
            return None, search_results, f"ç”Ÿæˆç­”æ¡ˆæ—¶å‡ºé”™: {e}"


# ä¸»ç•Œé¢
st.title("ğŸ“š RAG é—®ç­”åŠ©æ‰‹")
st.markdown("---")

# ä¾§è¾¹æ ï¼šæ–‡æ¡£ä¸Šä¼ å’Œå¤„ç†
with st.sidebar:
    st.header("ğŸ“„ æ–‡æ¡£ç®¡ç†")
    
    # åŠ è½½ç°æœ‰ç´¢å¼•
    if st.session_state.index is None:
        index, meta = load_index_if_exists()
        if index is not None:
            st.session_state.index = index
            st.session_state.meta = meta
            st.success(f"å·²åŠ è½½ç´¢å¼•ï¼ˆ{index.ntotal} ä¸ªå‘é‡ï¼‰")
    
    # æ–‡ä»¶ä¸Šä¼ 
    uploaded_file = st.file_uploader(
        "ä¸Šä¼  PDF æ–‡æ¡£",
        type=["pdf"],
        help="æ”¯æŒä¸Šä¼  PDF æ ¼å¼çš„æ–‡æ¡£"
    )
    
    if uploaded_file is not None:
        if st.button("ğŸš€ å¤„ç†æ–‡æ¡£", type="primary", use_container_width=True):
            with st.spinner("æ­£åœ¨å¤„ç†æ–‡æ¡£..."):
                index, meta, status_msg = process_uploaded_file(uploaded_file)
                
                if index is not None:
                    st.session_state.index = index
                    st.session_state.meta = meta
                    st.session_state.processed_files.append(uploaded_file.name)
                    st.success(status_msg)
                    st.rerun()
                else:
                    st.error(status_msg)
    
    # æ˜¾ç¤ºç´¢å¼•çŠ¶æ€
    st.markdown("---")
    st.subheader("ç´¢å¼•çŠ¶æ€")
    if st.session_state.index is not None:
        st.info(f"**å‘é‡æ•°é‡**: {st.session_state.index.ntotal}\n\n**å·²å¤„ç†æ–‡ä»¶**: {len(st.session_state.processed_files)}")
        if st.session_state.processed_files:
            for fname in st.session_state.processed_files:
                st.text(f"â€¢ {fname}")
    else:
        st.warning("å°šæœªåˆ›å»ºç´¢å¼•ï¼Œè¯·ä¸Šä¼ æ–‡æ¡£")

# ä¸»å†…å®¹åŒºï¼šé—®ç­”ç•Œé¢
if st.session_state.index is None or not st.session_state.meta:
    st.info("ğŸ‘ˆ è¯·å…ˆåœ¨å·¦ä¾§ä¸Šä¼ å¹¶å¤„ç†æ–‡æ¡£ï¼Œç„¶åæ‰èƒ½è¿›è¡Œé—®ç­”")
else:
    # é—®é¢˜è¾“å…¥
    st.subheader("ğŸ’¬ æé—®")
    question = st.text_input(
        "è¾“å…¥æ‚¨çš„é—®é¢˜",
        placeholder="ä¾‹å¦‚ï¼šè§£é‡Šä¸€ä¸‹ç»Ÿè®¡è¯­è¨€æ¨¡å‹",
        key="question_input"
    )
    
    col1, col2 = st.columns([1, 4])
    with col1:
        submit_button = st.button("ğŸ” æäº¤", type="primary", use_container_width=True)
    
    with col2:
        top_k = st.slider("æ£€ç´¢æ–‡æ¡£æ•°é‡", min_value=3, max_value=20, value=DEFAULT_TOP_K, step=1)
    
    # åˆå§‹åŒ–å†å²è®°å½•
    if "qa_history" not in st.session_state:
        st.session_state.qa_history = []
    
    # å¤„ç†é—®ç­”
    if submit_button and question:
        answer, search_results, error = search_and_answer(question, top_k=top_k)
        
        if error:
            st.error(error)
        elif answer:
            # ä¿å­˜åˆ°å†å²è®°å½•
            st.session_state.qa_history.append({
                "question": question,
                "answer": answer,
                "sources": search_results
            })
            
            # æ˜¾ç¤ºç­”æ¡ˆ
            st.markdown("---")
            st.subheader("ğŸ“ ç­”æ¡ˆ")
            st.markdown(answer)
            
            # æ˜¾ç¤ºæ¥æºå¼•ç”¨
            if search_results:
                st.markdown("---")
                st.subheader("ğŸ“š ç›¸å…³æ–‡æ¡£ç‰‡æ®µ")
                
                for i, result in enumerate(search_results, 1):
                    with st.expander(f"ç‰‡æ®µ {i} (ç›¸ä¼¼åº¦: {result['score']:.4f})"):
                        st.text(result['text'])
        else:
            st.warning("æœªèƒ½ç”Ÿæˆç­”æ¡ˆï¼Œè¯·æ£€æŸ¥é—®é¢˜æˆ–é‡è¯•")
    
    # æ˜¾ç¤ºå†å²é—®ç­”
    if st.session_state.qa_history:
        st.markdown("---")
        st.subheader("ğŸ“œ å†å²é—®ç­”")
        for qa in reversed(st.session_state.qa_history[-5:]):  # åªæ˜¾ç¤ºæœ€è¿‘5æ¡
            with st.expander(f"Q: {qa['question']}"):
                st.markdown(f"**A:** {qa['answer']}")
                if qa.get('sources'):
                    st.caption(f"æ¥æº: {len(qa['sources'])} ä¸ªç›¸å…³ç‰‡æ®µ")

